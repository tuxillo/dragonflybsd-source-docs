{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DragonFly BSD Kernel Documentation","text":"<p>Welcome to the comprehensive documentation for the DragonFly BSD kernel source code.</p>"},{"location":"#about-this-documentation","title":"About This Documentation","text":"<p>This documentation project aims to provide clear, accessible explanations of the DragonFly BSD kernel's internals, making it easier for developers, researchers, and enthusiasts to understand how this sophisticated operating system works.</p>"},{"location":"#what-is-dragonfly-bsd","title":"What is DragonFly BSD?","text":"<p>DragonFly BSD is a fork of FreeBSD designed with a focus on:</p> <ul> <li>Multiprocessor scalability \u2014 Efficient performance on multi-core systems</li> <li>Message-passing architecture \u2014 LWKT (Lightweight Kernel Threading) for lock-free concurrency</li> <li>Advanced filesystems \u2014 HAMMER and HAMMER2 with clustering capabilities</li> <li>Innovation \u2014 Modern kernel design patterns while maintaining UNIX heritage</li> </ul>"},{"location":"#documentation-organization","title":"Documentation Organization","text":"<p>The documentation is organized to mirror the kernel source tree structure, making it easy to find information about specific subsystems:</p>"},{"location":"#kernel-core-syskern","title":"Kernel Core (<code>sys/kern/</code>)","text":"<p>The heart of the kernel, containing:</p> <ul> <li>LWKT Threading \u2014 DragonFly's unique message-passing concurrency model</li> <li>Process &amp; Thread Management \u2014 How processes and threads are created, scheduled, and managed</li> <li>Virtual Filesystem (VFS) \u2014 The abstraction layer for filesystems</li> <li>IPC &amp; Sockets \u2014 Inter-process communication and networking foundations</li> <li>Memory Management \u2014 Kernel memory allocation and management</li> <li>Device Framework \u2014 How devices and drivers integrate with the kernel</li> </ul>"},{"location":"#virtual-memory-sysvm","title":"Virtual Memory (<code>sys/vm/</code>)","text":"<p>The virtual memory subsystem managing:</p> <ul> <li>VM objects and pages</li> <li>Paging and swap</li> <li>Memory mapping</li> <li>Page cache</li> </ul>"},{"location":"#cpu-architecture-syscpux86_64","title":"CPU Architecture (<code>sys/cpu/x86_64/</code>)","text":"<p>Machine-dependent code for x86-64:</p> <ul> <li>Low-level CPU interfaces</li> <li>MMU management</li> <li>Trap handling</li> <li>Assembly routines</li> </ul>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<ol> <li>Start with the basics \u2014 If you're new to DragonFly, begin with Getting Started</li> <li>Explore by subsystem \u2014 Navigate through the Kernel Subsystems section</li> <li>Follow the architecture \u2014 Documentation mirrors the source tree for easy cross-referencing</li> <li>Deep dive \u2014 Each subsystem page provides overviews, key concepts, and code flows</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>This documentation is a living project. If you find areas that need clarification or expansion, contributions are welcome.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Check out the Getting Started guide to learn how to navigate this documentation and understand DragonFly's unique architecture.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you navigate the DragonFly BSD kernel documentation and understand the unique aspects of DragonFly's architecture.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>To get the most out of this documentation, you should have:</p> <ul> <li>C programming experience \u2014 The kernel is written in C</li> <li>Basic OS concepts \u2014 Understanding of processes, memory management, and I/O</li> <li>UNIX/BSD familiarity \u2014 Knowledge of UNIX system architecture is helpful</li> </ul>"},{"location":"getting-started/#understanding-dragonflys-unique-architecture","title":"Understanding DragonFly's Unique Architecture","text":"<p>DragonFly BSD differs from traditional BSD systems in several key ways:</p>"},{"location":"getting-started/#lwkt-lightweight-kernel-threading","title":"LWKT: Lightweight Kernel Threading","text":"<p>The most distinctive feature of DragonFly is its message-passing concurrency model:</p> <ul> <li>Traditional kernels use locks to protect shared data</li> <li>DragonFly uses message passing and tokens for most synchronization</li> <li>Reduces lock contention and improves multiprocessor scalability</li> <li>Start with the LWKT Threading documentation to understand this foundational concept</li> </ul>"},{"location":"getting-started/#token-based-synchronization","title":"Token-Based Synchronization","text":"<p>Instead of traditional mutexes and read-write locks for most operations, DragonFly uses:</p> <ul> <li>Tokens \u2014 Serializing tokens that can be held across blocking operations</li> <li>Message ports \u2014 Each thread has message ports for asynchronous communication</li> <li>IPIQs \u2014 Inter-Processor Interrupt Queues for cross-CPU messaging</li> </ul> <p>See Synchronization for details.</p>"},{"location":"getting-started/#documentation-structure","title":"Documentation Structure","text":""},{"location":"getting-started/#mirror-of-source-tree","title":"Mirror of Source Tree","text":"<p>The documentation mirrors the kernel source tree at <code>~/s/dragonfly/sys/</code>:</p> <pre><code>Source: ~/s/dragonfly/sys/kern/kern_proc.c\n  Docs: docs/sys/kern/processes.md\n\nSource: ~/s/dragonfly/sys/vm/vm_page.c\n  Docs: docs/sys/vm/index.md\n</code></pre> <p>This makes it easy to:</p> <ul> <li>Find documentation for specific source directories</li> <li>Cross-reference between code and docs</li> <li>Navigate familiar territory if you know the source layout</li> </ul>"},{"location":"getting-started/#documentation-pages","title":"Documentation Pages","text":"<p>Each subsystem documentation page follows a consistent structure:</p> <ol> <li>Overview \u2014 What the subsystem does and why</li> <li>Key Concepts \u2014 Important ideas and terminology</li> <li>Data Structures \u2014 Core structures and their roles</li> <li>Key Functions \u2014 Important entry points and operations</li> <li>Subsystem Interactions \u2014 How it connects to other parts</li> <li>Code Flow Examples \u2014 Walkthrough of typical operations</li> <li>Files \u2014 Relevant source files</li> <li>References \u2014 Links to related topics</li> </ol>"},{"location":"getting-started/#recommended-reading-order","title":"Recommended Reading Order","text":""},{"location":"getting-started/#for-first-time-readers","title":"For First-Time Readers","text":"<ol> <li>LWKT Threading \u2014 Understand DragonFly's concurrency model first</li> <li>Synchronization \u2014 Learn about tokens, locks, and message passing</li> <li>Processes &amp; Threads \u2014 How processes and threads work</li> <li>Virtual Filesystem \u2014 VFS layer and file operations</li> <li>Memory Management \u2014 Kernel memory allocation</li> </ol>"},{"location":"getting-started/#for-specific-interests","title":"For Specific Interests","text":"<ul> <li>Filesystem developers \u2192 Start with VFS</li> <li>Network programmers \u2192 Begin with IPC &amp; Sockets</li> <li>Driver developers \u2192 Check out Devices &amp; Drivers</li> <li>Scheduler hackers \u2192 Head to Scheduling</li> <li>Architecture enthusiasts \u2192 Explore CPU/x86_64</li> </ul>"},{"location":"getting-started/#code-references","title":"Code References","text":"<p>Throughout the documentation, you'll see references like:</p> <ul> <li><code>kern_proc.c:142</code> \u2014 File and line number</li> <li><code>fork1()</code> \u2014 Function name</li> <li><code>struct proc</code> \u2014 Data structure</li> </ul> <p>These help you locate the relevant source code in <code>~/s/dragonfly/sys/</code>.</p>"},{"location":"getting-started/#understanding-the-planning-documents","title":"Understanding the Planning Documents","text":"<p>The repository also contains planning documents (in <code>planning/</code> directory) that outline:</p> <ul> <li>Reading order for source code</li> <li>Phases for documentation development</li> <li>Subsystem categorization</li> </ul> <p>These are primarily for documentation maintainers but can be useful if you want to understand how the source tree is organized.</p>"},{"location":"getting-started/#viewing-the-documentation","title":"Viewing the Documentation","text":"<p>This documentation is built with MkDocs and can be:</p> <ul> <li>Viewed locally: Run <code>make serve</code> in the repository root</li> <li>Built as static HTML: Run <code>make build</code> to generate <code>site/</code> directory</li> <li>Read as Markdown: All <code>.md</code> files in <code>docs/</code> are readable as plain text</li> </ul>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<p>Now that you understand how the documentation is organized, you're ready to explore:</p> <ul> <li>Kernel Subsystems Overview \u2014 High-level view of all subsystems</li> <li>kern/ Overview \u2014 Start with the kernel core</li> <li>LWKT Threading \u2014 Dive into DragonFly's unique concurrency model</li> </ul> <p>Happy exploring!</p>"},{"location":"sys/","title":"Kernel Subsystems Overview","text":"<p>The DragonFly BSD kernel (<code>sys/</code>) is organized into subsystems that handle different aspects of the operating system. This page provides a high-level overview of the major components.</p>"},{"location":"sys/#core-kernel-kern","title":"Core Kernel (<code>kern/</code>)","text":"<p>The kernel core is the heart of DragonFly, containing fundamental subsystems:</p> <ul> <li>LWKT Threading \u2014 Message-passing based threading model unique to DragonFly</li> <li>Synchronization \u2014 Tokens, locks, and synchronization primitives</li> <li>Memory Management \u2014 Kernel memory allocation (malloc, slab allocator, object caches)</li> <li>Processes &amp; Threads \u2014 Process lifecycle (fork, exec, exit) and thread management</li> <li>Scheduling \u2014 CPU scheduling framework and policies</li> <li>Virtual Filesystem (VFS) \u2014 Filesystem abstraction layer</li> <li>IPC &amp; Sockets \u2014 Inter-process communication and socket layer</li> <li>Devices &amp; Drivers \u2014 Device framework and driver infrastructure</li> <li>System Calls \u2014 System call infrastructure and implementation</li> </ul> <p>Explore kern/ in detail \u2192</p>"},{"location":"sys/#virtual-memory-vm","title":"Virtual Memory (<code>vm/</code>)","text":"<p>The virtual memory subsystem manages memory at the page level:</p> <ul> <li>VM objects and shadow objects</li> <li>Physical page management</li> <li>Paging and page replacement</li> <li>Swap management</li> <li>Memory mapping (<code>mmap</code>)</li> <li>Buffer cache integration</li> </ul> <p>Explore vm/ in detail \u2192</p>"},{"location":"sys/#cpu-architecture-cpux86_64","title":"CPU Architecture (<code>cpu/x86_64/</code>)","text":"<p>Machine-dependent code for the x86-64 architecture:</p> <ul> <li>CPU initialization and control</li> <li>MMU and page table management</li> <li>Trap and interrupt handling</li> <li>Context switching</li> <li>Assembly routines</li> <li>Low-level primitives</li> </ul> <p>Explore cpu/x86_64/ in detail \u2192</p>"},{"location":"sys/#networking","title":"Networking","text":""},{"location":"sys/#core-networking-net","title":"Core Networking (<code>net/</code>)","text":"<p>Generic networking infrastructure:</p> <ul> <li>Network interfaces (<code>struct ifnet</code>)</li> <li>Routing tables</li> <li>Network message passing (<code>netisr</code>)</li> <li>BPF (Berkeley Packet Filter)</li> </ul>"},{"location":"sys/#ipv6-netinet6","title":"IPv6 (<code>netinet6/</code>)","text":"<p>IPv6 protocol stack:</p> <ul> <li>IPv6 packet processing</li> <li>Neighbor discovery</li> <li>Routing and forwarding</li> <li>ICMPv6</li> <li>Multicast support</li> </ul>"},{"location":"sys/#bluetooth-netbt","title":"Bluetooth (<code>netbt/</code>)","text":"<p>Bluetooth protocol stack:</p> <ul> <li>HCI (Host Controller Interface)</li> <li>L2CAP (Logical Link Control and Adaptation Protocol)</li> <li>RFCOMM</li> <li>SCO (Synchronous Connection-Oriented)</li> </ul>"},{"location":"sys/#storage-and-filesystems","title":"Storage and Filesystems","text":""},{"location":"sys/#device-io","title":"Device I/O","text":"<ul> <li>Disk layer and partitioning</li> <li>Device statistics</li> <li>I/O scheduling</li> <li>DMA support</li> </ul>"},{"location":"sys/#vfs-layer","title":"VFS Layer","text":"<p>See kern/vfs/ for the filesystem abstraction layer.</p>"},{"location":"sys/#security","title":"Security","text":"<ul> <li>Capabilities \u2014 Capability-based security model</li> <li>ACLs \u2014 Access control lists</li> <li>Jails \u2014 Container-like isolation</li> </ul>"},{"location":"sys/#debugging-and-monitoring","title":"Debugging and Monitoring","text":"<ul> <li>DDB \u2014 In-kernel debugger</li> <li>KTR \u2014 Kernel trace buffer</li> <li>ktrace \u2014 System call tracing</li> <li>sysctl \u2014 Runtime configuration and monitoring</li> </ul>"},{"location":"sys/#libraries","title":"Libraries","text":""},{"location":"sys/#kernel-libraries","title":"Kernel Libraries","text":"<ul> <li>libkern \u2014 C library functions for kernel use</li> <li>libiconv \u2014 Character set conversion</li> <li>libprop \u2014 Property lists (kernel/userland shared)</li> </ul>"},{"location":"sys/#cryptography","title":"Cryptography","text":"<ul> <li>opencrypto \u2014 Cryptographic framework</li> <li>Software crypto implementations</li> <li>Hardware crypto driver support</li> </ul>"},{"location":"sys/#kernel-configuration","title":"Kernel Configuration","text":"<ul> <li>config/ \u2014 Kernel configuration files</li> <li>compile/ \u2014 Build output directories</li> </ul>"},{"location":"sys/#key-architectural-principles","title":"Key Architectural Principles","text":""},{"location":"sys/#message-passing-over-locking","title":"Message Passing Over Locking","text":"<p>DragonFly minimizes traditional locking by using:</p> <ul> <li>Message-based IPC between threads</li> <li>Tokens for serialization when needed</li> <li>Per-CPU data structures to avoid contention</li> </ul>"},{"location":"sys/#scalability-focus","title":"Scalability Focus","text":"<p>Design choices emphasize multiprocessor performance:</p> <ul> <li>Lock-free algorithms where possible</li> <li>Cache-friendly data structures</li> <li>Minimal serialization points</li> </ul>"},{"location":"sys/#cache-coherency","title":"Cache Coherency","text":"<p>DragonFly's HAMMER filesystem and buffer cache leverage:</p> <ul> <li>Distributed caching</li> <li>Cluster-aware design</li> <li>Cache coherency protocols</li> </ul>"},{"location":"sys/#next-steps","title":"Next Steps","text":"<ul> <li>Start with kern/ to explore the kernel core</li> <li>Learn about LWKT Threading to understand DragonFly's unique approach</li> <li>Dive into specific subsystems based on your interests</li> </ul>"},{"location":"sys/#subsystem-dependencies","title":"Subsystem Dependencies","text":"<p>Understanding dependencies helps navigate the kernel:</p> <pre><code>LWKT Threading (foundational)\n    \u2193\nSynchronization Primitives\n    \u2193\nMemory Management\n    \u2193\nProcess Management \u2192 VFS \u2192 Filesystems\n    \u2193              \u2193\nScheduling    Device I/O\n</code></pre> <p>Start with LWKT to understand the foundation, then explore other subsystems.</p>"},{"location":"sys/cpu/x86_64/","title":"CPU Architecture: x86_64","text":"<p>The <code>sys/cpu/x86_64/</code> directory contains machine-dependent headers and utility code for the x86-64 (AMD64/Intel 64) architecture.  Platform-specific implementations (PC hardware) live in <code>sys/platform/pc64/</code>.</p>"},{"location":"sys/cpu/x86_64/#directory-structure","title":"Directory Structure","text":"<pre><code>sys/cpu/x86_64/\n    include/            Machine-dependent headers\n        cpu.h           CPU definitions and macros\n        cpufunc.h       Inline assembly for CPU instructions\n        pmap.h          Page table entry definitions\n        frame.h         Trap/interrupt frame layout\n        segments.h      GDT/IDT segment descriptors\n        specialreg.h    Control registers and CPUID features\n        atomic.h        Atomic operations\n        ...\n    misc/               Assembly and utility implementations\n        bzeront.s       Non-temporal zero fill\n        cputimer_tsc.c  TSC-based timers\n        db_disasm.c     DDB disassembler\n        elf_machdep.c   ELF relocation handling\n        in_cksum2.s     Optimized IP checksum\n        lwbuf.c         Lightweight buffer mapping\n        ...\n\nsys/platform/pc64/x86_64/\n    machdep.c           Platform initialization\n    pmap.c              Page table management\n    trap.c              Trap/exception handling\n    mp_machdep.c        SMP support\n    exception.S         Low-level exception entry\n    ...\n</code></pre>"},{"location":"sys/cpu/x86_64/#control-registers","title":"Control Registers","text":"<p>x86-64 provides several control registers that govern CPU behavior:</p>"},{"location":"sys/cpu/x86_64/#cr0-specialregh","title":"CR0 (specialreg.h)","text":"Bit Name Description 0 <code>CR0_PE</code> Protected Mode Enable 1 <code>CR0_MP</code> Math Present (FPU) 2 <code>CR0_EM</code> FPU Emulation 3 <code>CR0_TS</code> Task Switched 16 <code>CR0_WP</code> Write Protect (honor page protection in supervisor mode) 29 <code>CR0_NW</code> Not Write-through 30 <code>CR0_CD</code> Cache Disable 31 <code>CR0_PG</code> Paging Enable"},{"location":"sys/cpu/x86_64/#cr4-specialregh","title":"CR4 (specialreg.h)","text":"Bit Name Description 4 <code>CR4_PSE</code> Page Size Extensions (2MB pages) 5 <code>CR4_PAE</code> Physical Address Extension 7 <code>CR4_PGE</code> Page Global Enable 9 <code>CR4_OSFXSR</code> OS supports FXSAVE/FXRSTOR 10 <code>CR4_OSXMMEXCPT</code> OS handles SIMD exceptions 16 <code>CR4_FSGSBASE</code> Enable RDFSBASE/WRFSBASE 17 <code>CR4_PCIDE</code> Process Context Identifiers 18 <code>CR4_OSXSAVE</code> OS supports XSAVE 20 <code>CR4_SMEP</code> Supervisor-Mode Execution Prevention 21 <code>CR4_SMAP</code> Supervisor-Mode Access Prevention"},{"location":"sys/cpu/x86_64/#cr2-and-cr3","title":"CR2 and CR3","text":"<ul> <li>CR2 - Holds the faulting linear address on a page fault</li> <li>CR3 - Holds the physical address of the PML4 page table root</li> </ul>"},{"location":"sys/cpu/x86_64/#cpu-feature-detection-cpufunch-specialregh","title":"CPU Feature Detection (cpufunc.h, specialreg.h)","text":"<p>CPUID instruction returns feature flags in %edx and %ecx:</p> <p>CPUID Fn0000_0001 %edx: - <code>CPUID_FPU</code> - x87 FPU present - <code>CPUID_VME</code> - Virtual 8086 extensions - <code>CPUID_TSC</code> - Time Stamp Counter - <code>CPUID_MSR</code> - Model Specific Registers - <code>CPUID_PAE</code> - Physical Address Extension - <code>CPUID_APIC</code> - On-chip APIC - <code>CPUID_MTRR</code> - Memory Type Range Registers - <code>CPUID_PGE</code> - Page Global Enable - <code>CPUID_SSE</code>, <code>CPUID_SSE2</code> - SIMD extensions</p> <p>CPUID Fn0000_0001 %ecx: - <code>CPUID2_SSE3</code>, <code>CPUID2_SSSE3</code>, <code>CPUID2_SSE41</code>, <code>CPUID2_SSE42</code> - <code>CPUID2_VMX</code> - Intel VMX (virtualization) - <code>CPUID2_AESNI</code> - AES instruction set - <code>CPUID2_AVX</code> - Advanced Vector Extensions - <code>CPUID2_XSAVE</code> - XSAVE/XRSTOR support</p> <pre><code>/* cpufunc.h:134 - CPUID wrapper */\nstatic __inline void\ndo_cpuid(u_int ax, u_int *p)\n{\n    __asm __volatile(\"cpuid\"\n        : \"=a\" (p[0]), \"=b\" (p[1]), \"=c\" (p[2]), \"=d\" (p[3])\n        :  \"0\" (ax));\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#cpu-inline-functions-cpufunch","title":"CPU Inline Functions (cpufunc.h)","text":"<p>The <code>cpufunc.h</code> header provides inline assembly wrappers for privileged instructions:</p>"},{"location":"sys/cpu/x86_64/#interrupt-control","title":"Interrupt Control","text":"<pre><code>static __inline void cpu_disable_intr(void)\n{\n    __asm __volatile(\"cli\" : : : \"memory\");\n}\n\nstatic __inline void cpu_enable_intr(void)\n{\n    __asm __volatile(\"sti\");\n}\n\nstatic __inline register_t intr_disable(void)\n{\n    register_t rflags = read_rflags();\n    cpu_disable_intr();\n    return rflags;\n}\n\nstatic __inline void intr_restore(register_t rflags)\n{\n    write_rflags(rflags);\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#memory-barriers-cpufunch177","title":"Memory Barriers (cpufunc.h:177)","text":"<pre><code>/* Full memory fence */\nstatic __inline void cpu_mfence(void)\n{\n    __asm __volatile(\"mfence\" : : : \"memory\");\n}\n\n/* Load fence - orders reads */\nstatic __inline void cpu_lfence(void)\n{\n    __asm __volatile(\"lfence\" : : : \"memory\");\n}\n\n/* Store fence - orders writes (mostly compiler barrier on Intel) */\nstatic __inline void cpu_sfence(void)\n{\n    __asm __volatile(\"\" : : : \"memory\");\n}\n\n/* Compiler-only fence */\nstatic __inline void cpu_ccfence(void)\n{\n    __asm __volatile(\"\" : : : \"memory\");\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#tlb-management","title":"TLB Management","text":"<pre><code>/* Invalidate single TLB entry */\nstatic __inline void cpu_invlpg(void *addr)\n{\n    __asm __volatile(\"invlpg %0\" : : \"m\" (*(char *)addr) : \"memory\");\n}\n\n/* Flush entire TLB (reload CR3) */\nstatic __inline void cpu_invltlb(void)\n{\n    load_cr3(rcr3());\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#msr-access-cpufunch524","title":"MSR Access (cpufunc.h:524)","text":"<pre><code>static __inline u_int64_t rdmsr(u_int msr)\n{\n    u_int32_t low, high;\n    __asm __volatile(\"rdmsr\" : \"=a\" (low), \"=d\" (high) : \"c\" (msr));\n    return (low | ((u_int64_t)high &lt;&lt; 32));\n}\n\nstatic __inline void wrmsr(u_int msr, u_int64_t newval)\n{\n    u_int32_t low = newval, high = newval &gt;&gt; 32;\n    __asm __volatile(\"wrmsr\" : : \"a\" (low), \"d\" (high), \"c\" (msr) : \"memory\");\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#tsc-time-stamp-counter","title":"TSC (Time Stamp Counter)","text":"<pre><code>static __inline tsc_uclock_t rdtsc(void)\n{\n    u_int32_t low, high;\n    __asm __volatile(\"rdtsc\" : \"=a\" (low), \"=d\" (high));\n    return (low | ((tsc_uclock_t)high &lt;&lt; 32));\n}\n\n/* Ordered TSC read with appropriate fence */\nstatic __inline tsc_uclock_t rdtsc_ordered(void)\n{\n    if (cpu_vendor_id == CPU_VENDOR_INTEL)\n        cpu_lfence();\n    else\n        cpu_mfence();\n    return rdtsc();\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#page-table-entries-pmaph","title":"Page Table Entries (pmap.h)","text":"<p>x86-64 uses 4-level paging with 48-bit virtual addresses:</p> <pre><code>PML4 (level 4) -&gt; PDP (level 3) -&gt; PD (level 2) -&gt; PT (level 1) -&gt; Page\n   9 bits           9 bits          9 bits          9 bits        12 bits\n</code></pre>"},{"location":"sys/cpu/x86_64/#page-table-entry-bits-pmaph67","title":"Page Table Entry Bits (pmap.h:67)","text":"Bit Name Description 0 <code>X86_PG_V</code> Valid/Present 1 <code>X86_PG_RW</code> Read/Write 2 <code>X86_PG_U</code> User/Supervisor 3 <code>X86_PG_NC_PWT</code> Write-Through 4 <code>X86_PG_NC_PCD</code> Cache Disable 5 <code>X86_PG_A</code> Accessed 6 <code>X86_PG_M</code> Dirty (Modified) 7 <code>X86_PG_PS</code> Page Size (2MB if set at PD level) 8 <code>X86_PG_G</code> Global (not flushed on CR3 reload) 63 <code>X86_PG_NX</code> No Execute"},{"location":"sys/cpu/x86_64/#page-protection-exceptions-pmaph126","title":"Page Protection Exceptions (pmap.h:126)","text":"<pre><code>#define PGEX_P      0x01    /* Protection violation (vs not present) */\n#define PGEX_W      0x02    /* Write access */\n#define PGEX_U      0x04    /* User mode access */\n#define PGEX_RSV    0x08    /* Reserved bit set in PTE */\n#define PGEX_I      0x10    /* Instruction fetch */\n</code></pre>"},{"location":"sys/cpu/x86_64/#trap-frame-frameh55","title":"Trap Frame (frame.h:55)","text":"<p>The <code>trapframe</code> structure captures CPU state on exception/interrupt entry:</p> <pre><code>struct trapframe {\n    /* Syscall arguments (rdi, rsi, rdx, rcx, r8, r9) come first */\n    register_t  tf_rdi;\n    register_t  tf_rsi;\n    register_t  tf_rdx;\n    register_t  tf_rcx;\n    register_t  tf_r8;\n    register_t  tf_r9;\n    register_t  tf_rax;\n    register_t  tf_rbx;\n    register_t  tf_rbp;\n    register_t  tf_r10;\n    register_t  tf_r11;\n    register_t  tf_r12;\n    register_t  tf_r13;\n    register_t  tf_r14;\n    register_t  tf_r15;\n    register_t  tf_xflags;      /* Software flags */\n    register_t  tf_trapno;\n    register_t  tf_addr;\n    register_t  tf_flags;\n    /* Hardware-pushed portion */\n    register_t  tf_err;         /* Error code */\n    register_t  tf_rip;\n    register_t  tf_cs;\n    register_t  tf_rflags;\n    register_t  tf_rsp;\n    register_t  tf_ss;\n} __packed;\n</code></pre> <p>The first six registers match the System V AMD64 ABI calling convention, allowing direct syscall argument extraction.</p>"},{"location":"sys/cpu/x86_64/#segment-descriptors-segmentsh","title":"Segment Descriptors (segments.h)","text":""},{"location":"sys/cpu/x86_64/#global-descriptor-table-layout-segmentsh236","title":"Global Descriptor Table Layout (segments.h:236)","text":"Selector Name Description 0 <code>GNULL_SEL</code> Null descriptor 1 <code>GCODE_SEL</code> Kernel 64-bit code 2 <code>GDATA_SEL</code> Kernel data 3 <code>GUCODE32_SEL</code> User 32-bit code (compat) 4 <code>GUDATA_SEL</code> User data (32/64) 5 <code>GUCODE_SEL</code> User 64-bit code 6-7 <code>GPROC0_SEL</code> TSS (128-bit system descriptor) 8 <code>GUGS32_SEL</code> User 32-bit GS"},{"location":"sys/cpu/x86_64/#idt-entries-segmentsh206","title":"IDT Entries (segments.h:206)","text":"<pre><code>#define IDT_DE      0       /* Divide Error */\n#define IDT_DB      1       /* Debug */\n#define IDT_NMI     2       /* Non-Maskable Interrupt */\n#define IDT_BP      3       /* Breakpoint */\n#define IDT_OF      4       /* Overflow */\n#define IDT_BR      5       /* Bound Range Exceeded */\n#define IDT_UD      6       /* Invalid Opcode */\n#define IDT_NM      7       /* Device Not Available */\n#define IDT_DF      8       /* Double Fault */\n#define IDT_TS      10      /* Invalid TSS */\n#define IDT_NP      11      /* Segment Not Present */\n#define IDT_SS      12      /* Stack Segment Fault */\n#define IDT_GP      13      /* General Protection */\n#define IDT_PF      14      /* Page Fault */\n#define IDT_MF      16      /* x87 FPU Error */\n#define IDT_AC      17      /* Alignment Check */\n#define IDT_MC      18      /* Machine Check */\n#define IDT_XF      19      /* SIMD Exception */\n#define IDT_SYSCALL 0x80    /* System call vector */\n</code></pre>"},{"location":"sys/cpu/x86_64/#atomic-operations-atomich","title":"Atomic Operations (atomic.h)","text":"<p>x86-64 atomic operations use the <code>LOCK</code> prefix for SMP safety:</p> <pre><code>#define MPLOCKED    \"lock ; \"\n\n/* Example: atomic_add_int */\nstatic __inline void\natomic_add_int(volatile u_int *p, u_int v)\n{\n    __asm __volatile(MPLOCKED \"addl %1,%0\"\n        : \"+m\" (*p)\n        : \"iq\" (v));\n}\n</code></pre> <p>Supported operations (char/short/int/long variants): - <code>atomic_set_*</code> - OR value - <code>atomic_clear_*</code> - AND with complement - <code>atomic_add_*</code> - Add value - <code>atomic_subtract_*</code> - Subtract value - <code>atomic_cmpset_*</code> - Compare and swap - <code>atomic_fetchadd_*</code> - Fetch and add (returns old value) - <code>atomic_readandclear_*</code> - Read and zero</p> <p>Lock elision variants (<code>_xacquire</code>, <code>_xrelease</code>) use Intel TSX hints.</p>"},{"location":"sys/cpu/x86_64/#trap-handling-platformpc64x86_64trapc","title":"Trap Handling (platform/pc64/x86_64/trap.c)","text":"<p>The <code>trap()</code> function dispatches exceptions based on <code>tf_trapno</code>:</p> Trap Name Action T_PAGEFLT (12) Page Fault Call <code>trap_pfault()</code> -&gt; <code>vm_fault()</code> T_PROTFLT (9) General Protection Signal or panic T_DIVIDE (18) Divide Error SIGFPE T_BPTFLT (3) Breakpoint SIGTRAP or DDB T_TRCTRAP (10) Trace Trap SIGTRAP T_NMI (19) NMI DDB or panic T_DNA (22) Device Not Available FPU state restore T_DOUBLEFLT (23) Double Fault Panic <p>Page faults (T_PAGEFLT) are the most common, handled by: 1. Reading faulting address from CR2 2. Calling <code>vm_fault()</code> to resolve the fault 3. Returning to retry the instruction, or sending SIGSEGV</p>"},{"location":"sys/cpu/x86_64/#cpu-scheduling-interface-cpuh","title":"CPU Scheduling Interface (cpu.h)","text":"<p>Macros for requesting reschedule via <code>gd_reqflags</code>:</p> <pre><code>#define need_lwkt_resched()  \\\n    atomic_set_int(&amp;mycpu-&gt;gd_reqflags, RQF_AST_LWKT_RESCHED)\n#define need_user_resched()  \\\n    atomic_set_int(&amp;mycpu-&gt;gd_reqflags, RQF_AST_USER_RESCHED)\n#define signotify()          \\\n    atomic_set_int(&amp;mycpu-&gt;gd_reqflags, RQF_AST_SIGNAL)\n</code></pre> <p>These set bits that are checked on return from trap/syscall to trigger context switches or signal delivery.</p>"},{"location":"sys/cpu/x86_64/#smp-and-inter-processor-communication","title":"SMP and Inter-Processor Communication","text":""},{"location":"sys/cpu/x86_64/#tlb-shootdown","title":"TLB Shootdown","text":"<p>When page mappings change, other CPUs must invalidate their TLB:</p> <pre><code>void smp_invltlb(void);     /* Broadcast TLB invalidation */\n</code></pre> <p>Uses IPI (Inter-Processor Interrupt) via the LAPIC.</p>"},{"location":"sys/cpu/x86_64/#ipiq-inter-processor-interrupt-queue","title":"IPIQ (Inter-Processor Interrupt Queue)","text":"<p>The <code>need_ipiq()</code> macro signals pending cross-CPU work.  IPIQs are used for TLB shootdowns, scheduler requests, and other SMP coordination.</p>"},{"location":"sys/cpu/x86_64/#meltdownspectre-mitigations","title":"Meltdown/Spectre Mitigations","text":"<p>The <code>trampframe</code> structure (frame.h:126) supports isolated page tables for mitigating Meltdown (CVE-2017-5754):</p> <pre><code>struct trampframe {\n    register_t  tr_cr2;\n    register_t  tr_rax, tr_rcx, tr_rdx;\n    register_t  tr_err, tr_rip, tr_cs, tr_rflags, tr_rsp, tr_ss;\n    register_t  tr_pcb_rsp;         /* Trampoline stack */\n    register_t  tr_pcb_flags;\n    register_t  tr_pcb_cr3_iso;     /* Isolated PML4 */\n    register_t  tr_pcb_cr3;         /* Full kernel PML4 */\n    uint32_t    tr_pcb_spec_ctrl[2]; /* SPEC_CTRL MSR values */\n    ...\n};\n</code></pre> <p>On syscall/interrupt entry, the trampoline switches from the isolated user page table to the full kernel page table.</p>"},{"location":"sys/cpu/x86_64/#see-also","title":"See Also","text":"<ul> <li>Virtual Memory Subsystem - Page fault handling</li> <li>Processes and Threads - Context switching</li> <li>LWKT Scheduler - Thread scheduling</li> <li>Synchronization - Locking primitives</li> </ul>"},{"location":"sys/kern/","title":"Kernel Core (<code>kern/</code>) Overview","text":"<p>The <code>sys/kern/</code> directory contains the core kernel subsystems \u2014 the fundamental building blocks of the DragonFly BSD operating system. With approximately 180 source files, <code>kern/</code> implements everything from threading and process management to filesystems and device drivers.</p>"},{"location":"sys/kern/#what-makes-dragonfly-unique","title":"What Makes DragonFly Unique","text":"<p>Before diving into specifics, understand that DragonFly's kernel core differs significantly from traditional BSD kernels:</p>"},{"location":"sys/kern/#lwkt-the-foundation","title":"LWKT: The Foundation","text":"<p>Lightweight Kernel Threading (LWKT) is DragonFly's message-passing based concurrency model. Instead of pervasive locking, DragonFly uses:</p> <ul> <li>Message ports attached to threads</li> <li>Tokens for serialization (can be held across blocking operations)</li> <li>Cross-CPU message passing via IPIQs (Inter-Processor Interrupt Queues)</li> </ul> <p>This architecture reduces lock contention and enables better scalability on multi-core systems.</p> <p>Start here: LWKT Threading is essential to understand before exploring other subsystems.</p>"},{"location":"sys/kern/#major-subsystems","title":"Major Subsystems","text":""},{"location":"sys/kern/#core-infrastructure","title":"Core Infrastructure","text":""},{"location":"sys/kern/#lwkt-threading","title":"LWKT Threading","text":"<p>DragonFly's message-passing threading model \u2014 the architectural foundation.</p> <ul> <li><code>lwkt_thread.c</code> \u2014 Thread management</li> <li><code>lwkt_msgport.c</code> \u2014 Message ports and message passing</li> <li><code>lwkt_token.c</code> \u2014 Serializing tokens</li> <li><code>lwkt_ipiq.c</code> \u2014 Inter-processor interrupt queues</li> <li><code>lwkt_serialize.c</code> \u2014 Serialization helpers</li> </ul>"},{"location":"sys/kern/#synchronization","title":"Synchronization","text":"<p>Locks, tokens, and synchronization primitives.</p> <ul> <li>Spinlocks, mutexes, condition variables</li> <li>Token-based serialization</li> <li>Sleep queues</li> <li>Reference counting</li> </ul>"},{"location":"sys/kern/#memory-management","title":"Memory Management","text":"<p>Kernel memory allocation and management.</p> <ul> <li><code>kern_kmalloc.c</code> \u2014 Kernel malloc wrapper</li> <li><code>kern_slaballoc.c</code> \u2014 Slab allocator</li> <li><code>kern_objcache.c</code> \u2014 Per-CPU object caches</li> <li><code>kern_mpipe.c</code> \u2014 Lock-free message pipe allocator</li> </ul>"},{"location":"sys/kern/#process-and-thread-management","title":"Process and Thread Management","text":""},{"location":"sys/kern/#processes-threads","title":"Processes &amp; Threads","text":"<p>Process lifecycle and thread management.</p> <ul> <li><code>kern_fork.c</code> \u2014 Process creation (fork, rfork)</li> <li><code>kern_exec.c</code> \u2014 Program execution (execve)</li> <li><code>kern_exit.c</code> \u2014 Process termination and wait</li> <li><code>kern_proc.c</code> \u2014 Process structure management</li> <li><code>kern_threads.c</code> \u2014 Thread management</li> <li><code>kern_sig.c</code> \u2014 Signal handling</li> </ul>"},{"location":"sys/kern/#scheduling","title":"Scheduling","text":"<p>CPU scheduling framework and policies.</p> <ul> <li><code>kern_sched.c</code> \u2014 Generic scheduler framework</li> <li><code>usched_dfly.c</code> \u2014 DragonFly's message-based scheduler</li> <li><code>usched_bsd4.c</code> \u2014 Traditional BSD4 scheduler</li> <li><code>kern_synch.c</code> \u2014 Sleep/wakeup primitives</li> </ul>"},{"location":"sys/kern/#storage-and-filesystems","title":"Storage and Filesystems","text":""},{"location":"sys/kern/#virtual-filesystem-vfs","title":"Virtual Filesystem (VFS)","text":"<p>Filesystem abstraction layer.</p> <ul> <li>VFS core (<code>vfs_*.c</code>, 23 files)</li> <li>Name lookup and caching</li> <li>Buffer cache and I/O clustering</li> <li>Mount/unmount operations</li> <li>Journaling support</li> <li>VFS/VM integration</li> </ul>"},{"location":"sys/kern/#communication","title":"Communication","text":""},{"location":"sys/kern/#ipc-sockets","title":"IPC &amp; Sockets","text":"<p>Inter-process communication and networking foundation.</p> <ul> <li>Unix domain sockets (<code>uipc_*.c</code>, 11 files)</li> <li>Mbuf management</li> <li>Socket layer</li> <li>System V IPC (messages, semaphores, shared memory)</li> <li>Pipes and POSIX message queues</li> </ul>"},{"location":"sys/kern/#hardware-interface","title":"Hardware Interface","text":""},{"location":"sys/kern/#devices-drivers","title":"Devices &amp; Drivers","text":"<p>Device framework and driver infrastructure.</p> <ul> <li>Device registration and management</li> <li>Bus/driver model (newbus)</li> <li>Disk layer and partitioning</li> <li>DMA framework</li> <li>I/O scheduling</li> </ul>"},{"location":"sys/kern/#system-calls","title":"System Calls","text":"<p>System call infrastructure and kernel module loading.</p> <ul> <li>System call dispatch</li> <li>Dynamic kernel linking</li> <li>Module management</li> </ul>"},{"location":"sys/kern/#subsystem-organization","title":"Subsystem Organization","text":"<p>The ~180 files in <code>kern/</code> cluster by prefix:</p> <ul> <li><code>kern_*</code> (70 files) \u2014 Core kernel services</li> <li><code>subr_*</code> (36 files) \u2014 Kernel subroutines and utilities</li> <li><code>vfs_*</code> (23 files) \u2014 Virtual filesystem</li> <li><code>uipc_*</code> (11 files) \u2014 Unix IPC</li> <li><code>lwkt_*</code> (5 files) \u2014 Lightweight kernel threading</li> <li><code>sys_*</code> (5 files) \u2014 System call implementations</li> <li><code>tty_*</code> (5 files) \u2014 Terminal I/O</li> <li><code>usched_*</code> (3 files) \u2014 User schedulers</li> <li>Plus image activators, dynamic linkers, and utilities</li> </ul>"},{"location":"sys/kern/#reading-order","title":"Reading Order","text":"<p>If you're new to DragonFly's kernel core:</p> <ol> <li>LWKT Threading \u2190 Start here! Essential foundation</li> <li>Synchronization \u2014 Tokens, locks, message passing</li> <li>Memory Management \u2014 Kernel allocation</li> <li>Processes &amp; Threads \u2014 Process lifecycle</li> <li>Scheduling \u2014 CPU scheduling</li> <li>VFS \u2014 Filesystem layer</li> <li>IPC &amp; Sockets \u2014 Communication</li> <li>Devices \u2014 Device framework</li> <li>System Calls \u2014 Syscall infrastructure</li> </ol>"},{"location":"sys/kern/#key-concepts","title":"Key Concepts","text":""},{"location":"sys/kern/#message-passing","title":"Message Passing","text":"<p>Instead of calling functions directly across threads, DragonFly often uses asynchronous messages:</p> <pre><code>Thread A                    Thread B\n   |                           |\n   |-- send message ----------&gt;|\n   |                           | process message\n   |&lt;-- reply message ---------|\n   |                           |\n</code></pre> <p>This reduces lock contention and cache coherency traffic.</p>"},{"location":"sys/kern/#tokens-vs-locks","title":"Tokens vs. Locks","text":"<p>Tokens can be held across blocking operations:</p> <pre><code>lwkt_gettoken(&amp;mp-&gt;mnt_token);\n/* Can sleep here - token is retained */\nlwkt_reltoken(&amp;mp-&gt;mnt_token);\n</code></pre> <p>Traditional locks typically cannot be held across sleeps.</p>"},{"location":"sys/kern/#per-cpu-data","title":"Per-CPU Data","text":"<p>Many data structures are per-CPU to avoid cache-line bouncing:</p> <pre><code>struct globaldata *gd = mycpu;\n/* Access CPU-local data without locks */\n</code></pre>"},{"location":"sys/kern/#dependencies","title":"Dependencies","text":"<p>Understanding dependencies helps navigate <code>kern/</code>:</p> <pre><code>LWKT (foundation)\n  \u2193\nSynchronization\n  \u2193\nMemory allocation\n  \u2193\nProcesses/Threads \u2192 VFS \u2192 Filesystems\n  \u2193                 \u2193\nScheduling      Device I/O\n</code></pre>"},{"location":"sys/kern/#source-location","title":"Source Location","text":"<p>All source discussed here lives in:</p> <pre><code>~/s/dragonfly/sys/kern/\n</code></pre> <p>With documentation mirrored at:</p> <pre><code>~/s/dragonfly-docs/docs/sys/kern/\n</code></pre>"},{"location":"sys/kern/#whats-next","title":"What's Next?","text":"<p>Ready to dive deeper? Start with LWKT Threading to understand DragonFly's unique concurrency model, then explore other subsystems based on your interests.</p> <p>For a detailed reading plan covering all ~180 files, see the planning document at <code>planning/sys/kern/PLAN.md</code>.</p>"},{"location":"sys/kern/accounting/","title":"Accounting, Sensors, and Watchdog","text":"<p>This document covers kernel subsystems for system monitoring and resource tracking: process accounting records resource usage for completed processes, the hardware sensor framework collects environmental data from device drivers, and the watchdog subsystem prevents system hangs through timer-based monitoring.</p> <p>Source files: - <code>sys/kern/kern_acct.c</code> - Process accounting - <code>sys/kern/kern_sensors.c</code> - Hardware sensor framework - <code>sys/kern/kern_wdog.c</code> - Watchdog timer support</p>"},{"location":"sys/kern/accounting/#process-accounting","title":"Process Accounting","text":"<p>Process accounting writes a record to a designated file each time a process exits, capturing resource usage statistics. This BSD-standard mechanism allows system administrators to track command execution, CPU time, and I/O activity.</p>"},{"location":"sys/kern/accounting/#the-acct-structure","title":"The acct Structure","text":"<p>Each accounting record uses a fixed-format structure (<code>sys/acct.h:53</code>):</p> <pre><code>struct acct {\n    char      ac_comm[16];   /* command name */\n    comp_t    ac_utime;      /* user time */\n    comp_t    ac_stime;      /* system time */\n    comp_t    ac_etime;      /* elapsed time */\n    time_t    ac_btime;      /* starting time */\n    uid_t     ac_uid;        /* user id */\n    gid_t     ac_gid;        /* group id */\n    u_int16_t ac_mem;        /* average memory usage */\n    comp_t    ac_io;         /* count of IO blocks */\n    dev_t     ac_tty;        /* controlling tty */\n    u_int8_t  ac_flag;       /* accounting flags */\n};\n</code></pre> <p>The <code>comp_t</code> type is a compressed floating-point representation using a 3-bit base-8 exponent and 13-bit mantissa, providing compact storage for time values in units of 1/64 seconds (AHZ = 64).</p>"},{"location":"sys/kern/accounting/#accounting-flags","title":"Accounting Flags","text":"Flag Value Description <code>AFORK</code> 0x01 Process forked but never exec'd <code>ASU</code> 0x02 Used superuser permissions <code>ACOMPAT</code> 0x04 Used compatibility mode <code>ACORE</code> 0x08 Dumped core on exit <code>AXSIG</code> 0x10 Killed by a signal"},{"location":"sys/kern/accounting/#enabling-accounting","title":"Enabling Accounting","text":"<p>The <code>acct()</code> system call enables or disables process accounting (<code>kern_acct.c:122</code>):</p> <pre><code>int sys_acct(struct sysmsg *sysmsg, const struct acct_args *uap)\n{\n    /* Requires SYSCAP_NOACCT capability */\n    error = caps_priv_check_self(SYSCAP_NOACCT);\n\n    if (uap-&gt;path != NULL) {\n        /* Open file for append, must be regular file */\n        error = nlookup_init(&amp;nd, uap-&gt;path, UIO_USERSPACE, NLC_LOCKVP);\n        error = vn_open(&amp;nd, NULL, FWRITE | O_APPEND, 0);\n        if (nd.nl_open_vp-&gt;v_type != VREG)\n            error = EACCES;\n    }\n\n    /* Close previous accounting file if any */\n    if (acctp != NULLVP || savacctp != NULLVP) {\n        callout_stop(&amp;acctwatch_handle);\n        vn_close(...);\n    }\n\n    /* Start space watcher for new file */\n    acctp = vp;\n    acctwatch(NULL);\n}\n</code></pre>"},{"location":"sys/kern/accounting/#writing-accounting-records","title":"Writing Accounting Records","text":"<p>The <code>acct_process()</code> function writes an accounting record on process exit (<code>kern_acct.c:196</code>):</p> <pre><code>int acct_process(struct proc *p)\n{\n    struct acct acct;\n\n    /* Skip if accounting disabled */\n    if (acctp == NULLVP)\n        return 0;\n\n    /* (1) Command name */\n    bcopy(p-&gt;p_comm, acct.ac_comm, sizeof acct.ac_comm);\n\n    /* (2) User and system time */\n    calcru_proc(p, &amp;ru);\n    acct.ac_utime = encode_comp_t(ru.ru_utime.tv_sec, ru.ru_utime.tv_usec);\n    acct.ac_stime = encode_comp_t(ru.ru_stime.tv_sec, ru.ru_stime.tv_usec);\n\n    /* (3) Elapsed time and start time */\n    acct.ac_btime = p-&gt;p_start.tv_sec;\n    acct.ac_etime = encode_comp_t(elapsed.tv_sec, elapsed.tv_usec);\n\n    /* (4) Average memory usage */\n    acct.ac_mem = (r-&gt;ru_ixrss + r-&gt;ru_idrss + r-&gt;ru_isrss) / t;\n\n    /* (5) I/O block count */\n    acct.ac_io = encode_comp_t(r-&gt;ru_inblock + r-&gt;ru_oublock, 0);\n\n    /* (6) UID and GID */\n    acct.ac_uid = p-&gt;p_ucred-&gt;cr_ruid;\n    acct.ac_gid = p-&gt;p_ucred-&gt;cr_rgid;\n\n    /* (7) Controlling terminal */\n    acct.ac_tty = devid_from_dev(p-&gt;p_pgrp-&gt;pg_session-&gt;s_ttyp-&gt;t_dev);\n\n    /* (8) Process flags */\n    acct.ac_flag = p-&gt;p_acflag;\n\n    /* Write record (with no file size limit) */\n    vn_rdwr(UIO_WRITE, vp, &amp;acct, sizeof(acct), 0,\n            UIO_SYSSPACE, IO_APPEND|IO_UNIT, ...);\n}\n</code></pre>"},{"location":"sys/kern/accounting/#disk-space-monitoring","title":"Disk Space Monitoring","text":"<p>The accounting subsystem monitors available disk space and suspends accounting when space runs low (<code>kern_acct.c:329</code>):</p> <pre><code>static void acctwatch(void *a)\n{\n    VFS_STATFS(acctp-&gt;v_mount, &amp;sb, proc0.p_ucred);\n\n    if (sb.f_bavail &lt;= acctsuspend * sb.f_blocks / 100) {\n        /* Suspend accounting when below threshold */\n        savacctp = acctp;\n        acctp = NULLVP;\n        log(LOG_NOTICE, \"Accounting suspended\\n\");\n    }\n\n    /* Reschedule check */\n    callout_reset(&amp;acctwatch_handle, acctchkfreq * hz, acctwatch, NULL);\n}\n</code></pre>"},{"location":"sys/kern/accounting/#accounting-sysctls","title":"Accounting Sysctls","text":"Sysctl Default Description <code>kern.acct_suspend</code> 2 Suspend when free space below this % <code>kern.acct_resume</code> 4 Resume when free space above this % <code>kern.acct_chkfreq</code> 15 Space check frequency (seconds)"},{"location":"sys/kern/accounting/#hardware-sensor-framework","title":"Hardware Sensor Framework","text":"<p>The sensor framework provides a unified interface for hardware monitoring devices to report environmental data (temperature, voltage, fan speed, etc.) through the sysctl tree. Originally from OpenBSD, this framework supports per-CPU sensor task scheduling for efficient data collection.</p>"},{"location":"sys/kern/accounting/#sensor-types","title":"Sensor Types","text":"<p>The framework supports many sensor types (<code>sys/sensors.h:35</code>):</p> Type Sysctl Name Unit Description <code>SENSOR_TEMP</code> temp \u00b5K Temperature (microkelvin) <code>SENSOR_FANRPM</code> fan RPM Fan speed <code>SENSOR_VOLTS_DC</code> volt \u00b5V DC voltage <code>SENSOR_VOLTS_AC</code> acvolt \u00b5V AC voltage <code>SENSOR_OHMS</code> resistance \u03a9 Resistance <code>SENSOR_WATTS</code> power W Power <code>SENSOR_AMPS</code> current \u00b5A Current <code>SENSOR_WATTHOUR</code> watthour Wh Energy capacity <code>SENSOR_AMPHOUR</code> amphour Ah Charge capacity <code>SENSOR_INDICATOR</code> indicator bool Boolean state <code>SENSOR_INTEGER</code> raw - Generic integer <code>SENSOR_PERCENT</code> percent % Percentage <code>SENSOR_LUX</code> illuminance mlx Light level <code>SENSOR_DRIVE</code> drive - Disk status <code>SENSOR_TIMEDELTA</code> timedelta ns Time error <code>SENSOR_ECC</code> ecc - Memory ECC errors <code>SENSOR_FREQ</code> freq Hz Frequency"},{"location":"sys/kern/accounting/#sensor-status","title":"Sensor Status","text":"<p>Each sensor reports its health status:</p> <pre><code>enum sensor_status {\n    SENSOR_S_UNSPEC,    /* status unspecified */\n    SENSOR_S_OK,        /* normal operation */\n    SENSOR_S_WARN,      /* warning threshold */\n    SENSOR_S_CRIT,      /* critical threshold */\n    SENSOR_S_UNKNOWN    /* status unknown */\n};\n</code></pre>"},{"location":"sys/kern/accounting/#data-structures","title":"Data Structures","text":"<p>Kernel sensor (<code>sys/sensors.h:142</code>):</p> <pre><code>struct ksensor {\n    SLIST_ENTRY(ksensor) list;   /* device sensor list */\n    char desc[32];               /* description */\n    struct timeval tv;           /* last update time */\n    int64_t value;               /* current value */\n    enum sensor_type type;       /* sensor type */\n    enum sensor_status status;   /* health status */\n    int numt;                    /* index within type */\n    int flags;                   /* SENSOR_FINVALID, SENSOR_FUNKNOWN */\n    struct sysctl_oid *oid;      /* sysctl node */\n};\n</code></pre> <p>Sensor device (<code>sys/sensors.h:156</code>):</p> <pre><code>struct ksensordev {\n    TAILQ_ENTRY(ksensordev) list;\n    int num;                          /* device number */\n    char xname[16];                   /* device name (e.g., \"cpu0\") */\n    int maxnumt[SENSOR_MAX_TYPES];    /* count per type */\n    int sensors_count;                /* total sensors */\n    struct ksensors_head sensors_list;\n    struct sysctl_oid *oid;           /* sysctl node */\n    struct sysctl_ctx_list clist;     /* sysctl context */\n};\n</code></pre>"},{"location":"sys/kern/accounting/#device-registration","title":"Device Registration","text":"<p>Drivers register sensor devices to make sensors visible (<code>kern_sensors.c:78</code>):</p> <pre><code>void sensordev_install(struct ksensordev *sensdev)\n{\n    SYSCTL_XLOCK();\n\n    /* Find next available device number */\n    TAILQ_FOREACH(v, &amp;sensordev_list, list) {\n        if (v-&gt;num == num)\n            ++num;\n        else if (v-&gt;num &gt; num)\n            break;\n    }\n    sensdev-&gt;num = num;\n\n    /* Insert into list */\n    TAILQ_INSERT_AFTER(&amp;sensordev_list, after, sensdev, list);\n\n    /* Create sysctl node: hw.sensors.&lt;device&gt; */\n    sensordev_sysctl_install(sensdev);\n\n    SYSCTL_XUNLOCK();\n}\n</code></pre>"},{"location":"sys/kern/accounting/#sensor-attachment","title":"Sensor Attachment","text":"<p>Individual sensors are attached to devices (<code>kern_sensors.c:112</code>):</p> <pre><code>void sensor_attach(struct ksensordev *sensdev, struct ksensor *sens)\n{\n    SYSCTL_XLOCK();\n\n    /* Assign sensor number within type */\n    /* Sensors of same type are kept consecutive */\n    sens-&gt;numt = v-&gt;numt + 1;  /* or 0 if first of type */\n\n    SLIST_INSERT_AFTER(v, sens, list);\n    sensdev-&gt;maxnumt[sens-&gt;type]++;\n    sensdev-&gt;sensors_count++;\n\n    /* Create sysctl: hw.sensors.&lt;device&gt;.&lt;type&gt;&lt;n&gt; */\n    sensor_sysctl_install(sensdev, sens);\n\n    SYSCTL_XUNLOCK();\n}\n</code></pre>"},{"location":"sys/kern/accounting/#task-scheduling","title":"Task Scheduling","text":"<p>Sensor drivers use periodic tasks to poll hardware. Tasks are distributed across CPUs for efficiency (<code>kern_sensors.c:269</code>):</p> <pre><code>struct sensor_task *\nsensor_task_register2(void *arg, void (*func)(void *), int period, int cpu)\n{\n    if (cpu &lt; 0)\n        cpu = sensor_task_default_cpu;  /* Usually first package CPU */\n\n    thr = &amp;sensor_task_threads[cpu];\n\n    st = kmalloc(sizeof(struct sensor_task), M_DEVBUF, M_WAITOK);\n    st-&gt;arg = arg;\n    st-&gt;func = func;\n    st-&gt;period = period;\n    st-&gt;cpuid = cpu;\n    st-&gt;running = 1;\n    st-&gt;nextrun = 0;  /* Run immediately */\n\n    TAILQ_INSERT_HEAD(&amp;thr-&gt;list, st, entry);\n    wakeup(&amp;thr-&gt;list);\n\n    return st;\n}\n</code></pre> <p>Each CPU runs a sensor task thread that processes its task list (<code>kern_sensors.c:300</code>):</p> <pre><code>static void sensor_task_thread(void *xthr)\n{\n    for (;;) {\n        /* Wait for tasks */\n        while (TAILQ_EMPTY(&amp;thr-&gt;list))\n            lksleep(&amp;thr-&gt;list, &amp;thr-&gt;lock, 0, \"waittask\", 0);\n\n        /* Wait until next task is due */\n        while (nst-&gt;nextrun &gt; time_uptime)\n            lksleep(&amp;thr-&gt;list, &amp;thr-&gt;lock, 0, \"timeout\",\n                    (nst-&gt;nextrun - now) * hz);\n\n        /* Run due tasks */\n        TAILQ_FOREACH_SAFE(st, &amp;thr-&gt;list, entry, nst) {\n            if (st-&gt;nextrun &gt; now)\n                break;\n\n            TAILQ_REMOVE(&amp;thr-&gt;list, st, entry);\n\n            if (!st-&gt;running) {\n                kfree(st, M_DEVBUF);\n                continue;\n            }\n\n            st-&gt;func(st-&gt;arg);           /* Poll hardware */\n            sensor_task_schedule(thr, st); /* Reschedule */\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/accounting/#sysctl-interface","title":"Sysctl Interface","text":"<p>Sensors appear under <code>hw.sensors</code>:</p> <pre><code>hw.sensors.cpu0.temp0           # First temperature sensor on cpu0\nhw.sensors.acpi_tz0.temp0       # ACPI thermal zone temperature\nhw.sensors.aibs0.volt0          # First voltage sensor\nhw.sensors.dev_idmax            # Maximum sensor device ID\n</code></pre> <p>The legacy MIB interface (<code>hw._sensors</code>) supports OpenBSD-compatible access (<code>kern_sensors.c:499</code>):</p> <pre><code>static int sysctl_sensors_handler(SYSCTL_HANDLER_ARGS)\n{\n    /* name[0] = device, name[1] = type, name[2] = numt */\n    if (namelen == 1)\n        return sysctl_handle_sensordev(...);  /* Return device info */\n\n    type = name[1];\n    numt = name[2];\n    ks = sensor_find(ksd, type, numt);\n    return sysctl_handle_sensor(...);  /* Return sensor data */\n}\n</code></pre>"},{"location":"sys/kern/accounting/#helper-functions","title":"Helper Functions","text":"<p>Convenience functions for setting sensor values (<code>sys/sensors.h:191</code>):</p> <pre><code>/* Mark sensor as invalid (hardware error) */\nstatic inline void sensor_set_invalid(struct ksensor *sens);\n\n/* Mark sensor value as unknown */\nstatic inline void sensor_set_unknown(struct ksensor *sens);\n\n/* Set sensor value and status */\nstatic inline void sensor_set(struct ksensor *sens, int64_t val,\n                              enum sensor_status status);\n\n/* Set temperature from degrees Celsius */\nstatic inline void sensor_set_temp_degc(struct ksensor *sens, int degc,\n                                        enum sensor_status status);\n</code></pre>"},{"location":"sys/kern/accounting/#driver-example","title":"Driver Example","text":"<p>A typical sensor driver pattern:</p> <pre><code>struct mydev_softc {\n    struct ksensordev sensordev;\n    struct ksensor temp_sensor;\n    struct sensor_task *task;\n};\n\nstatic void mydev_refresh(void *arg)\n{\n    struct mydev_softc *sc = arg;\n    int temp = read_hw_temp(sc);\n\n    /* Temperature in microkelvin: (degC * 1000000) + 273150000 */\n    sensor_set_temp_degc(&amp;sc-&gt;temp_sensor, temp, SENSOR_S_OK);\n}\n\nstatic int mydev_attach(device_t dev)\n{\n    struct mydev_softc *sc = device_get_softc(dev);\n\n    strlcpy(sc-&gt;sensordev.xname, device_get_nameunit(dev),\n            sizeof(sc-&gt;sensordev.xname));\n\n    sc-&gt;temp_sensor.type = SENSOR_TEMP;\n    strlcpy(sc-&gt;temp_sensor.desc, \"CPU temperature\",\n            sizeof(sc-&gt;temp_sensor.desc));\n    sensor_attach(&amp;sc-&gt;sensordev, &amp;sc-&gt;temp_sensor);\n\n    sensordev_install(&amp;sc-&gt;sensordev);\n\n    /* Poll every 5 seconds on default CPU */\n    sc-&gt;task = sensor_task_register2(sc, mydev_refresh, 5, -1);\n}\n</code></pre>"},{"location":"sys/kern/accounting/#watchdog-timer-support","title":"Watchdog Timer Support","text":"<p>The watchdog subsystem manages hardware watchdog timers that reset the system if not periodically serviced. This prevents system hangs by ensuring the kernel remains responsive.</p>"},{"location":"sys/kern/accounting/#watchdog-structure","title":"Watchdog Structure","text":"<p>Drivers register watchdog devices using (<code>sys/wdog.h:47</code>):</p> <pre><code>typedef int (wdog_fn)(void *, int);\n\nstruct watchdog {\n    const char *name;       /* watchdog name */\n    wdog_fn *wdog_fn;       /* reset function */\n    void *arg;              /* driver argument */\n    int period_max;         /* maximum period (seconds) */\n\n    /* Internal fields */\n    int period;             /* current period */\n    LIST_ENTRY(watchdog) link;\n};\n</code></pre> <p>The reset function receives the argument and requested period, returning the actual period set (which may be clamped to hardware limits).</p>"},{"location":"sys/kern/accounting/#registration","title":"Registration","text":"<p>Watchdog drivers register with the framework (<code>kern_wdog.c:60</code>):</p> <pre><code>void wdog_register(struct watchdog *wd)\n{\n    spin_lock(&amp;wdogmtx);\n    wd-&gt;period = WDOG_DEFAULT_PERIOD;  /* 30 seconds */\n    LIST_INSERT_HEAD(&amp;wdoglist, wd, link);\n    spin_unlock(&amp;wdogmtx);\n\n    wdog_reset_all(NULL);  /* Start watchdog immediately */\n\n    kprintf(\"wdog: Watchdog %s registered, max period = %ds\\n\",\n            wd-&gt;name, wd-&gt;period_max);\n}\n</code></pre>"},{"location":"sys/kern/accounting/#automatic-reset","title":"Automatic Reset","text":"<p>By default, the kernel automatically resets all watchdogs before they expire (<code>kern_wdog.c:90</code>):</p> <pre><code>static void wdog_reset_all(void *unused)\n{\n    int min_period = INT_MAX;\n\n    spin_lock(&amp;wdogmtx);\n\n    LIST_FOREACH(wd, &amp;wdoglist, link) {\n        period = wdog_reset(wd);  /* Call driver */\n        if (period &lt; min_period)\n            min_period = period;\n    }\n\n    if (wdog_auto_enable) {\n        /* Reset at half the minimum period */\n        callout_reset(&amp;wdog_callout, min_period * hz / 2,\n                      wdog_reset_all, NULL);\n    }\n\n    spin_unlock(&amp;wdogmtx);\n}\n</code></pre>"},{"location":"sys/kern/accounting/#sysctl-interface_1","title":"Sysctl Interface","text":"Sysctl Default Description <code>kern.watchdog.auto</code> 1 Enable automatic kernel reset <code>kern.watchdog.period</code> 30 Watchdog period (seconds) <p>When <code>kern.watchdog.auto</code> is disabled, userspace must reset the watchdog via the <code>/dev/wdog</code> device.</p>"},{"location":"sys/kern/accounting/#device-interface","title":"Device Interface","text":"<p>The <code>/dev/wdog</code> device provides userspace watchdog control (<code>kern_wdog.c:188</code>):</p> <pre><code>static int wdog_ioctl(struct dev_ioctl_args *ap)\n{\n    if (wdog_auto_enable)\n        return EINVAL;  /* Must disable auto-reset first */\n\n    if (ap-&gt;a_cmd == WDIOCRESET) {\n        wdog_reset_all(NULL);\n        return 0;\n    }\n    return EINVAL;\n}\n</code></pre> <p>Userspace usage pattern:</p> <pre><code>int fd = open(\"/dev/wdog\", O_RDWR);\n\n/* Disable kernel auto-reset via sysctl first */\nsysctlbyname(\"kern.watchdog.auto\", NULL, NULL, &amp;zero, sizeof(zero));\n\n/* Reset watchdog periodically */\nwhile (running) {\n    ioctl(fd, WDIOCRESET, NULL);\n    sleep(period / 2);\n}\n</code></pre>"},{"location":"sys/kern/accounting/#disabling-watchdogs","title":"Disabling Watchdogs","text":"<p>The <code>wdog_disable()</code> function stops all watchdog timers (<code>kern_wdog.c:171</code>):</p> <pre><code>void wdog_disable(void)\n{\n    callout_stop(&amp;wdog_callout);\n    wdog_set_period(0);  /* Period 0 disables hardware */\n    wdog_reset_all(NULL);\n}\n</code></pre> <p>This is called during controlled shutdown to prevent spurious resets.</p>"},{"location":"sys/kern/accounting/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph userspace[\"User Space\"]\n        apps[\"Applications(monitoring daemons)\"]\n    end\n\n    subgraph devwdog[\"/dev/wdog (if auto=0)\"]\n        wdogdev[\"ioctl(WDIOCRESET)\"]\n    end\n\n    subgraph framework[\"Watchdog Framework\"]\n        callout[\"wdog_callout(auto reset @ period/2)\"]\n        subgraph wdoglist[\"wdoglist\"]\n            ichwd[\"ichwd\"]\n            amdsbwd[\"amdsbwd\"]\n            other[\"other\"]\n            ichwd --- amdsbwd --- other\n        end\n        callout --&gt; wdoglist\n    end\n\n    subgraph hardware[\"Hardware Watchdog Timers\"]\n        timers[\"Chipset-specificcountdown timers\"]\n    end\n\n    apps --&gt; wdogdev\n    wdogdev --&gt; framework\n    wdoglist --&gt; hardware\n</code></pre>"},{"location":"sys/kern/accounting/#see-also","title":"See Also","text":"<ul> <li>Sysctl Framework - MIB tree for sensor and watchdog sysctls</li> <li>Tracing - KTR ring buffers for debugging</li> <li>Devices - Device driver framework</li> <li>Time Keeping - Callout infrastructure for periodic tasks</li> </ul>"},{"location":"sys/kern/bus-resources/","title":"Bus Resource Management and DMA","text":"<p>This document covers the resource manager (rman) subsystem for managing hardware resources and the bus DMA subsystem for DMA memory management.</p> <p>Source files: - <code>sys/kern/subr_rman.c</code> - Resource manager implementation (~735 lines) - <code>sys/kern/subr_busdma.c</code> - Bus DMA helper functions (~144 lines) - <code>sys/platform/pc64/x86_64/busdma_machdep.c</code> - x86_64 DMA implementation (~1470 lines) - <code>sys/sys/rman.h</code> - Resource manager data structures - <code>sys/sys/bus_dma.h</code> - Bus DMA interface - <code>sys/cpu/x86_64/include/bus_dma.h</code> - Architecture-specific types</p>"},{"location":"sys/kern/bus-resources/#resource-manager-rman","title":"Resource Manager (rman)","text":"<p>The resource manager provides generic infrastructure for tracking and allocating hardware resources like IRQs, I/O ports, and memory regions. It is used by NewBus to manage bus resources.</p>"},{"location":"sys/kern/bus-resources/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/bus-resources/#struct-resource","title":"struct resource","text":"<p>Represents an allocated resource:</p> <pre><code>struct resource {\n    TAILQ_ENTRY(resource)   r_link;         /* list linkage */\n    LIST_ENTRY(resource)    r_sharelink;    /* sharing list link */\n    LIST_HEAD(, resource)   *r_sharehead;   /* head of sharing list */\n    u_long  r_start;        /* first index in resource */\n    u_long  r_end;          /* last index (inclusive) */\n    u_int   r_flags;        /* RF_* flags */\n    void    *r_virtual;     /* virtual address */\n    bus_space_tag_t r_bustag;       /* bus_space tag */\n    bus_space_handle_t r_bushandle; /* bus_space handle */\n    device_t r_dev;         /* owning device */\n    struct  rman *r_rm;     /* owning resource manager */\n    int     r_rid;          /* resource identifier */\n};\n</code></pre> <p>Defined in <code>sys/sys/rman.h:98-111</code>.</p>"},{"location":"sys/kern/bus-resources/#struct-rman","title":"struct rman","text":"<p>The resource manager:</p> <pre><code>struct rman {\n    struct  resource_head   rm_list;    /* list of resources */\n    struct  lwkt_token      *rm_slock;  /* mutex for rm_list */\n    TAILQ_ENTRY(rman)       rm_link;    /* link in global list */\n    u_long  rm_start;       /* globally first entry */\n    u_long  rm_end;         /* globally last entry */\n    enum    rman_type rm_type;  /* RMAN_ARRAY or RMAN_GAUGE */\n    const   char *rm_descr; /* text description */\n    int     rm_cpuid;       /* owner CPU ID */\n    int     rm_hold;        /* destruction interlock */\n};\n</code></pre> <p>Defined in <code>sys/sys/rman.h:115-125</code>.</p>"},{"location":"sys/kern/bus-resources/#resource-types","title":"Resource Types","text":"<pre><code>enum rman_type { RMAN_UNINIT = 0, RMAN_GAUGE, RMAN_ARRAY };\n</code></pre> <ul> <li><code>RMAN_ARRAY</code> - Sequential, individually-allocatable resources (common case)</li> <li><code>RMAN_GAUGE</code> - Fungible resources like power budgets (not currently used)</li> </ul> <p>Defined in <code>sys/sys/rman.h:60</code>.</p>"},{"location":"sys/kern/bus-resources/#resource-flags","title":"Resource Flags","text":"Flag Value Description <code>RF_ALLOCATED</code> 0x0001 Resource has been reserved <code>RF_ACTIVE</code> 0x0002 Resource allocation activated <code>RF_SHAREABLE</code> 0x0004 Permits contemporaneous sharing <code>RF_TIMESHARE</code> 0x0008 Permits time-division sharing <code>RF_WANTED</code> 0x0010 Someone is waiting for resource <code>RF_FIRSTSHARE</code> 0x0020 First in sharing list <code>RF_PREFETCHABLE</code> 0x0040 Memory is prefetchable <code>RF_OPTIONAL</code> 0x0080 For bus_alloc_resources() <p>Alignment can be encoded in flags using: <pre><code>#define RF_ALIGNMENT_SHIFT  10\n#define RF_ALIGNMENT_LOG2(x) ((x) &lt;&lt; RF_ALIGNMENT_SHIFT)\n#define RF_ALIGNMENT(x)     (((x) &amp; RF_ALIGNMENT_MASK) &gt;&gt; RF_ALIGNMENT_SHIFT)\n</code></pre></p> <p>Defined in <code>sys/sys/rman.h:45-58</code>.</p>"},{"location":"sys/kern/bus-resources/#api-functions","title":"API Functions","text":""},{"location":"sys/kern/bus-resources/#rman_init","title":"rman_init","text":"<p>Initializes a resource manager:</p> <pre><code>int rman_init(struct rman *rm, int cpuid);\n</code></pre> <ul> <li>Creates a per-rman lwkt_token for locking</li> <li>Adds the rman to global <code>rman_head</code> list</li> <li>Sets <code>rm_type</code> to <code>RMAN_ARRAY</code></li> </ul> <p>See <code>sys/kern/subr_rman.c:86-116</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_manage_region","title":"rman_manage_region","text":"<p>Adds a range of resources to the manager:</p> <pre><code>int rman_manage_region(struct rman *rm, u_long start, u_long end);\n</code></pre> <ul> <li>Maintains sorted order by start address</li> <li>Does NOT check for overlapping regions</li> <li>Caller must ensure regions don't overlap</li> </ul> <p>See <code>sys/kern/subr_rman.c:122-152</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_reserve_resource","title":"rman_reserve_resource","text":"<p>Reserves resources from the manager:</p> <pre><code>struct resource *rman_reserve_resource(struct rman *rm, u_long start,\n                                       u_long end, u_long count,\n                                       u_int flags, device_t dev);\n</code></pre> <p>Allocation algorithm: 1. Search for unshared region that fits the request 2. Split region into 1-3 parts as needed:    - Allocating from beginning: split into 2 parts    - Allocating from end: split into 2 parts    - Allocating from middle: split into 3 parts 3. If <code>RF_SHAREABLE</code> or <code>RF_TIMESHARE</code>, search for exact match 4. If <code>RF_ACTIVE</code> specified, atomically activate</p> <p>See <code>sys/kern/subr_rman.c:204-404</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_activate_resource-rman_deactivate_resource","title":"rman_activate_resource / rman_deactivate_resource","text":"<p>Activate or deactivate a resource:</p> <pre><code>int rman_activate_resource(struct resource *r);\nint rman_deactivate_resource(struct resource *r);\n</code></pre> <ul> <li>Activation marks resource as <code>RF_ACTIVE</code></li> <li>Time-shared resources: only one active at a time</li> <li>Deactivation wakes waiters via <code>wakeup(r-&gt;r_sharehead)</code></li> </ul> <p>See <code>sys/kern/subr_rman.c:406-515</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_release_resource","title":"rman_release_resource","text":"<p>Releases a resource back to the pool:</p> <pre><code>int rman_release_resource(struct resource *r);\n</code></pre> <p>Merging algorithm: 1. If sharing list exists, update the list 2. Try to merge with previous adjacent segment 3. Try to merge with next adjacent segment 4. If both neighbors are free, merge all three 5. If neither neighbor is free, just mark as unallocated</p> <p>See <code>sys/kern/subr_rman.c:517-613</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_fini","title":"rman_fini","text":"<p>Destroys a resource manager:</p> <pre><code>int rman_fini(struct rman *rm);\n</code></pre> <ul> <li>Fails if any resources are still allocated</li> <li>Waits for <code>rm_hold</code> to drop to zero before destroying</li> </ul> <p>See <code>sys/kern/subr_rman.c:154-202</code>.</p>"},{"location":"sys/kern/bus-resources/#helper-macros","title":"Helper Macros","text":"<p>Accessor macros for resource fields:</p> <pre><code>#define rman_get_start(r)       ((r)-&gt;r_start)\n#define rman_get_end(r)         ((r)-&gt;r_end)\n#define rman_get_size(r)        ((r)-&gt;r_end - (r)-&gt;r_start + 1)\n#define rman_get_device(r)      ((r)-&gt;r_dev)\n#define rman_get_flags(r)       ((r)-&gt;r_flags)\n#define rman_get_virtual(r)     ((r)-&gt;r_virtual)\n#define rman_get_bustag(r)      ((r)-&gt;r_bustag)\n#define rman_get_bushandle(r)   ((r)-&gt;r_bushandle)\n#define rman_get_rid(r)         ((r)-&gt;r_rid)\n#define rman_get_cpuid(r)       ((r)-&gt;r_rm-&gt;rm_cpuid)\n</code></pre> <p>Defined in <code>sys/sys/rman.h:141-157</code>.</p>"},{"location":"sys/kern/bus-resources/#sysctl-interface","title":"Sysctl Interface","text":"<p>Resource manager information is exported via <code>hw.bus.rman</code> sysctl for userspace introspection. The exported structures are:</p> <pre><code>struct u_rman {\n    uintptr_t      rm_handle;\n    char           rm_descr[RM_TEXTLEN];\n    u_long         rm_start;\n    u_long         rm_size;\n    enum rman_type rm_type;\n};\n\nstruct u_resource {\n    uintptr_t r_handle;\n    uintptr_t r_parent;\n    uintptr_t r_device;\n    char      r_devname[RM_TEXTLEN];\n    u_long    r_start;\n    u_long    r_size;\n    u_int     r_flags;\n};\n</code></pre> <p>Defined in <code>sys/sys/rman.h:70-88</code>.</p>"},{"location":"sys/kern/bus-resources/#bus-dma-subsystem","title":"Bus DMA Subsystem","text":"<p>The bus DMA subsystem provides portable DMA memory management with support for devices that have address limitations requiring bounce buffers.</p>"},{"location":"sys/kern/bus-resources/#data-structures_1","title":"Data Structures","text":""},{"location":"sys/kern/bus-resources/#bus_dma_tag_t-struct-bus_dma_tag","title":"bus_dma_tag_t (struct bus_dma_tag)","text":"<p>Defines constraints for DMA operations:</p> <pre><code>struct bus_dma_tag {\n    bus_size_t      alignment;      /* required alignment */\n    bus_size_t      boundary;       /* boundary that segments can't cross */\n    bus_addr_t      lowaddr;        /* low address constraint */\n    bus_addr_t      highaddr;       /* high address constraint */\n    bus_size_t      maxsize;        /* maximum mapping size */\n    u_int           nsegments;      /* max number of segments */\n    bus_size_t      maxsegsz;       /* max size per segment */\n    int             flags;          /* BUS_DMA_* flags */\n    int             map_count;      /* number of active maps */\n    bus_dma_segment_t *segments;    /* segment array */\n    struct bounce_zone *bounce_zone;/* bounce buffer zone */\n    struct spinlock spin;           /* lock for segment array */\n};\n</code></pre> <p>Defined in <code>sys/platform/pc64/x86_64/busdma_machdep.c:64-77</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_t-struct-bus_dmamap","title":"bus_dmamap_t (struct bus_dmamap)","text":"<p>Tracks a DMA mapping:</p> <pre><code>struct bus_dmamap {\n    struct bp_list  bpages;         /* list of bounce pages */\n    int             pagesneeded;    /* pages needed for transfer */\n    int             pagesreserved;  /* pages currently reserved */\n    bus_dma_tag_t   dmat;           /* associated tag */\n    void            *buf;           /* original buffer pointer */\n    bus_size_t      buflen;         /* original buffer length */\n    bus_dmamap_callback_t *callback;/* completion callback */\n    void            *callback_arg;  /* callback argument */\n    STAILQ_ENTRY(bus_dmamap) links; /* waitlist linkage */\n};\n</code></pre> <p>Defined in <code>sys/platform/pc64/x86_64/busdma_machdep.c:140-150</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dma_segment_t","title":"bus_dma_segment_t","text":"<p>Describes a DMA segment:</p> <pre><code>typedef struct bus_dma_segment {\n    bus_addr_t  ds_addr;    /* DMA address */\n    bus_size_t  ds_len;     /* length of transfer */\n} bus_dma_segment_t;\n</code></pre> <p>Defined in <code>sys/sys/bus_dma.h:146-149</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamem_t","title":"bus_dmamem_t","text":"<p>Convenience structure for coherent memory:</p> <pre><code>typedef struct bus_dmamem {\n    bus_dma_tag_t   dmem_tag;       /* tag used */\n    bus_dmamap_t    dmem_map;       /* map created */\n    void            *dmem_addr;     /* virtual address */\n    bus_addr_t      dmem_busaddr;   /* bus address */\n} bus_dmamem_t;\n</code></pre> <p>Defined in <code>sys/sys/bus_dma.h:151-156</code>.</p>"},{"location":"sys/kern/bus-resources/#dma-flags","title":"DMA Flags","text":"Flag Value Description <code>BUS_DMA_WAITOK</code> 0x0000 Safe to sleep (pseudo-flag) <code>BUS_DMA_NOWAIT</code> 0x0001 Not safe to sleep <code>BUS_DMA_ALLOCNOW</code> 0x0002 Perform resource allocation now <code>BUS_DMA_COHERENT</code> 0x0004 Map memory to not require sync <code>BUS_DMA_ZERO</code> 0x0008 Allocate zero'd memory <code>BUS_DMA_ONEBPAGE</code> 0x0100 Allocate one bounce page per map <code>BUS_DMA_ALIGNED</code> 0x0200 Memory is already properly aligned <code>BUS_DMA_PRIVBZONE</code> 0x0400 Need private bounce zone <code>BUS_DMA_ALLOCALL</code> 0x0800 Allocate all needed resources <code>BUS_DMA_PROTECTED</code> 0x1000 Functions are already protected <code>BUS_DMA_KEEP_PG_OFFSET</code> 0x2000 Preserve page offset in first segment <code>BUS_DMA_NOCACHE</code> 0x4000 Map memory uncached <p>Defined in <code>sys/sys/bus_dma.h:83-104</code>.</p>"},{"location":"sys/kern/bus-resources/#sync-operations","title":"Sync Operations","text":"<pre><code>typedef int bus_dmasync_op_t;\n#define BUS_DMASYNC_PREREAD     0x01  /* before device reads from memory */\n#define BUS_DMASYNC_POSTREAD    0x02  /* after device reads from memory */\n#define BUS_DMASYNC_PREWRITE    0x04  /* before device writes to memory */\n#define BUS_DMASYNC_POSTWRITE   0x08  /* after device writes to memory */\n</code></pre> <p>On x86, these operations primarily handle bounce buffer data copying: - <code>PREWRITE</code>: Copy data from client buffer to bounce buffer - <code>POSTREAD</code>: Copy data from bounce buffer to client buffer - <code>PREREAD</code>/<code>POSTWRITE</code>: No-ops on cache-coherent x86</p> <p>Defined in <code>sys/sys/bus_dma.h:116-121</code>.</p>"},{"location":"sys/kern/bus-resources/#api-functions_1","title":"API Functions","text":""},{"location":"sys/kern/bus-resources/#bus_dma_tag_create","title":"bus_dma_tag_create","text":"<p>Creates a DMA tag with specified constraints:</p> <pre><code>int bus_dma_tag_create(bus_dma_tag_t parent, bus_size_t alignment,\n                       bus_size_t boundary, bus_addr_t lowaddr,\n                       bus_addr_t highaddr, bus_size_t maxsize,\n                       int nsegments, bus_size_t maxsegsz,\n                       int flags, bus_dma_tag_t *dmat);\n</code></pre> <p>Key behavior: - Validates alignment/boundary are powers of 2 - Inherits constraints from parent tag - Sets bounce flags if <code>lowaddr &lt; Maxmem</code> or <code>alignment &gt; 1</code> - Pre-allocates bounce pages if <code>BUS_DMA_ALLOCNOW</code></p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:222-331</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dma_tag_destroy","title":"bus_dma_tag_destroy","text":"<p>Destroys a DMA tag:</p> <pre><code>int bus_dma_tag_destroy(bus_dma_tag_t dmat);\n</code></pre> <ul> <li>Fails with <code>EBUSY</code> if maps still exist</li> <li>Frees bounce zone (for private zones)</li> <li>Frees segment array and tag</li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:333-346</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_create","title":"bus_dmamap_create","text":"<p>Creates a DMA map:</p> <pre><code>int bus_dmamap_create(bus_dma_tag_t dmat, int flags, bus_dmamap_t *mapp);\n</code></pre> <ul> <li>Returns NULL map if no bouncing needed</li> <li>Allocates map structure and initializes bounce page list</li> <li>Allocates bounce pages incrementally up to <code>max_bounce_pages</code></li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:358-433</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_destroy","title":"bus_dmamap_destroy","text":"<p>Destroys a DMA map:</p> <pre><code>int bus_dmamap_destroy(bus_dma_tag_t dmat, bus_dmamap_t map);\n</code></pre> <ul> <li>Fails with <code>EBUSY</code> if bounce pages still attached</li> <li>Decrements <code>map_count</code> on tag</li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:439-449</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamem_alloc","title":"bus_dmamem_alloc","text":"<p>Allocates DMA-safe memory:</p> <pre><code>int bus_dmamem_alloc(bus_dma_tag_t dmat, void **vaddr, int flags,\n                     bus_dmamap_t *mapp);\n</code></pre> <p>Allocation method selection: - Small allocations (<code>maxsize &lt;= PAGE_SIZE</code>, <code>lowaddr &gt;= Maxmem</code>): <code>kmalloc</code> - Large/constrained allocations: <code>contigmalloc</code></p> <p>The map pointer encodes which allocator was used for later freeing.</p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:480-546</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamem_free","title":"bus_dmamem_free","text":"<p>Frees DMA-safe memory:</p> <pre><code>void bus_dmamem_free(bus_dma_tag_t dmat, void *vaddr, bus_dmamap_t map);\n</code></pre> <ul> <li><code>map == NULL</code>: uses <code>kfree</code></li> <li><code>map == (void *)-1</code>: uses <code>contigfree</code></li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:552-565</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_load","title":"bus_dmamap_load","text":"<p>Loads a buffer for DMA:</p> <pre><code>int bus_dmamap_load(bus_dma_tag_t dmat, bus_dmamap_t map, void *buf,\n                    bus_size_t buflen, bus_dmamap_callback_t *callback,\n                    void *callback_arg, int flags);\n</code></pre> <p>Algorithm: 1. Count bounce pages needed 2. Reserve bounce pages with zone lock held 3. Per-page loop:    - Extract physical address    - If needs bouncing, substitute bounce page address    - Coalesce contiguous segments    - Handle boundary and <code>maxsegsz</code> constraints 4. If insufficient bounce pages, return <code>EINPROGRESS</code> (deferred) 5. Otherwise call callback immediately with segments</p> <p>Bounce page need determination: <pre><code>static __inline int addr_needs_bounce(bus_dma_tag_t dmat, bus_addr_t paddr)\n{\n    if ((paddr &gt; dmat-&gt;lowaddr &amp;&amp; paddr &lt;= dmat-&gt;highaddr) ||\n         (bounce_alignment &amp;&amp; (paddr &amp; (dmat-&gt;alignment - 1)) != 0))\n        return (1);\n    return (0);\n}\n</code></pre></p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:766-807</code>.</p>"},{"location":"sys/kern/bus-resources/#specialized-load-functions","title":"Specialized Load Functions","text":"<p>bus_dmamap_load_mbuf() - Loads an mbuf chain: <pre><code>int bus_dmamap_load_mbuf(bus_dma_tag_t dmat, bus_dmamap_t map,\n                         struct mbuf *m0,\n                         bus_dmamap_callback2_t *callback,\n                         void *callback_arg, int flags);\n</code></pre> See <code>sys/platform/pc64/x86_64/busdma_machdep.c:836-867</code>.</p> <p>bus_dmamap_load_mbuf_segment() - Loads mbuf with direct segment return: <pre><code>int bus_dmamap_load_mbuf_segment(bus_dma_tag_t dmat, bus_dmamap_t map,\n                                 struct mbuf *m0,\n                                 bus_dma_segment_t *segs, int maxsegs,\n                                 int *nsegs, int flags);\n</code></pre> See <code>sys/platform/pc64/x86_64/busdma_machdep.c:869-923</code>.</p> <p>bus_dmamap_load_uio() - Loads user I/O vector: <pre><code>int bus_dmamap_load_uio(bus_dma_tag_t dmat, bus_dmamap_t map,\n                        struct uio *uio,\n                        bus_dmamap_callback2_t *callback,\n                        void *callback_arg, int flags);\n</code></pre> See <code>sys/platform/pc64/x86_64/busdma_machdep.c:928-1017</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_unload","title":"bus_dmamap_unload","text":"<p>Unloads a DMA mapping:</p> <pre><code>void bus_dmamap_unload(bus_dma_tag_t dmat, bus_dmamap_t map);\n</code></pre> <ul> <li>Frees all bounce pages associated with map</li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1022-1031</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_sync","title":"bus_dmamap_sync","text":"<p>Synchronizes DMA memory:</p> <pre><code>void bus_dmamap_sync(bus_dma_tag_t dmat, bus_dmamap_t map,\n                     bus_dmasync_op_t op);\n</code></pre> <p>Bounce buffer handling on x86: - <code>PREWRITE</code>: Copy data from client to bounce buffer, then <code>cpu_sfence()</code> - <code>POSTREAD</code>: <code>cpu_lfence()</code>, then copy data from bounce to client buffer - <code>PREREAD</code>/<code>POSTWRITE</code>: No operation (cache coherent)</p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1033-1067</code>.</p>"},{"location":"sys/kern/bus-resources/#bounce-buffer-infrastructure","title":"Bounce Buffer Infrastructure","text":""},{"location":"sys/kern/bus-resources/#struct-bounce_page","title":"struct bounce_page","text":"<p>Individual bounce page:</p> <pre><code>struct bounce_page {\n    vm_offset_t vaddr;          /* kva of bounce buffer */\n    bus_addr_t  busaddr;        /* physical address */\n    vm_offset_t datavaddr;      /* kva of client data */\n    bus_size_t  datacount;      /* client data count */\n    STAILQ_ENTRY(bounce_page) links;\n};\n</code></pre> <p>Defined in <code>sys/platform/pc64/x86_64/busdma_machdep.c:93-99</code>.</p>"},{"location":"sys/kern/bus-resources/#struct-bounce_zone","title":"struct bounce_zone","text":"<p>Zone managing bounce pages:</p> <pre><code>struct bounce_zone {\n    STAILQ_ENTRY(bounce_zone) links;\n    STAILQ_HEAD(bp_list, bounce_page) bounce_page_list;\n    STAILQ_HEAD(, bus_dmamap) bounce_map_waitinglist;\n    struct spinlock spin;\n    int             total_bpages;       /* total bounce pages */\n    int             free_bpages;        /* free bounce pages */\n    int             reserved_bpages;    /* reserved bounce pages */\n    int             active_bpages;      /* in-use bounce pages */\n    int             total_bounced;      /* total transfers bounced */\n    int             total_deferred;     /* total deferred operations */\n    int             reserve_failed;     /* failed reservations */\n    bus_size_t      alignment;          /* zone alignment */\n    bus_addr_t      lowaddr;            /* zone low address limit */\n    char            zoneid[8];          /* zone identifier */\n    char            lowaddrid[20];      /* low address string */\n    struct sysctl_ctx_list sysctl_ctx;\n    struct sysctl_oid *sysctl_tree;\n};\n</code></pre> <p>Defined in <code>sys/platform/pc64/x86_64/busdma_machdep.c:101-119</code>.</p>"},{"location":"sys/kern/bus-resources/#zone-management","title":"Zone Management","text":"<p>Bounce zones are shared by default. Multiple tags with compatible constraints share a zone. Private zones can be requested with <code>BUS_DMA_PRIVBZONE</code>.</p> <p>alloc_bounce_zone() - Creates or finds compatible zone: See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1069-1167</code>.</p> <p>alloc_bounce_pages() - Allocates pages using <code>contigmalloc</code>: See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1169-1206</code>.</p> <p>reserve_bounce_pages() - Reserves pages from free pool: See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1262-1283</code>.</p> <p>return_bounce_pages() - Returns pages to free pool, wakes waiters: See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1285-1312</code>.</p>"},{"location":"sys/kern/bus-resources/#deferred-operations","title":"Deferred Operations","text":"<p>When bounce pages are unavailable, <code>bus_dmamap_load()</code> returns <code>EINPROGRESS</code> and the map is added to a waiting list. The <code>busdma_swi()</code> software interrupt handler processes waiting maps when bounce pages become available.</p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1436-1450</code>.</p>"},{"location":"sys/kern/bus-resources/#helper-functions","title":"Helper Functions","text":""},{"location":"sys/kern/bus-resources/#bus_dmamem_coherent","title":"bus_dmamem_coherent","text":"<p>Allocates coherent DMA memory in one call:</p> <pre><code>int bus_dmamem_coherent(bus_dma_tag_t parent,\n                        bus_size_t alignment, bus_size_t boundary,\n                        bus_addr_t lowaddr, bus_addr_t highaddr,\n                        bus_size_t maxsize, int flags,\n                        bus_dmamem_t *dmem);\n</code></pre> <p>Creates tag, allocates memory, and loads mapping.</p> <p>See <code>sys/kern/subr_busdma.c:53-95</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamem_coherent_any","title":"bus_dmamem_coherent_any","text":"<p>Simplified coherent allocation with no boundary:</p> <pre><code>void *bus_dmamem_coherent_any(bus_dma_tag_t parent,\n                              bus_size_t alignment, bus_size_t size,\n                              int flags,\n                              bus_dma_tag_t *dtag, bus_dmamap_t *dmap,\n                              bus_addr_t *busaddr);\n</code></pre> <p>See <code>sys/kern/subr_busdma.c:97-117</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_load_mbuf_defrag","title":"bus_dmamap_load_mbuf_defrag","text":"<p>Loads mbuf with automatic defragmentation:</p> <pre><code>int bus_dmamap_load_mbuf_defrag(bus_dma_tag_t dmat, bus_dmamap_t map,\n                                struct mbuf **m_head,\n                                bus_dma_segment_t *segs, int maxsegs,\n                                int *nsegs, int flags);\n</code></pre> <p>Tries normal load first; if <code>EFBIG</code> (too many segments), defragments the mbuf and retries.</p> <p>See <code>sys/kern/subr_busdma.c:119-143</code>.</p>"},{"location":"sys/kern/bus-resources/#bus-space-operations","title":"Bus Space Operations","text":""},{"location":"sys/kern/bus-resources/#types-x86_64","title":"Types (x86_64)","text":"<pre><code>typedef uint64_t bus_addr_t;\ntypedef uint64_t bus_size_t;\ntypedef uint64_t bus_space_tag_t;\ntypedef uint64_t bus_space_handle_t;\n</code></pre> <p>Bus space tags: - <code>X86_64_BUS_SPACE_IO</code> (0) - I/O port space - <code>X86_64_BUS_SPACE_MEM</code> (1) - Memory-mapped space</p> <p>Address limits: <pre><code>#define BUS_SPACE_MAXADDR_24BIT 0xFFFFFFUL\n#define BUS_SPACE_MAXADDR_32BIT 0xFFFFFFFFUL\n#define BUS_SPACE_MAXADDR       0xFFFFFFFFFFFFFFFFUL\n</code></pre></p> <p>Defined in <code>sys/cpu/x86_64/include/bus_dma.h:36-55</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_space_map-bus_space_unmap","title":"bus_space_map / bus_space_unmap","text":"<p>Maps bus space to kernel virtual address:</p> <pre><code>int bus_space_map(bus_space_tag_t t, bus_addr_t addr, bus_size_t size,\n                  int flags, bus_space_handle_t *bshp);\nvoid bus_space_unmap(bus_space_tag_t t, bus_space_handle_t bsh,\n                     bus_size_t size);\n</code></pre> <ul> <li>Memory space: uses <code>pmap_mapdev()</code>/<code>pmap_unmapdev()</code></li> <li>I/O space: returns address directly (no mapping needed)</li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1452-1469</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_space_barrier","title":"bus_space_barrier","text":"<p>Memory barrier for bus operations:</p> <pre><code>static __inline void\nbus_space_barrier(bus_space_tag_t tag, bus_space_handle_t bsh,\n                  bus_size_t offset, bus_size_t len, int flags)\n{\n    if (flags &amp; BUS_SPACE_BARRIER_READ)\n        __asm __volatile(\"lock; addl $0,0(%%rsp)\" : : : \"memory\");\n    else\n        __asm __volatile(\"\" : : : \"memory\");\n}\n</code></pre> <ul> <li>Read barrier: uses locked instruction for MFENCE semantics</li> <li>Write barrier: compiler barrier only (x86 has strong ordering)</li> </ul> <p>Defined in <code>sys/cpu/x86_64/include/bus_dma.h:893-901</code>.</p>"},{"location":"sys/kern/bus-resources/#sysctl-interface_1","title":"Sysctl Interface","text":"<p>Global tunables: <pre><code>hw.busdma.max_bpages     - Maximum bounce pages (default 1024)\nhw.busdma.bounce_alignment - Enable alignment bouncing (default 1)\n</code></pre></p> <p>Per-zone statistics via <code>hw.busdma.zoneN.*</code>: - <code>total_bpages</code> - Total bounce pages in zone - <code>free_bpages</code> - Free bounce pages - <code>reserved_bpages</code> - Reserved bounce pages - <code>active_bpages</code> - Active (in-use) bounce pages - <code>total_bounced</code> - Total bounce operations - <code>total_deferred</code> - Total deferred operations - <code>reserve_failed</code> - Failed reservations - <code>lowaddr</code> - Zone low address constraint - <code>alignment</code> - Zone alignment constraint</p>"},{"location":"sys/kern/bus-resources/#example-dma-buffer-allocation","title":"Example: DMA Buffer Allocation","text":"<pre><code>bus_dma_tag_t   tag;\nbus_dmamap_t    map;\nvoid            *vaddr;\nbus_addr_t      paddr;\nbus_dma_segment_t seg;\nint             nseg;\n\n/* Create a tag for 4K-aligned, 32-bit addressable memory */\nerror = bus_dma_tag_create(NULL,            /* parent */\n                           4096,            /* alignment */\n                           0,               /* boundary */\n                           BUS_SPACE_MAXADDR_32BIT,  /* lowaddr */\n                           BUS_SPACE_MAXADDR,        /* highaddr */\n                           4096,            /* maxsize */\n                           1,               /* nsegments */\n                           4096,            /* maxsegsz */\n                           0,               /* flags */\n                           &amp;tag);\n\n/* Allocate DMA-safe memory */\nerror = bus_dmamem_alloc(tag, &amp;vaddr, BUS_DMA_WAITOK | BUS_DMA_ZERO, &amp;map);\n\n/* Load the buffer to get physical address */\nerror = bus_dmamap_load(tag, map, vaddr, 4096, callback, &amp;paddr, BUS_DMA_NOWAIT);\n\n/* Use the buffer... */\n\n/* Before device reads: */\nbus_dmamap_sync(tag, map, BUS_DMASYNC_PREREAD);\n\n/* After device reads: */\nbus_dmamap_sync(tag, map, BUS_DMASYNC_POSTREAD);\n\n/* Cleanup */\nbus_dmamap_unload(tag, map);\nbus_dmamem_free(tag, vaddr, map);\nbus_dmamap_destroy(tag, map);\nbus_dma_tag_destroy(tag);\n</code></pre>"},{"location":"sys/kern/bus-resources/#cross-references","title":"Cross-References","text":"<ul> <li>NewBus Framework - Device/driver infrastructure using rman</li> <li>Device Framework - Character device layer</li> <li>Memory Management - Kernel memory allocation</li> <li>Buffer Cache - BIO and buffer management</li> </ul>"},{"location":"sys/kern/checkpoint/","title":"Process Checkpoint/Restart","text":"<p>The DragonFly BSD kernel provides a process checkpoint/restart facility that allows running processes to be frozen to a file and later resumed. This feature enables application migration, fault tolerance, and long-running computation management.</p>"},{"location":"sys/kern/checkpoint/#overview","title":"Overview","text":"<p>Process checkpointing captures the complete state of a running process, including:</p> <ul> <li>CPU register state (general-purpose and floating-point registers)</li> <li>Virtual memory mappings and contents</li> <li>Open file descriptors</li> <li>Signal handlers and masks</li> <li>Process identity information</li> </ul> <p>The checkpoint file uses the ELF format, leveraging the existing core dump infrastructure with extensions for restoration.</p> <pre><code>flowchart LR\n    subgraph Freeze[\"Checkpoint (Freeze)\"]\n        direction LR\n        P1[\"Running Process\"] --&gt;|\"SIGCKPT / sys_checkpoint()\"| F1[\"Checkpoint File(ELF format)\"]\n    end\n    subgraph Thaw[\"Restore (Thaw)\"]\n        direction LR\n        F2[\"Checkpoint File(ELF format)\"] --&gt;|\"checkpt -r file.ckpt\"| P2[\"Running Process\"]\n    end\n    Freeze --&gt; Thaw\n</code></pre>"},{"location":"sys/kern/checkpoint/#architecture","title":"Architecture","text":""},{"location":"sys/kern/checkpoint/#checkpoint-file-format","title":"Checkpoint File Format","text":"<p>The checkpoint file is an ELF core file with the following structure:</p> <pre><code>block-beta\n    columns 1\n    block:elf[\"ELF Header (e_type = ET_CORE)\"]\n        columns 1\n    end\n    block:phdr[\"Program Headers (PT_NOTE, PT_LOAD)\"]\n        columns 1\n    end\n    block:notes[\"Notes Section\"]\n        columns 1\n        n1[\"NT_PRPSINFO (psinfo)\"]\n        n2[\"NT_PRSTATUS (regs)\"]\n        n3[\"NT_FPREGSET (fpregs)\"]\n        n4[\"Per-thread state...\"]\n    end\n    block:vminfo[\"VM Info Section\"]\n        columns 1\n        v1[\"Text/data addresses\"]\n        v2[\"Segment sizes\"]\n    end\n    block:vnode[\"Vnode Headers\"]\n        columns 1\n        vh1[\"File handles\"]\n        vh2[\"Memory mappings\"]\n    end\n    block:sig[\"Signal Info\"]\n        columns 1\n        s1[\"Signal actions\"]\n        s2[\"Signal masks\"]\n        s3[\"Interval timers\"]\n    end\n    block:fd[\"File Descriptors\"]\n        columns 1\n        f1[\"Open file list\"]\n        f2[\"File handles\"]\n    end\n    block:mem[\"Memory Segments\"]\n        columns 1\n        m1[\"Writable data\"]\n        m2[\"Stack contents\"]\n    end\n</code></pre>"},{"location":"sys/kern/checkpoint/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/kern/checkpoint/#checkpoint-vm-info","title":"Checkpoint VM Info","text":"<pre><code>/* sys/ckpt.h */\nstruct ckpt_vminfo {\n    segsz_t     cvm_dsize;      /* Data segment size (pages) */\n    segsz_t     cvm_tsize;      /* Text segment size (pages) */\n    segsz_t     cvm_reserved1[4];\n    caddr_t     cvm_daddr;      /* Data segment address */\n    caddr_t     cvm_taddr;      /* Text segment address */\n    caddr_t     cvm_reserved2[4];\n};\n</code></pre>"},{"location":"sys/kern/checkpoint/#checkpoint-file-info","title":"Checkpoint File Info","text":"<pre><code>struct ckpt_fileinfo {\n    int         cfi_index;      /* File descriptor number */\n    u_int       cfi_flags;      /* Saved f_flag */\n    off_t       cfi_offset;     /* Saved f_offset */\n    fhandle_t   cfi_fh;         /* File handle for VFS lookup */\n    int         cfi_type;       /* File type */\n    int         cfi_ckflags;    /* Checkpoint flags */\n    int         cfi_reserved[6];\n};\n\n#define CKFIF_ISCKPTFD  0x0001  /* This FD is the checkpoint file itself */\n</code></pre>"},{"location":"sys/kern/checkpoint/#checkpoint-signal-info","title":"Checkpoint Signal Info","text":"<pre><code>struct ckpt_siginfo {\n    int             csi_ckptpisz;   /* Structure size for validation */\n    struct sigacts  csi_sigacts;    /* Signal action table */\n    struct itimerval csi_itimerval; /* Interval timer */\n    int             csi_sigparent;  /* Signal to parent on exit */\n    sigset_t        csi_sigmask;    /* Current signal mask */\n    int             csi_reserved[6];\n};\n</code></pre>"},{"location":"sys/kern/checkpoint/#vnode-header","title":"Vnode Header","text":"<pre><code>struct vn_hdr {\n    fhandle_t   vnh_fh;         /* File handle for mapped file */\n    Elf_Phdr    vnh_phdr;       /* Program header for mapping */\n    int         vnh_reserved[8];\n};\n</code></pre>"},{"location":"sys/kern/checkpoint/#system-call-interface","title":"System Call Interface","text":""},{"location":"sys/kern/checkpoint/#sys_checkpoint","title":"sys_checkpoint","text":"<p>The <code>sys_checkpoint()</code> system call provides the user interface:</p> <pre><code>int sys_checkpoint(int type, int fd, pid_t pid, int retval);\n</code></pre> <p>Parameters:</p> Parameter Description <code>type</code> Operation: <code>CKPT_FREEZE</code> or <code>CKPT_THAW</code> <code>fd</code> File descriptor for checkpoint file <code>pid</code> Process ID (-1 for current process) <code>retval</code> Return value after restore <p>Operation Types:</p> Type Value Description <code>CKPT_FREEZE</code> 0x1 Create checkpoint of current process <code>CKPT_THAW</code> 0x2 Restore process from checkpoint <code>CKPT_FREEZEPID</code> 0x3 Checkpoint another process (not implemented) <code>CKPT_THAWBIN</code> 0x4 Restore with binary replacement (not implemented) <p>Return Value:</p> <ul> <li>On freeze: Returns 0 on success</li> <li>On thaw: Returns the <code>retval</code> parameter passed during restore</li> <li>Programs can distinguish between checkpoint creation and restoration by   checking this return value</li> </ul>"},{"location":"sys/kern/checkpoint/#implementation","title":"Implementation","text":"<p>Source: <code>sys/kern/kern_checkpoint.c:715-770</code></p> <pre><code>int \nsys_sys_checkpoint(struct sysmsg *sysmsg,\n                   const struct sys_checkpoint_args *uap)\n{\n    int error = 0;\n    struct thread *td = curthread;\n    struct proc *p = td-&gt;td_proc;\n    struct file *fp;\n\n    /* Only certain groups can checkpoint (security) */\n    if (ckptgroup &gt;= 0 &amp;&amp; groupmember(ckptgroup, td-&gt;td_ucred) == 0)\n        return (EPERM);\n\n    /* For now only checkpoint current process */\n    if (uap-&gt;pid != -1 &amp;&amp; uap-&gt;pid != p-&gt;p_pid)\n        return (EINVAL);\n\n    get_mplock();\n\n    switch (uap-&gt;type) {\n    case CKPT_FREEZE:\n        fp = NULL;\n        if (uap-&gt;fd == -1 &amp;&amp; uap-&gt;pid == (pid_t)-1)\n            error = checkpoint_signal_handler(td-&gt;td_lwp);\n        else if ((fp = holdfp(td, uap-&gt;fd, FWRITE)) == NULL)\n            error = EBADF;\n        else\n            error = ckpt_freeze_proc(td-&gt;td_lwp, fp);\n        if (fp)\n            dropfp(td, uap-&gt;fd, fp);\n        break;\n    case CKPT_THAW:\n        if (uap-&gt;pid != -1) {\n            error = EINVAL;\n            break;\n        }\n        if ((fp = holdfp(td, uap-&gt;fd, FREAD)) == NULL) {\n            error = EBADF;\n            break;\n        }\n        sysmsg-&gt;sysmsg_result = uap-&gt;retval;\n        error = ckpt_thaw_proc(td-&gt;td_lwp, fp);\n        dropfp(td, uap-&gt;fd, fp);\n        break;\n    default:\n        error = EOPNOTSUPP;\n        break;\n    }\n    rel_mplock();\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#checkpoint-signals","title":"Checkpoint Signals","text":"<p>DragonFly provides two signals for checkpoint control:</p> Signal Number Default Action Description <code>SIGCKPT</code> 33 Checkpoint and continue Process creates checkpoint, continues running <code>SIGCKPTEXIT</code> 34 Checkpoint and exit Process creates checkpoint, then terminates"},{"location":"sys/kern/checkpoint/#signal-properties","title":"Signal Properties","text":"<p>Source: <code>sys/kern/kern_sig.c:162-163</code></p> <pre><code>SA_CKPT,            /* SIGCKPT */\nSA_KILL|SA_CKPT,    /* SIGCKPTEXIT */\n</code></pre>"},{"location":"sys/kern/checkpoint/#triggering-via-tty","title":"Triggering via TTY","text":"<p>Users can trigger checkpoints using the checkpoint character (default: Ctrl+E):</p> <p>Source: <code>sys/kern/tty.c:662-665</code></p> <pre><code>if (CCEQ(cc[VCHECKPT], c) &amp;&amp; ISSET(lflag, IEXTEN)) {\n    if (ISSET(lflag, ISIG))\n        pgsignal(tp-&gt;t_pgrp, SIGCKPT, 1);\n    goto endcase;\n}\n</code></pre> <p>The checkpoint character can be configured using <code>stty(1)</code>:</p> <pre><code># View current checkpoint character\nstty -a | grep ckpt\n\n# Change checkpoint character\nstty ckpt '^T'\n\n# Disable checkpoint character\nstty ckpt undef\n</code></pre>"},{"location":"sys/kern/checkpoint/#freeze-operation","title":"Freeze Operation","text":""},{"location":"sys/kern/checkpoint/#process-flow","title":"Process Flow","text":"<pre><code>flowchart TD\n    A[\"sys_checkpoint(CKPT_FREEZE)\"] --&gt; B[\"ckpt_freeze_proc()\"]\n    B --&gt; C[\"proc_stop(p, SCORE)Stop all threads\"]\n    C --&gt; D[\"Wait for threads to stop\"]\n    D --&gt; E[\"generic_elf_coredump()Write checkpoint\"]\n    E --&gt; F[\"proc_unstop(p, SCORE)Resume threads\"]\n    F --&gt; G[\"Return to caller\"]\n</code></pre>"},{"location":"sys/kern/checkpoint/#freeze-implementation","title":"Freeze Implementation","text":"<p>Source: <code>sys/kern/kern_checkpoint.c:684-710</code></p> <pre><code>static int\nckpt_freeze_proc(struct lwp *lp, struct file *fp)\n{\n    struct proc *p = lp-&gt;lwp_proc;\n    rlim_t limit;\n    int error;\n\n    lwkt_gettoken(&amp;p-&gt;p_token);\n\n    limit = p-&gt;p_rlimit[RLIMIT_CORE].rlim_cur;\n    if (limit) {\n        if (p-&gt;p_stat != SCORE) {\n            /* Stop all threads in the process */\n            proc_stop(p, SCORE);\n\n            /* Wait for all threads to stop */\n            while (p-&gt;p_nstopped &lt; p-&gt;p_nthreads - 1)\n                tsleep(&amp;p-&gt;p_nstopped, 0, \"freeze\", 1);\n\n            /* Generate checkpoint using core dump machinery */\n            error = generic_elf_coredump(lp, SIGCKPT, fp, limit);\n\n            /* Resume execution */\n            proc_unstop(p, SCORE);\n        } else {\n            error = ERANGE;\n        }\n    } else {\n        error = ERANGE;\n    }\n    lwkt_reltoken(&amp;p-&gt;p_token);\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#signal-handler-checkpoint","title":"Signal Handler Checkpoint","text":"<p>When triggered by <code>SIGCKPT</code>, the checkpoint goes through a signal handler:</p> <p>Source: <code>sys/kern/kern_checkpoint.c:772-828</code></p> <pre><code>int\ncheckpoint_signal_handler(struct lwp *lp)\n{\n    struct thread *td = lp-&gt;lwp_thread;\n    struct proc *p = lp-&gt;lwp_proc;\n    char *buf;\n    struct file *fp;\n    struct nlookupdata nd;\n    int error;\n\n    chptinuse++;\n\n    /* Security: prevent checkpointing setuid/setgid programs */\n    if (sugid_coredump == 0 &amp;&amp; (p-&gt;p_flags &amp; P_SUGID)) {\n        chptinuse--;\n        return (EPERM);\n    }\n\n    /* Generate checkpoint filename */\n    buf = ckpt_expand_name(p-&gt;p_comm, td-&gt;td_ucred-&gt;cr_uid, p-&gt;p_pid);\n    if (buf == NULL) {\n        chptinuse--;\n        return (ENOMEM);\n    }\n\n    log(LOG_INFO, \"pid %d (%s), uid %d: checkpointing to %s\\n\",\n        p-&gt;p_pid, p-&gt;p_comm,\n        (td-&gt;td_ucred ? td-&gt;td_ucred-&gt;cr_uid : -1), buf);\n\n    /* Remove any previous checkpoint file (important for re-checkpointing\n     * restored processes - otherwise we corrupt the memory mappings) */\n    error = nlookup_init(&amp;nd, buf, UIO_SYSSPACE, 0);\n    if (error == 0)\n        error = kern_unlink(&amp;nd);\n    nlookup_done(&amp;nd);\n\n    /* Create and write checkpoint file */\n    error = fp_open(buf, O_WRONLY|O_CREAT|O_TRUNC|O_NOFOLLOW, 0600, &amp;fp);\n    if (error == 0) {\n        error = ckpt_freeze_proc(lp, fp);\n        fp_close(fp);\n    }\n    kfree(buf, M_TEMP);\n    chptinuse--;\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#thaw-restore-operation","title":"Thaw (Restore) Operation","text":""},{"location":"sys/kern/checkpoint/#process-flow_1","title":"Process Flow","text":"<pre><code>flowchart TD\n    A[\"checkpt -r file.ckpt\"] --&gt; B[\"sys_checkpoint(CKPT_THAW)\"]\n    B --&gt; C[\"ckpt_thaw_proc()\"]\n    C --&gt; D[\"elf_gethdr()Read ELF header\"]\n    D --&gt; E[\"elf_getphdrs()Read program headers\"]\n    E --&gt; F[\"elf_getnotes()Restore register state\"]\n    F --&gt; G[\"elf_gettextvp()Restore text mappings\"]\n    G --&gt; H[\"elf_getsigs()Restore signal state\"]\n    H --&gt; I[\"elf_getfiles()Restore file descriptors\"]\n    I --&gt; J[\"elf_loadphdrs()Map memory segments\"]\n    J --&gt; K[\"Set p_textvpMark as checkpoint-restored\"]\n    K --&gt; L[\"Resume execution with retval\"]\n</code></pre>"},{"location":"sys/kern/checkpoint/#thaw-implementation","title":"Thaw Implementation","text":"<p>Source: <code>sys/kern/kern_checkpoint.c:217-281</code></p> <pre><code>static int\nckpt_thaw_proc(struct lwp *lp, struct file *fp)\n{\n    struct proc *p = lp-&gt;lwp_proc;\n    Elf_Phdr *phdr = NULL;\n    Elf_Ehdr *ehdr = NULL;\n    int error;\n    size_t nbyte;\n\n    ehdr = kmalloc(sizeof(Elf_Ehdr), M_TEMP, M_ZERO | M_WAITOK);\n\n    /* Read and validate ELF header */\n    if ((error = elf_gethdr(fp, ehdr)) != 0)\n        goto done;\n\n    nbyte = sizeof(Elf_Phdr) * ehdr-&gt;e_phnum;\n    phdr = kmalloc(nbyte, M_TEMP, M_WAITOK);\n\n    /* Read program headers */\n    if ((error = elf_getphdrs(fp, phdr, nbyte)) != 0)\n        goto done;\n\n    /* Restore register state from notes section */\n    if ((error = elf_getnotes(lp, fp, phdr-&gt;p_filesz)) != 0)\n        goto done;\n\n    /* Restore text segment mappings */\n    if ((error = elf_gettextvp(p, fp)) != 0)\n        goto done;\n\n    /* Restore signal handlers and masks */\n    if ((error = elf_getsigs(lp, fp)) != 0)\n        goto done;\n\n    /* Restore file descriptors */\n    if ((error = elf_getfiles(lp, fp)) != 0)\n        goto done;\n\n    /* Map memory segments from checkpoint file */\n    error = elf_loadphdrs(fp, phdr, ehdr-&gt;e_phnum);\n\n    /* Mark process as checkpoint-restored to handle re-checkpointing */\n    if (error == 0 &amp;&amp; fp-&gt;f_data &amp;&amp; fp-&gt;f_type == DTYPE_VNODE) {\n        if (p-&gt;p_textvp)\n            vrele(p-&gt;p_textvp);\n        p-&gt;p_textvp = (struct vnode *)fp-&gt;f_data;\n        vsetflags(p-&gt;p_textvp, VCKPT);\n        vref(p-&gt;p_textvp);\n    }\ndone:\n    if (ehdr)\n        kfree(ehdr, M_TEMP);\n    if (phdr)\n        kfree(phdr, M_TEMP);\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#register-state-restoration","title":"Register State Restoration","text":"<p>Source: <code>sys/kern/kern_checkpoint.c:283-311</code></p> <pre><code>static int\nelf_loadnotes(struct lwp *lp, prpsinfo_t *psinfo, prstatus_t *status,\n           prfpregset_t *fpregset)\n{\n    struct proc *p = lp-&gt;lwp_proc;\n    int error;\n\n    /* Validate note structures */\n    if (status-&gt;pr_version != PRSTATUS_VERSION ||\n        status-&gt;pr_statussz != sizeof(prstatus_t) ||\n        status-&gt;pr_gregsetsz != sizeof(gregset_t) ||\n        status-&gt;pr_fpregsetsz != sizeof(fpregset_t) ||\n        psinfo-&gt;pr_version != PRPSINFO_VERSION ||\n        psinfo-&gt;pr_psinfosz != sizeof(prpsinfo_t)) {\n        return EINVAL;\n    }\n\n    /* Restore general-purpose registers */\n    if ((error = set_regs(lp, &amp;status-&gt;pr_reg)) != 0)\n        return error;\n\n    /* Restore floating-point registers */\n    error = set_fpregs(lp, fpregset);\n\n    /* Restore process name */\n    strlcpy(p-&gt;p_comm, psinfo-&gt;pr_fname, sizeof(p-&gt;p_comm));\n\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#file-descriptor-restoration","title":"File Descriptor Restoration","text":"<p>Source: <code>sys/kern/kern_checkpoint.c:581-682</code></p> <p>The file restoration process:</p> <ol> <li>Closes all file descriptors &gt;= 3 (inherited from <code>checkpt</code> utility)</li> <li>Iterates through saved file descriptors</li> <li>Uses file handles (<code>fhandle_t</code>) to locate vnodes via VFS</li> <li>Reopens files with saved flags and offsets</li> <li>Special handling for checkpoint file descriptor itself</li> </ol> <pre><code>/* If this FD is the checkpoint file, reuse current fp */\nif (cfi-&gt;cfi_ckflags &amp; CKFIF_ISCKPTFD) {\n    fhold(fp);\n    tempfp = fp;\n    error = 0;\n} else {\n    /* Convert file handle to vnode and open */\n    error = ckpt_fhtovp(&amp;cfi-&gt;cfi_fh, &amp;vp);\n    if (error == 0) {\n        error = fp_vpopen(vp, OFLAGS(cfi-&gt;cfi_flags), &amp;tempfp);\n        if (error)\n            vput(vp);\n    }\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#memory-segment-restoration","title":"Memory Segment Restoration","text":"<p>Memory segments are mapped directly from the checkpoint file:</p> <p>Source: <code>sys/kern/kern_checkpoint.c:398-427</code></p> <pre><code>static int\nmmap_phdr(struct file *fp, Elf_Phdr *phdr)\n{\n    int error;\n    size_t len;\n    int prot;\n    void *addr;\n    int flags;\n    off_t pos;\n\n    pos = phdr-&gt;p_offset;\n    len = phdr-&gt;p_filesz;\n    addr = (void *)phdr-&gt;p_vaddr;\n    flags = MAP_FIXED | MAP_NOSYNC | MAP_PRIVATE;\n\n    prot = 0;\n    if (phdr-&gt;p_flags &amp; PF_R)\n        prot |= PROT_READ;\n    if (phdr-&gt;p_flags &amp; PF_W)\n        prot |= PROT_WRITE;\n    if (phdr-&gt;p_flags &amp; PF_X)\n        prot |= PROT_EXEC;\n\n    error = fp_mmap(addr, len, prot, flags, fp, pos, &amp;addr);\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#configuration","title":"Configuration","text":""},{"location":"sys/kern/checkpoint/#sysctl-variables","title":"Sysctl Variables","text":"Sysctl Default Description <code>kern.ckptgroup</code> 0 (wheel) Group allowed to checkpoint (-1 = any) <code>kern.ckptfile</code> <code>%N.ckpt</code> Checkpoint filename template"},{"location":"sys/kern/checkpoint/#filename-template","title":"Filename Template","text":"<p>The checkpoint filename supports format specifiers:</p> Specifier Expansion <code>%N</code> Process name (comm) <code>%P</code> Process ID <code>%U</code> User ID <code>%%</code> Literal <code>%</code> <p>Examples: <pre><code># Default: creates \"programname.ckpt\" in current directory\nsysctl kern.ckptfile=\"%N.ckpt\"\n\n# Store by user and process name\nsysctl kern.ckptfile=\"/var/checkpoints/%U/%N-%P.ckpt\"\n\n# Store all checkpoints centrally\nsysctl kern.ckptfile=\"/cores/%N.ckpt\"\n</code></pre></p>"},{"location":"sys/kern/checkpoint/#userland-interface","title":"Userland Interface","text":""},{"location":"sys/kern/checkpoint/#checkpt-utility","title":"checkpt Utility","text":"<p>The <code>checkpt(1)</code> utility restores checkpointed processes:</p> <pre><code># Restore a checkpoint\ncheckpt -r myprogram.ckpt\n</code></pre> <p>Source: <code>usr.bin/checkpt/checkpt.c</code></p> <pre><code>int main(int ac, char **av)\n{\n    int fd;\n    int error;\n    const char *filename = NULL;\n\n    /* Parse arguments */\n    while ((ch = getopt(ac, av, \"r:\")) != -1) {\n        switch(ch) {\n        case 'r':\n            filename = optarg;\n            break;\n        }\n    }\n\n    fd = open(filename, O_RDONLY);\n    if (fd &lt; 0) {\n        fprintf(stderr, \"unable to open %s\\n\", filename);\n        exit(1);\n    }\n\n    /* Restore process - on success, does not return */\n    error = sys_checkpoint(CKPT_THAW, fd, -1, 1);\n\n    /* Only reach here on error */\n    fprintf(stderr, \"thaw failed error %d %s\\n\", errno, strerror(errno));\n    return(5);\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#creating-checkpoints","title":"Creating Checkpoints","text":"<p>Programs can be checkpointed in several ways:</p> <ol> <li>Keyboard: Press Ctrl+E (configurable via <code>stty</code>)</li> <li>Signal: Send <code>SIGCKPT</code> or <code>SIGCKPTEXIT</code></li> <li>Programmatic: Call <code>sys_checkpoint(CKPT_FREEZE, fd, -1, 0)</code></li> </ol>"},{"location":"sys/kern/checkpoint/#application-aware-checkpointing","title":"Application-Aware Checkpointing","text":"<p>Programs can actively support checkpointing:</p> <pre><code>#include &lt;sys/checkpoint.h&gt;\n#include &lt;signal.h&gt;\n\nvolatile sig_atomic_t checkpoint_requested = 0;\n\nvoid sigckpt_handler(int sig) {\n    checkpoint_requested = 1;\n}\n\nint main() {\n    int fd, result;\n\n    /* Install checkpoint signal handler */\n    signal(SIGCKPT, sigckpt_handler);\n\n    while (1) {\n        /* Application work... */\n\n        if (checkpoint_requested) {\n            checkpoint_requested = 0;\n\n            /* Clean up transient state */\n            close_network_connections();\n            flush_caches();\n\n            /* Create checkpoint */\n            fd = open(\"myapp.ckpt\", O_WRONLY|O_CREAT|O_TRUNC, 0600);\n            result = sys_checkpoint(CKPT_FREEZE, fd, -1, 42);\n            close(fd);\n\n            if (result == 42) {\n                /* We were just restored */\n                reopen_network_connections();\n                printf(\"Resumed from checkpoint\\n\");\n            } else {\n                /* Checkpoint created, continue running */\n                printf(\"Checkpoint created\\n\");\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#limitations","title":"Limitations","text":""},{"location":"sys/kern/checkpoint/#what-can-be-checkpointed","title":"What Can Be Checkpointed","text":"<ul> <li>Regular processes with normal file descriptors</li> <li>Memory-mapped regular files</li> <li>Signal handlers and masks</li> <li>CPU register state (general and FP)</li> <li>Process credentials and identity</li> </ul>"},{"location":"sys/kern/checkpoint/#what-cannot-be-checkpointed","title":"What Cannot Be Checkpointed","text":"Resource Reason Network sockets Connection state is external Pipes No persistent backing Device files Hardware state cannot be saved Shared memory Cross-process state issues Semaphores/mutexes Synchronization state lost Setuid/setgid programs Security restriction"},{"location":"sys/kern/checkpoint/#file-system-requirements","title":"File System Requirements","text":"<ul> <li>Files must be accessible by file handle after restore</li> <li>File system must support <code>VFS_FHTOVP</code> operation</li> <li>Files should not be modified between checkpoint and restore</li> </ul>"},{"location":"sys/kern/checkpoint/#thread-limitations","title":"Thread Limitations","text":"<p>The current implementation has limited multi-thread support:</p> <pre><code>#define CKPT_MAXTHREADS 256\n\n/* Thread state is saved, but restoration may be incomplete */\nnthreads = (notesz - sizeof(prpsinfo_t)) /\n           (sizeof(prstatus_t) + sizeof(prfpregset_t));\n</code></pre>"},{"location":"sys/kern/checkpoint/#security-considerations","title":"Security Considerations","text":""},{"location":"sys/kern/checkpoint/#group-restriction","title":"Group Restriction","text":"<p>By default, only the wheel group can checkpoint:</p> <pre><code>static int ckptgroup = 0;  /* wheel only */\nSYSCTL_INT(_kern, OID_AUTO, ckptgroup, CTLFLAG_RW, &amp;ckptgroup, 0, \"\");\n\n/* In sys_sys_checkpoint(): */\nif (ckptgroup &gt;= 0 &amp;&amp; groupmember(ckptgroup, td-&gt;td_ucred) == 0)\n    return (EPERM);\n</code></pre>"},{"location":"sys/kern/checkpoint/#setuidsetgid-protection","title":"Setuid/Setgid Protection","text":"<p>Checkpointing privileged processes is prevented:</p> <pre><code>if (sugid_coredump == 0 &amp;&amp; (p-&gt;p_flags &amp; P_SUGID)) {\n    return (EPERM);\n}\n</code></pre>"},{"location":"sys/kern/checkpoint/#checkpoint-file-permissions","title":"Checkpoint File Permissions","text":"<p>Checkpoint files are created with restrictive permissions:</p> <pre><code>error = fp_open(buf, O_WRONLY|O_CREAT|O_TRUNC|O_NOFOLLOW, 0600, &amp;fp);\n</code></pre>"},{"location":"sys/kern/checkpoint/#re-checkpointing","title":"Re-Checkpointing","text":"<p>A checkpoint-restored process can be checkpointed again. Special handling ensures the new checkpoint contains actual memory contents rather than references to the old checkpoint file:</p> <pre><code>/* Mark the vnode so future checkpoints copy data instead of recording\n * vnode references to the checkpoint file */\nif (error == 0 &amp;&amp; fp-&gt;f_data &amp;&amp; fp-&gt;f_type == DTYPE_VNODE) {\n    p-&gt;p_textvp = (struct vnode *)fp-&gt;f_data;\n    vsetflags(p-&gt;p_textvp, VCKPT);  /* Mark as checkpoint source */\n    vref(p-&gt;p_textvp);\n}\n</code></pre> <p>When checkpointing a restored process, the old checkpoint file is removed first to avoid corrupting the running process's memory mappings:</p> <pre><code>/* Remove previous checkpoint before creating new one */\nerror = nlookup_init(&amp;nd, buf, UIO_SYSSPACE, 0);\nif (error == 0)\n    error = kern_unlink(&amp;nd);\nnlookup_done(&amp;nd);\n</code></pre>"},{"location":"sys/kern/checkpoint/#related-documentation","title":"Related Documentation","text":"<ul> <li>Processes and Threads - Process state management</li> <li>Signals - Signal delivery and handling</li> <li>Virtual Memory - Memory mapping operations</li> <li>VFS Operations - File handle operations</li> </ul>"},{"location":"sys/kern/checkpoint/#source-files","title":"Source Files","text":"File Description <code>sys/kern/kern_checkpoint.c</code> Checkpoint/restart implementation <code>sys/sys/checkpoint.h</code> User interface definitions <code>sys/sys/ckpt.h</code> Kernel structures <code>sys/kern/imgact_elf.c</code> ELF core dump generation <code>sys/kern/kern_sig.c</code> Signal handling integration <code>sys/kern/tty.c</code> TTY checkpoint character handling <code>usr.bin/checkpt/checkpt.c</code> Restore utility"},{"location":"sys/kern/devices/","title":"Device Framework","text":"<p>The device framework provides the infrastructure for creating, managing, and operating on character and block devices in DragonFly BSD. It consists of two main components: device number management (<code>kern_conf.c</code>) and device operations dispatch (<code>kern_device.c</code>).</p> <p>Source files: - <code>sys/kern/kern_conf.c</code> - Device creation, destruction, aliases - <code>sys/kern/kern_device.c</code> - Device operations dispatch layer - <code>sys/sys/device.h</code> - Data structures and prototypes - <code>sys/sys/conf.h</code> - Device type definitions</p>"},{"location":"sys/kern/devices/#overview","title":"Overview","text":"<p>DragonFly's device framework evolved from traditional BSD but incorporates several DragonFly-specific enhancements:</p> <ol> <li>DEVFS Integration - All device creation goes through devfs</li> <li>MPSAFE Support - Per-device MPSAFE flags control locking</li> <li>KVABIO Support - Efficient buffer handling for capable devices</li> <li>Reference Counting - Sysref-based lifecycle management</li> <li>Operation Interception - Console and layered device support</li> </ol>"},{"location":"sys/kern/devices/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/devices/#struct-dev_ops","title":"struct dev_ops","text":"<p>The device operations switch table. Each device driver provides one of these to define how the device responds to operations.</p> <pre><code>struct dev_ops {\n    struct {\n        const char  *name;   /* base name, e.g. 'da' */\n        int          maj;    /* major device number */\n        u_int        flags;  /* D_XXX flags */\n        void        *data;   /* custom driver data */\n        int          refs;   /* ref count */\n        int          id;\n    } head;\n\n    d_default_t     *d_default;\n    d_open_t        *d_open;\n    d_close_t       *d_close;\n    d_read_t        *d_read;\n    d_write_t       *d_write;\n    d_ioctl_t       *d_ioctl;\n    d_mmap_t        *d_mmap;\n    d_mmap_single_t *d_mmap_single;\n    d_strategy_t    *d_strategy;\n    d_dump_t        *d_dump;\n    d_psize_t       *d_psize;\n    d_kqfilter_t    *d_kqfilter;\n    d_clone_t       *d_clone;\n    d_revoke_t      *d_revoke;\n    int (*d_uksmap)(...);\n};\n</code></pre> <p>Defined in <code>sys/sys/device.h:229-257</code>.</p>"},{"location":"sys/kern/devices/#device-flags-headflags","title":"Device Flags (head.flags)","text":"<p>Type flags (mutually exclusive):</p> Flag Value Description <code>D_TAPE</code> 0x0001 Tape device <code>D_DISK</code> 0x0002 Disk device <code>D_TTY</code> 0x0004 Terminal device <code>D_MEM</code> 0x0008 Memory device <p>Behavior flags:</p> Flag Value Description <code>D_MEMDISK</code> 0x00010000 Memory-type disk <code>D_CANFREE</code> 0x00040000 Supports TRIM/free blocks <code>D_TRACKCLOSE</code> 0x00080000 Track all close calls <code>D_MASTER</code> 0x00100000 Used by pty/tty code <code>D_NOEMERGPGR</code> 0x00200000 Skip in emergency pager <code>D_MPSAFE</code> 0x00400000 All operations are MPSAFE <code>D_KVABIO</code> 0x00800000 Supports KVABIO API <code>D_QUICK</code> 0x01000000 No fancy open/close needed <p>Defined in <code>sys/sys/device.h:263-287</code>.</p>"},{"location":"sys/kern/devices/#cdev_t","title":"cdev_t","text":"<p>Opaque pointer to a device structure. The actual structure (<code>struct cdev</code>) is managed by devfs and contains:</p> <ul> <li><code>si_ops</code> - Pointer to <code>struct dev_ops</code></li> <li><code>si_umajor</code>, <code>si_uminor</code> - Major/minor numbers</li> <li><code>si_name</code> - Device name</li> <li><code>si_uid</code>, <code>si_gid</code>, <code>si_perms</code> - Ownership and permissions</li> <li><code>si_sysref</code> - Reference count</li> <li><code>si_track_read</code>, <code>si_track_write</code> - BIO tracking</li> <li><code>si_lastread</code>, <code>si_lastwrite</code> - Access timestamps</li> </ul>"},{"location":"sys/kern/devices/#operation-argument-structures","title":"Operation Argument Structures","text":"<p>Each device operation receives a typed argument structure derived from <code>struct dev_generic_args</code>:</p> <pre><code>struct dev_generic_args {\n    struct syslink_desc *a_desc;  /* operation descriptor */\n    struct cdev *a_dev;           /* device pointer */\n};\n</code></pre> <p>Common argument structures:</p> Structure Operation Key Fields <code>dev_open_args</code> <code>d_open</code> <code>a_oflags</code>, <code>a_devtype</code>, <code>a_cred</code>, <code>a_fpp</code> <code>dev_close_args</code> <code>d_close</code> <code>a_fflag</code>, <code>a_devtype</code>, <code>a_fp</code> <code>dev_read_args</code> <code>d_read</code> <code>a_uio</code>, <code>a_ioflag</code>, <code>a_fp</code> <code>dev_write_args</code> <code>d_write</code> <code>a_uio</code>, <code>a_ioflag</code>, <code>a_fp</code> <code>dev_ioctl_args</code> <code>d_ioctl</code> <code>a_cmd</code>, <code>a_data</code>, <code>a_fflag</code>, <code>a_cred</code> <code>dev_strategy_args</code> <code>d_strategy</code> <code>a_bio</code> <code>dev_mmap_args</code> <code>d_mmap</code> <code>a_offset</code>, <code>a_nprot</code>, <code>a_result</code> <code>dev_dump_args</code> <code>d_dump</code> <code>a_virtual</code>, <code>a_physical</code>, <code>a_offset</code>, <code>a_length</code> <code>dev_psize_args</code> <code>d_psize</code> <code>a_result</code> <p>Defined in <code>sys/sys/device.h:59-204</code>.</p>"},{"location":"sys/kern/devices/#device-creation","title":"Device Creation","text":""},{"location":"sys/kern/devices/#make_dev","title":"make_dev","text":"<p>Creates a device node visible in <code>/dev</code>:</p> <pre><code>cdev_t make_dev(struct dev_ops *ops, int minor,\n                uid_t uid, gid_t gid, int perms,\n                const char *fmt, ...);\n</code></pre> <p>Operation: 1. Call <code>compile_dev_ops()</code> to fill in NULL handlers 2. Create device via <code>devfs_new_cdev()</code> 3. Format device name from <code>fmt</code> and varargs 4. Create devfs entry via <code>devfs_create_dev()</code> 5. Return unreferenced device pointer</p> <p>The returned <code>cdev_t</code> is an ad-hoc reference. Callers who store it long-term must call <code>reference_dev()</code>.</p> <p>See <code>sys/kern/kern_conf.c:188-211</code>.</p>"},{"location":"sys/kern/devices/#make_dev_covering","title":"make_dev_covering","text":"<p>Creates a device that layers over another device:</p> <pre><code>cdev_t make_dev_covering(struct dev_ops *ops, struct dev_ops *bops,\n                         int minor, uid_t uid, gid_t gid, int perms,\n                         const char *fmt, ...);\n</code></pre> <p>Used by disk label code to create partition devices that cover the base disk device.</p> <p>See <code>sys/kern/kern_conf.c:218-241</code>.</p>"},{"location":"sys/kern/devices/#make_only_dev","title":"make_only_dev","text":"<p>Creates a device without a devfs entry (internal use):</p> <pre><code>cdev_t make_only_dev(struct dev_ops *ops, int minor,\n                     uid_t uid, gid_t gid, int perms,\n                     const char *fmt, ...);\n</code></pre> <p>Unlike <code>make_dev()</code>, this returns a referenced device.</p> <p>See <code>sys/kern/kern_conf.c:270-296</code>.</p>"},{"location":"sys/kern/devices/#make_autoclone_dev","title":"make_autoclone_dev","text":"<p>Creates an auto-cloning device:</p> <pre><code>cdev_t make_autoclone_dev(struct dev_ops *ops, struct devfs_bitmap *bitmap,\n                          d_clone_t *nhandler, uid_t uid, gid_t gid,\n                          int perms, const char *fmt, ...);\n</code></pre> <p>Operation: 1. Initialize clone bitmap (if provided) 2. Register clone handler with devfs 3. Create base device covering <code>default_dev_ops</code> 4. Clone handler called on-demand for new instances</p> <p>Used for devices like <code>/dev/pty*</code> that create instances dynamically.</p> <p>See <code>sys/kern/kern_conf.c:407-427</code>.</p>"},{"location":"sys/kern/devices/#device-destruction","title":"Device Destruction","text":""},{"location":"sys/kern/devices/#destroy_dev","title":"destroy_dev","text":"<p>Destroys a device and revectors its ops to <code>dead_dev_ops</code>:</p> <pre><code>void destroy_dev(cdev_t dev);\n</code></pre> <p>Important: The caller must hold a reference to the device. The ad-hoc reference from <code>make_dev()</code> is not sufficient:</p> <pre><code>/* Wrong: */\ndestroy_dev(make_dev(...));\n\n/* Correct: */\ncdev_t dev = make_dev(...);\nreference_dev(dev);\n/* ... use device ... */\ndestroy_dev(dev);  /* releases caller's reference + ad-hoc reference */\n</code></pre> <p>See <code>sys/kern/kern_conf.c:345-354</code>.</p>"},{"location":"sys/kern/devices/#sync_devs","title":"sync_devs","text":"<p>Synchronizes asynchronous disk and devfs operations:</p> <pre><code>void sync_devs(void);\n</code></pre> <p>Called before mountroot and on module unload to ensure all devices are fully probed and ops structures dereferenced.</p> <p>See <code>sys/kern/kern_conf.c:364-371</code>.</p>"},{"location":"sys/kern/devices/#device-aliases","title":"Device Aliases","text":""},{"location":"sys/kern/devices/#make_dev_alias","title":"make_dev_alias","text":"<p>Creates a symbolic alias for an existing device:</p> <pre><code>int make_dev_alias(cdev_t target, const char *fmt, ...);\n</code></pre> <p>See <code>sys/kern/kern_conf.c:373-387</code>.</p>"},{"location":"sys/kern/devices/#destroy_dev_alias","title":"destroy_dev_alias","text":"<p>Removes a device alias:</p> <pre><code>int destroy_dev_alias(cdev_t target, const char *fmt, ...);\n</code></pre> <p>See <code>sys/kern/kern_conf.c:389-403</code>.</p>"},{"location":"sys/kern/devices/#reference-counting","title":"Reference Counting","text":"<p>Devices use sysref-based reference counting:</p>"},{"location":"sys/kern/devices/#reference_dev","title":"reference_dev","text":"<p>Adds a reference to a device:</p> <pre><code>cdev_t reference_dev(cdev_t dev);\n</code></pre> <p>Returns the device pointer for convenience. Callers storing device pointers long-term should call this to prevent premature destruction.</p> <p>See <code>sys/kern/kern_conf.c:453-467</code>.</p>"},{"location":"sys/kern/devices/#release_dev","title":"release_dev","text":"<p>Releases a device reference:</p> <pre><code>void release_dev(cdev_t dev);\n</code></pre> <p>The device is freed when the last reference is released.</p> <p>See <code>sys/kern/kern_conf.c:476-484</code>.</p>"},{"location":"sys/kern/devices/#device-operations-dispatch","title":"Device Operations Dispatch","text":"<p>The <code>dev_d*()</code> functions in <code>kern_device.c</code> dispatch operations to drivers while handling MPSAFE and KVABIO requirements.</p>"},{"location":"sys/kern/devices/#mpsafe-handling","title":"MPSAFE Handling","text":"<p>Each dispatch function checks the <code>D_MPSAFE</code> flag:</p> <pre><code>static __inline int\ndev_needmplock(cdev_t dev)\n{\n    return ((dev-&gt;si_ops-&gt;head.flags &amp; D_MPSAFE) == 0);\n}\n</code></pre> <p>If the device is not MPSAFE, the dispatch function acquires/releases the big kernel lock:</p> <pre><code>if (needmplock)\n    get_mplock();\nerror = dev-&gt;si_ops-&gt;d_open(&amp;ap);\nif (needmplock)\n    rel_mplock();\n</code></pre> <p>See <code>sys/kern/kern_device.c:117-122</code>.</p>"},{"location":"sys/kern/devices/#kvabio-handling","title":"KVABIO Handling","text":"<p>For strategy operations, if the device doesn't support KVABIO but the buffer uses it, data is synchronized to all CPUs:</p> <pre><code>if (dev_nokvabio(dev) &amp;&amp; (bp-&gt;b_flags &amp; B_KVABIO))\n    bkvasync_all(bp);\n</code></pre> <p>See <code>sys/kern/kern_device.c:366-367</code>.</p>"},{"location":"sys/kern/devices/#dev_dopen","title":"dev_dopen","text":"<p>Opens a device:</p> <pre><code>int dev_dopen(cdev_t dev, int oflags, int devtype,\n              struct ucred *cred, struct file **fpp, struct vnode *vp);\n</code></pre> <p>The <code>fpp</code> parameter allows the driver to replace the file pointer during open (used by some devices for per-open state).</p> <p>See <code>sys/kern/kern_device.c:137-168</code>.</p>"},{"location":"sys/kern/devices/#dev_dstrategy","title":"dev_dstrategy","text":"<p>Issues I/O to a device:</p> <pre><code>void dev_dstrategy(cdev_t dev, struct bio *bio);\n</code></pre> <p>Operation: 1. Handle KVABIO synchronization if needed 2. Select read or write tracking based on <code>bio-&gt;bio_buf-&gt;b_cmd</code> 3. Reference the appropriate <code>bio_track</code> 4. Call <code>dsched_buf_enter()</code> for disk scheduling 5. Dispatch to driver's <code>d_strategy</code></p> <p>The BIO tracking allows <code>sync_devs()</code> to wait for outstanding I/O.</p> <p>See <code>sys/kern/kern_device.c:354-389</code>.</p>"},{"location":"sys/kern/devices/#dev_dstrategy_chain","title":"dev_dstrategy_chain","text":"<p>Chained strategy call (reuses existing BIO setup):</p> <pre><code>void dev_dstrategy_chain(cdev_t dev, struct bio *bio);\n</code></pre> <p>Used when forwarding I/O through device layers. Unlike <code>dev_dstrategy()</code>, it doesn't add new tracking.</p> <p>See <code>sys/kern/kern_device.c:391-416</code>.</p>"},{"location":"sys/kern/devices/#dev_dpsize","title":"dev_dpsize","text":"<p>Gets device/partition size:</p> <pre><code>int64_t dev_dpsize(cdev_t dev);\n</code></pre> <p>Returns the size in device blocks, or -1 on error.</p> <p>See <code>sys/kern/kern_device.c:448-467</code>.</p>"},{"location":"sys/kern/devices/#operation-compilation","title":"Operation Compilation","text":""},{"location":"sys/kern/devices/#compile_dev_ops","title":"compile_dev_ops","text":"<p>Fills in NULL operation pointers with defaults:</p> <pre><code>void compile_dev_ops(struct dev_ops *ops);\n</code></pre> <p>For each NULL function pointer: - If <code>d_default</code> is set, use that - Otherwise, use the corresponding function from <code>default_dev_ops</code></p> <p>Called automatically by <code>make_dev()</code> and related functions.</p> <p>See <code>sys/kern/kern_device.c:588-606</code>.</p>"},{"location":"sys/kern/devices/#default_dev_ops","title":"default_dev_ops","text":"<p>Default operations that return <code>ENODEV</code> for most calls:</p> <pre><code>struct dev_ops default_dev_ops = {\n    { \"null\" },\n    .d_default = NULL,\n    .d_open = noopen,      /* returns ENODEV */\n    .d_close = noclose,    /* returns ENODEV */\n    .d_read = noread,      /* returns ENODEV */\n    ...\n};\n</code></pre> <p>See <code>sys/kern/kern_device.c:99-115</code>.</p>"},{"location":"sys/kern/devices/#dead_dev_ops","title":"dead_dev_ops","text":"<p>Operations for destroyed devices. When a device is destroyed, its <code>si_ops</code> is revectored to point here.</p> <p>See <code>sys/kern/kern_device.c:83</code>.</p>"},{"location":"sys/kern/devices/#operation-interception","title":"Operation Interception","text":""},{"location":"sys/kern/devices/#dev_ops_intercept","title":"dev_ops_intercept","text":"<p>Intercepts device operations (used by console code):</p> <pre><code>struct dev_ops *dev_ops_intercept(cdev_t dev, struct dev_ops *iops);\n</code></pre> <p>Operation: 1. Save original ops 2. Copy major, data, and flags to interceptor ops 3. Replace device's ops with interceptor 4. Set <code>SI_INTERCEPTED</code> flag 5. Return original ops</p> <p>See <code>sys/kern/kern_device.c:654-667</code>.</p>"},{"location":"sys/kern/devices/#dev_ops_restore","title":"dev_ops_restore","text":"<p>Restores original operations after interception:</p> <pre><code>void dev_ops_restore(cdev_t dev, struct dev_ops *oops);\n</code></pre> <p>See <code>sys/kern/kern_device.c:669-679</code>.</p>"},{"location":"sys/kern/devices/#major-number-management","title":"Major Number Management","text":"<p>Major numbers are tracked in a red-black tree for efficient lookup:</p> <pre><code>struct dev_ops_rb_tree dev_ops_rbhead;\n</code></pre> <p>The tree maps major numbers to <code>struct dev_ops_maj</code> entries, which link to the associated <code>dev_ops</code> structures.</p> <p>See <code>sys/kern/kern_device.c:638-640</code>.</p>"},{"location":"sys/kern/devices/#device-number-primitives","title":"Device Number Primitives","text":""},{"location":"sys/kern/devices/#major-minor","title":"major / minor","text":"<p>Extract major/minor numbers from a device:</p> <pre><code>int major(cdev_t dev);\nint minor(cdev_t dev);\n</code></pre> <p>Note: Major number comes from <code>si_umajor</code>, not <code>si_ops</code>, because <code>si_ops</code> may be replaced when a device is destroyed.</p> <p>See <code>sys/kern/kern_conf.c:61-75</code>.</p>"},{"location":"sys/kern/devices/#devid_from_dev-dev_from_devid","title":"devid_from_dev / dev_from_devid","text":"<p>Convert between <code>cdev_t</code> and old-style <code>dev_t</code>:</p> <pre><code>dev_t devid_from_dev(cdev_t dev);\ncdev_t dev_from_devid(dev_t x, int b);\n</code></pre> <p>The <code>dev_from_devid()</code> function looks up the device through devfs.</p> <p>See <code>sys/kern/kern_conf.c:99-124</code>.</p>"},{"location":"sys/kern/devices/#dev_is_good","title":"dev_is_good","text":"<p>Check if a device is valid (not dead):</p> <pre><code>int dev_is_good(cdev_t dev);\n</code></pre> <p>Returns 1 if the device exists and its ops is not <code>dead_dev_ops</code>.</p> <p>See <code>sys/kern/kern_conf.c:126-132</code>.</p>"},{"location":"sys/kern/devices/#helper-functions","title":"Helper Functions","text":""},{"location":"sys/kern/devices/#devtoname","title":"devtoname","text":"<p>Gets the device name:</p> <pre><code>const char *devtoname(cdev_t dev);\n</code></pre> <p>Returns the device name, or constructs one from major/minor if no name is set.</p> <p>See <code>sys/kern/kern_conf.c:486-512</code>.</p>"},{"location":"sys/kern/devices/#dev_dname-dev_dflags-dev_dmaj","title":"dev_dname / dev_dflags / dev_dmaj","text":"<p>Quick accessors for device properties:</p> <pre><code>const char *dev_dname(cdev_t dev);   /* ops-&gt;head.name */\nint dev_dflags(cdev_t dev);          /* ops-&gt;head.flags */\nint dev_dmaj(cdev_t dev);            /* ops-&gt;head.maj */\n</code></pre> <p>See <code>sys/kern/kern_device.c:515-537</code>.</p>"},{"location":"sys/kern/devices/#dev_drefs","title":"dev_drefs","text":"<p>Gets the reference count:</p> <pre><code>int dev_drefs(cdev_t dev);\n</code></pre> <p>See <code>sys/kern/kern_device.c:506-510</code>.</p>"},{"location":"sys/kern/devices/#debugging","title":"Debugging","text":""},{"location":"sys/kern/devices/#debugdev_refs","title":"debug.dev_refs","text":"<p>Sysctl to enable device reference debugging:</p> <pre><code>sysctl debug.dev_refs=2\n</code></pre> <p>When set to 2, prints reference/release messages with device name and reference count.</p> <p>See <code>sys/kern/kern_conf.c:52-54</code>.</p>"},{"location":"sys/kern/devices/#example-simple-character-device","title":"Example: Simple Character Device","text":"<pre><code>static d_open_t     mydev_open;\nstatic d_close_t    mydev_close;\nstatic d_read_t     mydev_read;\nstatic d_write_t    mydev_write;\n\nstatic struct dev_ops mydev_ops = {\n    { \"mydev\", 0, D_MPSAFE },\n    .d_open =   mydev_open,\n    .d_close =  mydev_close,\n    .d_read =   mydev_read,\n    .d_write =  mydev_write,\n};\n\nstatic int\nmydev_open(struct dev_open_args *ap)\n{\n    /* ap-&gt;a_head.a_dev is the device */\n    /* ap-&gt;a_oflags has open flags */\n    /* ap-&gt;a_cred has credentials */\n    return 0;\n}\n\nstatic int\nmydev_read(struct dev_read_args *ap)\n{\n    return uiomove(data, len, ap-&gt;a_uio);\n}\n\n/* Module init */\nstatic int\nmydev_modevent(module_t mod, int type, void *data)\n{\n    static cdev_t dev;\n\n    switch (type) {\n    case MOD_LOAD:\n        dev = make_dev(&amp;mydev_ops, 0, UID_ROOT, GID_WHEEL,\n                       0600, \"mydev\");\n        reference_dev(dev);\n        break;\n    case MOD_UNLOAD:\n        destroy_dev(dev);\n        break;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/devices/#cross-references","title":"Cross-References","text":"<ul> <li>NewBus Framework - Device/driver attachment model</li> <li>Disk Layer - Block device and partition handling</li> <li>Buffer Cache - BIO and buffer management</li> <li>LWKT - Threading and MPSAFE concepts</li> </ul>"},{"location":"sys/kern/disk/","title":"Disk Subsystem","text":"<p>The disk subsystem provides a unified framework for managing block storage devices, including disk registration, slice/partition handling, and I/O routing. It bridges device drivers with the filesystem layer through a multi-level abstraction: whole disk, slices (MBR/GPT partitions), and BSD partitions (within slices).</p> <p>Source files: - <code>sys/kern/subr_disk.c</code> - Core disk management (~1,600 lines) - <code>sys/kern/subr_diskslice.c</code> - Slice management (~900 lines) - <code>sys/kern/subr_diskmbr.c</code> - MBR parsing (~560 lines) - <code>sys/kern/subr_diskgpt.c</code> - GPT parsing (~240 lines) - <code>sys/kern/subr_disklabel32.c</code> - BSD 32-bit disklabel (~660 lines) - <code>sys/kern/subr_disklabel64.c</code> - BSD 64-bit disklabel (~540 lines)</p> <p>Header files: - <code>sys/sys/disk.h</code> - Disk structures - <code>sys/sys/diskslice.h</code> - Slice structures and helpers</p>"},{"location":"sys/kern/disk/#architecture-overview","title":"Architecture Overview","text":"<p>The disk subsystem implements a three-level hierarchy:</p> <pre><code>flowchart TD\n    WD[\"Whole DiskWHOLE_DISK_SLICE(e.g., da0)\"]\n    WD --&gt; S0[\"Slice 0(s0/compat)\"]\n    WD --&gt; S1[\"Slice 1(s1)\"]\n    WD --&gt; SN[\"Slice N(sN)\"]\n    S0 --&gt; P0[\"Partitions(a-p)\"]\n    S1 --&gt; P1[\"Partitions(a-p)\"]\n</code></pre>"},{"location":"sys/kern/disk/#device-naming","title":"Device Naming","text":"<p>Disk devices follow a hierarchical naming scheme:</p> Device Description <code>da0</code> Whole disk (raw, no interpretation) <code>da0s0</code> Compatibility slice (MBR) or GPT s0 <code>da0s1</code> First slice (MBR partition 1) <code>da0s1a</code> Partition 'a' within slice 1 <code>da0s1c</code> Raw slice 1 (whole slice)"},{"location":"sys/kern/disk/#minor-number-encoding","title":"Minor Number Encoding","text":"<p>Minor numbers encode unit, slice, and partition:</p> <pre><code>/* sys/sys/diskslice.h:240-256 */\nstatic __inline u_int32_t\ndkmakeminor(u_int32_t unit, u_int32_t slice, u_int32_t part)\n{\n    u_int32_t val;\n    val = ((unit &amp; 0x001f) &lt;&lt; 3) | ((unit &amp; 0x01e0) &lt;&lt; 16) |\n          ((slice &amp; 0x000f) &lt;&lt; 16) | ((slice &amp; 0x0070) &lt;&lt; 25) |\n          (part &amp; 0x0007) | ((part &amp; 0x0008) &lt;&lt; 17) |\n          ((part &amp; 0x00F0) &lt;&lt; 21);\n    return(val);\n}\n</code></pre> <p>Bit layout (32 bits): <pre><code>| SL2 | PART3 |UNIT_2 |P| SLICE |  MAJOR?  |  UNIT   |PART |\n</code></pre></p>"},{"location":"sys/kern/disk/#core-data-structures","title":"Core Data Structures","text":""},{"location":"sys/kern/disk/#struct-disk","title":"struct disk","text":"<p>The primary structure representing a disk device:</p> <pre><code>/* sys/sys/disk.h:132-148 */\nstruct disk {\n    struct dev_ops      *d_dev_ops;     /* disk layer dev ops */\n    struct dev_ops      *d_raw_ops;     /* raw device driver ops */\n    u_int               d_flags;\n    int                 d_opencount;    /* current open count */\n    cdev_t              d_rawdev;       /* backing raw device */\n    cdev_t              d_cdev;         /* whole-disk device */\n    struct diskslices   *d_slice;       /* slice table */\n    struct disk_info    d_info;         /* media parameters */\n    const char          *d_disktype;    /* disk type string */\n    LIST_ENTRY(disk)    d_list;         /* global disk list */\n    kdmsg_iocom_t       d_iocom;        /* cluster import/export */\n    int                 d_refs;         /* destruction interlock */\n};\n</code></pre> <p>Flags: - <code>DISKFLAG_LOCK</code> - Disk operations locked - <code>DISKFLAG_WANTED</code> - Someone waiting for lock - <code>DISKFLAG_MARKER</code> - Enumeration marker (not a real disk)</p>"},{"location":"sys/kern/disk/#struct-disk_info","title":"struct disk_info","text":"<p>Media parameters provided by the device driver:</p> <pre><code>/* sys/sys/disk.h:70-99 */\nstruct disk_info {\n    u_int64_t   d_media_size;       /* media size in bytes */\n    u_int64_t   d_media_blocks;     /* media size in sectors */\n    int         d_media_blksize;    /* sector size (bytes) */\n    u_int       d_dsflags;          /* management flags */\n\n    /* Optional geometry (legacy CHS) */\n    u_int       d_type;             /* DTYPE_xxx */\n    u_int       d_nheads;\n    u_int       d_ncylinders;\n    u_int       d_secpertrack;\n    u_int       d_secpercyl;\n    u_int       d_trimflag;\n    char        *d_serialno;        /* serial number string */\n};\n</code></pre> <p>DSO flags (<code>d_dsflags</code>):</p> Flag Value Description <code>DSO_NOLABELS</code> 0x0001 Don't probe for labels <code>DSO_ONESLICE</code> 0x0002 Single slice only <code>DSO_COMPATLABEL</code> 0x0004 Create compatibility label if none <code>DSO_COMPATPARTA</code> 0x0008 Create partition 'a' in compat label <code>DSO_RAWEXTENSIONS</code> 0x0020 Allow raw partition extensions <code>DSO_MBRQUIET</code> 0x0040 Silent MBR probe failures <code>DSO_DEVICEMAPPER</code> 0x0080 Device mapper (use \".\" separator) <code>DSO_RAWPSIZE</code> 0x0100 Use raw device psize on failure"},{"location":"sys/kern/disk/#struct-diskslices","title":"struct diskslices","text":"<p>Container for all slices on a disk:</p> <pre><code>/* sys/sys/diskslice.h:167-177 */\nstruct diskslices {\n    struct cdevsw   *dss_cdevsw;        /* containing device switch */\n    int             dss_first_bsd_slice; /* COMPATIBILITY_SLICE mapped here */\n    u_int           dss_nslices;        /* actual number of slices */\n    u_int           dss_oflags;         /* flags for first open */\n    int             dss_secmult;        /* block-to-sector multiplier */\n    int             dss_secshift;       /* block-to-sector shift (-1 if N/A) */\n    int             dss_secsize;        /* sector size */\n    struct diskslice dss_slices[MAX_SLICES];\n};\n</code></pre>"},{"location":"sys/kern/disk/#struct-diskslice","title":"struct diskslice","text":"<p>Individual slice (MBR partition or GPT entry):</p> <pre><code>/* sys/sys/diskslice.h:142-163 */\nstruct diskslice {\n    cdev_t          ds_dev;             /* device for this slice */\n    u_int64_t       ds_offset;          /* starting sector */\n    u_int64_t       ds_size;            /* number of sectors */\n    u_int32_t       ds_reserved;        /* reserved sectors (label area) */\n    struct uuid     ds_type_uuid;       /* slice type UUID (GPT) */\n    struct uuid     ds_stor_uuid;       /* storage UUID (GPT) */\n    int             ds_type;            /* MBR partition type */\n    int             ds_flags;           /* DSF_ flags */\n    disklabel_t     ds_label;           /* BSD label (if present) */\n    struct disklabel_ops *ds_ops;       /* label operations */\n    void            *ds_devs[MAXPARTITIONS];\n    u_int32_t       ds_openmask[DKMAXPARTITIONS/32];\n    u_char          ds_wlabel;          /* label writable flag */\n    int             ds_ttlopens;        /* total opens */\n};\n</code></pre> <p>Special slices:</p> Constant Value Description <code>COMPATIBILITY_SLICE</code> 0 Compatibility (s0) <code>WHOLE_DISK_SLICE</code> 1 Entire disk <code>BASE_SLICE</code> 2 First real slice (s1) <code>WHOLE_SLICE_PART</code> 255 Entire slice partition"},{"location":"sys/kern/disk/#disk-registration-and-lifecycle","title":"Disk Registration and Lifecycle","text":""},{"location":"sys/kern/disk/#creating-a-disk","title":"Creating a Disk","text":"<p>Device drivers register disks using <code>disk_create()</code>:</p> <pre><code>/* sys/kern/subr_disk.c:664-765 */\ncdev_t\ndisk_create(int unit, struct disk *dp, struct dev_ops *raw_ops)\n{\n    return _disk_create_named(NULL, unit, dp, raw_ops, 0);\n}\n\nstatic cdev_t\n_disk_create_named(const char *name, int unit, struct disk *dp,\n                   struct dev_ops *raw_ops, int clone)\n{\n    cdev_t rawdev;\n    struct dev_ops *dops;\n\n    /* Create raw device (no slice/partition interpretation) */\n    if (name) {\n        rawdev = make_only_dev(raw_ops, dkmakewholedisk(unit),\n                               UID_ROOT, GID_OPERATOR, 0640, \"%s\", name);\n    } else {\n        rawdev = make_only_dev(raw_ops, dkmakewholedisk(unit),\n                               UID_ROOT, GID_OPERATOR, 0640,\n                               \"%s%d\", raw_ops-&gt;head.name, unit);\n    }\n\n    bzero(dp, sizeof(*dp));\n\n    /* Select disk ops based on D_NOEMERGPGR flag */\n    dops = (raw_ops-&gt;head.flags &amp; D_NOEMERGPGR) ? &amp;disk2_ops : &amp;disk1_ops;\n\n    dp-&gt;d_rawdev = rawdev;\n    dp-&gt;d_raw_ops = raw_ops;\n    dp-&gt;d_dev_ops = dops;\n\n    /* Create covering device for slice/partition management */\n    if (clone) {\n        dp-&gt;d_cdev = make_only_dev_covering(...);\n    } else {\n        dp-&gt;d_cdev = make_dev_covering(...);\n    }\n\n    dp-&gt;d_cdev-&gt;si_disk = dp;\n\n    /* Initialize I/O scheduling */\n    dsched_disk_create(dp, name, unit);\n\n    /* Add to global disk list */\n    lwkt_gettoken(&amp;disklist_token);\n    LIST_INSERT_HEAD(&amp;disklist, dp, d_list);\n    lwkt_reltoken(&amp;disklist_token);\n\n    /* Initialize cluster support */\n    disk_iocom_init(dp);\n\n    return dp-&gt;d_rawdev;\n}\n</code></pre>"},{"location":"sys/kern/disk/#setting-disk-information","title":"Setting Disk Information","text":"<p>After creating the disk, drivers provide media parameters:</p> <pre><code>/* sys/kern/subr_disk.c:853-869 */\nvoid\ndisk_setdiskinfo(struct disk *disk, struct disk_info *info)\n{\n    _setdiskinfo(disk, info);\n    disk_msg_send(DISK_DISK_PROBE, disk, NULL);\n}\n\nstatic void\n_setdiskinfo(struct disk *disk, struct disk_info *info)\n{\n    /* Copy info, duplicate serial number */\n    bcopy(info, &amp;disk-&gt;d_info, sizeof(disk-&gt;d_info));\n\n    if (info-&gt;d_serialno &amp;&amp; info-&gt;d_serialno[0]) {\n        info-&gt;d_serialno = kstrdup(info-&gt;d_serialno, M_TEMP);\n        disk_cleanserial(info-&gt;d_serialno);\n        make_dev_alias(disk-&gt;d_cdev, \"serno/%s\", info-&gt;d_serialno);\n    }\n\n    /* Calculate size from blocks or vice versa */\n    if (info-&gt;d_media_size == 0 &amp;&amp; info-&gt;d_media_blocks) {\n        info-&gt;d_media_size = (u_int64_t)info-&gt;d_media_blocks *\n                             info-&gt;d_media_blksize;\n    }\n\n    dsched_disk_update(disk, info);\n}\n</code></pre>"},{"location":"sys/kern/disk/#disk-message-thread","title":"Disk Message Thread","text":"<p>A dedicated kernel thread processes disk operations asynchronously:</p> <pre><code>/* sys/kern/subr_disk.c:500-601 */\nstatic void\ndisk_msg_core(void *arg)\n{\n    struct disk *dp;\n    struct diskslice *sp;\n    disk_msg_t msg;\n\n    lwkt_initport_thread(&amp;disk_msg_port, curthread);\n\n    while (run) {\n        msg = (disk_msg_t)lwkt_waitport(&amp;disk_msg_port, 0);\n\n        switch (msg-&gt;hdr.u.ms_result) {\n        case DISK_DISK_PROBE:\n            dp = (struct disk *)msg-&gt;load;\n            disk_iocom_update(dp);\n            disk_probe(dp, 0);\n            break;\n\n        case DISK_DISK_DESTROY:\n            dp = (struct disk *)msg-&gt;load;\n            disk_iocom_uninit(dp);\n\n            /* Wait for references to drain */\n            while (dp-&gt;d_refs)\n                tsleep(&amp;dp-&gt;d_refs, 0, \"diskdel\", hz / 10);\n            LIST_REMOVE(dp, d_list);\n\n            dsched_disk_destroy(dp);\n            devfs_destroy_related(dp-&gt;d_cdev);\n            destroy_dev(dp-&gt;d_cdev);\n            destroy_only_dev(dp-&gt;d_rawdev);\n            break;\n\n        case DISK_SLICE_REPROBE:\n            dp = (struct disk *)msg-&gt;load;\n            sp = (struct diskslice *)msg-&gt;load2;\n            disk_probe_slice(dp, sp-&gt;ds_dev, dkslice(sp-&gt;ds_dev), 1);\n            break;\n\n        case DISK_DISK_REPROBE:\n            dp = (struct disk *)msg-&gt;load;\n            disk_probe(dp, 1);\n            break;\n        }\n        lwkt_replymsg(&amp;msg-&gt;hdr, 0);\n    }\n}\n</code></pre> <p>Message types:</p> Message Description <code>DISK_DISK_PROBE</code> Initial disk probe (parse MBR/GPT) <code>DISK_DISK_DESTROY</code> Disk removal <code>DISK_SLICE_REPROBE</code> Re-probe single slice <code>DISK_DISK_REPROBE</code> Re-probe entire disk <code>DISK_UNPROBE</code> Remove slice devices <code>DISK_SYNC</code> Synchronization barrier"},{"location":"sys/kern/disk/#slice-and-partition-probing","title":"Slice and Partition Probing","text":""},{"location":"sys/kern/disk/#mbr-parsing","title":"MBR Parsing","text":"<p>MBR (Master Boot Record) parsing handles traditional PC partition tables:</p> <pre><code>/* sys/kern/subr_diskmbr.c:89-322 */\nint\nmbrinit(cdev_t dev, struct disk_info *info, struct diskslices **sspp)\n{\n    struct buf *bp;\n    struct dos_partition *dp;\n    struct dos_partition dpcopy[NDOSPART];\n\n    /* Read sector 0 */\n    bp = getpbuf_mem(NULL);\n    bp-&gt;b_bio1.bio_offset = (off_t)mbr_offset * info-&gt;d_media_blksize;\n    bp-&gt;b_bcount = info-&gt;d_media_blksize;\n    bp-&gt;b_cmd = BUF_CMD_READ;\n    dev_dstrategy(wdev, &amp;bp-&gt;b_bio1);\n\n    if (biowait(&amp;bp-&gt;b_bio1, \"mbrrd\") != 0)\n        return EIO;\n\n    /* Verify magic number */\n    cp = bp-&gt;b_data;\n    if (cp[0x1FE] != 0x55 || cp[0x1FF] != 0xAA)\n        return EINVAL;\n\n    /* Copy partition table */\n    memcpy(&amp;dpcopy[0], cp + DOSPARTOFF, sizeof(dpcopy));\n\n    /* Check for GPT (PMBR) */\n    for (dospart = 0, dp = dp0; dospart &lt; NDOSPART; dospart++, dp++) {\n        if (dospart == 0 &amp;&amp; dp-&gt;dp_typ == DOSPTYP_PMBR) {\n            return gptinit(dev, info, sspp);  /* Switch to GPT */\n        }\n        if (dp-&gt;dp_typ == DOSPTYP_ONTRACK) {\n            /* Ontrack Disk Manager - retry at sector 63 */\n            mbr_offset = 63;\n            goto reread_mbr;\n        }\n    }\n\n    /* Create slice structures */\n    ssp = dsmakeslicestruct(MAX_SLICES, info);\n    *sspp = ssp;\n\n    /* Initialize slices from MBR entries */\n    sp = &amp;ssp-&gt;dss_slices[BASE_SLICE];\n    for (dospart = 0; dospart &lt; NDOSPART; dospart++, dp++, sp++) {\n        mbr_setslice(sname, info, sp, dp, mbr_offset);\n    }\n    ssp-&gt;dss_nslices = BASE_SLICE + NDOSPART;\n\n    /* Handle extended partitions (recursive) */\n    for (dospart = 0; dospart &lt; NDOSPART; dospart++) {\n        if (sp-&gt;ds_type == DOSPTYP_EXT || sp-&gt;ds_type == DOSPTYP_EXTLBA) {\n            mbr_extended(wdev, info, ssp, sp-&gt;ds_offset, sp-&gt;ds_size,\n                         sp-&gt;ds_offset, max_nsectors, max_ntracks,\n                         mbr_offset, 1);\n        }\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/disk/#gpt-parsing","title":"GPT Parsing","text":"<p>GPT (GUID Partition Table) provides modern partition support:</p> <pre><code>/* sys/kern/subr_diskgpt.c:62-229 */\nint\ngptinit(cdev_t dev, struct disk_info *info, struct diskslices **sspp)\n{\n    struct gpt_hdr *gpt;\n    struct gpt_ent *ent;\n\n    /* Read GPT header at LBA 1 */\n    bp1-&gt;b_bio1.bio_offset = info-&gt;d_media_blksize;\n    bp1-&gt;b_bcount = info-&gt;d_media_blksize;\n    bp1-&gt;b_cmd = BUF_CMD_READ;\n    dev_dstrategy(wdev, &amp;bp1-&gt;b_bio1);\n\n    gpt = (void *)bp1-&gt;b_data;\n\n    /* Validate header */\n    len = le32toh(gpt-&gt;hdr_size);\n    if (len &lt; GPT_MIN_HDR_SIZE || len &gt; info-&gt;d_media_blksize)\n        return EINVAL;\n\n    /* Verify CRC32 */\n    crc = le32toh(gpt-&gt;hdr_crc_self);\n    gpt-&gt;hdr_crc_self = 0;\n    if (crc32(gpt, len) != crc)\n        return EINVAL;\n\n    /* Read partition entries */\n    entries = le32toh(gpt-&gt;hdr_entries);\n    entsz = le32toh(gpt-&gt;hdr_entsz);\n    table_lba = le32toh(gpt-&gt;hdr_lba_table);\n\n    bp2-&gt;b_bio1.bio_offset = (off_t)table_lba * info-&gt;d_media_blksize;\n    bp2-&gt;b_bcount = table_blocks * info-&gt;d_media_blksize;\n    dev_dstrategy(wdev, &amp;bp2-&gt;b_bio1);\n\n    /* Create slice structure for up to 128 partitions */\n    ssp = dsmakeslicestruct(BASE_SLICE + 128, info);\n    *sspp = ssp;\n\n    /* Process each GPT entry */\n    for (i = 0; i &lt; entries &amp;&amp; i &lt; 128; ++i) {\n        ent = (void *)((char *)bp2-&gt;b_data + i * entsz);\n\n        /* Convert from little-endian */\n        le_uuid_dec(&amp;ent-&gt;ent_type, &amp;sent.ent_type);\n        le_uuid_dec(&amp;ent-&gt;ent_uuid, &amp;sent.ent_uuid);\n        sent.ent_lba_start = le64toh(ent-&gt;ent_lba_start);\n        sent.ent_lba_end = le64toh(ent-&gt;ent_lba_end);\n\n        if (kuuid_is_nil(&amp;sent.ent_type))\n            continue;\n\n        /* GPT entry 0 -&gt; COMPATIBILITY_SLICE, others -&gt; BASE_SLICE+i-1 */\n        if (i == 0)\n            sp = &amp;ssp-&gt;dss_slices[COMPATIBILITY_SLICE];\n        else\n            sp = &amp;ssp-&gt;dss_slices[BASE_SLICE + i - 1];\n\n        gpt_setslice(sname, info, sp, &amp;sent);\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/disk/#disklabel-probing","title":"Disklabel Probing","text":"<p>BSD disklabels are probed within slices:</p> <pre><code>/* sys/kern/subr_disk.c:184-340 */\nstatic int\ndisk_probe_slice(struct disk *dp, cdev_t dev, int slice, int reprobe)\n{\n    struct disk_info *info = &amp;dp-&gt;d_info;\n    struct diskslice *sp = &amp;dp-&gt;d_slice-&gt;dss_slices[slice];\n    disklabel_ops_t ops;\n    const char *msg;\n\n    /* Try 32-bit disklabel first */\n    ops = &amp;disklabel32_ops;\n    msg = ops-&gt;op_readdisklabel(dev, sp, &amp;sp-&gt;ds_label, info);\n\n    /* If not found, try 64-bit disklabel */\n    if (msg &amp;&amp; !strcmp(msg, \"no disk label\")) {\n        ops = &amp;disklabel64_ops;\n        msg = ops-&gt;op_readdisklabel(dev, sp, &amp;sp-&gt;ds_label, info);\n    }\n\n    if (msg == NULL) {\n        /* Label found - create partition devices */\n        sp-&gt;ds_ops = ops;\n        for (i = 0; i &lt; ops-&gt;op_getnumparts(sp-&gt;ds_label); i++) {\n            ops-&gt;op_loadpartinfo(sp-&gt;ds_label, i, &amp;part);\n            if (part.fstype) {\n                ndev = make_dev_covering(dops, dp-&gt;d_rawdev-&gt;si_ops,\n                        dkmakeminor(dkunit(dp-&gt;d_cdev), slice, i),\n                        UID_ROOT, GID_OPERATOR, 0640,\n                        \"%s%c\", dev-&gt;si_name, 'a' + i);\n                ndev-&gt;si_disk = dp;\n\n                /* Create UUID alias if available */\n                if (!kuuid_is_nil(&amp;part.storage_uuid)) {\n                    snprintf_uuid(uuid_buf, sizeof(uuid_buf),\n                                  &amp;part.storage_uuid);\n                    make_dev_alias(ndev, \"part-by-uuid/%s\", uuid_buf);\n                }\n            }\n        }\n    } else if (info-&gt;d_dsflags &amp; DSO_COMPATLABEL) {\n        /* Create compatibility label */\n        sp-&gt;ds_label = ops-&gt;op_clone_label(info, sp);\n    }\n\n    return (msg ? EINVAL : 0);\n}\n</code></pre>"},{"location":"sys/kern/disk/#disklabel-operations","title":"Disklabel Operations","text":""},{"location":"sys/kern/disk/#disklabel-operations-structure","title":"Disklabel Operations Structure","text":"<p>Both 32-bit and 64-bit disklabels use a common operations interface:</p> <pre><code>/* sys/sys/disklabel.h (conceptual) */\nstruct disklabel_ops {\n    int labelsize;\n\n    const char *(*op_readdisklabel)(cdev_t dev, struct diskslice *sp,\n                                    disklabel_t *lpp, struct disk_info *info);\n    int (*op_setdisklabel)(disklabel_t olp, disklabel_t nlp,\n                           struct diskslices *ssp, struct diskslice *sp,\n                           u_int32_t *openmask);\n    int (*op_writedisklabel)(cdev_t dev, struct diskslices *ssp,\n                             struct diskslice *sp, disklabel_t lp);\n    disklabel_t (*op_clone_label)(struct disk_info *info, struct diskslice *sp);\n    void (*op_adjust_label_reserved)(struct diskslices *ssp, int slice,\n                                     struct diskslice *sp);\n    int (*op_getpartbounds)(struct diskslices *ssp, disklabel_t lp,\n                            u_int32_t part, u_int64_t *start, u_int64_t *blocks);\n    void (*op_loadpartinfo)(disklabel_t lp, u_int32_t part,\n                            struct partinfo *dpart);\n    u_int32_t (*op_getnumparts)(disklabel_t lp);\n    void (*op_makevirginlabel)(disklabel_t lp, struct diskslices *ssp,\n                               struct diskslice *sp, struct disk_info *info);\n    int (*op_getpackname)(disklabel_t lp, char *buf, size_t bytes);\n    void (*op_freedisklabel)(disklabel_t *lpp);\n};\n</code></pre>"},{"location":"sys/kern/disk/#32-bit-disklabel","title":"32-bit Disklabel","text":"<p>Traditional BSD disklabel format (up to 2TB):</p> <pre><code>/* sys/kern/subr_disklabel32.c:647-660 */\nstruct disklabel_ops disklabel32_ops = {\n    .labelsize = sizeof(struct disklabel32),\n    .op_readdisklabel = l32_readdisklabel,\n    .op_setdisklabel = l32_setdisklabel,\n    .op_writedisklabel = l32_writedisklabel,\n    .op_clone_label = l32_clone_label,\n    .op_adjust_label_reserved = l32_adjust_label_reserved,\n    .op_getpartbounds = l32_getpartbounds,\n    .op_loadpartinfo = l32_loadpartinfo,\n    .op_getnumparts = l32_getnumparts,\n    .op_makevirginlabel = l32_makevirginlabel,\n    .op_getpackname = l32_getpackname,\n    .op_freedisklabel = l32_freedisklabel\n};\n</code></pre> <p>32-bit label location: Sector 1 (<code>LABELSECTOR32</code>)</p> <p>Key features: - Up to 16 partitions (a-p) - 32-bit sector addresses (2TB limit) - CHS geometry stored - Checksum-based integrity</p>"},{"location":"sys/kern/disk/#64-bit-disklabel","title":"64-bit Disklabel","text":"<p>DragonFly's native 64-bit disklabel format:</p> <pre><code>/* sys/kern/subr_disklabel64.c:529-542 */\nstruct disklabel_ops disklabel64_ops = {\n    .labelsize = sizeof(struct disklabel64),\n    .op_readdisklabel = l64_readdisklabel,\n    .op_setdisklabel = l64_setdisklabel,\n    .op_writedisklabel = l64_writedisklabel,\n    .op_clone_label = l64_clone_label,\n    .op_adjust_label_reserved = l64_adjust_label_reserved,\n    .op_getpartbounds = l64_getpartbounds,\n    .op_loadpartinfo = l64_loadpartinfo,\n    .op_getnumparts = l64_getnumparts,\n    .op_makevirginlabel = l64_makevirginlabel,\n    .op_getpackname = l64_getpackname,\n    .op_freedisklabel = l64_freedisklabel\n};\n</code></pre> <p>64-bit label location: Offset 0 (sector-agnostic)</p> <p>Key features: - Up to 16 partitions - 64-bit byte offsets (no practical size limit) - UUID-based partition identification - CRC32 integrity check - 1MB-aligned partitions by default - Reserved boot area (32KB)</p> <pre><code>/* sys/kern/subr_disklabel64.c:437-509 */\nstatic void\nl64_makevirginlabel(disklabel_t lpx, struct diskslices *ssp,\n                    struct diskslice *sp, struct disk_info *info)\n{\n    struct disklabel64 *lp = lpx.lab64;\n\n    /* Use 4KB minimum block size for alignment calculations */\n    if ((blksize = info-&gt;d_media_blksize) &lt; 4096)\n        blksize = 4096;\n\n    lp-&gt;d_magic = DISKMAGIC64;\n    lp-&gt;d_align = blksize;\n    lp-&gt;d_npartitions = MAXPARTITIONS64;\n    kern_uuidgen(&amp;lp-&gt;d_stor_uuid, 1);\n\n    /* Reserve space for label (rounded to block size) */\n    ressize = offsetof(struct disklabel64, d_partitions[RESPARTITIONS64]);\n    ressize = (ressize + blkmask) &amp; ~blkmask;\n\n    lp-&gt;d_bbase = ressize;                          /* boot area start */\n    lp-&gt;d_pbase = lp-&gt;d_bbase + BOOT2SIZE64;        /* partition area start */\n    lp-&gt;d_abase = lp-&gt;d_total_size - ressize;       /* backup label */\n\n    /* Align partition boundaries to 1MB (physical alignment) */\n    lp-&gt;d_pbase = ((doffset + lp-&gt;d_pbase + PALIGN_MASK) &amp;\n                   ~(uint64_t)PALIGN_MASK) - doffset;\n    lp-&gt;d_pstop = ((lp-&gt;d_abase - lp-&gt;d_pbase) &amp;\n                   ~(uint64_t)PALIGN_MASK) + lp-&gt;d_pbase;\n}\n</code></pre>"},{"location":"sys/kern/disk/#io-path","title":"I/O Path","text":""},{"location":"sys/kern/disk/#disk-device-operations","title":"Disk Device Operations","text":"<p>The disk layer interposes its own device operations:</p> <pre><code>/* sys/kern/subr_disk.c:136-159 */\nstatic struct dev_ops disk1_ops = {\n    { \"disk\", 0, D_DISK | D_MPSAFE | D_TRACKCLOSE | D_KVABIO },\n    .d_open = diskopen,\n    .d_close = diskclose,\n    .d_read = physread,\n    .d_write = physwrite,\n    .d_ioctl = diskioctl,\n    .d_strategy = diskstrategy,\n    .d_dump = diskdump,\n    .d_psize = diskpsize,\n};\n\n/* Variant without emergency pager */\nstatic struct dev_ops disk2_ops = {\n    { \"disk\", 0, D_DISK | D_MPSAFE | D_TRACKCLOSE | D_KVABIO | D_NOEMERGPGR },\n    /* ... same operations ... */\n};\n</code></pre>"},{"location":"sys/kern/disk/#strategy-routine","title":"Strategy Routine","text":"<p>The disk strategy routine translates slice-relative offsets:</p> <pre><code>/* sys/kern/subr_disk.c:1221-1253 */\nstatic int\ndiskstrategy(struct dev_strategy_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct bio *bio = ap-&gt;a_bio;\n    struct bio *nbio;\n    struct disk *dp;\n\n    dp = dev-&gt;si_disk;\n    if (dp == NULL) {\n        bio-&gt;bio_buf-&gt;b_error = ENXIO;\n        bio-&gt;bio_buf-&gt;b_flags |= B_ERROR;\n        biodone(bio);\n        return 0;\n    }\n\n    /*\n     * dscheck() transforms slice-relative offset to absolute offset.\n     * Returns NULL if I/O should not proceed (EOF, error, etc.)\n     */\n    if ((nbio = dscheck(dev, bio, dp-&gt;d_slice)) != NULL) {\n        dev_dstrategy(dp-&gt;d_rawdev, nbio);\n    } else {\n        biodone(bio);\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/disk/#slice-checking-and-translation","title":"Slice Checking and Translation","text":"<p><code>dscheck()</code> validates and translates I/O requests:</p> <pre><code>/* sys/kern/subr_diskslice.c:90-310 */\nstruct bio *\ndscheck(cdev_t dev, struct bio *bio, struct diskslices *ssp)\n{\n    struct buf *bp = bio-&gt;bio_buf;\n    struct diskslice *sp;\n    u_int64_t secno, endsecno, slicerel_secno;\n\n    slice = dkslice(dev);\n    part = dkpart(dev);\n    sp = &amp;ssp-&gt;dss_slices[slice];\n\n    /* Calculate sector number from byte offset */\n    if (ssp-&gt;dss_secshift != -1) {\n        secno = bio-&gt;bio_offset &gt;&gt; (DEV_BSHIFT + ssp-&gt;dss_secshift);\n        nsec = bp-&gt;b_bcount &gt;&gt; (DEV_BSHIFT + ssp-&gt;dss_secshift);\n    } else {\n        secno = bio-&gt;bio_offset / ssp-&gt;dss_secsize;\n        nsec = bp-&gt;b_bcount / ssp-&gt;dss_secsize;\n    }\n\n    /* Handle WHOLE_DISK_SLICE - no label interpretation */\n    if (slice == WHOLE_DISK_SLICE) {\n        if (part &gt;= 128 &amp;&amp; part != WHOLE_SLICE_PART) {\n            /* Encode partition in high bits for raw pass-through */\n            nbio = push_bio(bio);\n            nbio-&gt;bio_offset = bio-&gt;bio_offset | (u_int64_t)part &lt;&lt; 56;\n            return nbio;\n        }\n        endsecno = sp-&gt;ds_size;\n        slicerel_secno = secno;\n    }\n    /* Handle whole-slice partition */\n    else if (part == WHOLE_SLICE_PART) {\n        endsecno = sp-&gt;ds_size;\n        slicerel_secno = secno;\n    }\n    /* Handle labeled partition */\n    else if ((lp = sp-&gt;ds_label).opaque != NULL) {\n        ops = sp-&gt;ds_ops;\n        if (ops-&gt;op_getpartbounds(ssp, lp, part, &amp;slicerel_secno, &amp;endsecno))\n            goto bad;\n        slicerel_secno += secno;\n    }\n    else {\n        /* No label - can't access partition */\n        goto bad;\n    }\n\n    /* Check reserved area (label protection) */\n    if (slicerel_secno &lt; sp-&gt;ds_reserved &amp;&amp; nsec &amp;&amp;\n        bp-&gt;b_cmd == BUF_CMD_WRITE) {\n        if (sp-&gt;ds_wlabel == 0) {\n            bp-&gt;b_error = EROFS;\n            goto error;\n        }\n        sp-&gt;ds_flags |= DSF_REPROBE;\n    }\n\n    /* Handle EOF */\n    if (secno + nsec &gt; endsecno) {\n        if (secno &gt;= endsecno) {\n            bp-&gt;b_resid = bp-&gt;b_bcount;\n            bp-&gt;b_flags |= B_INVAL;\n            return NULL;\n        }\n        /* Truncate to fit */\n        nsec = endsecno - secno;\n        bp-&gt;b_bcount = nsec * ssp-&gt;dss_secsize;\n    }\n\n    /* Translate to absolute offset */\n    nbio = push_bio(bio);\n    nbio-&gt;bio_offset = (off_t)(sp-&gt;ds_offset + slicerel_secno) *\n                       ssp-&gt;dss_secsize;\n    return nbio;\n}\n</code></pre>"},{"location":"sys/kern/disk/#io-ordering","title":"I/O Ordering","text":""},{"location":"sys/kern/disk/#bio-queue-management","title":"BIO Queue Management","text":"<p>The disk layer provides I/O ordering for drive zone cache optimization:</p> <pre><code>/* sys/kern/subr_disk.c:1353-1409 */\nvoid\nbioqdisksort(struct bio_queue_head *bioq, struct bio *bio)\n{\n    switch (bio-&gt;bio_buf-&gt;b_cmd) {\n    case BUF_CMD_READ:\n        if (bioq-&gt;transition) {\n            /*\n             * Insert reads before the first write to prioritize reads.\n             * Periodically bleed writes through to prevent starvation.\n             */\n            TAILQ_INSERT_BEFORE(bioq-&gt;transition, bio, bio_act);\n            ++bioq-&gt;reorder;\n\n            /* Minor interval: trickle small writes */\n            if (bioq-&gt;reorder % bioq_reorder_minor_interval == 0) {\n                bioqwritereorder(bioq);\n                if (bioq-&gt;reorder &gt;= bioq_reorder_burst_interval) {\n                    bioq-&gt;reorder = 0;\n                }\n            }\n        } else {\n            /* No writes queued, append to tail */\n            TAILQ_INSERT_TAIL(&amp;bioq-&gt;queue, bio, bio_act);\n        }\n        break;\n\n    case BUF_CMD_WRITE:\n        /* Writes always append; track transition point */\n        TAILQ_INSERT_TAIL(&amp;bioq-&gt;queue, bio, bio_act);\n        if (bioq-&gt;transition == NULL)\n            bioq-&gt;transition = bio;\n        break;\n\n    default:\n        /* Other requests force ordering */\n        bioq_insert_tail(bioq, bio);\n        break;\n    }\n}\n</code></pre> <p>Reorder tuning parameters:</p> Sysctl Default Description <code>kern.bioq_reorder_burst_interval</code> 60 Burst interval (reads) <code>kern.bioq_reorder_minor_interval</code> 5 Minor bleed interval <code>kern.bioq_reorder_burst_bytes</code> 3000000 Bytes per burst <code>kern.bioq_reorder_minor_bytes</code> 262144 Bytes per minor bleed"},{"location":"sys/kern/disk/#disk-enumeration","title":"Disk Enumeration","text":"<p>Enumerate all registered disks safely:</p> <pre><code>/* sys/kern/subr_disk.c:967-1008 */\nstruct disk *\ndisk_enumerate(struct disk *marker, struct disk *dp)\n{\n    lwkt_gettoken(&amp;disklist_token);\n\n    if (dp) {\n        --dp-&gt;d_refs;\n        dp = LIST_NEXT(marker, d_list);\n        LIST_REMOVE(marker, d_list);\n    } else {\n        bzero(marker, sizeof(*marker));\n        marker-&gt;d_flags = DISKFLAG_MARKER;\n        dp = LIST_FIRST(&amp;disklist);\n    }\n\n    /* Skip markers */\n    while (dp) {\n        if ((dp-&gt;d_flags &amp; DISKFLAG_MARKER) == 0)\n            break;\n        dp = LIST_NEXT(dp, d_list);\n    }\n\n    if (dp) {\n        ++dp-&gt;d_refs;\n        LIST_INSERT_AFTER(dp, marker, d_list);\n    }\n\n    lwkt_reltoken(&amp;disklist_token);\n    return dp;\n}\n\nvoid\ndisk_enumerate_stop(struct disk *marker, struct disk *dp)\n{\n    lwkt_gettoken(&amp;disklist_token);\n    LIST_REMOVE(marker, d_list);\n    if (dp)\n        --dp-&gt;d_refs;\n    lwkt_reltoken(&amp;disklist_token);\n}\n</code></pre> <p>Usage pattern: <pre><code>struct disk marker;\nstruct disk *dp = NULL;\n\nwhile ((dp = disk_enumerate(&amp;marker, dp)) != NULL) {\n    /* Process disk */\n    if (some_condition) {\n        disk_enumerate_stop(&amp;marker, dp);\n        break;\n    }\n}\n</code></pre></p>"},{"location":"sys/kern/disk/#ioctls","title":"Ioctls","text":""},{"location":"sys/kern/disk/#diskslice-ioctls","title":"Disk/Slice Ioctls","text":"<pre><code>/* sys/kern/subr_diskslice.c:365-688 */\nint\ndsioctl(cdev_t dev, u_long cmd, caddr_t data, int flags,\n        struct diskslices **sspp, struct disk_info *info)\n{\n    switch (cmd) {\n    case DIOCGDVIRGIN32:\n    case DIOCGDVIRGIN64:\n        /* Get virgin (default) disklabel */\n        ops-&gt;op_makevirginlabel(lp, ssp, sp, info);\n        return 0;\n\n    case DIOCGDINFO32:\n    case DIOCGDINFO64:\n        /* Read disklabel from disk */\n        if (sp-&gt;ds_label.opaque == NULL)\n            error = dsreadandsetlabel(dev, info-&gt;d_dsflags, ssp, sp, info);\n        if (error == 0)\n            bcopy(sp-&gt;ds_label.opaque, data, ops-&gt;labelsize);\n        return error;\n\n    case DIOCGMEDIASIZE:\n        /* Get media size in bytes */\n        *(off_t *)data = (u_int64_t)sp-&gt;ds_size * info-&gt;d_media_blksize;\n        return 0;\n\n    case DIOCGSECTORSIZE:\n        /* Get sector size */\n        *(u_int *)data = info-&gt;d_media_blksize;\n        return 0;\n\n    case DIOCGPART:\n        /* Get partition info */\n        dpart-&gt;media_offset = (u_int64_t)sp-&gt;ds_offset * info-&gt;d_media_blksize;\n        dpart-&gt;media_size = (u_int64_t)sp-&gt;ds_size * info-&gt;d_media_blksize;\n        dpart-&gt;media_blocks = sp-&gt;ds_size;\n        dpart-&gt;media_blksize = info-&gt;d_media_blksize;\n        return 0;\n\n    case DIOCGSLICEINFO:\n        /* Get all slice info */\n        bcopy(ssp, data, ...);\n        return 0;\n\n    case DIOCSDINFO32:\n    case DIOCSDINFO64:\n        /* Set disklabel in memory */\n        error = ops-&gt;op_setdisklabel(lp, lptmp, ssp, sp, openmask);\n        return error;\n\n    case DIOCWDINFO32:\n    case DIOCWDINFO64:\n        /* Write disklabel to disk */\n        error = ops-&gt;op_writedisklabel(dev, ssp, sp, sp-&gt;ds_label);\n        return error;\n\n    case DIOCSYNCSLICEINFO:\n        /* Reprobe entire disk */\n        disk_msg_send_sync(DISK_DISK_REPROBE, dev-&gt;si_disk, NULL);\n        devfs_config();\n        return 0;\n\n    case DIOCWLABEL:\n        /* Enable/disable label writes */\n        set_ds_wlabel(ssp, slice, *(int *)data != 0);\n        return 0;\n    }\n}\n</code></pre>"},{"location":"sys/kern/disk/#kernel-dumps","title":"Kernel Dumps","text":""},{"location":"sys/kern/disk/#dump-configuration","title":"Dump Configuration","text":"<pre><code>/* sys/kern/subr_disk.c:915-940 */\nint\ndisk_dumpconf(cdev_t dev, u_int onoff)\n{\n    struct dumperinfo di;\n    u_int64_t size, blkno;\n    u_int32_t secsize;\n\n    if (!onoff)\n        return set_dumper(NULL);\n\n    error = disk_dumpcheck(dev, &amp;size, &amp;blkno, &amp;secsize);\n    if (error)\n        return ENXIO;\n\n    bzero(&amp;di, sizeof(di));\n    di.dumper = diskdump;\n    di.priv = dev;\n    di.blocksize = secsize;\n    di.maxiosize = dev-&gt;si_iosize_max;\n    di.mediaoffset = blkno * DEV_BSIZE;\n    di.mediasize = size * DEV_BSIZE;\n\n    return set_dumper(&amp;di);\n}\n</code></pre>"},{"location":"sys/kern/disk/#device-aliases","title":"Device Aliases","text":"<p>The disk layer creates convenient device aliases:</p> Alias Pattern Example Description <code>serno/&lt;serial&gt;</code> <code>serno/WD-12345</code> By serial number <code>serno/&lt;serial&gt;.sN</code> <code>serno/WD-12345.s1</code> Slice by serial <code>serno/&lt;serial&gt;.sNX</code> <code>serno/WD-12345.s1a</code> Partition by serial <code>slice-by-uuid/&lt;uuid&gt;</code> <code>slice-by-uuid/abc...</code> GPT slice by UUID <code>part-by-uuid/&lt;uuid&gt;</code> <code>part-by-uuid/def...</code> Partition by UUID <code>by-label/&lt;packname&gt;</code> <code>by-label/root</code> By disklabel pack name <code>part-by-label/&lt;&gt;.X</code> <code>part-by-label/root.a</code> Partition by label name"},{"location":"sys/kern/disk/#initialization","title":"Initialization","text":"<pre><code>/* sys/kern/subr_disk.c:1540-1564 */\nstatic void\ndisk_init(void)\n{\n    struct thread *td_core;\n\n    /* Create object cache for disk messages */\n    disk_msg_cache = objcache_create(\"disk-msg-cache\", 0, 0,\n                                     NULL, NULL, NULL,\n                                     objcache_malloc_alloc,\n                                     objcache_malloc_free,\n                                     &amp;disk_msg_malloc_args);\n\n    /* Initialize tokens */\n    lwkt_token_init(&amp;disklist_token, \"disks\");\n    lwkt_token_init(&amp;ds_token, \"ds\");\n\n    /* Initialize message drain port */\n    lwkt_initport_replyonly(&amp;disk_dispose_port, disk_msg_autofree_reply);\n\n    /* Create disk message processing thread */\n    lwkt_gettoken(&amp;disklist_token);\n    lwkt_create(disk_msg_core, NULL, &amp;td_core, NULL, 0, -1, \"disk_msg_core\");\n    tsleep(td_core, 0, \"diskcore\", 0);\n    lwkt_reltoken(&amp;disklist_token);\n}\n\nSYSINIT(disk_register, SI_SUB_PRE_DRIVERS, SI_ORDER_FIRST, disk_init, NULL);\n</code></pre>"},{"location":"sys/kern/disk/#io-scheduling-dsched","title":"I/O Scheduling (dsched)","text":"<p>The disk subsystem includes stub entry points for I/O scheduling (<code>dsched</code>):</p> <pre><code>/* sys/kern/kern_dsched.c - stubs only */\nvoid dsched_disk_create(struct disk *dp, const char *head_name, int unit) { }\nvoid dsched_disk_update(struct disk *dp, struct disk_info *info) { }\nvoid dsched_disk_destroy(struct disk *dp) { }\n</code></pre> <p>These functions are called from <code>disk_create()</code>, <code>disk_setdiskinfo()</code>, and <code>disk_destroy()</code> but do nothing. The original dsched framework was removed in November 2015 (commit <code>3573cf7bf6</code>) with this explanation:</p> <p>After consultation, remove dsched from the kernel. The original idea is still valid but the current implementation has had lingering bugs for several years now and we've determined that it's just got its fingers into too many structures.</p> <p>Also, the implementation was designed before SSDs, and doesn't play well with SSDs.</p> <p>Leave various empty entry points in so we can revisit at some future date.</p> <p>The removal deleted ~5,800 lines of code including three I/O schedulers: - BFQ (Budget Fair Queueing) - ~2,100 lines - FQ (Fair Queueing) - ~900 lines - AS (Anticipatory Scheduler) - ~290 lines</p> <p>The stub entry points remain to allow potential future reimplementation.</p>"},{"location":"sys/kern/disk/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Type Description <code>kern.disks</code> string Space-separated list of disk names <code>kern.disk_debug</code> int Enable disk subsystem debugging <code>debug.sizeof.disk</code> int Size of struct disk <code>debug.sizeof.diskslices</code> int Size of struct diskslices"},{"location":"sys/kern/disk/#see-also","title":"See Also","text":"<ul> <li>Devices - Device framework</li> <li>Buffer Cache - Block I/O buffering</li> <li>VFS Operations - Filesystem integration</li> </ul>"},{"location":"sys/kern/dmsg/","title":"Distributed Messaging (dmsg)","text":"<p>The DragonFly BSD kernel includes a distributed messaging protocol (dmsg) that enables communication between kernel components across network links. This subsystem is primarily used by HAMMER2 filesystem clustering and the xdisk virtual block device driver.</p>"},{"location":"sys/kern/dmsg/#overview","title":"Overview","text":"<p>The dmsg protocol provides:</p> <ul> <li>Point-to-point streaming message links</li> <li>Transaction-based state management</li> <li>Mesh network topology support</li> <li>Service advertisement via SPAN protocol</li> <li>Block device and filesystem protocol extensions</li> </ul> <pre><code>flowchart TD\n    CC[\"Cluster Controller(hammer2 service daemon)\"]\n    CC --&gt; LC1[\"LNK_CONN\"]\n    CC --&gt; LC2[\"LNK_CONN\"]\n    CC --&gt; LC3[\"LNK_CONN\"]\n    LC1 --&gt; H1[\"HAMMER2Mount\"]\n    LC1 --&gt; X1[\"xdiskDevice\"]\n    LC2 --&gt; H2[\"HAMMER2Mount\"]\n    LC2 --&gt; CL[\"Client\"]\n    LC3 --&gt; H3[\"HAMMER2Mount\"]\n    LC3 --&gt; X2[\"xdiskDevice\"]\n</code></pre>"},{"location":"sys/kern/dmsg/#architecture","title":"Architecture","text":""},{"location":"sys/kern/dmsg/#protocol-layers","title":"Protocol Layers","text":"<p>The dmsg protocol operates in layers:</p> Protocol Code Purpose <code>DMSG_PROTO_LNK</code> <code>0x00</code> Link layer (CONN, SPAN, PING) <code>DMSG_PROTO_DBG</code> <code>0x01</code> Debug shell access <code>DMSG_PROTO_HM2</code> <code>0x02</code> HAMMER2 filesystem operations <code>DMSG_PROTO_BLK</code> <code>0x05</code> Block device operations <code>DMSG_PROTO_VOP</code> <code>0x06</code> VFS operations"},{"location":"sys/kern/dmsg/#connection-model","title":"Connection Model","text":"<pre><code>flowchart LR\n    subgraph NodeA[\"Node A\"]\n        iocomA[\"kdmsg_iocom\"]\n        msgrdA[\"msgrd_td\"]\n        msgwrA[\"msgwr_td\"]\n        stateA1[\"staterd_tree\"]\n        stateA2[\"statewr_tree\"]\n    end\n    subgraph NodeB[\"Node B\"]\n        iocomB[\"kdmsg_iocom\"]\n        rcvB[\"rcvmsg()\"]\n        msgqB[\"msgq\"]\n        stateB1[\"statewr_tree\"]\n        stateB2[\"staterd_tree\"]\n    end\n    msgrdA --&gt;|\"Socket\"| rcvB\n    msgqB --&gt;|\"Socket\"| msgwrA\n</code></pre> <p>Each connection (<code>kdmsg_iocom</code>) has: - Separate reader and writer kernel threads - Red-black trees tracking active transactions - Message queue for outbound messages - Callback functions for message handling</p>"},{"location":"sys/kern/dmsg/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/kern/dmsg/#message-header","title":"Message Header","text":"<p>Source: <code>sys/sys/dmsg.h:213-229</code></p> <pre><code>struct dmsg_hdr {\n    uint16_t    magic;          /* 00 sanity, synchro, endian */\n    uint16_t    reserved02;     /* 02 */\n    uint32_t    salt;           /* 04 random salt helps w/crypto */\n    uint64_t    msgid;          /* 08 message transaction id */\n    uint64_t    circuit;        /* 10 circuit id or 0 */\n    uint64_t    link_verifier;  /* 18 link verifier */\n    uint32_t    cmd;            /* 20 flags | cmd | hdr_size / ALIGN */\n    uint32_t    aux_crc;        /* 24 auxillary data crc */\n    uint32_t    aux_bytes;      /* 28 auxillary data length (bytes) */\n    uint32_t    error;          /* 2C error code or 0 */\n    uint64_t    aux_descr;      /* 30 negotiated OOB data descr */\n    uint32_t    reserved38;     /* 38 */\n    uint32_t    hdr_crc;        /* 3C (aligned) extended header crc */\n};\n</code></pre> <p>The header is always 64 bytes and must be aligned on a 64-byte boundary.</p>"},{"location":"sys/kern/dmsg/#command-flags","title":"Command Flags","text":"<pre><code>#define DMSGF_CREATE    0x80000000U  /* Transaction start */\n#define DMSGF_DELETE    0x40000000U  /* Transaction end */\n#define DMSGF_REPLY     0x20000000U  /* Reply direction */\n#define DMSGF_ABORT     0x10000000U  /* Abort request */\n#define DMSGF_REVTRANS  0x08000000U  /* Opposite direction msgid */\n#define DMSGF_REVCIRC   0x04000000U  /* Opposite direction circuit */\n</code></pre>"},{"location":"sys/kern/dmsg/#io-communication-structure","title":"I/O Communication Structure","text":"<p>Source: <code>sys/sys/dmsg.h:806-828</code></p> <pre><code>struct kdmsg_iocom {\n    struct malloc_type  *mmsg;          /* Memory allocator */\n    struct file         *msg_fp;        /* File pointer for I/O */\n    thread_t            msgrd_td;       /* Reader thread */\n    thread_t            msgwr_td;       /* Writer thread */\n    int                 msg_ctl;        /* Wakeup flags */\n    int                 msg_seq;        /* Message sequence id */\n    uint32_t            flags;          /* KDMSG_IOCOMF_* */\n    struct lock         msglk;          /* Lock for message queue */\n    TAILQ_HEAD(, kdmsg_msg) msgq;       /* Transmit queue */\n    void                *handle;        /* Caller's handle */\n    void (*auto_callback)(kdmsg_msg_t *);\n    int  (*rcvmsg)(kdmsg_msg_t *);\n    void (*exit_func)(struct kdmsg_iocom *);\n    struct kdmsg_state  state0;         /* Root state for stacking */\n    struct kdmsg_state  *conn_state;    /* Active LNK_CONN state */\n    struct kdmsg_state_tree staterd_tree; /* Received transactions */\n    struct kdmsg_state_tree statewr_tree; /* Sent transactions */\n    dmsg_lnk_conn_t     auto_lnk_conn;\n    dmsg_lnk_span_t     auto_lnk_span;\n};\n</code></pre>"},{"location":"sys/kern/dmsg/#transaction-state","title":"Transaction State","text":"<p>Source: <code>sys/sys/dmsg.h:735-757</code></p> <pre><code>struct kdmsg_state {\n    RB_ENTRY(kdmsg_state) rbnode;       /* Indexed by msgid */\n    struct kdmsg_state  *scan;          /* Scan check */\n    struct kdmsg_state_list subq;       /* Active stacked states */\n    TAILQ_ENTRY(kdmsg_state) entry;     /* On parent subq */\n    struct kdmsg_iocom  *iocom;\n    struct kdmsg_state  *parent;\n    int                 refs;           /* Reference count */\n    uint32_t            icmd;           /* Initial command */\n    uint32_t            txcmd;          /* Transmit command flags */\n    uint32_t            rxcmd;          /* Receive command flags */\n    uint64_t            msgid;          /* Transaction ID */\n    int                 flags;\n    int                 error;\n    void                *chain;         /* Caller's state */\n    int (*func)(struct kdmsg_state *, struct kdmsg_msg *);\n    union {\n        void *any;\n        struct hammer2_mount *hmp;\n        struct xa_softc *xa_sc;\n    } any;\n};\n</code></pre> <p>State flags:</p> Flag Description <code>KDMSG_STATE_SUBINSERTED</code> Inserted in parent's subq <code>KDMSG_STATE_DYNAMIC</code> Dynamically allocated <code>KDMSG_STATE_ABORTING</code> Being aborted <code>KDMSG_STATE_OPPOSITE</code> Opposite direction transaction <code>KDMSG_STATE_DYING</code> Connection dying <code>KDMSG_STATE_RBINSERTED</code> Inserted in RB tree <code>KDMSG_STATE_NEW</code> Newly created, defer abort"},{"location":"sys/kern/dmsg/#connection-protocol","title":"Connection Protocol","text":""},{"location":"sys/kern/dmsg/#lnk_conn","title":"LNK_CONN","text":"<p>The <code>LNK_CONN</code> message establishes a connection and identifies the peer:</p> <p>Source: <code>sys/sys/dmsg.h:410-424</code></p> <pre><code>struct dmsg_lnk_conn {\n    dmsg_hdr_t  head;\n    uuid_t      media_id;       /* Media configuration id */\n    uuid_t      peer_id;        /* Unique peer uuid */\n    uuid_t      reserved01;\n    uint64_t    peer_mask;      /* PEER mask for SPAN filtering */\n    uint8_t     peer_type;      /* DMSG_PEER_xxx */\n    uint8_t     reserved02;\n    uint16_t    proto_version;  /* High level protocol support */\n    uint32_t    status;         /* Status flags */\n    uint32_t    rnss;           /* Node's generated rnss */\n    /* ... */\n    char        peer_label[DMSG_LABEL_SIZE];\n};\n</code></pre> <p>Peer types:</p> Type Value Description <code>DMSG_PEER_NONE</code> 0 None <code>DMSG_PEER_ROUTER</code> 1 Cluster controller <code>DMSG_PEER_BLOCK</code> 2 Block device server <code>DMSG_PEER_HAMMER2</code> 3 HAMMER2 mounted volume <code>DMSG_PEER_CLIENT</code> 63 Client connection"},{"location":"sys/kern/dmsg/#lnk_span","title":"LNK_SPAN","text":"<p>The <code>LNK_SPAN</code> message advertises a service over an open <code>LNK_CONN</code>:</p> <p>Source: <code>sys/sys/dmsg.h:503-528</code></p> <pre><code>struct dmsg_lnk_span {\n    dmsg_hdr_t  head;\n    uuid_t      peer_id;\n    uuid_t      pfs_id;         /* Unique PFS id */\n    uint8_t     pfs_type;       /* PFS type */\n    uint8_t     peer_type;      /* PEER type */\n    uint16_t    proto_version;  /* Protocol version */\n    uint32_t    status;         /* Status flags */\n    /* ... */\n    uint32_t    dist;           /* Span distance */\n    uint32_t    rnss;           /* Random number sub-sort */\n    union {\n        uint32_t reserved03[14];\n        dmsg_media_block_t block;\n    } media;\n    char        peer_label[DMSG_LABEL_SIZE];\n    char        pfs_label[DMSG_LABEL_SIZE];\n};\n</code></pre> <p>SPANs can be relayed through intermediate nodes (cluster controllers) to build a mesh network. The <code>dist</code> field tracks hop count for path selection.</p>"},{"location":"sys/kern/dmsg/#transaction-state-machine","title":"Transaction State Machine","text":""},{"location":"sys/kern/dmsg/#message-flow","title":"Message Flow","text":"<pre><code>sequenceDiagram\n    participant I as Initiator\n    participant R as Responder\n    I-&gt;&gt;R: CREATE\n    R-&gt;&gt;I: CREATE | REPLY\n    I-&gt;&gt;R: (data messages)\n    R-&gt;&gt;I: (data messages)\n    I-&gt;&gt;R: DELETE\n    R-&gt;&gt;I: DELETE | REPLY\n    Note over I,R: Transaction closed\n</code></pre>"},{"location":"sys/kern/dmsg/#state-transitions","title":"State Transitions","text":"<p>Transactions are tracked by the <code>txcmd</code> and <code>rxcmd</code> fields:</p> <pre><code>CREATE sent:     txcmd |= CREATE\nCREATE received: rxcmd |= CREATE\nDELETE sent:     txcmd |= DELETE\nDELETE received: rxcmd |= DELETE\n\nTransaction fully closed when:\n    (txcmd &amp; DELETE) &amp;&amp; (rxcmd &amp; DELETE)\n</code></pre>"},{"location":"sys/kern/dmsg/#abort-handling","title":"Abort Handling","text":"<p>Transactions can be aborted in several ways:</p> <ol> <li>Mid-stream abort: Send message with <code>DMSGF_ABORT</code> flag</li> <li>Abort on create: <code>DMSGF_ABORT | DMSGF_CREATE</code> for non-blocking</li> <li>Abort after delete: <code>DMSGF_ABORT | DMSGF_DELETE</code> for error cleanup</li> </ol>"},{"location":"sys/kern/dmsg/#api-reference","title":"API Reference","text":""},{"location":"sys/kern/dmsg/#initialization","title":"Initialization","text":"<pre><code>void kdmsg_iocom_init(kdmsg_iocom_t *iocom, void *handle,\n                      uint32_t flags, struct malloc_type *mmsg,\n                      int (*rcvmsg)(kdmsg_msg_t *msg));\n</code></pre> <p>Initialize an iocom structure. The <code>rcvmsg</code> callback handles received messages.</p> <p>Flags:</p> Flag Description <code>KDMSG_IOCOMF_AUTOCONN</code> Auto-handle LNK_CONN transactions <code>KDMSG_IOCOMF_AUTORXSPAN</code> Auto-handle received LNK_SPAN <code>KDMSG_IOCOMF_AUTOTXSPAN</code> Auto-transmit LNK_SPAN"},{"location":"sys/kern/dmsg/#connection-management","title":"Connection Management","text":"<pre><code>void kdmsg_iocom_reconnect(kdmsg_iocom_t *iocom, struct file *fp,\n                           const char *subsysname);\n</code></pre> <p>Connect or reconnect using the provided file pointer. Creates reader and writer threads named <code>&lt;subsysname&gt;-msgrd</code> and <code>&lt;subsysname&gt;-msgwr</code>.</p> <pre><code>void kdmsg_iocom_autoinitiate(kdmsg_iocom_t *iocom,\n                              void (*auto_callback)(kdmsg_msg_t *msg));\n</code></pre> <p>Automatically initiate <code>LNK_CONN</code> and optionally <code>LNK_SPAN</code> transactions.</p> <pre><code>void kdmsg_iocom_uninit(kdmsg_iocom_t *iocom);\n</code></pre> <p>Disconnect and clean up. Waits for all transactions to complete or abort.</p>"},{"location":"sys/kern/dmsg/#message-operations","title":"Message Operations","text":"<pre><code>kdmsg_msg_t *kdmsg_msg_alloc(kdmsg_state_t *state, uint32_t cmd,\n                             int (*func)(kdmsg_state_t *, kdmsg_msg_t *),\n                             void *data);\n</code></pre> <p>Allocate a message. If <code>cmd</code> includes <code>DMSGF_CREATE</code>, a new transaction state is created and registered.</p> <pre><code>void kdmsg_msg_write(kdmsg_msg_t *msg);\n</code></pre> <p>Queue a message for transmission.</p> <pre><code>void kdmsg_msg_reply(kdmsg_msg_t *msg, uint32_t error);\n</code></pre> <p>Reply to a message and terminate the transaction.</p> <pre><code>void kdmsg_msg_result(kdmsg_msg_t *msg, uint32_t error);\n</code></pre> <p>Reply to a message but keep the transaction open.</p> <pre><code>void kdmsg_msg_free(kdmsg_msg_t *msg);\n</code></pre> <p>Free a message. Automatically drops the state reference.</p>"},{"location":"sys/kern/dmsg/#state-operations","title":"State Operations","text":"<pre><code>void kdmsg_state_reply(kdmsg_state_t *state, uint32_t error);\n</code></pre> <p>Terminate a transaction from state context.</p> <pre><code>void kdmsg_state_result(kdmsg_state_t *state, uint32_t error);\n</code></pre> <p>Send a result but keep the transaction open.</p>"},{"location":"sys/kern/dmsg/#implementation-details","title":"Implementation Details","text":""},{"location":"sys/kern/dmsg/#reader-thread","title":"Reader Thread","text":"<p>Source: <code>sys/kern/kern_dmsg.c:324-424</code></p> <p>The reader thread (<code>kdmsg_iocom_thread_rd</code>) handles incoming messages:</p> <ol> <li>Read message header from socket/pipe</li> <li>Validate magic number and header size</li> <li>Allocate message structure</li> <li>Read extended header and auxiliary data</li> <li>Call <code>kdmsg_msg_receive_handling()</code> for state machine processing</li> </ol> <pre><code>static void\nkdmsg_iocom_thread_rd(void *arg)\n{\n    kdmsg_iocom_t *iocom = arg;\n    dmsg_hdr_t hdr;\n    kdmsg_msg_t *msg = NULL;\n    int error = 0;\n\n    while ((iocom-&gt;msg_ctl &amp; KDMSG_CLUSTERCTL_KILLRX) == 0) {\n        /* Read and validate header */\n        error = fp_read(iocom-&gt;msg_fp, &amp;hdr, sizeof(hdr),\n                        NULL, 1, UIO_SYSSPACE);\n        if (error || hdr.magic != DMSG_HDR_MAGIC)\n            break;\n\n        /* Allocate and populate message */\n        msg = kdmsg_msg_alloc(&amp;iocom-&gt;state0,\n                              hdr.cmd &amp; DMSGF_BASECMDMASK,\n                              NULL, NULL);\n        msg-&gt;any.head = hdr;\n\n        /* Read auxiliary data if present */\n        if (msg-&gt;aux_size) {\n            msg-&gt;aux_data = kmalloc(DMSG_DOALIGN(msg-&gt;aux_size),\n                                    iocom-&gt;mmsg, M_WAITOK);\n            error = fp_read(iocom-&gt;msg_fp, msg-&gt;aux_data, ...);\n        }\n\n        /* Process message */\n        error = kdmsg_msg_receive_handling(msg);\n        msg = NULL;\n    }\n\n    /* Shutdown handling... */\n    lwkt_exit();\n}\n</code></pre>"},{"location":"sys/kern/dmsg/#writer-thread","title":"Writer Thread","text":"<p>Source: <code>sys/kern/kern_dmsg.c:426-611</code></p> <p>The writer thread (<code>kdmsg_iocom_thread_wr</code>) handles outgoing messages:</p> <ol> <li>Sleep waiting for messages in queue</li> <li>Dequeue message and process state machine</li> <li>Write header and auxiliary data to socket/pipe</li> <li>Clean up state on connection termination</li> </ol> <p>The writer thread is also responsible for final cleanup when the connection terminates, simulating failures for any remaining open transactions.</p>"},{"location":"sys/kern/dmsg/#state-machine","title":"State Machine","text":"<p>Source: <code>sys/kern/kern_dmsg.c:771-1098</code></p> <p>The <code>kdmsg_state_msgrx()</code> function processes received message state:</p> <ul> <li><code>CREATE</code>: Allocates new state, inserts in RB tree</li> <li><code>DELETE</code>: Marks state for deletion</li> <li><code>REPLY</code>: Updates state with response flags</li> <li><code>ABORT</code>: Handles various abort scenarios</li> </ul> <p>Transaction states are tracked in two RB trees: - <code>staterd_tree</code>: Transactions initiated by the remote side - <code>statewr_tree</code>: Transactions initiated locally</p>"},{"location":"sys/kern/dmsg/#stacked-transactions","title":"Stacked Transactions","text":"<p>Transactions can be stacked by specifying a parent's <code>msgid</code> in the <code>circuit</code> field. This creates a hierarchy:</p> <pre><code>state0 (root)\n   |\n   +-- LNK_CONN (circuit = 0)\n          |\n          +-- LNK_SPAN (circuit = conn.msgid)\n                 |\n                 +-- BLK_OPEN (circuit = span.msgid)\n                        |\n                        +-- BLK_READ (circuit = open.msgid)\n</code></pre> <p>When a parent transaction terminates, all child transactions are automatically aborted by <code>kdmsg_simulate_failure()</code>.</p>"},{"location":"sys/kern/dmsg/#block-device-protocol","title":"Block Device Protocol","text":"<p>The <code>DMSG_PROTO_BLK</code> protocol provides remote block device access:</p>"},{"location":"sys/kern/dmsg/#commands","title":"Commands","text":"Command Description <code>DMSG_BLK_OPEN</code> Open device <code>DMSG_BLK_CLOSE</code> Close device <code>DMSG_BLK_READ</code> Read data <code>DMSG_BLK_WRITE</code> Write data <code>DMSG_BLK_FLUSH</code> Flush data <code>DMSG_BLK_FREEBLKS</code> Free blocks (TRIM)"},{"location":"sys/kern/dmsg/#readwrite-structure","title":"Read/Write Structure","text":"<pre><code>struct dmsg_blk_read {\n    dmsg_hdr_t  head;\n    uint64_t    keyid;      /* From BLK_OPEN */\n    uint64_t    offset;     /* Byte offset */\n    uint32_t    bytes;      /* Byte count */\n    uint32_t    flags;\n    uint32_t    reserved01;\n    uint32_t    reserved02;\n};\n</code></pre>"},{"location":"sys/kern/dmsg/#error-codes","title":"Error Codes","text":"Error Value Description <code>DMSG_ERR_NOSUPP</code> 0x20 Operation not supported <code>DMSG_ERR_LOSTLINK</code> 0x21 Link lost <code>DMSG_ERR_IO</code> 0x22 I/O error <code>DMSG_ERR_PARAM</code> 0x23 Parameter error <code>DMSG_ERR_CANTCIRC</code> 0x24 Cannot circuit (lost span)"},{"location":"sys/kern/dmsg/#configuration","title":"Configuration","text":""},{"location":"sys/kern/dmsg/#sysctl-variables","title":"Sysctl Variables","text":"Sysctl Default Description <code>kdmsg.debug</code> 1 Debug output level"},{"location":"sys/kern/dmsg/#usage-example","title":"Usage Example","text":""},{"location":"sys/kern/dmsg/#server-side-receiving-connections","title":"Server Side (Receiving Connections)","text":"<pre><code>static int my_rcvmsg(kdmsg_msg_t *msg);\n\nvoid\nmy_start_server(struct file *fp)\n{\n    kdmsg_iocom_t *iocom;\n\n    iocom = kmalloc(sizeof(*iocom), M_MYDEV, M_WAITOK | M_ZERO);\n\n    /* Initialize with auto-handling of CONN and SPAN */\n    kdmsg_iocom_init(iocom, mydev,\n                     KDMSG_IOCOMF_AUTOCONN | KDMSG_IOCOMF_AUTORXSPAN,\n                     M_MYDEV, my_rcvmsg);\n\n    /* Set up CONN info */\n    iocom-&gt;auto_lnk_conn.peer_type = DMSG_PEER_BLOCK;\n    snprintf(iocom-&gt;auto_lnk_conn.peer_label,\n             sizeof(iocom-&gt;auto_lnk_conn.peer_label),\n             \"mydevice\");\n\n    /* Connect */\n    kdmsg_iocom_reconnect(iocom, fp, \"mydev\");\n    kdmsg_iocom_autoinitiate(iocom, my_auto_callback);\n}\n\nstatic int\nmy_rcvmsg(kdmsg_msg_t *msg)\n{\n    switch(msg-&gt;tcmd) {\n    case DMSG_BLK_READ | DMSGF_CREATE | DMSGF_DELETE:\n        /* Handle read request */\n        /* ... */\n        kdmsg_msg_reply(msg, 0);\n        break;\n    default:\n        kdmsg_msg_reply(msg, DMSG_ERR_NOSUPP);\n        break;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/dmsg/#client-side-initiating-connections","title":"Client Side (Initiating Connections)","text":"<pre><code>void\nmy_send_read(kdmsg_state_t *span_state, uint64_t offset, uint32_t bytes)\n{\n    kdmsg_msg_t *msg;\n\n    msg = kdmsg_msg_alloc(span_state,\n                          DMSG_BLK_READ | DMSGF_CREATE | DMSGF_DELETE,\n                          my_read_callback, NULL);\n    msg-&gt;any.blk_read.offset = offset;\n    msg-&gt;any.blk_read.bytes = bytes;\n    kdmsg_msg_write(msg);\n}\n\nstatic int\nmy_read_callback(kdmsg_state_t *state, kdmsg_msg_t *msg)\n{\n    if (msg-&gt;any.head.cmd &amp; DMSGF_DELETE) {\n        if (msg-&gt;any.head.error == 0) {\n            /* Success - process msg-&gt;aux_data */\n        } else {\n            /* Error */\n        }\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/dmsg/#consumers","title":"Consumers","text":"<p>The dmsg protocol is used by:</p> <ul> <li>HAMMER2 Filesystem: Cluster synchronization and distributed storage</li> <li>xdisk Driver: Virtual block devices over network connections</li> <li>hammer2 Service: Userland cluster controller daemon</li> </ul>"},{"location":"sys/kern/dmsg/#related-documentation","title":"Related Documentation","text":"<ul> <li>LWKT Threading - Threading model used by dmsg</li> <li>Synchronization - Lock mechanisms</li> <li>Sockets - Socket layer used for transport</li> <li>Disk Subsystem - Block device layer</li> </ul>"},{"location":"sys/kern/dmsg/#source-files","title":"Source Files","text":"File Description <code>sys/kern/kern_dmsg.c</code> Kernel dmsg implementation <code>sys/sys/dmsg.h</code> Protocol definitions and structures <code>sys/kern/subr_diskiocom.c</code> Disk I/O over dmsg <code>sys/dev/disk/xdisk/xdisk.c</code> Virtual disk using dmsg"},{"location":"sys/kern/firmware/","title":"Firmware Loading","text":"<p>The firmware subsystem provides an interface for loading firmware images into the kernel and making them available to device drivers. Firmware images are typically embedded in kernel modules and can be loaded on-demand or pre-loaded.</p> <p>Source files: - <code>sys/kern/subr_firmware.c</code> - Firmware management (~540 lines)</p> <p>Header files: - <code>sys/sys/firmware.h</code> - Public API</p>"},{"location":"sys/kern/firmware/#overview","title":"Overview","text":"<p>The firmware framework solves a common driver problem: many hardware devices require proprietary microcode or configuration data to operate. This subsystem:</p> <ol> <li>Provides a registry for firmware images with reference counting</li> <li>Supports automatic loading of firmware modules on demand</li> <li>Handles parent/child relationships for multi-image modules</li> <li>Manages unloading of unreferenced firmware</li> </ol>"},{"location":"sys/kern/firmware/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/firmware/#struct-firmware","title":"struct firmware","text":"<p>The user-visible firmware image descriptor:</p> <pre><code>/* sys/sys/firmware.h:51-56 */\nstruct firmware {\n    const char      *name;      /* system-wide name */\n    const uint8_t   *data;      /* location of image */\n    size_t          datasize;   /* size of image in bytes */\n    unsigned int    version;    /* version of the image */\n};\n</code></pre>"},{"location":"sys/kern/firmware/#struct-priv_fw","title":"struct priv_fw","text":"<p>Internal firmware registry entry:</p> <pre><code>/* sys/kern/subr_firmware.c:81-109 */\nstruct priv_fw {\n    int             refcnt;     /* reference count */\n    struct priv_fw  *parent;    /* parent image for subimages */\n    int             flags;      /* FW_UNLOAD flag */\n    linker_file_t   file;       /* module file (if autoloaded) */\n    struct firmware fw;         /* externally visible information */\n};\n</code></pre> <p>State transitions: <pre><code>firmware_register()    --&gt;  fw.name = image_name\n(autoloaded image)     --&gt;  file = module reference\nfirmware_unregister()  --&gt;  fw.name = NULL\n(unload complete)      --&gt;  file = NULL\n</code></pre></p>"},{"location":"sys/kern/firmware/#registry","title":"Registry","text":"<p>The firmware registry uses a static array:</p> <pre><code>/* sys/kern/subr_firmware.c:124-125 */\n#define FIRMWARE_MAX    30\nstatic struct priv_fw firmware_table[FIRMWARE_MAX];\n</code></pre> <p>A slot is in use if either <code>file != NULL</code> or <code>fw.name != NULL</code>:</p> <pre><code>#define FW_INUSE(p)     ((p)-&gt;file != NULL || (p)-&gt;fw.name != NULL)\n</code></pre>"},{"location":"sys/kern/firmware/#public-api","title":"Public API","text":""},{"location":"sys/kern/firmware/#firmware_register","title":"firmware_register()","text":"<p>Register a firmware image with the kernel:</p> <pre><code>/* sys/kern/subr_firmware.c:172-209 */\nconst struct firmware *\nfirmware_register(const char *imagename, const void *data, size_t datasize,\n    unsigned int version, const struct firmware *parent)\n{\n    struct priv_fw *match, *frp;\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n\n    /* Check name is unique and find free slot */\n    match = lookup(imagename, &amp;frp);\n    if (match != NULL) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        kprintf(\"%s: image %s already registered!\\n\", __func__, imagename);\n        return NULL;\n    }\n    if (frp == NULL) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        kprintf(\"%s: cannot register image %s, firmware table full!\\n\",\n            __func__, imagename);\n        return NULL;\n    }\n\n    bzero(frp, sizeof(*frp));\n    frp-&gt;fw.name = imagename;\n    frp-&gt;fw.data = data;\n    frp-&gt;fw.datasize = datasize;\n    frp-&gt;fw.version = version;\n\n    /* Link to parent if specified */\n    if (parent != NULL) {\n        frp-&gt;parent = PRIV_FW(parent);\n        frp-&gt;parent-&gt;refcnt++;\n    }\n\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n    return &amp;frp-&gt;fw;\n}\n</code></pre> <p>Parameters: - <code>imagename</code> - Unique name for the firmware - <code>data</code> - Pointer to firmware data - <code>datasize</code> - Size in bytes - <code>version</code> - Version number - <code>parent</code> - Parent firmware (for multi-image modules)</p>"},{"location":"sys/kern/firmware/#firmware_get","title":"firmware_get()","text":"<p>Look up and optionally load firmware:</p> <pre><code>/* sys/kern/subr_firmware.c:304-347 */\nconst struct firmware *\nfirmware_get(const char *imagename)\n{\n    struct task fwload_task;\n    struct priv_fw *fp;\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    fp = lookup(imagename, NULL);\n    if (fp != NULL)\n        goto found;\n\n    /*\n     * Image not present, try to load the module holding it.\n     */\n    if (caps_priv_check_self(SYSCAP_NOKLD) != 0 || securelevel &gt; 0) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        kprintf(\"%s: insufficient privileges to \"\n            \"load firmware image %s\\n\", __func__, imagename);\n        return NULL;\n    }\n\n    /*\n     * Defer load to a thread with known context.  linker_reference_module\n     * may do filesystem i/o which requires root &amp; current dirs, etc.\n     */\n    if (!cold) {\n        TASK_INIT(&amp;fwload_task, 0, loadimage, __DECONST(void *, imagename));\n        taskqueue_enqueue(firmware_tq, &amp;fwload_task);\n        lksleep(__DECONST(void *, imagename), &amp;firmware_lock, 0, \"fwload\", 0);\n    }\n\n    /* After load attempt, check if image registered */\n    fp = lookup(imagename, NULL);\n    if (fp == NULL) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        return NULL;\n    }\n\nfound:\n    fp-&gt;refcnt++;\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n    return &amp;fp-&gt;fw;\n}\n</code></pre> <p>Auto-loading:</p> <ol> <li>If firmware not in registry, attempt to load kernel module</li> <li>Module loading delegated to taskqueue thread (needs filesystem context)</li> <li>Module calls <code>firmware_register()</code> during initialization</li> <li>Caller blocks until load completes or fails</li> </ol>"},{"location":"sys/kern/firmware/#firmware_put","title":"firmware_put()","text":"<p>Release a firmware reference:</p> <pre><code>/* sys/kern/subr_firmware.c:358-372 */\nvoid\nfirmware_put(const struct firmware *p, int flags)\n{\n    struct priv_fw *fp = PRIV_FW(p);\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    fp-&gt;refcnt--;\n    if (fp-&gt;refcnt == 0) {\n        if (flags &amp; FIRMWARE_UNLOAD)\n            fp-&gt;flags |= FW_UNLOAD;\n        if (fp-&gt;file)\n            taskqueue_enqueue(firmware_tq, &amp;firmware_unload_task);\n    }\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n}\n</code></pre> <p>Flags: - <code>FIRMWARE_UNLOAD</code> (0x0001) - Request unload when refcount reaches zero</p>"},{"location":"sys/kern/firmware/#firmware_unregister","title":"firmware_unregister()","text":"<p>Remove a firmware image from the registry:</p> <pre><code>/* sys/kern/subr_firmware.c:216-250 */\nint\nfirmware_unregister(const char *imagename)\n{\n    struct priv_fw *fp;\n    int err;\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    fp = lookup(imagename, NULL);\n\n    if (fp == NULL) {\n        /* OK - happens on module unload after firmware_put() */\n        err = 0;\n    } else if (fp-&gt;refcnt != 0) {\n        err = EBUSY;  /* still in use */\n    } else {\n        linker_file_t x = fp-&gt;file;  /* preserve for unload */\n\n        if (fp-&gt;parent != NULL)\n            fp-&gt;parent-&gt;refcnt--;\n\n        bzero(fp, sizeof(struct priv_fw));\n        fp-&gt;file = x;  /* restore for unload completion */\n        err = 0;\n    }\n\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n    return err;\n}\n</code></pre>"},{"location":"sys/kern/firmware/#module-loading","title":"Module Loading","text":""},{"location":"sys/kern/firmware/#load-task","title":"Load Task","text":"<p>Firmware loading is performed by a dedicated taskqueue thread:</p> <pre><code>/* sys/kern/subr_firmware.c:252-295 */\nstatic void\nloadimage(void *arg, int npending)\n{\n    char *imagename = arg;\n    struct priv_fw *fp;\n    linker_file_t result;\n    int error;\n\n    /* synchronize with the thread that dispatched us */\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n\n    error = linker_reference_module(imagename, NULL, &amp;result);\n    if (error != 0) {\n        kprintf(\"%s: could not load firmware image, error %d\\n\",\n            imagename, error);\n        goto done;\n    }\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    fp = lookup(imagename, NULL);\n    if (fp == NULL || fp-&gt;file != NULL) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        if (fp == NULL)\n            kprintf(\"%s: firmware image loaded, \"\n                \"but did not register\\n\", imagename);\n        (void) linker_release_module(imagename, NULL, NULL);\n        goto done;\n    }\n    fp-&gt;file = result;  /* record module identity */\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n\ndone:\n    wakeup_one(imagename);  /* wake waiting caller */\n}\n</code></pre>"},{"location":"sys/kern/firmware/#unload-task","title":"Unload Task","text":"<p>Unreferenced autoloaded modules are unloaded by a background task:</p> <pre><code>/* sys/kern/subr_firmware.c:427-475 */\nstatic void\nunloadentry(void *unused1, int unused2)\n{\n    int limit = FIRMWARE_MAX;\n    int i;\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n\n    /* Scan table, may need multiple passes for cross-linked images */\n    for (i = 0; i &lt; limit; i++) {\n        struct priv_fw *fp;\n        int err;\n\n        fp = &amp;firmware_table[i % FIRMWARE_MAX];\n        if (fp-&gt;fw.name == NULL || fp-&gt;file == NULL ||\n            fp-&gt;refcnt != 0 || (fp-&gt;flags &amp; FW_UNLOAD) == 0)\n            continue;\n\n        /* Found entry to unload */\n        limit = i + FIRMWARE_MAX;   /* another full round */\n        fp-&gt;flags &amp;= ~FW_UNLOAD;    /* don't retry */\n\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        err = linker_release_module(NULL, NULL, fp-&gt;file);\n        lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n\n        if (err == 0)\n            fp-&gt;file = NULL;\n    }\n\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n}\n</code></pre>"},{"location":"sys/kern/firmware/#parentchild-relationships","title":"Parent/Child Relationships","text":"<p>Modules can contain multiple firmware images. One image (typically named after the module) is the parent; others are children:</p> <pre><code>flowchart TD\n    subgraph Module[\"Module: iwn5000fw.ko\"]\n        P[\"iwn5000fw(parent, master image)\"]\n        C1[\"iwn5000init(child, references parent)\"]\n        C2[\"iwn5000boot(child, references parent)\"]\n    end\n    P --&gt; C1\n    P --&gt; C2\n</code></pre> <p>Behavior: - Children increment parent's refcount on registration - Children decrement parent's refcount on unregistration - Module cannot unload until all children are released - Auto-loading uses parent name (module name) to find the module</p>"},{"location":"sys/kern/firmware/#usage-example","title":"Usage Example","text":"<p>Driver requesting firmware:</p> <pre><code>static int\nmydriver_attach(device_t dev)\n{\n    const struct firmware *fw;\n\n    fw = firmware_get(\"mydevice_fw\");\n    if (fw == NULL) {\n        device_printf(dev, \"could not load firmware\\n\");\n        return ENXIO;\n    }\n\n    /* Use fw-&gt;data and fw-&gt;datasize */\n    error = load_microcode(dev, fw-&gt;data, fw-&gt;datasize);\n\n    /* Release when done (or keep reference if needed later) */\n    firmware_put(fw, FIRMWARE_UNLOAD);\n\n    return error;\n}\n</code></pre> <p>Firmware module:</p> <pre><code>/* In the firmware module */\nstatic const uint8_t mydevice_fw_data[] = {\n    /* firmware binary data */\n};\n\nstatic int\nmydevice_fw_modevent(module_t mod, int type, void *unused)\n{\n    switch (type) {\n    case MOD_LOAD:\n        return firmware_register(\"mydevice_fw\",\n            mydevice_fw_data, sizeof(mydevice_fw_data),\n            1, NULL) == NULL ? ENOMEM : 0;\n    case MOD_UNLOAD:\n        return firmware_unregister(\"mydevice_fw\");\n    }\n    return EINVAL;\n}\n</code></pre>"},{"location":"sys/kern/firmware/#initialization","title":"Initialization","text":"<pre><code>/* sys/kern/subr_firmware.c:480-504 */\nstatic int\nfirmware_modevent(module_t mod, int type, void *unused)\n{\n    switch (type) {\n    case MOD_LOAD:\n        TASK_INIT(&amp;firmware_unload_task, 0, unloadentry, NULL);\n        lockinit(&amp;firmware_lock, \"firmware table\", 0, LK_CANRECURSE);\n        firmware_tq = taskqueue_create(\"taskqueue_firmware\", M_WAITOK,\n            taskqueue_thread_enqueue, &amp;firmware_tq);\n        (void) taskqueue_start_threads(&amp;firmware_tq, 1, TDPRI_KERN_DAEMON,\n            -1, \"firmware taskq\");\n        return 0;\n\n    case MOD_UNLOAD:\n        /* Mark all for unload, drain, verify empty */\n        ...\n    }\n}\n\nDECLARE_MODULE(firmware, firmware_mod, SI_SUB_DRIVERS, SI_ORDER_FIRST);\n</code></pre>"},{"location":"sys/kern/firmware/#security","title":"Security","text":"<p>Firmware loading is restricted:</p> <pre><code>if (caps_priv_check_self(SYSCAP_NOKLD) != 0 || securelevel &gt; 0) {\n    /* Cannot load firmware - insufficient privileges */\n}\n</code></pre> <ul> <li>Requires <code>SYSCAP_NOKLD</code> capability</li> <li>Blocked when <code>securelevel &gt; 0</code></li> </ul>"},{"location":"sys/kern/firmware/#see-also","title":"See Also","text":"<ul> <li>Devices - Device driver framework</li> <li>NewBus - Device attachment</li> </ul>"},{"location":"sys/kern/initialization/","title":"Kernel Initialization and Bootstrap","text":"<p>Source Files: - <code>sys/kern/init_main.c</code> - Main kernel initialization - <code>sys/kern/init_sysent.c</code> - System call table (generated) - <code>sys/kern/subr_param.c</code> - Kernel parameter computation - <code>sys/kern/kern_environment.c</code> - Kernel environment variables</p> <p>This document describes how the DragonFly BSD kernel boots and initializes, from the hand-off from machine-dependent code through the creation of the first userspace process (init).</p>"},{"location":"sys/kern/initialization/#overview","title":"Overview","text":"<p>DragonFly's kernel initialization follows a carefully orchestrated sequence:</p> <pre><code>Machine-dependent startup (e.g., init386)\n  \u2193\nmi_proc0init() - Initialize proc0/thread0/lwp0\n  \u2193\nmi_startup() - Machine-independent initialization\n  \u2193\nSYSINIT framework - Ordered subsystem initialization\n  \u2193\nCreate init process (PID 1)\n  \u2193\nStart init - exec /sbin/init\n  \u2193\nUserspace initialization begins\n</code></pre> <p>The initialization process relies on the SYSINIT framework, which allows subsystems to register initialization functions that are executed in a well-defined order based on subsystem (SI_SUB_) and order (SI_ORDER_) values.</p>"},{"location":"sys/kern/initialization/#architecture","title":"Architecture","text":""},{"location":"sys/kern/initialization/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/kern/initialization/#process-0-swapper","title":"Process 0 (swapper)","text":"<p>Process 0 is the kernel's initial context, created statically:</p> <pre><code>/* sys/kern/init_main.c:79-91 */\nstatic struct session session0;\nstatic struct pgrp pgrp0;\nstatic struct sigacts sigacts0;\nstatic struct filedesc filedesc0;\nstatic struct plimit limit0;\nstatic struct vmspace vmspace0;\nstatic struct sysreaper initreaper;\n\nstruct proc *initproc;        /* Will point to init (PID 1) */\nstruct proc proc0;            /* Process 0 (swapper) */\nstruct lwp lwp0;              /* LWP 0 */\nstruct thread thread0;        /* Thread 0 */\n</code></pre> <p>These static structures are never freed and serve as the template for forking other processes.</p>"},{"location":"sys/kern/initialization/#sysinit-structure","title":"SYSINIT Structure","text":"<pre><code>struct sysinit {\n    unsigned int    subsystem;    /* SI_SUB_* */\n    unsigned int    order;        /* SI_ORDER_* within subsystem */\n    void            (*func)(void *);\n    void            *udata;\n};\n</code></pre> <p>Subsystem levels (SI_SUB_*): - <code>SI_BOOT1_COPYRIGHT</code> - Print copyright - <code>SI_BOOT1_POST</code> - Early boot tasks - <code>SI_BOOT2_LEAVE_CRIT</code> - Enable interrupts - <code>SI_BOOT2_PROC0</code> - Initialize proc0 - <code>SI_BOOT2_FINISH_SMP</code> - SMP initialization complete - <code>SI_SUB_PROC0_POST</code> - Post-proc0 initialization - <code>SI_SUB_CREATE_INIT</code> - Create init process - <code>SI_SUB_KTHREAD_INIT</code> - Start init process - Many more (see <code>&lt;sys/kernel.h&gt;</code>)</p> <p>Order values (SI_ORDER_*): - <code>SI_ORDER_FIRST</code> - First in subsystem - <code>SI_ORDER_SECOND</code> - Second - <code>SI_ORDER_MIDDLE</code> - Middle - <code>SI_ORDER_ANY</code> - No specific order - <code>SI_ORDER_LAST</code> - Last in subsystem</p>"},{"location":"sys/kern/initialization/#initialization-sequence","title":"Initialization Sequence","text":""},{"location":"sys/kern/initialization/#phase-1-machine-dependent-setup","title":"Phase 1: Machine-Dependent Setup","text":"<p>Before <code>mi_proc0init()</code> is called, the machine-dependent code (e.g., <code>init386()</code> on x86-64) has:</p> <ol> <li>Set up the boot stack</li> <li>Initialized basic CPU structures</li> <li>Set up memory management (paging)</li> <li>Initialized the GDT/IDT</li> <li>Set up the boot CPU's globaldata structure</li> <li>Prepared the initial kernel environment</li> </ol>"},{"location":"sys/kern/initialization/#phase-2-process-0-creation","title":"Phase 2: Process 0 Creation","text":"<p>Function: <code>mi_proc0init()</code> (<code>init_main.c:162</code>)</p> <p>Called from very low-level boot code to initialize CPU #0's structures:</p> <pre><code>void mi_proc0init(struct globaldata *gd, struct user *proc0paddr)\n{\n    /* Initialize thread0 */\n    lwkt_init_thread(&amp;thread0, proc0paddr, LWKT_THREAD_STACK, 0, gd);\n    lwkt_set_comm(&amp;thread0, \"thread0\");\n\n    /* Initialize proc0 */\n    RB_INIT(&amp;proc0.p_lwp_tree);\n    spin_init(&amp;proc0.p_spin, \"iproc_proc0\");\n    lwkt_token_init(&amp;proc0.p_token, \"iproc\");\n\n    /* Initialize lwp0 */\n    lwp0.lwp_tid = 1;\n    proc0.p_lasttid = lwp0.lwp_tid;\n    lwp_rb_tree_RB_INSERT(&amp;proc0.p_lwp_tree, &amp;lwp0);\n    lwp0.lwp_thread = &amp;thread0;\n    lwp0.lwp_proc = &amp;proc0;\n\n    /* Set up user scheduler */\n    proc0.p_usched = usched_init();\n\n    /* Thread0 linkage */\n    thread0.td_flags |= TDF_RUNNING;\n    thread0.td_proc = &amp;proc0;\n    thread0.td_lwp = &amp;lwp0;\n    thread0.td_switch = cpu_lwkt_switch;\n\n    lwkt_schedule_self(curthread);\n}\n</code></pre> <p>At this point: - Thread0 is running on CPU 0 - Proc0 exists but is incomplete - LWKT threading is operational - Interrupts are still disabled (critical section)</p>"},{"location":"sys/kern/initialization/#phase-3-machine-independent-startup","title":"Phase 3: Machine-Independent Startup","text":"<p>Function: <code>mi_startup()</code> (<code>init_main.c:199</code>)</p> <p>This is the main initialization orchestrator:</p> <pre><code>void mi_startup(void)\n{\n    struct sysinit *sip;\n    struct sysinit **sipp, **xipp;\n\n    /* Get sysinit array from linker set */\n    if (sysinit == NULL) {\n        sysinit = SET_BEGIN(sysinit_set);\n        sysinit_end = SET_LIMIT(sysinit_set);\n    }\n\nrestart:\n    /* Bubble sort sysinits by subsystem and order */\n    for (sipp = sysinit; sipp &lt; sysinit_end; sipp++) {\n        for (xipp = sipp + 1; xipp &lt; sysinit_end; xipp++) {\n            if ((*sipp)-&gt;subsystem &lt; (*xipp)-&gt;subsystem ||\n                ((*sipp)-&gt;subsystem == (*xipp)-&gt;subsystem &amp;&amp;\n                 (*sipp)-&gt;order &lt;= (*xipp)-&gt;order))\n                continue;\n            /* Swap */\n            save = *sipp;\n            *sipp = *xipp;\n            *xipp = save;\n        }\n    }\n\n    /* Execute all sysinit functions in order */\n    for (sipp = sysinit; sipp &lt; sysinit_end; sipp++) {\n        sip = *sipp;\n\n        if (sip-&gt;subsystem == SI_SPECIAL_DUMMY)\n            continue;\n        if (sip-&gt;subsystem == SI_SPECIAL_DONE)\n            continue;\n\n        /* Call initialization function */\n        (*(sip-&gt;func))(sip-&gt;udata);\n\n        /* Mark as done */\n        sip-&gt;subsystem = SI_SPECIAL_DONE;\n\n        /* Check if KLDs added more sysinits */\n        if (newsysinit != NULL) {\n            sysinit = newsysinit;\n            sysinit_end = newsysinit_end;\n            newsysinit = NULL;\n            goto restart;\n        }\n    }\n\n    panic(\"Shouldn't get here!\");  /* Scheduler never returns */\n}\n</code></pre> <p>Key aspects:</p> <ol> <li>Sorting: SYSINITs are sorted by subsystem (primary key) and order (secondary key)</li> <li>Execution: Each function is called in order</li> <li>Dynamic loading: If kernel modules (KLDs) are loaded during initialization, their SYSINITs are merged and the process restarts</li> <li>Never returns: The last SYSINIT starts the scheduler, which never returns</li> </ol>"},{"location":"sys/kern/initialization/#phase-4-important-sysinit-stages","title":"Phase 4: Important SYSINIT Stages","text":""},{"location":"sys/kern/initialization/#41-copyright-and-early-boot","title":"4.1 Copyright and Early Boot","text":"<pre><code>/* Print copyright */\nSYSINIT(announce, SI_BOOT1_COPYRIGHT, SI_ORDER_FIRST,\n        print_caddr_t, copyright);\n\n/* Initialize kernel environment */\nSYSINIT(kenv, SI_BOOT1_POST, SI_ORDER_ANY, kenv_init, NULL);\n</code></pre> <p>The <code>kenv_init()</code> function (<code>kern_environment.c:571</code>): - Creates dynamic environment array - Copies static environment from bootloader - Sets up kenv spinlock - Enables dynamic environment operations</p>"},{"location":"sys/kern/initialization/#42-leave-critical-section","title":"4.2 Leave Critical Section","text":"<pre><code>SYSINIT(leavecrit, SI_BOOT2_LEAVE_CRIT, SI_ORDER_ANY,\n        leavecrit, NULL);\n</code></pre> <p>The <code>leavecrit()</code> function (<code>init_main.c:305</code>): - Stabilizes machine interrupt ABI - Enables CPU interrupts - Exits critical section - Allows device probes to work</p> <p>After this point, interrupts are enabled and the system can respond to external events.</p>"},{"location":"sys/kern/initialization/#43-enable-tsleep","title":"4.3 Enable tsleep","text":"<pre><code>SYSINIT(tsleepworks, SI_BOOT2_FINISH_SMP, SI_ORDER_SECOND,\n        tsleepworks, NULL);\n</code></pre> <p>Sets <code>tsleep_now_works = 1</code>, allowing threads to block/sleep.</p>"},{"location":"sys/kern/initialization/#44-initialize-process-0","title":"4.4 Initialize Process 0","text":"<pre><code>SYSINIT(p0init, SI_BOOT2_PROC0, SI_ORDER_FIRST, proc0_init, NULL);\n</code></pre> <p>The <code>proc0_init()</code> function (<code>init_main.c:373</code>) completes proc0 setup:</p> <pre><code>static void proc0_init(void *dummy)\n{\n    struct proc *p = &amp;proc0;\n\n    /* Initialize osrel */\n    p-&gt;p_osrel = osreldate;\n\n    /* Initialize process and pgrp structures */\n    procinit();\n    vm_init2();\n\n    /* Create process 0 (the swapper) */\n    procinsertinit(p);\n    pgrpinsertinit(&amp;pgrp0);\n    LIST_INIT(&amp;pgrp0.pg_members);\n    LIST_INSERT_HEAD(&amp;pgrp0.pg_members, p, p_pglist);\n\n    /* Set up session */\n    pgrp0.pg_session = &amp;session0;\n    session0.s_count = 1;\n    session0.s_leader = p;\n\n    /* Set process attributes */\n    p-&gt;p_flags = P_SYSTEM;\n    p-&gt;p_stat = SACTIVE;\n    lwp0.lwp_stat = LSRUN;\n    p-&gt;p_nice = NZERO;\n    p-&gt;p_sysent = &amp;null_sysvec;\n\n    bcopy(\"swapper\", p-&gt;p_comm, sizeof(\"swapper\"));\n\n    /* Create credentials (root) */\n    uip = uicreate(0);\n    p-&gt;p_ucred = crget();\n    p-&gt;p_ucred-&gt;cr_ruidinfo = uip;\n    p-&gt;p_ucred-&gt;cr_uidinfo = uip;\n    p-&gt;p_ucred-&gt;cr_ngroups = 1;\n\n    /* Create sigacts */\n    p-&gt;p_sigacts = &amp;sigacts0;\n    refcount_init(&amp;p-&gt;p_sigacts-&gt;ps_refcnt, 1);\n    siginit(p);\n\n    /* Create file descriptor table */\n    fdinit_bootstrap(p, &amp;filedesc0, cmask);\n\n    /* Create limits */\n    plimit_init0(&amp;limit0);\n    p-&gt;p_limit = &amp;limit0;\n\n    /* Set up address space */\n    pmap_pinit0(vmspace_pmap(&amp;vmspace0));\n    p-&gt;p_vmspace = &amp;vmspace0;\n    lwp0.lwp_vmspace = p-&gt;p_vmspace;\n    vmspace_initrefs(&amp;vmspace0);\n    vm_map_init(&amp;vmspace0.vm_map, VM_MIN_USER_ADDRESS,\n                VM_MAX_USER_ADDRESS, vmspace_pmap(&amp;vmspace0));\n\n    /* Initialize kqueue */\n    kqueue_init(&amp;lwp0.lwp_kqueue, &amp;filedesc0);\n\n    /* Charge root for one process */\n    chgproccnt(p-&gt;p_ucred-&gt;cr_uidinfo, 1, 0);\n    vm_init_limits(p);\n}\n</code></pre> <p>After this, proc0 is a fully functional kernel process.</p>"},{"location":"sys/kern/initialization/#45-end-cold-boot","title":"4.5 End Cold Boot","text":"<pre><code>SYSINIT(endofcoldboot, SI_SUB_ISWARM, SI_ORDER_ANY,\n        endofcoldboot, NULL);\n</code></pre> <p>Sets <code>cold = 0</code>, indicating the system is no longer in cold boot phase. Device drivers use this flag to determine if they're being initialized at boot or hotplugged later.</p>"},{"location":"sys/kern/initialization/#46-create-init-process","title":"4.6 Create Init Process","text":"<pre><code>SYSINIT(init, SI_SUB_CREATE_INIT, SI_ORDER_FIRST, create_init, NULL);\n</code></pre> <p>The <code>create_init()</code> function (<code>init_main.c:716</code>):</p> <pre><code>static void create_init(const void *udata)\n{\n    int error;\n    struct lwp *lp;\n\n    crit_enter();\n\n    /* Fork process 1 from lwp0 */\n    error = fork1(&amp;lwp0, RFFDG | RFPROC, &amp;initproc);\n    if (error)\n        panic(\"cannot fork init: %d\", error);\n\n    initproc-&gt;p_flags |= P_SYSTEM;\n    reaper_init(initproc, &amp;initreaper);\n\n    lp = ONLY_LWP_IN_PROC(initproc);\n\n    /* Set up fork handler to call start_init() */\n    cpu_set_fork_handler(lp, start_init, NULL);\n\n    crit_exit();\n}\n</code></pre> <p>At this point: - Init process (PID 1) is created but not running - It's marked as a system process - Fork handler is set to <code>start_init()</code></p>"},{"location":"sys/kern/initialization/#47-start-init-process","title":"4.7 Start Init Process","text":"<pre><code>SYSINIT(kickinit, SI_SUB_KTHREAD_INIT, SI_ORDER_FIRST, kick_init, NULL);\n</code></pre> <p>The <code>kick_init()</code> function (<code>init_main.c:737</code>):</p> <pre><code>static void kick_init(const void *udata)\n{\n    start_forked_proc(&amp;lwp0, initproc);\n}\n</code></pre> <p>This makes the init process runnable. When it's scheduled, it will execute <code>start_init()</code>.</p>"},{"location":"sys/kern/initialization/#phase-5-starting-userspace","title":"Phase 5: Starting Userspace","text":"<p>Function: <code>start_init()</code> (<code>init_main.c:556</code>)</p> <p>This function executes in the context of the init process (PID 1):</p> <pre><code>static void start_init(void *dummy, struct trapframe *frame)\n{\n    struct proc *p = curproc;\n    struct mount *mp;\n    struct vnode *vp;\n    char *path, *next;\n    int error;\n\n    /* Get kernel name from environment */\n    env = kgetenv(\"kernelname\");\n    if (env != NULL)\n        strlcpy(kernelname, env, sizeof(kernelname));\n\n    /* Set up root vnode */\n    mp = mountlist_boot_getfirst();\n    if (VFS_ROOT(mp, &amp;vp))\n        panic(\"cannot find root vnode\");\n\n    p-&gt;p_fd-&gt;fd_cdir = vp;\n    vref(p-&gt;p_fd-&gt;fd_cdir);\n    p-&gt;p_fd-&gt;fd_rdir = vp;\n    vref(p-&gt;p_fd-&gt;fd_rdir);\n\n    /* Mount devfs */\n    kprintf(\"Mounting devfs\\n\");\n    vfs_mountroot_devfs();\n\n    /* Allocate stack space for execve args */\n    addr = trunc_page(USRSTACK - PAGE_SIZE);\n    error = vm_map_find(&amp;p-&gt;p_vmspace-&gt;vm_map, NULL, NULL,\n                        0, &amp;addr, PAGE_SIZE, PAGE_SIZE, FALSE,\n                        VM_MAPTYPE_NORMAL, VM_SUBSYS_INIT,\n                        VM_PROT_ALL, VM_PROT_ALL, 0);\n    if (error)\n        panic(\"init: couldn't allocate argument space\");\n\n    /* Try each path in init_path */\n    for (path = init_path; *path != '\\0'; path = next) {\n        /* Parse next path component */\n        while (*path == ':')\n            path++;\n        if (*path == '\\0')\n            break;\n        for (next = path; *next != '\\0' &amp;&amp; *next != ':'; next++)\n            ;\n\n        /* Build argv for exec */\n        /* argv[0] = path (e.g., \"/sbin/init\") */\n        /* argv[1] = boot flags (e.g., \"-s\" for single-user) */\n\n        /* Try to exec */\n        if ((error = sys_execve(&amp;sysmsg, &amp;args)) == 0) {\n            /* Success! Now running /sbin/init */\n            lp-&gt;lwp_proc-&gt;p_usched-&gt;acquire_curproc(lp);\n            return;\n        }\n\n        if (error != ENOENT)\n            kprintf(\"exec %.*s: error %d\\n\",\n                   (int)(next - path), path, error);\n    }\n\n    kprintf(\"init: not found in path %s\\n\", init_path);\n    panic(\"no init\");\n}\n</code></pre> <p>Default init_path: <pre><code>/sbin/init:/sbin/oinit:/sbin/init.bak\n</code></pre></p> <p>This can be overridden with <code>kern.init_path</code> environment variable.</p> <p>Boot flags passed to init: - <code>-s</code>: Single-user mode (if <code>RB_SINGLE</code> set) - <code>-C</code>: Boot from CDROM (if <code>BOOTCDROM</code> defined)</p> <p>Once <code>sys_execve()</code> succeeds: 1. Init's address space is replaced with <code>/sbin/init</code> 2. Init becomes PID 1 in userspace 3. Kernel initialization is complete 4. Userspace initialization begins (init spawns getty, starts services, etc.)</p>"},{"location":"sys/kern/initialization/#kernel-parameters","title":"Kernel Parameters","text":"<p>Source: <code>sys/kern/subr_param.c</code></p>"},{"location":"sys/kern/initialization/#parameter-computation","title":"Parameter Computation","text":"<p>DragonFly computes many kernel parameters at boot time based on available physical memory and tunables. This happens in two phases:</p>"},{"location":"sys/kern/initialization/#phase-1-init-param1-fixed-parameters","title":"Phase 1: Init Param1 - Fixed Parameters","text":"<p>Function: <code>init_param1()</code> (<code>subr_param.c:197</code>)</p> <p>Called very early, before memory-dependent calculations:</p> <pre><code>void init_param1(void)\n{\n    /* Timer frequency */\n    hz = HZ_DEFAULT;  /* Default: 100 Hz */\n    TUNABLE_INT_FETCH(\"kern.hz\", &amp;hz);\n\n    ustick = 1000000 / hz;        /* microseconds per tick */\n    nstick = 1000000000 / hz;     /* nanoseconds per tick */\n\n    /* Statistics clock */\n    stathz = hz + 1;\n    TUNABLE_INT_FETCH(\"kern.stathz\", &amp;stathz);\n\n    /* Profiling clock */\n    profhz = stathz;\n\n    /* NTP tick delta */\n    ntp_default_tick_delta = howmany(30000000, 60 * hz);\n\n    /* Size limits (can be tuned) */\n    maxswzone = VM_SWZONE_SIZE_MAX;\n    TUNABLE_LONG_FETCH(\"kern.maxswzone\", &amp;maxswzone);\n\n    maxbcache = VM_BCACHE_SIZE_MAX;\n    TUNABLE_LONG_FETCH(\"kern.maxbcache\", &amp;maxbcache);\n\n    /* User address space limits */\n    maxtsiz = MAXTSIZ;    /* Max text size */\n    dfldsiz = DFLDSIZ;    /* Default data size */\n    maxdsiz = MAXDSIZ;    /* Max data size */\n    dflssiz = DFLSSIZ;    /* Default stack size */\n    maxssiz = MAXSSIZ;    /* Max stack size */\n    sgrowsiz = SGROWSIZ;  /* Stack growth size */\n    maxthrssiz = MAXTHRSSIZ;  /* Thread stack area */\n\n    /* Each can be overridden via tunable */\n    TUNABLE_QUAD_FETCH(\"kern.maxtsiz\", &amp;maxtsiz);\n    TUNABLE_QUAD_FETCH(\"kern.dfldsiz\", &amp;dfldsiz);\n    /* ... etc ... */\n}\n</code></pre>"},{"location":"sys/kern/initialization/#phase-2-init-param2-memory-scaled-parameters","title":"Phase 2: Init Param2 - Memory-Scaled Parameters","text":"<p>Function: <code>init_param2(int physpages)</code> (<code>subr_param.c:237</code>)</p> <p>Called after physical memory detection:</p> <pre><code>void init_param2(int physpages)\n{\n    size_t limsize;\n\n    /* Calculate memory size in bytes, limited by KVA */\n    limsize = (size_t)physpages * PAGE_SIZE;\n    if (limsize &gt; KvaSize)\n        limsize = KvaSize;\n\n    /* Limit maxswzone to 1/2 of physical memory */\n    if (maxswzone &gt; limsize / 2)\n        maxswzone = limsize / 2;\n\n    limsize /= 1024 * 1024;  /* Convert to MB */\n\n    /* Compute maxusers (affects many other limits) */\n    maxusers = MAXUSERS;\n    TUNABLE_INT_FETCH(\"kern.maxusers\", &amp;maxusers);\n\n    if (maxusers == 0) {\n        /* Auto-compute: ~384 per 3GB */\n        maxusers = limsize / 8;\n        if (maxusers &lt; 32)\n            maxusers = 32;\n    }\n\n    /* Maximum number of processes */\n    maxproc = NPROC;  /* 20 + 16 * maxusers */\n    TUNABLE_INT_FETCH(\"kern.maxproc\", &amp;maxproc);\n\n    if (maxproc &lt; 32)\n        maxproc = 32;\n    if (maxproc &gt; limsize * 40)\n        maxproc = limsize * 40;  /* Prevent kmap exhaustion */\n\n    /* Maximum open files */\n    maxfiles = MAXFILES;  /* maxproc * 16 */\n    TUNABLE_INT_FETCH(\"kern.maxfiles\", &amp;maxfiles);\n\n    if (maxfiles &lt; 128)\n        maxfiles = 128;\n\n    /* Per-user/per-proc limits */\n    maxprocperuid = maxproc / 4;\n    if (maxprocperuid &lt; 128)\n        maxprocperuid = maxproc / 2;\n\n    minfilesperproc = 8;\n    maxfilesperproc = maxfiles / 4;\n    maxfilesperuser = maxfilesperproc * 2;\n    maxfilesrootres = maxfiles / 20;  /* Reserved for root */\n\n    /* POSIX locks */\n    maxposixlocksperuid = MAXPOSIXLOCKSPERUID;  /* maxproc * 4 */\n    TUNABLE_INT_FETCH(\"kern.maxposixlocksperuid\", &amp;maxposixlocksperuid);\n\n    /* Buffer cache */\n    nbuf = NBUF;  /* Usually 0 (auto-sized) */\n    TUNABLE_LONG_FETCH(\"kern.nbuf\", &amp;nbuf);\n\n    /* Callout wheel size */\n    ncallout = 16 + maxproc + maxfiles;\n    if (ncallout &gt; 5*60*hz)  /* Limit to ~5 minutes worth */\n        ncallout = 5*60*hz;\n    TUNABLE_INT_FETCH(\"kern.ncallout\", &amp;ncallout);\n}\n</code></pre>"},{"location":"sys/kern/initialization/#key-computed-parameters","title":"Key Computed Parameters","text":"Parameter Formula Description <code>maxusers</code> <code>limsize / 8</code> (if not set) Tuning knob affecting many limits <code>maxproc</code> <code>20 + 16 * maxusers</code> Maximum processes <code>maxfiles</code> <code>maxproc * 16</code> Maximum open files system-wide <code>maxprocperuid</code> <code>maxproc / 4</code> Max processes per user <code>maxfilesperproc</code> <code>maxfiles / 4</code> Max files per process <code>maxfilesperuser</code> <code>maxfilesperproc * 2</code> Max files per user <code>maxposixlocksperuid</code> <code>maxproc * 4</code> Max POSIX locks per user <code>ncallout</code> <code>16 + maxproc + maxfiles</code> Callout wheel size"},{"location":"sys/kern/initialization/#virtual-machine-detection","title":"Virtual Machine Detection","text":"<p>DragonFly can detect if it's running as a VM guest:</p> <pre><code>enum vmm_guest_type detect_virtual(void)\n{\n    char *sysenv;\n\n    /* Check SMBIOS BIOS vendor */\n    sysenv = kgetenv(\"smbios.bios.vendor\");\n    if (sysenv != NULL) {\n        /* Check for QEMU, Xen, BHYVE, KVM, etc. */\n        for (i = 0; vmm_bnames[i].str != NULL; i++)\n            if (strcmp(sysenv, vmm_bnames[i].str) == 0)\n                return (vmm_bnames[i].type);\n    }\n\n    /* Check SMBIOS system product */\n    sysenv = kgetenv(\"smbios.system.product\");\n    if (sysenv != NULL) {\n        /* Check for VMware, Hyper-V, VirtualBox, etc. */\n        for (i = 0; vmm_pnames[i].str != NULL; i++)\n            if (strcmp(sysenv, vmm_pnames[i].str) == 0)\n                return (vmm_pnames[i].type);\n    }\n\n    return (VMM_GUEST_NONE);\n}\n</code></pre> <p>Detected VM types: - VMM_GUEST_QEMU - VMM_GUEST_XEN - VMM_GUEST_BHYVE - VMM_GUEST_KVM - VMM_GUEST_VMWARE - VMM_GUEST_HYPERV - VMM_GUEST_VBOX - VMM_GUEST_PARALLELS</p> <p>Exposed via <code>sysctl kern.vmm_guest</code>.</p>"},{"location":"sys/kern/initialization/#kernel-environment","title":"Kernel Environment","text":"<p>Source: <code>sys/kern/kern_environment.c</code></p> <p>The kernel environment provides a key-value store passed from the bootloader, similar to Unix environment variables.</p>"},{"location":"sys/kern/initialization/#architecture_1","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph BOOT[\"Boot time\"]\n        LOADER[\"Bootloader (loader(8))Sets up static environment\u2022 kernelname=/boot/kernel\u2022 kern.hz=1000\u2022 etc.\"]\n    end\n\n    subgraph STATIC[\"Static environment\"]\n        SENV[\"(null-terminated strings)name=value\\\\0name2=value2\\\\0\\\\0\"]\n    end\n\n    subgraph DYNAMIC[\"Dynamic environment\"]\n        DENV[\"(kmalloc'd array of strings)kenv_dynp[] = {  'name=value',  'name2=value2',  NULL}+ spinlock protection+ ksetenv/kunsetenv support\"]\n    end\n\n    LOADER --&gt;|\"kern_envp pointer\"| STATIC\n    STATIC --&gt;|\"kenv_init() - SI_BOOT1_POST\"| DYNAMIC\n</code></pre>"},{"location":"sys/kern/initialization/#static-environment","title":"Static Environment","text":"<p>At boot, the bootloader passes a pointer to a static environment:</p> <pre><code>char *kern_envp;  /* Pointer to static environment */\n</code></pre> <p>Format: <pre><code>\"var1=value1\\0var2=value2\\0var3=value3\\0\\0\"\n           ^              ^              ^\n           |              |              |\n         NUL            NUL          Double NUL (end)\n</code></pre></p>"},{"location":"sys/kern/initialization/#dynamic-environment","title":"Dynamic Environment","text":"<p>After <code>kenv_init()</code> (SI_BOOT1_POST), the environment becomes dynamic:</p> <pre><code>/* kern_environment.c:570 */\nstatic void kenv_init(void *dummy)\n{\n    char *cp;\n    int len, i;\n\n    /* Allocate dynamic array (max 512 entries) */\n    kenv_dynp = kmalloc(KENV_DYNMAXNUM * sizeof(char *),\n                        M_KENV, M_WAITOK | M_ZERO);\n\n    /* Copy static environment to dynamic */\n    for (i = 0, cp = kern_envp; cp != NULL; cp = kernenv_next(cp)) {\n        len = strlen(cp) + 1;\n        if (i &lt; (KENV_DYNMAXNUM - 1)) {\n            kenv_dynp[i] = kmalloc(len, M_KENV, M_WAITOK);\n            strcpy(kenv_dynp[i++], cp);\n        } else {\n            kprintf(\"WARNING: kenv: exhausted dynamic storage\\n\");\n        }\n    }\n    kenv_dynp[i] = NULL;\n\n    spin_init(&amp;kenv_dynlock, \"kenvdynlock\");\n    kenv_isdynamic = 1;\n}\n</code></pre> <p>After this: - Static environment is still accessible (read-only) - Dynamic environment is modifiable - Protected by spinlock (<code>kenv_dynlock</code>)</p>"},{"location":"sys/kern/initialization/#api","title":"API","text":""},{"location":"sys/kern/initialization/#get-environment-variable","title":"Get Environment Variable","text":"<pre><code>char *kgetenv(const char *name);\n</code></pre> <p>Returns a kmalloc'd copy of the value, or NULL if not found. Must be freed with <code>kfreeenv()</code>.</p> <p>Example: <pre><code>char *hz_str = kgetenv(\"kern.hz\");\nif (hz_str != NULL) {\n    hz = atoi(hz_str);\n    kfreeenv(hz_str);\n}\n</code></pre></p>"},{"location":"sys/kern/initialization/#set-environment-variable","title":"Set Environment Variable","text":"<pre><code>int ksetenv(const char *name, const char *value);\n</code></pre> <p>Sets or replaces an environment variable. Returns 0 on success, -1 on error.</p> <p>Example: <pre><code>ksetenv(\"debug.verbose\", \"1\");\n</code></pre></p>"},{"location":"sys/kern/initialization/#unset-environment-variable","title":"Unset Environment Variable","text":"<pre><code>int kunsetenv(const char *name);\n</code></pre> <p>Removes an environment variable. Returns 0 on success, -1 if not found.</p>"},{"location":"sys/kern/initialization/#test-environment-variable","title":"Test Environment Variable","text":"<pre><code>int ktestenv(const char *name);\n</code></pre> <p>Returns 1 if variable exists, 0 otherwise.</p>"},{"location":"sys/kern/initialization/#typed-getters","title":"Typed Getters","text":"<pre><code>int kgetenv_string(const char *name, char *data, int size);\nint kgetenv_int(const char *name, int *data);\nint kgetenv_long(const char *name, long *data);\nint kgetenv_ulong(const char *name, unsigned long *data);\nint kgetenv_quad(const char *name, quad_t *data);\n</code></pre> <p>These return 1 on success, 0 if variable not found or parse error.</p> <p>kgetenv_quad() supports size suffixes: - <code>k</code> or <code>K</code>: \u00d7 1024 - <code>m</code> or <code>M</code>: \u00d7 1024\u00b2 - <code>g</code> or <code>G</code>: \u00d7 1024\u00b3 - <code>t</code> or <code>T</code>: \u00d7 1024\u2074</p> <p>Example: <pre><code>quad_t size;\nif (kgetenv_quad(\"vm.kmem_size\", &amp;size)) {\n    /* size now contains value in bytes */\n    /* \"512m\" \u2192 536870912 */\n    /* \"2g\" \u2192 2147483648 */\n}\n</code></pre></p>"},{"location":"sys/kern/initialization/#free-environment-value","title":"Free Environment Value","text":"<pre><code>void kfreeenv(char *env);\n</code></pre> <p>Frees a string returned by <code>kgetenv()</code>.</p>"},{"location":"sys/kern/initialization/#userspace-access","title":"Userspace Access","text":"<p>The <code>kenv(2)</code> system call provides userspace access:</p> <pre><code>int kenv(int action, const char *name, char *value, int len);\n</code></pre> <p>Actions: - <code>KENV_GET</code>: Get variable value - <code>KENV_SET</code>: Set variable (requires SYSCAP_NOKENV_WR) - <code>KENV_UNSET</code>: Unset variable (requires SYSCAP_NOKENV_WR) - <code>KENV_DUMP</code>: Dump all variables</p> <p>Userspace tools: <pre><code># Get variable\nkenv kern.hz\n\n# Set variable\nkenv kern.verbose=1\n\n# Unset variable\nkenv -u debug.trace\n\n# Dump all\nkenv\n</code></pre></p>"},{"location":"sys/kern/initialization/#tunables","title":"Tunables","text":"<p>The TUNABLE macros simplify reading boot-time configuration:</p> <pre><code>/* Definition (from subr_param.c:74) */\nint maxusers;\n\nTUNABLE_INT_FETCH(\"kern.maxusers\", &amp;maxusers);\n</code></pre> <p>TUNABLE types: - <code>TUNABLE_INT_FETCH(path, var)</code> - <code>TUNABLE_LONG_FETCH(path, var)</code> - <code>TUNABLE_ULONG_FETCH(path, var)</code> - <code>TUNABLE_QUAD_FETCH(path, var)</code> - <code>TUNABLE_STR_FETCH(path, var, size)</code></p> <p>These are typically called during <code>init_param1()</code> or <code>init_param2()</code>.</p>"},{"location":"sys/kern/initialization/#common-kernel-environment-variables","title":"Common Kernel Environment Variables","text":"Variable Description Example <code>kern.hz</code> Timer frequency (Hz) <code>1000</code> <code>kern.maxusers</code> Base tuning parameter <code>384</code> <code>kern.maxproc</code> Max processes <code>4096</code> <code>kern.maxfiles</code> Max open files <code>65536</code> <code>kern.ipc.maxsockets</code> Max sockets <code>16384</code> <code>vm.kmem_size</code> Kernel memory size <code>512m</code> <code>vm.kmem_size_max</code> Max kernel memory <code>2g</code> <code>debug.bootverbose</code> Verbose boot messages <code>1</code> <code>init_path</code> Path to init <code>/sbin/init:/sbin/oinit</code> <code>kernelname</code> Kernel path <code>/boot/kernel/kernel</code>"},{"location":"sys/kern/initialization/#system-call-table","title":"System Call Table","text":"<p>Source: <code>sys/kern/init_sysent.c</code> (generated from <code>syscalls.master</code>)</p>"},{"location":"sys/kern/initialization/#structure","title":"Structure","text":"<p>The system call table is an array of <code>struct sysent</code>:</p> <pre><code>struct sysent {\n    int         sy_narg;        /* Number of arguments */\n    int         sy_flags;       /* Flags (reserved) */\n    sy_call_t   *sy_call;       /* System call handler */\n};\n\n/* The table */\nstruct sysent sysent[] = {\n    { 0, 4, (sy_call_t *)sys_xsyscall },    /* 0 = syscall */\n    { AS(exit_args), 4, (sy_call_t *)sys_exit },  /* 1 = exit */\n    { 0, 4, (sy_call_t *)sys_fork },        /* 2 = fork */\n    { AS(read_args), 8, (sy_call_t *)sys_read },  /* 3 = read */\n    { AS(write_args), 8, (sy_call_t *)sys_write },/* 4 = write */\n    /* ... 500+ more ... */\n};\n</code></pre> <p>AS() macro: <pre><code>#define AS(name) (sizeof(struct name) / sizeof(register_t))\n</code></pre></p> <p>Computes the number of register-sized arguments.</p>"},{"location":"sys/kern/initialization/#system-call-dispatch","title":"System Call Dispatch","text":"<p>When userspace issues a system call:</p> <ol> <li>Trap to kernel (syscall instruction)</li> <li>Kernel trap handler extracts syscall number</li> <li>Lookup in <code>sysent[]</code> table</li> <li>Validate syscall number and argument count</li> <li>Copy arguments from userspace</li> <li>Call <code>sy_call()</code> function</li> <li>Return result to userspace</li> </ol> <p>Example syscalls: <pre><code>/* 1 = exit */\n{ AS(exit_args), 4, (sy_call_t *)sys_exit }\n\nstruct exit_args {\n    int rval;  /* Exit status */\n};\n\nint sys_exit(struct sysmsg *sysmsg, const struct exit_args *uap)\n{\n    /* Never returns */\n    exit1(W_EXITCODE(uap-&gt;rval, 0));\n    /* NOTREACHED */\n}\n</code></pre></p> <pre><code>/* 59 = execve */\n{ AS(execve_args), 4, (sy_call_t *)sys_execve }\n\nstruct execve_args {\n    char *fname;      /* Path to executable */\n    char **argv;      /* Argument vector */\n    char **envv;      /* Environment vector */\n};\n\nint sys_execve(struct sysmsg *sysmsg, const struct execve_args *uap)\n{\n    /* Implemented in kern_exec.c */\n    return kern_execve(...);\n}\n</code></pre>"},{"location":"sys/kern/initialization/#obsolete-syscalls","title":"Obsolete Syscalls","text":"<p>Many syscall numbers are marked obsolete:</p> <pre><code>{ 0, 4, (sy_call_t *)sys_nosys },  /* 8 = __nosys */\n{ 0, 4, (sy_call_t *)sys_nosys },  /* 11 = obsolete execv */\n</code></pre> <p>These return <code>ENOSYS</code> (function not implemented).</p>"},{"location":"sys/kern/initialization/#generating-the-table","title":"Generating the Table","text":"<p>The syscall table is generated from <code>sys/kern/syscalls.master</code>:</p> <pre><code>0   STD     { int syscall(void); }\n1   STD     { void exit(int rval); }\n2   STD     { int fork(void); }\n3   STD     { ssize_t read(int fd, void *buf, size_t nbyte); }\n4   STD     { ssize_t write(int fd, const void *buf, size_t nbyte); }\n...\n</code></pre> <p>Build process: <pre><code>cd sys/kern\nmake sysent\n</code></pre></p> <p>This regenerates: - <code>init_sysent.c</code> - syscall table - <code>syscalls.c</code> - syscall names array - <code>sysproto.h</code> - argument structure declarations - <code>sysmsg.h</code> - message structure declarations - <code>sysunion.h</code> - union of all argument structures</p>"},{"location":"sys/kern/initialization/#global-data-initialization","title":"Global Data Initialization","text":"<p>Function: <code>mi_gdinit()</code> (<code>init_main.c:772</code>)</p> <p>Called for each CPU to initialize per-CPU global data:</p> <pre><code>void mi_gdinit(struct globaldata *gd, int cpuid)\n{\n    /* Initialize systimer queue */\n    TAILQ_INIT(&amp;gd-&gt;gd_systimerq);\n\n    /* Set CPU ID */\n    gd-&gt;gd_sysid_alloc = cpuid;\n    gd-&gt;gd_cpuid = cpuid;\n\n    /* Set CPU mask */\n    CPUMASK_ASSBIT(gd-&gt;gd_cpumask, cpuid);\n    gd-&gt;gd_cpumask_simple = CPUMASK_SIMPLE(cpuid);\n    gd-&gt;gd_cpumask_offset = ...;\n\n    /* Initialize LWKT for this CPU */\n    lwkt_gdinit(gd);\n\n    /* Initialize VM map entry reserve */\n    vm_map_entry_reserve_cpu_init(gd);\n\n    /* Initialize sleep queue */\n    if (gd-&gt;gd_cpuid == 0)\n        sleep_early_gdinit(gd);\n    else\n        sleep_gdinit(gd);\n\n    /* Initialize slab allocator for this CPU */\n    slab_gdinit(gd);\n\n    /* Add CPU to global mask */\n    ATOMIC_CPUMASK_ORBIT(usched_global_cpumask, cpuid);\n\n    /* Set up vmstats pointer */\n    gd-&gt;gd_vmstats = vmstats;\n}\n</code></pre> <p>This is called: - Once for CPU 0 during early boot - Once for each additional CPU during SMP initialization</p>"},{"location":"sys/kern/initialization/#kernel-process-map-kpmap","title":"Kernel Process Map (kpmap)","text":"<p>Function: <code>kpmap_init()</code> (<code>init_main.c:744</code>)</p> <p>Initializes a shared read-only page mapped into all processes:</p> <pre><code>static void kpmap_init(const void *udata)\n{\n    /* Allocate page-aligned kpmap structure */\n    kpmap = kmalloc(roundup2(sizeof(*kpmap), PAGE_SIZE),\n                    M_TEMP, M_ZERO | M_WAITOK);\n\n    /* Set up header describing available fields */\n    kpmap-&gt;header[0].type = UKPTYPE_VERSION;\n    kpmap-&gt;header[0].offset = offsetof(struct sys_kpmap, version);\n\n    kpmap-&gt;header[1].type = KPTYPE_UPTICKS;\n    kpmap-&gt;header[1].offset = offsetof(struct sys_kpmap, upticks);\n\n    kpmap-&gt;header[2].type = KPTYPE_TS_UPTIME;\n    kpmap-&gt;header[2].offset = offsetof(struct sys_kpmap, ts_uptime);\n\n    kpmap-&gt;header[3].type = KPTYPE_TS_REALTIME;\n    kpmap-&gt;header[3].offset = offsetof(struct sys_kpmap, ts_realtime);\n\n    kpmap-&gt;header[4].type = KPTYPE_TSC_FREQ;\n    kpmap-&gt;header[4].offset = offsetof(struct sys_kpmap, tsc_freq);\n\n    kpmap-&gt;header[5].type = KPTYPE_TICK_FREQ;\n    kpmap-&gt;header[5].offset = offsetof(struct sys_kpmap, tick_freq);\n\n    kpmap-&gt;header[6].type = KPTYPE_FAST_GTOD;\n    kpmap-&gt;header[6].offset = offsetof(struct sys_kpmap, fast_gtod);\n\n    kpmap-&gt;version = KPMAP_VERSION;\n}\nSYSINIT(kpmapinit, SI_BOOT1_POST, SI_ORDER_FIRST, kpmap_init, NULL);\n</code></pre> <p>This page is later mapped into userspace, allowing fast access to: - System uptime - Real-time clock - TSC frequency - Tick frequency - Fast gettimeofday</p> <p>This avoids syscall overhead for time queries.</p>"},{"location":"sys/kern/initialization/#summary","title":"Summary","text":"<p>DragonFly's kernel initialization follows a well-orchestrated sequence:</p> <ol> <li>Machine-dependent setup - CPU, memory, interrupts</li> <li>mi_proc0init() - Initialize proc0, thread0, lwp0</li> <li>mi_startup() - Execute SYSINIT functions in order</li> <li>proc0_init() - Complete proc0 setup</li> <li>create_init() - Fork init process (PID 1)</li> <li>kick_init() - Start init process</li> <li>start_init() - Exec /sbin/init</li> </ol> <p>Key mechanisms: - SYSINIT framework: Ordered subsystem initialization - Kernel parameters: Computed based on physical memory - Environment: Boot-time configuration key-value store - System call table: Dispatch table for userspace\u2192kernel transitions</p> <p>After initialization completes, the init process becomes the first userspace process, and normal system operation begins.</p>"},{"location":"sys/kern/ipc/","title":"IPC &amp; Sockets Overview","text":"<p>DragonFly BSD provides multiple inter-process communication mechanisms, from low-level kernel primitives to POSIX-standard APIs. This section documents the kernel implementation of these facilities.</p>"},{"location":"sys/kern/ipc/#socket-layer","title":"Socket Layer","text":"<p>The socket abstraction provides a uniform interface for network and local communication:</p> <ul> <li>Mbufs - Network memory buffer management</li> <li>Sockets - Socket API implementation and socket buffers</li> <li>Unix Domain Sockets - Local IPC via filesystem namespace</li> <li>Protocol Dispatch - Protocol family registration and netisr</li> </ul>"},{"location":"sys/kern/ipc/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    USER[\"User Space\"]\n\n    USER --&gt;|\"socket(), bind(), connect(), send(), recv()\"| SOCKET\n\n    subgraph SOCKET[\"Socket Layer\"]\n        STRUCT[\"socketstruct\"]\n        SOCKBUF[\"sockbuf(SSB)\"]\n        PROTOSW[\"protoswdispatch\"]\n        STRUCT --- SOCKBUF\n    end\n\n    PROTOSW --&gt; UNIX[\"UnixDomain\"]\n    PROTOSW --&gt; TCP[\"TCP/IP\"]\n    PROTOSW --&gt; UDP[\"UDP/IP\"]\n</code></pre>"},{"location":"sys/kern/ipc/#pipes","title":"Pipes","text":"<p>Traditional Unix pipes for unidirectional byte streams between related processes:</p> <ul> <li>Pipes - pipe() system call, direct data transfer optimization</li> </ul>"},{"location":"sys/kern/ipc/#message-queues","title":"Message Queues","text":"<p>Two message queue implementations:</p> <ul> <li>POSIX Message Queues - mq_open(), priority-based queuing</li> <li>System V Message Queues - msgget(), msgctl(), msgsnd(), msgrcv()</li> </ul>"},{"location":"sys/kern/ipc/#system-v-ipc","title":"System V IPC","text":"<p>Classic System V IPC primitives:</p> <ul> <li>System V Message Queues - Message passing</li> <li>System V Semaphores - Counting semaphores with undo support</li> <li>System V Shared Memory - Shared memory segments</li> </ul> <p>All System V IPC facilities share common permission checking via <code>ipcperm()</code> and use integer keys for namespace management.</p>"},{"location":"sys/kern/ipc/#comparison","title":"Comparison","text":"Mechanism Scope Data Model Persistence Best For Pipes Related processes Byte stream Process lifetime Simple parent-child Unix Sockets Local system Stream/Datagram Filesystem Local services, fd passing POSIX Mqueue System-wide Messages Kernel (can persist) Priority messaging SysV Mqueue System-wide Messages Kernel Legacy compatibility SysV Semaphores System-wide Counters Kernel Process synchronization SysV Shm System-wide Raw memory Kernel High-bandwidth sharing TCP/UDP Sockets Network Stream/Datagram Connection lifetime Network communication"},{"location":"sys/kern/ipc/#key-source-files","title":"Key Source Files","text":"File Description <code>sys/kern/uipc_socket.c</code> Socket layer core <code>sys/kern/uipc_usrreq.c</code> Unix domain sockets <code>sys/kern/uipc_mbuf.c</code> Mbuf allocation <code>sys/kern/sys_pipe.c</code> Pipe implementation <code>sys/kern/sys_mqueue.c</code> POSIX message queues <code>sys/kern/sysv_msg.c</code> System V messages <code>sys/kern/sysv_sem.c</code> System V semaphores <code>sys/kern/sysv_shm.c</code> System V shared memory"},{"location":"sys/kern/kevent/","title":"kqueue/kevent Event Notification","text":"<p>The kqueue/kevent subsystem provides a scalable, unified event notification mechanism for monitoring file descriptors, processes, signals, timers, and user-defined events. It supersedes traditional <code>select()</code> and <code>poll()</code> interfaces with O(1) event delivery and extensible filter-based architecture.</p> <p>Source files:</p> <ul> <li><code>sys/kern/kern_event.c</code> - Core kqueue/kevent implementation (2,133 lines)</li> <li><code>sys/sys/event.h</code> - Public API definitions and structures (285 lines)</li> </ul>"},{"location":"sys/kern/kevent/#overview","title":"Overview","text":""},{"location":"sys/kern/kevent/#design-philosophy","title":"Design Philosophy","text":"<p>The kqueue mechanism addresses fundamental limitations of <code>select()</code> and <code>poll()</code>:</p> Aspect select/poll kqueue Registration Per-call Persistent Scalability O(n) per call O(1) delivery Event types FD-centric Unified filters Edge/level Level only Both supported Extensibility None Filter plugins <p>Key design principles:</p> <ol> <li>Separation of registration and retrieval - Events are registered once    and remain active until explicitly removed</li> <li>Filter abstraction - All event sources use a common filter interface</li> <li>Edge-triggered delivery - Events report state changes, not just state</li> <li>Kernel note (knote) - Each registration creates a persistent kernel    object tracking the event source</li> </ol>"},{"location":"sys/kern/kevent/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph UserSpace[\"User Space\"]\n        kevent[\"kevent()\"]\n        changelist[\"changelisteventlist\"]\n    end\n\n    subgraph KernelSpace[\"Kernel Space\"]\n        sys_kevent[\"sys_kevent()\"]\n        kqueue_register[\"kqueue_register\"]\n\n        subgraph kqueue[\"struct kqueue\"]\n            kq_knlist[\"kq_knlist (active knotes)\"]\n            kq_knpend[\"kq_knpend (pending events)\"]\n            kq_count[\"kq_count (pending count)\"]\n        end\n\n        subgraph knotes[\"knotes\"]\n            knote1[\"struct knote(EVFILT_READ)\"]\n            knote2[\"struct knote(EVFILT_PROC)\"]\n            knote3[\"struct knote(EVFILT_TIMER)\"]\n        end\n\n        subgraph filterops[\"filterops\"]\n            fops1[\"filteropsf_attachf_detachf_event\"]\n            fops2[\"filteropsf_attachf_detachf_event\"]\n            fops3[\"filteropsf_attachf_detachf_event\"]\n        end\n\n        subgraph sources[\"Event Sources\"]\n            src1[\"file/socket\"]\n            src2[\"process\"]\n            src3[\"timer callout\"]\n        end\n    end\n\n    kevent --&gt; sys_kevent\n    changelist --&gt; kevent\n    sys_kevent --&gt; kqueue_register\n    kqueue_register --&gt; kqueue\n    kqueue --&gt; knote1 &amp; knote2 &amp; knote3\n    knote1 --&gt; fops1 --&gt; src1\n    knote2 --&gt; fops2 --&gt; src2\n    knote3 --&gt; fops3 --&gt; src3\n</code></pre>"},{"location":"sys/kern/kevent/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/kevent/#struct-kevent-user-facing","title":"struct kevent (User-Facing)","text":"<p>The user-visible event structure passed to/from <code>kevent()</code>:</p> <pre><code>/* sys/sys/event.h:70 */\nstruct kevent {\n    uintptr_t   ident;      /* identifier for this event */\n    short       filter;     /* filter for event */\n    u_short     flags;      /* action flags for kqueue */\n    u_int       fflags;     /* filter-specific flags */\n    intptr_t    data;       /* filter-specific data value */\n    void        *udata;     /* opaque user data identifier */\n};\n</code></pre> <p>Field semantics vary by filter type:</p> Field EVFILT_READ EVFILT_PROC EVFILT_TIMER ident File descriptor Process ID Arbitrary ID fflags (unused) NOTE_EXIT, etc. NOTE_SECONDS, etc. data Bytes available Exit status Expirations"},{"location":"sys/kern/kevent/#struct-knote-kernel-internal","title":"struct knote (Kernel-Internal)","text":"<p>The kernel's representation of a registered event:</p> <pre><code>/* sys/sys/event.h:117 */\nstruct knote {\n    SLIST_ENTRY(knote)  kn_link;    /* for fd/object list */\n    TAILQ_ENTRY(knote)  kn_kqlink;  /* for kq_knlist */\n    SLIST_ENTRY(knote)  kn_next;    /* for kqinfo */\n    TAILQ_ENTRY(knote)  kn_tqe;     /* for kq_knpend */\n    struct kqueue       *kn_kq;     /* owning kqueue */\n    struct kevent       kn_kevent;  /* copy of user's kevent */\n    int                 kn_status;  /* KN_* status flags */\n    int                 kn_sfflags; /* saved filter flags */\n    intptr_t            kn_sdata;   /* saved data field */\n    union {\n        struct file     *p_fp;      /* file pointer (FILTEROP_ISFD) */\n        struct proc     *p_proc;    /* process pointer */\n        struct kqinfo   *p_kqi;     /* kqinfo pointer */\n    } kn_ptr;\n    struct filterops    *kn_fop;    /* filter operations */\n    caddr_t             kn_hook;    /* filter-private data */\n    int                 kn_lkflags; /* sync lock flags */\n};\n\n#define kn_id       kn_kevent.ident\n#define kn_filter   kn_kevent.filter\n#define kn_flags    kn_kevent.flags\n#define kn_fflags   kn_kevent.fflags\n#define kn_data     kn_kevent.data\n#define kn_fp       kn_ptr.p_fp\n</code></pre>"},{"location":"sys/kern/kevent/#struct-kqueue","title":"struct kqueue","text":"<p>The kqueue descriptor structure:</p> <pre><code>/* sys/kern/kern_event.c:111 */\nstruct kqueue {\n    struct kqinfo       kq_kqinfo;      /* knotes attached to kqueue */\n    TAILQ_HEAD(, knote) kq_knpend;      /* pending knotes */\n    TAILQ_HEAD(, knote) kq_knlist;      /* all knotes for this kqueue */\n    int                 kq_count;       /* number of pending events */\n    int                 kq_state;       /* KQ_* state flags */\n    struct sigio        *kq_sigio;      /* for SIGIO delivery */\n    struct filedesc     *kq_fdp;        /* file descriptor table */\n    struct thread       *kq_sleep_owner;/* sleeping thread */\n    int                 kq_sleep_cnt;   /* sleep reference count */\n};\n\n/* kqueue state flags */\n#define KQ_ASYNC        0x0002  /* async I/O in progress */\n#define KQ_SLEEP        0x0004  /* kqueue is sleeping */\n</code></pre>"},{"location":"sys/kern/kevent/#struct-filterops","title":"struct filterops","text":"<p>Filter operation vectors defining event source behavior:</p> <pre><code>/* sys/sys/event.h:91 */\nstruct filterops {\n    u_short f_flags;                            /* FILTEROP_* flags */\n    int     (*f_attach)(struct knote *kn);      /* attach to event source */\n    void    (*f_detach)(struct knote *kn);      /* detach from source */\n    int     (*f_event)(struct knote *kn, long hint); /* check/filter event */\n};\n\n/* filterops flags */\n#define FILTEROP_ISFD       0x0001  /* ident is a file descriptor */\n#define FILTEROP_MPSAFE     0x0002  /* filter is MP-safe */\n</code></pre>"},{"location":"sys/kern/kevent/#knote-status-flags","title":"knote Status Flags","text":"<pre><code>/* sys/sys/event.h:106 */\n#define KN_ACTIVE       0x0001  /* event has been triggered */\n#define KN_QUEUED       0x0002  /* knote is on kq_knpend queue */\n#define KN_DISABLED     0x0004  /* event is disabled */\n#define KN_DETACHED     0x0008  /* knote detached from source */\n#define KN_REPROCESS    0x0010  /* force reprocessing after release */\n#define KN_DELETING     0x0020  /* knote being deleted */\n#define KN_PROCESSING   0x0040  /* event processing in progress */\n#define KN_WAITING      0x0080  /* thread waiting on processing */\n</code></pre>"},{"location":"sys/kern/kevent/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/kevent/#kqueue-create-event-queue","title":"kqueue() - Create Event Queue","text":"<pre><code>int kqueue(void);\n</code></pre> <p>Creates a new kqueue and returns a file descriptor:</p> <pre><code>/* sys/kern/kern_event.c:722 */\nint\nsys_kqueue(struct sysmsg *sysmsg, const struct kqueue_args *uap)\n{\n    struct thread *td = curthread;\n    struct kqueue *kq;\n    struct file *fp;\n    int fd, error;\n\n    error = falloc(td-&gt;td_lwp, &amp;fp, &amp;fd);\n    if (error)\n        return (error);\n\n    kq = kmalloc(sizeof(*kq), M_KQUEUE, M_WAITOK | M_ZERO);\n    TAILQ_INIT(&amp;kq-&gt;kq_knpend);\n    TAILQ_INIT(&amp;kq-&gt;kq_knlist);\n    kq-&gt;kq_fdp = td-&gt;td_proc-&gt;p_fd;\n\n    fp-&gt;f_type = DTYPE_KQUEUE;\n    fp-&gt;f_flag = FREAD | FWRITE;\n    fp-&gt;f_ops = &amp;kqueueops;\n    fp-&gt;f_data = kq;\n\n    fsetfd(kq-&gt;kq_fdp, fp, fd);\n    fdrop(fp);\n    sysmsg-&gt;sysmsg_result = fd;\n    return (0);\n}\n</code></pre> <p>The kqueue file descriptor supports:</p> <ul> <li><code>close()</code> - Destroys kqueue and all registered knotes</li> <li><code>kevent()</code> - Register and retrieve events</li> <li><code>poll()</code>/<code>select()</code> - Check for pending events</li> <li><code>ioctl(FIOASYNC)</code> - Enable async notification</li> </ul>"},{"location":"sys/kern/kevent/#kevent-register-and-retrieve-events","title":"kevent() - Register and Retrieve Events","text":"<pre><code>int kevent(int kq, const struct kevent *changelist, int nchanges,\n           struct kevent *eventlist, int nevents,\n           const struct timespec *timeout);\n</code></pre> <p>The primary interface for event registration and retrieval:</p> <p>Parameters:</p> Parameter Description <code>kq</code> kqueue file descriptor <code>changelist</code> Array of events to register/modify <code>nchanges</code> Number of changes to process <code>eventlist</code> Array to receive triggered events <code>nevents</code> Maximum events to return <code>timeout</code> Wait timeout (NULL = infinite, 0 = poll) <p>Return value: Number of events placed in <code>eventlist</code>, or -1 on error.</p> <p>Implementation flow:</p> <pre><code>/* sys/kern/kern_event.c:1053 */\nint\nsys_kevent(struct sysmsg *sysmsg, const struct kevent_args *uap)\n{\n    struct thread *td = curthread;\n    struct timespec ts, *tsp;\n    struct kqueue *kq;\n    struct file *fp;\n    struct kevent_copyin_args *kap, ka;\n    int error;\n\n    /* Validate and get kqueue */\n    fp = holdfp(td, uap-&gt;fd, -1);\n    if (fp == NULL)\n        return (EBADF);\n    if (fp-&gt;f_type != DTYPE_KQUEUE) {\n        fdrop(fp);\n        return (EBADF);\n    }\n    kq = fp-&gt;f_data;\n\n    /* Copy timeout from user space */\n    if (uap-&gt;timeout != NULL) {\n        error = copyin(uap-&gt;timeout, &amp;ts, sizeof(ts));\n        if (error)\n            goto done;\n        tsp = &amp;ts;\n    } else {\n        tsp = NULL;\n    }\n\n    /* Set up copyin/copyout args */\n    ka.ka_kq = kq;\n    ka.ka_uap = uap;\n\n    /* Process changes and scan for events */\n    error = kern_kevent(kq, uap-&gt;nchanges, &amp;ka, kqueue_copyin,\n                        uap-&gt;nevents, &amp;ka, kqueue_copyout, tsp);\n\n    sysmsg-&gt;sysmsg_result = ka.ka_nevents;\ndone:\n    fdrop(fp);\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#kern_kevent-core-processing","title":"kern_kevent() - Core Processing","text":"<p>The internal implementation processes changes then scans for events:</p> <pre><code>/* sys/kern/kern_event.c:796 */\nint\nkern_kevent(struct kqueue *kq, int nchanges, void *uchange,\n            kevent_copyin_fn *kcopy_in, int nevents, void *uevent,\n            kevent_copyout_fn *kcopy_out, struct timespec *tsp)\n{\n    struct kevent kev;\n    int i, n, error;\n\n    /* Phase 1: Process changelist */\n    for (i = 0; i &lt; nchanges; i++) {\n        error = kcopy_in(uchange, &amp;kev, i);\n        if (error)\n            break;\n\n        kev.flags &amp;= ~EV_SYSFLAGS;\n        error = kqueue_register(kq, &amp;kev);\n\n        if (error || (kev.flags &amp; EV_RECEIPT)) {\n            if (nevents &gt; 0) {\n                kev.flags = EV_ERROR;\n                kev.data = error;\n                kcopy_out(uevent, &amp;kev, n++);\n                nevents--;\n                error = 0;\n            }\n        }\n    }\n\n    /* Phase 2: Scan for pending events */\n    if (nevents &gt; 0 &amp;&amp; error == 0) {\n        error = kqueue_scan(kq, nevents, uevent, kcopy_out, tsp, &amp;n);\n    }\n\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#event-flags","title":"Event Flags","text":""},{"location":"sys/kern/kevent/#action-flags","title":"Action Flags","text":"<p>Specify operations when registering events:</p> Flag Value Description <code>EV_ADD</code> 0x0001 Add event to kqueue (or modify existing) <code>EV_DELETE</code> 0x0002 Remove event from kqueue <code>EV_ENABLE</code> 0x0004 Enable event for reporting <code>EV_DISABLE</code> 0x0008 Disable event (keep registered)"},{"location":"sys/kern/kevent/#behavior-flags","title":"Behavior Flags","text":"<p>Control event delivery behavior:</p> Flag Value Description <code>EV_ONESHOT</code> 0x0010 Delete after first delivery <code>EV_CLEAR</code> 0x0020 Clear state after retrieval <code>EV_RECEIPT</code> 0x0040 Force EV_ERROR return on success <code>EV_DISPATCH</code> 0x0080 Disable after delivery"},{"location":"sys/kern/kevent/#return-flags","title":"Return Flags","text":"<p>Set by kernel in returned events:</p> Flag Value Description <code>EV_EOF</code> 0x8000 EOF condition detected <code>EV_ERROR</code> 0x4000 Error occurred (data = errno) <code>EV_NODATA</code> 0x1000 EOF with no more data"},{"location":"sys/kern/kevent/#flag-combinations","title":"Flag Combinations","text":"<p>Common usage patterns:</p> <pre><code>/* One-shot read notification */\nEV_SET(&amp;kev, fd, EVFILT_READ, EV_ADD | EV_ONESHOT, 0, 0, NULL);\n\n/* Level-triggered (re-arm after each retrieval) */\nEV_SET(&amp;kev, fd, EVFILT_READ, EV_ADD | EV_CLEAR, 0, 0, NULL);\n\n/* Edge-triggered (default - reports state changes) */\nEV_SET(&amp;kev, fd, EVFILT_READ, EV_ADD, 0, 0, NULL);\n\n/* Disabled registration (enable later) */\nEV_SET(&amp;kev, fd, EVFILT_READ, EV_ADD | EV_DISABLE, 0, 0, NULL);\n\n/* Dispatch mode (disable after delivery, re-enable to re-arm) */\nEV_SET(&amp;kev, fd, EVFILT_READ, EV_ADD | EV_DISPATCH, 0, 0, NULL);\n</code></pre>"},{"location":"sys/kern/kevent/#filter-types","title":"Filter Types","text":""},{"location":"sys/kern/kevent/#evfilt_read-1-read-availability","title":"EVFILT_READ (-1) - Read Availability","text":"<p>Reports when data is available for reading:</p> Field Meaning ident File descriptor data Bytes available to read fflags NOTE_LOWAT (set low water mark) <p>For different descriptor types:</p> <ul> <li>Sockets: Bytes in receive buffer</li> <li>Pipes: Bytes in pipe buffer</li> <li>FIFOs: Bytes available</li> <li>TTYs: Input queue size</li> <li>Vnodes: (offset - filesize), EV_EOF at EOF</li> </ul>"},{"location":"sys/kern/kevent/#evfilt_write-2-write-availability","title":"EVFILT_WRITE (-2) - Write Availability","text":"<p>Reports when writing won't block:</p> Field Meaning ident File descriptor data Space available in write buffer fflags NOTE_LOWAT (set low water mark)"},{"location":"sys/kern/kevent/#evfilt_vnode-4-vnode-events","title":"EVFILT_VNODE (-4) - Vnode Events","text":"<p>Monitor filesystem object changes:</p> fflags Description NOTE_DELETE Vnode deleted NOTE_WRITE Write to file NOTE_EXTEND File extended NOTE_ATTRIB Attributes changed NOTE_LINK Link count changed NOTE_RENAME Vnode renamed NOTE_REVOKE Access revoked"},{"location":"sys/kern/kevent/#evfilt_proc-5-process-events","title":"EVFILT_PROC (-5) - Process Events","text":"<p>Monitor process state changes:</p> <pre><code>/* sys/kern/kern_event.c:394 */\nstatic struct filterops proc_filtops = {\n    .f_flags = 0,               /* not fd-based */\n    .f_attach = filt_procattach,\n    .f_detach = filt_procdetach,\n    .f_event = filt_proc,\n};\n</code></pre> fflags (input) Description NOTE_EXIT Process exited NOTE_FORK Process forked NOTE_EXEC Process exec'd NOTE_TRACK Follow across fork fflags (output) Description NOTE_EXIT Exit status in data NOTE_FORK Child PID in data NOTE_EXEC (no additional data) NOTE_CHILD Followed child (NOTE_TRACK) NOTE_TRACKERR Couldn't follow fork <p>Implementation:</p> <pre><code>/* sys/kern/kern_event.c:468 */\nstatic int\nfilt_proc(struct knote *kn, long hint)\n{\n    u_int event;\n\n    if (kn-&gt;kn_sfflags &amp; NOTE_TRACK) {\n        /* Handle fork tracking */\n    }\n\n    event = (u_int)hint &amp; NOTE_PCTRLMASK;\n    if (event == NOTE_EXIT) {\n        kn-&gt;kn_status |= KN_DETACHED;\n        kn-&gt;kn_flags |= EV_EOF | EV_NODATA;\n        kn-&gt;kn_data = kn-&gt;kn_ptr.p_proc-&gt;p_xstat;\n        return (1);\n    }\n\n    if (kn-&gt;kn_sfflags &amp; event) {\n        kn-&gt;kn_fflags |= event;\n        return (1);\n    }\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#evfilt_signal-6-signal-events","title":"EVFILT_SIGNAL (-6) - Signal Events","text":"<p>Monitor signal delivery to the process:</p> Field Meaning ident Signal number data Delivery count since last retrieval <p>Note: Signals are still delivered normally; this provides notification only.</p>"},{"location":"sys/kern/kevent/#evfilt_timer-7-timer-events","title":"EVFILT_TIMER (-7) - Timer Events","text":"<p>Create kernel timers:</p> <pre><code>/* sys/kern/kern_event.c:571 */\nstatic struct filterops timer_filtops = {\n    .f_flags = 0,\n    .f_attach = filt_timerattach,\n    .f_detach = filt_timerdetach,\n    .f_event = filt_timer,\n};\n</code></pre> fflags Description NOTE_SECONDS data is in seconds NOTE_MSECONDS data is in milliseconds NOTE_USECONDS data is in microseconds NOTE_NSECONDS data is in nanoseconds NOTE_ABSTIME Absolute time (not interval) NOTE_ONESHOT Fire once (alternative to EV_ONESHOT) Field Meaning ident User-chosen timer ID data Number of expirations <p>Implementation:</p> <pre><code>/* sys/kern/kern_event.c:607 */\nstatic int\nfilt_timerattach(struct knote *kn)\n{\n    struct callout *calloutp;\n    struct timeval tv;\n    int tticks;\n\n    /* Convert timeout to ticks */\n    tticks = filt_timer_ticks(kn-&gt;kn_sdata, kn-&gt;kn_sfflags);\n\n    /* Allocate callout */\n    calloutp = kmalloc(sizeof(*calloutp), M_KQUEUE, M_WAITOK);\n    callout_init_mp(calloutp);\n    kn-&gt;kn_hook = calloutp;\n\n    /* Start timer */\n    callout_reset(calloutp, tticks, filt_timerexpire, kn);\n\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#evfilt_user-9-user-triggered-events","title":"EVFILT_USER (-9) - User-Triggered Events","text":"<p>User-controlled event triggering:</p> <pre><code>/* sys/kern/kern_event.c:686 */\nstatic struct filterops user_filtops = {\n    .f_flags = 0,\n    .f_attach = filt_userattach,\n    .f_detach = filt_userdetach,\n    .f_event = filt_user,\n};\n</code></pre> fflags Description NOTE_TRIGGER Trigger the event NOTE_FFNOP No fflags operation NOTE_FFAND AND fflags NOTE_FFOR OR fflags NOTE_FFCOPY Copy fflags NOTE_FFCTRLMASK Mask for control flags NOTE_FFLAGSMASK Mask for user flags <p>Used for inter-thread signaling without pipes or signals.</p>"},{"location":"sys/kern/kevent/#evfilt_fs-10-filesystem-events","title":"EVFILT_FS (-10) - Filesystem Events","text":"<p>Monitor filesystem mount/unmount events:</p> <pre><code>/* sys/kern/kern_event.c:386 */\nstatic struct filterops fs_filtops = {\n    .f_flags = 0,\n    .f_attach = filt_fsattach,\n    .f_detach = filt_fsdetach,\n    .f_event = filt_fs,\n};\n</code></pre> fflags Description NOTE_FSMOUNT Filesystem mounted NOTE_FSUNMOUNT Filesystem unmounted NOTE_FSUNMOUNTING Unmount in progress"},{"location":"sys/kern/kevent/#evfilt_except-8-exceptional-conditions","title":"EVFILT_EXCEPT (-8) - Exceptional Conditions","text":"<p>Monitor out-of-band/exceptional conditions:</p> Field Meaning ident File descriptor fflags NOTE_OOB (out-of-band data)"},{"location":"sys/kern/kevent/#event-registration","title":"Event Registration","text":""},{"location":"sys/kern/kevent/#kqueue_register","title":"kqueue_register()","text":"<p>Registers a single event in the kqueue:</p> <pre><code>/* sys/kern/kern_event.c:1152 */\nstatic int\nkqueue_register(struct kqueue *kq, struct kevent *kev)\n{\n    struct filedesc *fdp;\n    struct filterops *fops;\n    struct file *fp;\n    struct knote *kn;\n    int error;\n\n    /* Look up filter operations */\n    if (kev-&gt;filter &lt; 0 &amp;&amp; kev-&gt;filter + EVFILT_SYSCOUNT &gt;= 0)\n        fops = sysfilt_ops[~kev-&gt;filter];\n    else\n        return (EINVAL);\n\n    /* Handle file descriptor based filters */\n    if (fops-&gt;f_flags &amp; FILTEROP_ISFD) {\n        fp = holdfp(curthread, kev-&gt;ident, -1);\n        if (fp == NULL)\n            return (EBADF);\n    }\n\n    /* Look for existing knote */\n    lwkt_getpooltoken(kq);\n    kn = kqueue_find(kq, kev-&gt;filter, kev-&gt;ident);\n\n    if (kn == NULL &amp;&amp; (kev-&gt;flags &amp; EV_ADD)) {\n        /* Create new knote */\n        kn = knote_alloc();\n        kn-&gt;kn_kq = kq;\n        kn-&gt;kn_fop = fops;\n        kn-&gt;kn_kevent = *kev;\n        kn-&gt;kn_sfflags = kev-&gt;fflags;\n        kn-&gt;kn_sdata = kev-&gt;data;\n\n        if (fops-&gt;f_flags &amp; FILTEROP_ISFD) {\n            kn-&gt;kn_fp = fp;\n        }\n\n        /* Attach to event source */\n        error = filter_attach(kn);\n        if (error) {\n            knote_free(kn);\n            goto done;\n        }\n\n        /* Add to kqueue's knote list */\n        TAILQ_INSERT_TAIL(&amp;kq-&gt;kq_knlist, kn, kn_kqlink);\n\n    } else if (kn != NULL) {\n        /* Modify existing knote */\n        if (kev-&gt;flags &amp; EV_DELETE) {\n            kn-&gt;kn_status |= KN_DELETING;\n            knote_detach_and_drop(kn);\n            kn = NULL;\n        } else {\n            /* Update flags */\n            if (kev-&gt;flags &amp; EV_DISABLE)\n                kn-&gt;kn_status |= KN_DISABLED;\n            if (kev-&gt;flags &amp; EV_ENABLE)\n                kn-&gt;kn_status &amp;= ~KN_DISABLED;\n            kn-&gt;kn_kevent.udata = kev-&gt;udata;\n\n            /* Re-evaluate filter */\n            if (filter_event(kn, 0))\n                KNOTE_ACTIVATE(kn);\n        }\n    } else if (!(kev-&gt;flags &amp; EV_ADD)) {\n        error = ENOENT;\n    }\n\ndone:\n    lwkt_relpooltoken(kq);\n    if (fp)\n        fdrop(fp);\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#filter-attachment","title":"Filter Attachment","text":"<p>When a knote is created, the filter's <code>f_attach</code> is called:</p> <pre><code>/* sys/kern/kern_event.c:1740 */\nstatic int\nfilter_attach(struct knote *kn)\n{\n    int error;\n\n    if (kn-&gt;kn_fop-&gt;f_flags &amp; FILTEROP_ISFD) {\n        /* Delegate to file's kqfilter operation */\n        error = fo_kqfilter(kn-&gt;kn_fp, kn);\n    } else {\n        /* Call filter's attach directly */\n        error = kn-&gt;kn_fop-&gt;f_attach(kn);\n    }\n    return (error);\n}\n</code></pre> <p>For file-based filters, <code>fo_kqfilter()</code> is called which eventually calls the file type's specific kqfilter routine (e.g., <code>soo_kqfilter()</code> for sockets).</p>"},{"location":"sys/kern/kevent/#event-delivery","title":"Event Delivery","text":""},{"location":"sys/kern/kevent/#kqueue_scan","title":"kqueue_scan()","text":"<p>Scans the pending queue and returns triggered events:</p> <pre><code>/* sys/kern/kern_event.c:1478 */\nstatic int\nkqueue_scan(struct kqueue *kq, int maxevents, void *uevent,\n            kevent_copyout_fn *kcopy_out, struct timespec *tsp, int *nresp)\n{\n    struct knote *kn, marker;\n    struct kevent kev;\n    struct timeval atv, rtv, ttv;\n    int count, timeout, error;\n\n    count = maxevents;\n    error = 0;\n\n    /* Calculate timeout */\n    if (tsp != NULL) {\n        if (tsp-&gt;tv_sec == 0 &amp;&amp; tsp-&gt;tv_nsec == 0)\n            timeout = 0;  /* Poll mode */\n        else\n            timeout = tstohz_high(tsp);\n    } else {\n        timeout = INFSLP;  /* Wait forever */\n    }\n\n    lwkt_getpooltoken(kq);\n\nretry:\n    /* Insert marker to track our position */\n    TAILQ_INSERT_TAIL(&amp;kq-&gt;kq_knpend, &amp;marker, kn_tqe);\n\n    while (count &gt; 0) {\n        /* Get next pending knote (before marker) */\n        kn = TAILQ_FIRST(&amp;kq-&gt;kq_knpend);\n        if (kn == &amp;marker) {\n            TAILQ_REMOVE(&amp;kq-&gt;kq_knpend, &amp;marker, kn_tqe);\n\n            if (count == maxevents &amp;&amp; timeout != 0) {\n                /* No events yet, sleep */\n                error = kqueue_sleep(kq, timeout);\n                if (error == 0 || error == EWOULDBLOCK)\n                    goto retry;\n                break;\n            }\n            break;\n        }\n        TAILQ_REMOVE(&amp;kq-&gt;kq_knpend, kn, kn_tqe);\n        kn-&gt;kn_status &amp;= ~KN_QUEUED;\n\n        /* Acquire knote for processing */\n        if (!knote_acquire(kn))\n            continue;\n\n        /* Skip disabled or deleted knotes */\n        if (kn-&gt;kn_status &amp; (KN_DISABLED | KN_DELETING)) {\n            knote_release(kn);\n            continue;\n        }\n\n        /* Re-evaluate filter */\n        kn-&gt;kn_status |= KN_PROCESSING;\n        if (!filter_event(kn, 0)) {\n            kn-&gt;kn_status &amp;= ~(KN_PROCESSING | KN_ACTIVE);\n            knote_release(kn);\n            continue;\n        }\n\n        /* Copy event to user */\n        kev = kn-&gt;kn_kevent;\n        kev.flags = kn-&gt;kn_flags;\n        kev.fflags = kn-&gt;kn_fflags;\n        kev.data = kn-&gt;kn_data;\n\n        lwkt_relpooltoken(kq);\n        error = kcopy_out(uevent, &amp;kev, *nresp);\n        lwkt_getpooltoken(kq);\n\n        if (error)\n            break;\n        (*nresp)++;\n        count--;\n\n        /* Handle flags */\n        if (kn-&gt;kn_flags &amp; EV_ONESHOT) {\n            kn-&gt;kn_status |= KN_DELETING;\n            knote_detach_and_drop(kn);\n        } else if (kn-&gt;kn_flags &amp; EV_CLEAR) {\n            kn-&gt;kn_fflags = 0;\n            kn-&gt;kn_data = 0;\n            kn-&gt;kn_status &amp;= ~(KN_PROCESSING | KN_ACTIVE);\n            knote_release(kn);\n        } else if (kn-&gt;kn_flags &amp; EV_DISPATCH) {\n            kn-&gt;kn_status |= KN_DISABLED;\n            kn-&gt;kn_status &amp;= ~(KN_PROCESSING | KN_ACTIVE);\n            knote_release(kn);\n        } else {\n            /* Re-queue if still active */\n            if (kn-&gt;kn_status &amp; KN_ACTIVE) {\n                TAILQ_INSERT_TAIL(&amp;kq-&gt;kq_knpend, kn, kn_tqe);\n                kn-&gt;kn_status |= KN_QUEUED;\n            }\n            kn-&gt;kn_status &amp;= ~KN_PROCESSING;\n            knote_release(kn);\n        }\n    }\n\n    lwkt_relpooltoken(kq);\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#event-activation","title":"Event Activation","text":"<p>When an event source triggers, it calls <code>knote()</code> to activate knotes:</p> <pre><code>/* sys/kern/kern_event.c:1809 */\nvoid\nknote(struct klist *list, long hint)\n{\n    struct knote *kn;\n\n    SLIST_FOREACH(kn, list, kn_next) {\n        if (filter_event(kn, hint))\n            KNOTE_ACTIVATE(kn);\n    }\n}\n</code></pre> <p>The <code>KNOTE_ACTIVATE</code> macro marks the knote active and queues it:</p> <pre><code>/* sys/kern/kern_event.c:164 */\n#define KNOTE_ACTIVATE(kn) do {                                         \\\n    kn-&gt;kn_status |= KN_ACTIVE;                                         \\\n    if ((kn-&gt;kn_status &amp; (KN_QUEUED | KN_DISABLED)) == 0)               \\\n        knote_enqueue(kn);                                              \\\n} while (0)\n</code></pre> <pre><code>/* sys/kern/kern_event.c:178 */\nstatic void\nknote_enqueue(struct knote *kn)\n{\n    struct kqueue *kq = kn-&gt;kn_kq;\n\n    KKASSERT((kn-&gt;kn_status &amp; KN_QUEUED) == 0);\n    TAILQ_INSERT_TAIL(&amp;kq-&gt;kq_knpend, kn, kn_tqe);\n    kn-&gt;kn_status |= KN_QUEUED;\n    kq-&gt;kq_count++;\n\n    /* Wake up any sleeping threads */\n    if (kq-&gt;kq_state &amp; KQ_SLEEP) {\n        kq-&gt;kq_state &amp;= ~KQ_SLEEP;\n        wakeup(kq);\n    }\n\n    /* Send SIGIO if requested */\n    KNOTE(&amp;kq-&gt;kq_kqinfo.ki_note, 0);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/kevent/#kqueue-locking","title":"Kqueue Locking","text":"<p>Each kqueue is protected by a pool token:</p> <pre><code>lwkt_getpooltoken(kq);  /* Acquire lock */\n/* ... critical section ... */\nlwkt_relpooltoken(kq);  /* Release lock */\n</code></pre> <p>Pool tokens provide per-object serialization without dedicated lock structures.</p>"},{"location":"sys/kern/kevent/#knote-acquirerelease","title":"Knote Acquire/Release","text":"<p>Knote processing is serialized with acquire/release:</p> <pre><code>/* sys/kern/kern_event.c:203 */\nstatic int\nknote_acquire(struct knote *kn)\n{\n    if (kn-&gt;kn_status &amp; KN_PROCESSING) {\n        kn-&gt;kn_status |= KN_WAITING;\n        tsleep(kn, 0, \"kqnote\", 0);\n        /* Knote may have been deleted while waiting */\n        return (0);\n    }\n    kn-&gt;kn_status |= KN_PROCESSING;\n    return (1);\n}\n\n/* sys/kern/kern_event.c:224 */\nstatic void\nknote_release(struct knote *kn)\n{\n    kn-&gt;kn_status &amp;= ~KN_PROCESSING;\n    if (kn-&gt;kn_status &amp; KN_WAITING) {\n        kn-&gt;kn_status &amp;= ~KN_WAITING;\n        wakeup(kn);\n    }\n}\n</code></pre>"},{"location":"sys/kern/kevent/#race-handling","title":"Race Handling","text":"<p>The <code>KN_REPROCESS</code> flag handles races between event activation and processing:</p> <ol> <li>Thread A is processing knote (KN_PROCESSING set)</li> <li>Event source triggers, sets KN_REPROCESS</li> <li>Thread A finishes, sees KN_REPROCESS, re-evaluates filter</li> <li>Ensures no events are lost during processing</li> </ol>"},{"location":"sys/kern/kevent/#mp-safe-filters","title":"MP-Safe Filters","text":"<p>Filters marked <code>FILTEROP_MPSAFE</code> can be called without the kqueue token:</p> <pre><code>/* sys/kern/kern_event.c:1786 */\nstatic int\nfilter_event(struct knote *kn, long hint)\n{\n    if (kn-&gt;kn_fop-&gt;f_flags &amp; FILTEROP_MPSAFE) {\n        return (kn-&gt;kn_fop-&gt;f_event(kn, hint));\n    } else {\n        /* Non-MPSAFE filters need additional serialization */\n        return (kn-&gt;kn_fop-&gt;f_event(kn, hint));\n    }\n}\n</code></pre>"},{"location":"sys/kern/kevent/#integration-points","title":"Integration Points","text":""},{"location":"sys/kern/kevent/#file-descriptor-integration","title":"File Descriptor Integration","text":"<p>File types implement <code>fo_kqfilter</code> to support kqueue:</p> <pre><code>/* Socket kqfilter - sys/kern/uipc_socket.c */\nint\nsoo_kqfilter(struct file *fp, struct knote *kn)\n{\n    struct socket *so = fp-&gt;f_data;\n    struct sockbuf *sb;\n\n    switch (kn-&gt;kn_filter) {\n    case EVFILT_READ:\n        kn-&gt;kn_fop = &amp;soread_filtops;\n        sb = &amp;so-&gt;so_rcv;\n        break;\n    case EVFILT_WRITE:\n        kn-&gt;kn_fop = &amp;sowrite_filtops;\n        sb = &amp;so-&gt;so_snd;\n        break;\n    case EVFILT_EXCEPT:\n        kn-&gt;kn_fop = &amp;soexcept_filtops;\n        sb = &amp;so-&gt;so_rcv;\n        break;\n    default:\n        return (EOPNOTSUPP);\n    }\n\n    kn-&gt;kn_hook = so;\n    SLIST_INSERT_HEAD(&amp;sb-&gt;sb_kq.ki_note, kn, kn_next);\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#socket-integration","title":"Socket Integration","text":"<p>Sockets call <code>KNOTE()</code> when buffer state changes:</p> <pre><code>/* sys/kern/uipc_socket.c - on receive */\nvoid\nsorwakeup(struct socket *so)\n{\n    /* ... */\n    KNOTE(&amp;so-&gt;so_rcv.sb_kq.ki_note, 0);\n}\n\n/* sys/kern/uipc_socket.c - on send buffer space */\nvoid\nsowwakeup(struct socket *so)\n{\n    /* ... */\n    KNOTE(&amp;so-&gt;so_snd.sb_kq.ki_note, 0);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#process-integration","title":"Process Integration","text":"<p>Process state changes trigger EVFILT_PROC notifications:</p> <pre><code>/* sys/kern/kern_exit.c - on exit */\nvoid\nexit1(int rv)\n{\n    /* ... */\n    KNOTE(&amp;p-&gt;p_klist, NOTE_EXIT);\n}\n\n/* sys/kern/kern_fork.c - on fork */\nint\nfork1(struct lwp *lp, int flags, struct proc **procp)\n{\n    /* ... */\n    KNOTE(&amp;p1-&gt;p_klist, NOTE_FORK | p2-&gt;p_pid);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#vnode-integration","title":"Vnode Integration","text":"<p>VFS operations trigger EVFILT_VNODE notifications:</p> <pre><code>/* sys/kern/vfs_subr.c */\nvoid\nvn_knote(struct vnode *vp, int flags)\n{\n    KNOTE(&amp;vp-&gt;v_pollinfo.vpi_kqinfo.ki_note, flags);\n}\n\n/* Called from various VFS operations */\nvn_knote(vp, NOTE_WRITE);   /* On write */\nvn_knote(vp, NOTE_ATTRIB);  /* On chmod/chown */\nvn_knote(vp, NOTE_DELETE);  /* On unlink */\n</code></pre>"},{"location":"sys/kern/kevent/#helper-macros","title":"Helper Macros","text":""},{"location":"sys/kern/kevent/#ev_set","title":"EV_SET","text":"<p>Initialize a kevent structure:</p> <pre><code>/* sys/sys/event.h:85 */\n#define EV_SET(kevp, a, b, c, d, e, f) do {     \\\n    struct kevent *__kevp = (kevp);             \\\n    __kevp-&gt;ident = (a);                        \\\n    __kevp-&gt;filter = (b);                       \\\n    __kevp-&gt;flags = (c);                        \\\n    __kevp-&gt;fflags = (d);                       \\\n    __kevp-&gt;data = (e);                         \\\n    __kevp-&gt;udata = (f);                        \\\n} while (0)\n</code></pre>"},{"location":"sys/kern/kevent/#usage-examples","title":"Usage Examples","text":"<pre><code>struct kevent changes[3];\nstruct kevent events[10];\nint kq, nev;\n\nkq = kqueue();\n\n/* Monitor socket for read/write */\nEV_SET(&amp;changes[0], sockfd, EVFILT_READ, EV_ADD, 0, 0, NULL);\nEV_SET(&amp;changes[1], sockfd, EVFILT_WRITE, EV_ADD, 0, 0, NULL);\n\n/* Monitor file for modifications */\nEV_SET(&amp;changes[2], filefd, EVFILT_VNODE, EV_ADD | EV_CLEAR,\n       NOTE_WRITE | NOTE_DELETE, 0, NULL);\n\n/* Register and wait for events */\nnev = kevent(kq, changes, 3, events, 10, NULL);\n\nfor (int i = 0; i &lt; nev; i++) {\n    if (events[i].flags &amp; EV_ERROR) {\n        /* Error in registration */\n        errno = events[i].data;\n    } else if (events[i].filter == EVFILT_READ) {\n        /* Data available: events[i].data bytes */\n    } else if (events[i].filter == EVFILT_VNODE) {\n        if (events[i].fflags &amp; NOTE_WRITE)\n            /* File was modified */;\n        if (events[i].fflags &amp; NOTE_DELETE)\n            /* File was deleted */;\n    }\n}\n</code></pre>"},{"location":"sys/kern/kevent/#filter-implementation-guide","title":"Filter Implementation Guide","text":"<p>To implement a custom filter:</p>"},{"location":"sys/kern/kevent/#1-define-filter-operations","title":"1. Define Filter Operations","text":"<pre><code>static struct filterops myfilter_filtops = {\n    .f_flags = FILTEROP_MPSAFE,     /* Or 0, or FILTEROP_ISFD */\n    .f_attach = myfilter_attach,\n    .f_detach = myfilter_detach,\n    .f_event = myfilter_event,\n};\n</code></pre>"},{"location":"sys/kern/kevent/#2-implement-attach","title":"2. Implement Attach","text":"<pre><code>static int\nmyfilter_attach(struct knote *kn)\n{\n    struct myobject *obj;\n\n    /* Validate and find object */\n    obj = myobject_find(kn-&gt;kn_id);\n    if (obj == NULL)\n        return (ENOENT);\n\n    /* Store reference */\n    kn-&gt;kn_hook = obj;\n\n    /* Add to object's knote list */\n    SLIST_INSERT_HEAD(&amp;obj-&gt;kq_list, kn, kn_next);\n\n    /* Initial event check */\n    kn-&gt;kn_data = obj-&gt;available;\n    return (obj-&gt;available &gt; 0);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#3-implement-detach","title":"3. Implement Detach","text":"<pre><code>static void\nmyfilter_detach(struct knote *kn)\n{\n    struct myobject *obj = kn-&gt;kn_hook;\n\n    SLIST_REMOVE(&amp;obj-&gt;kq_list, kn, knote, kn_next);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#4-implement-event","title":"4. Implement Event","text":"<pre><code>static int\nmyfilter_event(struct knote *kn, long hint)\n{\n    struct myobject *obj = kn-&gt;kn_hook;\n\n    /* Update event data */\n    kn-&gt;kn_data = obj-&gt;available;\n\n    /* Check filter flags */\n    if (kn-&gt;kn_sfflags &amp; MY_NOTE_THRESHOLD) {\n        return (obj-&gt;available &gt;= kn-&gt;kn_sdata);\n    }\n\n    return (obj-&gt;available &gt; 0);\n}\n</code></pre>"},{"location":"sys/kern/kevent/#5-trigger-events","title":"5. Trigger Events","text":"<pre><code>void\nmyobject_data_ready(struct myobject *obj)\n{\n    obj-&gt;available++;\n    KNOTE(&amp;obj-&gt;kq_list, 0);  /* hint = 0 */\n}\n</code></pre>"},{"location":"sys/kern/kevent/#performance-considerations","title":"Performance Considerations","text":""},{"location":"sys/kern/kevent/#scalability","title":"Scalability","text":"<ul> <li>O(1) registration: Adding/removing events is constant time</li> <li>O(active) retrieval: Only active events are scanned</li> <li>No fd_set copying: Unlike select(), no per-call data copying</li> </ul>"},{"location":"sys/kern/kevent/#best-practices","title":"Best Practices","text":"<ol> <li>Use EV_CLEAR for high-frequency events - Avoids re-registration overhead</li> <li>Use EV_DISPATCH for one-at-a-time processing - Prevents thundering herd</li> <li>Batch changes - Register multiple events in one kevent() call</li> <li>Use appropriate timeouts - Avoid tight polling loops</li> </ol>"},{"location":"sys/kern/kevent/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature kqueue epoll (Linux) poll Edge-triggered Yes Yes No One-shot Yes Yes No Timers Built-in timerfd No Signals Built-in signalfd No User events Yes eventfd No File monitoring Yes inotify No"},{"location":"sys/kern/kevent/#see-also","title":"See Also","text":"<ul> <li>Processes - Process management and lifecycle</li> <li>Signals - Signal handling subsystem</li> <li>IPC Overview - Inter-process communication mechanisms</li> <li>Sockets - Socket implementation</li> <li>LWKT - Lightweight kernel threads and tokens</li> </ul>"},{"location":"sys/kern/kld/","title":"Kernel Linker (KLD)","text":"<p>This document covers the kernel dynamic linker framework (KLD) that supports loading and unloading kernel modules at runtime.</p>"},{"location":"sys/kern/kld/#overview","title":"Overview","text":"<p>The Kernel Linker (KLD) system provides dynamic loading of kernel modules. It handles ELF object files, symbol resolution, relocation, and manages module dependencies. The system supports both:</p> <ul> <li>Preloaded modules - Loaded by the boot loader before the kernel starts</li> <li>Runtime loading - Loaded via the <code>kldload(2)</code> system call</li> </ul> <p>Key features: - ELF shared object and relocatable object file support - Automatic dependency resolution and loading - Symbol export/import between modules and kernel - SYSINIT/SYSUNINIT execution within modules - Reference counting for safe unloading</p>"},{"location":"sys/kern/kld/#key-source-files","title":"Key Source Files","text":"File Purpose <code>sys/kern/kern_linker.c</code> Core linker framework and syscalls <code>sys/kern/kern_module.c</code> Module registration and lifecycle <code>sys/kern/link_elf.c</code> ELF shared object (ET_DYN) loader <code>sys/kern/link_elf_obj.c</code> ELF relocatable object (ET_REL) loader <code>sys/sys/linker.h</code> Linker structures and function prototypes <code>sys/sys/module.h</code> Module metadata and macros"},{"location":"sys/kern/kld/#architecture","title":"Architecture","text":""},{"location":"sys/kern/kld/#linker-classes","title":"Linker Classes","text":"<p>The KLD system uses a class-based architecture to support different object file formats. Each class implements <code>struct linker_class_ops</code> (<code>sys/sys/linker.h:134</code>):</p> <pre><code>struct linker_class_ops {\n    int  (*load_file)(const char *filename, linker_file_t *result);\n    int  (*preload_file)(const char *filename, linker_file_t *result);\n};\n</code></pre> <p>DragonFly BSD registers two ELF linker classes at boot (<code>SI_BOOT2_KLD</code>):</p> <ol> <li>elf32/elf64 (link_elf.c) - For shared objects (ET_DYN type)</li> <li>elf32/elf64 (link_elf_obj.c) - For relocatable objects (ET_REL type)</li> </ol> <p>Classes are tried in order until one successfully loads the file.</p>"},{"location":"sys/kern/kld/#the-linker_file-structure","title":"The linker_file Structure","text":"<p>Every loaded module is represented by a <code>struct linker_file</code> (<code>sys/sys/linker.h:107</code>):</p> <pre><code>struct linker_file {\n    int              refs;        /* reference count */\n    int              userrefs;    /* kldload(2) count */\n    int              flags;\n#define LINKER_FILE_LINKED  0x1   /* file has been fully linked */\n    TAILQ_ENTRY(linker_file) link; /* list of all loaded files */\n    char            *filename;    /* file which was loaded */\n    char            *pathname;    /* file name with full path */\n    int              id;          /* unique id */\n    caddr_t          address;     /* load address */\n    size_t           size;        /* size of file */\n    int              ndeps;       /* number of dependencies */\n    linker_file_t   *deps;        /* list of dependencies */\n    STAILQ_HEAD(, common_symbol) common; /* list of common symbols */\n    TAILQ_HEAD(, module) modules; /* modules in this file */\n    void            *priv;        /* implementation data */\n    struct linker_file_ops *ops;\n};\n</code></pre> <p>File operations provide symbol lookup and unload callbacks:</p> <pre><code>struct linker_file_ops {\n    int   (*lookup_symbol)(linker_file_t, const char *name, c_linker_sym_t *sym);\n    int   (*symbol_values)(linker_file_t, c_linker_sym_t, linker_symval_t *);\n    int   (*search_symbol)(linker_file_t, caddr_t value, c_linker_sym_t *, long *);\n    int   (*preload_finish)(linker_file_t);\n    void  (*unload)(linker_file_t);\n    int   (*lookup_set)(linker_file_t, const char *name, void ***, void ***, int *);\n};\n</code></pre>"},{"location":"sys/kern/kld/#global-state","title":"Global State","text":"<p>The linker maintains several global structures (<code>kern_linker.c:63</code>):</p> <pre><code>linker_file_t linker_current_file;   /* file currently being loaded */\nlinker_file_t linker_kernel_file;    /* the kernel itself */\n\nstatic struct lock llf_lock;         /* lock for the file list */\nstatic struct lock kld_lock;         /* general kld lock */\nstatic linker_class_list_t classes;  /* registered file classes */\nstatic linker_file_list_t linker_files; /* all loaded files */\nstatic int next_file_id = 1;\n</code></pre>"},{"location":"sys/kern/kld/#module-loading","title":"Module Loading","text":""},{"location":"sys/kern/kld/#loading-from-filesystem","title":"Loading from Filesystem","text":"<p>The <code>sys_kldload()</code> syscall (<code>kern_linker.c:782</code>) loads a module:</p> <pre><code>int sys_kldload(struct sysmsg *sysmsg, const struct kldload_args *uap)\n{\n    /* Security checks */\n    if (securelevel &gt; 0 || kernel_mem_readonly)\n        return EPERM;\n    if ((error = caps_priv_check_self(SYSCAP_NOKLD)) != 0)\n        return error;\n\n    /* Determine if file path or module name */\n    if (strchr(file, '/') || strchr(file, '.')) {\n        kldname = file;    /* full path or .ko file */\n        modname = NULL;\n    } else {\n        kldname = NULL;\n        modname = file;    /* module name - search path */\n    }\n\n    lockmgr(&amp;kld_lock, LK_EXCLUSIVE);\n    error = linker_load_module(kldname, modname, NULL, NULL, &amp;lf);\n    lockmgr(&amp;kld_lock, LK_RELEASE);\n\n    if (!error) {\n        lf-&gt;userrefs++;\n        sysmsg-&gt;sysmsg_result = lf-&gt;id;\n    }\n    return error;\n}\n</code></pre> <p>The <code>linker_load_file()</code> function (<code>kern_linker.c:310</code>) is the core loader:</p> <pre><code>int linker_load_file(const char *filename, linker_file_t *result)\n{\n    /* Security check */\n    if (securelevel &gt; 0 || kernel_mem_readonly)\n        return EPERM;\n\n    /* Check if already loaded */\n    lf = linker_find_file_by_name(filename);\n    if (lf) {\n        lf-&gt;refs++;\n        *result = lf;\n        return 0;\n    }\n\n    /* Try each class until one succeeds */\n    TAILQ_FOREACH(lc, &amp;classes, link) {\n        error = lc-&gt;ops-&gt;load_file(filename, &amp;lf);\n        if (lf) {\n            linker_file_register_modules(lf);\n            linker_file_register_sysctls(lf);\n            linker_file_sysinit(lf);\n            lf-&gt;flags |= LINKER_FILE_LINKED;\n            *result = lf;\n            return 0;\n        }\n    }\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/kld/#module-search-path","title":"Module Search Path","text":"<p>The linker searches for modules using a configurable path (<code>kern_linker.c:1406</code>):</p> <pre><code>static char linker_path[MAXPATHLEN] = \"/boot/kernel;/boot/modules.local\";\n\nSYSCTL_STRING(_kern, OID_AUTO, module_path, CTLFLAG_RW, linker_path,\n              sizeof(linker_path), \"module load search path\");\n</code></pre> <p>The <code>linker_search_path()</code> function tries each path component with optional <code>.ko</code> extension until a file is found.</p>"},{"location":"sys/kern/kld/#preloaded-modules","title":"Preloaded Modules","text":"<p>Modules loaded by the boot loader are processed during kernel initialization by <code>linker_preload()</code> (<code>kern_linker.c:1208</code>):</p> <ol> <li>Iterate through preload metadata finding modules</li> <li>Call each linker class's <code>preload_file()</code> to parse the module</li> <li>Sort modules by dependency order (bubble sort)</li> <li>Call <code>preload_finish()</code> to complete relocation</li> <li>Register modules and run SYSINITs</li> </ol> <pre><code>static void linker_preload(void *arg)\n{\n    /* Find all preloaded modules */\n    while ((modptr = preload_search_next_name(modptr)) != NULL) {\n        TAILQ_FOREACH(lc, &amp;classes, link) {\n            error = lc-&gt;ops-&gt;preload_file(modname, &amp;lf);\n            if (!error) break;\n        }\n        if (lf)\n            TAILQ_INSERT_TAIL(&amp;loaded_files, lf, loaded);\n    }\n\n    /* Resolve dependencies and link order */\n    /* ... bubble sort by dependencies ... */\n\n    /* Complete loading */\n    TAILQ_FOREACH(lf, &amp;depended_files, loaded) {\n        lf-&gt;ops-&gt;preload_finish(lf);\n        linker_file_register_modules(lf);\n        linker_file_register_sysctls(lf);\n        lf-&gt;flags |= LINKER_FILE_LINKED;\n    }\n}\n\nSYSINIT(preload, SI_BOOT2_KLD, SI_ORDER_MIDDLE, linker_preload, 0);\n</code></pre>"},{"location":"sys/kern/kld/#elf-loading","title":"ELF Loading","text":""},{"location":"sys/kern/kld/#shared-objects-link_elfc","title":"Shared Objects (link_elf.c)","text":"<p>For ET_DYN (shared object) files, <code>link_elf_load_file()</code> (<code>link_elf.c:388</code>):</p> <ol> <li>Read and validate ELF header</li> <li>Parse program headers - Find PT_LOAD segments (text, data) and PT_DYNAMIC</li> <li>Allocate memory - <code>kmalloc()</code> for the module's address space</li> <li>Load segments - Read text/data, zero BSS</li> <li>Parse dynamic section - Extract symbol table, hash table, relocations</li> <li>Perform local relocations - <code>link_elf_reloc_local()</code></li> <li>Load dependencies - <code>linker_load_dependencies()</code></li> <li>Perform global relocations - <code>relocate_file()</code></li> <li>Load debug symbols (optional)</li> </ol> <p>The ELF file structure (<code>link_elf.c:89</code>):</p> <pre><code>typedef struct elf_file {\n    caddr_t          address;     /* Relocation address */\n    const Elf_Dyn   *dynamic;     /* Symbol table etc. */\n    Elf_Hashelt      nbuckets;    /* DT_HASH info */\n    Elf_Hashelt      nchains;\n    const Elf_Hashelt *buckets;\n    const Elf_Hashelt *chains;\n    caddr_t          strtab;      /* DT_STRTAB */\n    int              strsz;       /* DT_STRSZ */\n    const Elf_Sym   *symtab;      /* DT_SYMTAB */\n    Elf_Addr        *got;         /* DT_PLTGOT */\n    const Elf_Rela  *rela;        /* DT_RELA */\n    int              relasize;    /* DT_RELASZ */\n    const Elf_Sym   *ddbsymtab;   /* Symbol table for DDB */\n    long             ddbsymcnt;\n    caddr_t          ddbstrtab;   /* String table */\n    long             ddbstrcnt;\n} *elf_file_t;\n</code></pre>"},{"location":"sys/kern/kld/#relocatable-objects-link_elf_objc","title":"Relocatable Objects (link_elf_obj.c)","text":"<p>For ET_REL (relocatable object) files used by most kernel modules, <code>link_elf_obj_load_file()</code> (<code>link_elf_obj.c:390</code>):</p> <ol> <li>Read and validate ELF header - Must be ET_REL type</li> <li>Read section headers - Parse all sections</li> <li>Count sections - PROGBITS, NOBITS, REL, RELA</li> <li>Allocate symbol/string tables - Read into memory</li> <li>Allocate contiguous memory - For all code/data sections</li> <li>Load sections - Read PROGBITS, zero NOBITS</li> <li>Update symbol values - Add section base addresses</li> <li>Perform local relocations - <code>link_elf_obj_reloc_local()</code></li> <li>Load dependencies - <code>linker_load_dependencies()</code></li> <li>Perform global relocations - <code>relocate_file()</code></li> </ol> <p>The object file structure (<code>link_elf_obj.c:104</code>):</p> <pre><code>typedef struct elf_file {\n    int              preloaded;\n    caddr_t          address;     /* Relocation address */\n    size_t           bytes;       /* Chunk size in bytes */\n    vm_object_t      object;      /* VM object for pages */\n    Elf_Shdr        *e_shdr;      /* Section headers */\n    Elf_progent     *progtab;     /* PROGBITS/NOBITS sections */\n    int              nprogtab;\n    Elf_relaent     *relatab;     /* RELA relocations */\n    int              nrelatab;\n    Elf_relent      *reltab;      /* REL relocations */\n    int              nreltab;\n    Elf_Sym         *ddbsymtab;   /* Symbol table */\n    long             ddbsymcnt;\n    caddr_t          ddbstrtab;   /* String table */\n    long             ddbstrcnt;\n} *elf_file_t;\n</code></pre>"},{"location":"sys/kern/kld/#symbol-lookup","title":"Symbol Lookup","text":"<p>Symbol lookup uses the ELF hash table for efficiency (<code>link_elf.c:776</code>):</p> <pre><code>static unsigned long elf_hash(const char *name)\n{\n    /* Standard System V ABI hash function */\n    const unsigned char *p = (const unsigned char *)name;\n    unsigned long h = 0, g;\n\n    while (*p != '\\0') {\n        h = (h &lt;&lt; 4) + *p++;\n        if ((g = h &amp; 0xf0000000) != 0)\n            h ^= g &gt;&gt; 24;\n        h &amp;= ~g;\n    }\n    return h;\n}\n\nstatic int link_elf_lookup_symbol(linker_file_t lf, const char *name,\n                                  c_linker_sym_t *sym)\n{\n    unsigned long hash = elf_hash(name);\n    unsigned long symnum = ef-&gt;buckets[hash % ef-&gt;nbuckets];\n\n    while (symnum != STN_UNDEF) {\n        symp = ef-&gt;symtab + symnum;\n        strp = ef-&gt;strtab + symp-&gt;st_name;\n        if (strcmp(name, strp) == 0) {\n            if (symp-&gt;st_shndx != SHN_UNDEF) {\n                *sym = (c_linker_sym_t)symp;\n                return 0;\n            }\n        }\n        symnum = ef-&gt;chains[symnum];\n    }\n    return ENOENT;\n}\n</code></pre> <p>For undefined symbols, <code>linker_file_lookup_symbol()</code> (<code>kern_linker.c:605</code>) searches: 1. The file's own symbol table 2. Dependencies (if <code>deps</code> flag set) 3. All loaded files (global search) 4. Common symbol table (allocates storage if needed)</p>"},{"location":"sys/kern/kld/#relocation","title":"Relocation","text":"<p>Relocation applies fixups to resolve symbol references. The <code>relocate_file()</code> function processes both REL and RELA relocation entries:</p> <pre><code>static int relocate_file(linker_file_t lf)\n{\n    /* Process REL entries (addend in instruction) */\n    for (rel = ef-&gt;rel; rel &lt; rellim; rel++) {\n        if (elf_reloc(lf, ef-&gt;address, rel, ELF_RELOC_REL, elf_lookup)) {\n            kprintf(\"link_elf: symbol %s undefined\\n\", symname);\n            return ENOENT;\n        }\n    }\n\n    /* Process RELA entries (explicit addend) */\n    for (rela = ef-&gt;rela; rela &lt; relalim; rela++) {\n        if (elf_reloc(lf, ef-&gt;address, rela, ELF_RELOC_RELA, elf_lookup)) {\n            kprintf(\"link_elf: symbol %s undefined\\n\", symname);\n            return ENOENT;\n        }\n    }\n    return 0;\n}\n</code></pre> <p>Architecture-specific <code>elf_reloc()</code> and <code>elf_reloc_local()</code> functions handle the actual relocation types (e.g., R_X86_64_64, R_X86_64_PC32, etc.).</p>"},{"location":"sys/kern/kld/#module-system","title":"Module System","text":""},{"location":"sys/kern/kld/#module-registration","title":"Module Registration","text":"<p>Modules within a KLD file are registered via <code>module_register()</code> (<code>kern_module.c:120</code>):</p> <pre><code>int module_register(const moduledata_t *data, linker_file_t container)\n{\n    newmod = module_lookupbyname(data-&gt;name);\n    if (newmod != NULL)\n        return EEXIST;  /* already registered */\n\n    newmod = kmalloc(sizeof(struct module) + namelen, M_MODULE, M_WAITOK);\n    newmod-&gt;refs = 1;\n    newmod-&gt;id = nextid++;\n    newmod-&gt;name = (char *)(newmod + 1);\n    strcpy(newmod-&gt;name, data-&gt;name);\n    newmod-&gt;handler = data-&gt;evhand ? data-&gt;evhand : modevent_nop;\n    newmod-&gt;arg = data-&gt;priv;\n\n    TAILQ_INSERT_TAIL(&amp;modules, newmod, link);\n    if (container)\n        TAILQ_INSERT_TAIL(&amp;container-&gt;modules, newmod, flink);\n    newmod-&gt;file = container;\n    return 0;\n}\n</code></pre> <p>The <code>struct module</code> (<code>kern_module.c:43</code>):</p> <pre><code>struct module {\n    TAILQ_ENTRY(module) link;    /* chain all modules */\n    TAILQ_ENTRY(module) flink;   /* modules in this file */\n    struct linker_file *file;    /* containing file */\n    int              refs;       /* reference count */\n    int              id;         /* unique id number */\n    char            *name;       /* module name */\n    modeventhand_t   handler;    /* event handler */\n    void            *arg;        /* argument for handler */\n    modspecific_t    data;       /* module specific data */\n};\n</code></pre>"},{"location":"sys/kern/kld/#module-events","title":"Module Events","text":"<p>Modules receive lifecycle events via their handler:</p> <pre><code>typedef enum modeventtype {\n    MOD_LOAD,      /* Module is being loaded */\n    MOD_UNLOAD,    /* Module is being unloaded */\n    MOD_SHUTDOWN   /* System is shutting down */\n} modeventtype_t;\n</code></pre>"},{"location":"sys/kern/kld/#module-metadata","title":"Module Metadata","text":"<p>Modules declare metadata using macros from <code>sys/sys/module.h</code>:</p> <pre><code>/* Declare a module */\nDECLARE_MODULE(name, data, sub, order);\n\n/* Declare a dependency on another module */\nMODULE_DEPEND(module, mdepend, vmin, vpref, vmax);\n\n/* Declare module version */\nMODULE_VERSION(module, version);\n</code></pre> <p>These expand to <code>struct mod_metadata</code> entries in the <code>modmetadata_set</code> linker set:</p> <pre><code>struct mod_metadata {\n    int         md_version;   /* structure version */\n    int         md_type;      /* MDT_DEPEND, MDT_MODULE, or MDT_VERSION */\n    void       *md_data;      /* type-specific data */\n    const char *md_cval;      /* module/dependency name */\n};\n</code></pre>"},{"location":"sys/kern/kld/#dependency-resolution","title":"Dependency Resolution","text":"<p>The linker resolves dependencies during loading (<code>kern_linker.c:1582</code>):</p> <pre><code>int linker_load_dependencies(linker_file_t lf)\n{\n    /* All files depend on kernel */\n    if (linker_kernel_file) {\n        linker_kernel_file-&gt;refs++;\n        linker_file_add_dependancy(lf, linker_kernel_file);\n    }\n\n    /* Process MDT_DEPEND entries */\n    linker_file_lookup_set(lf, MDT_SETNAME, &amp;start, &amp;stop, NULL);\n    for (mdp = start; mdp &lt; stop; mdp++) {\n        if ((*mdp)-&gt;md_type != MDT_DEPEND)\n            continue;\n        modname = (*mdp)-&gt;md_cval;\n        verinfo = (*mdp)-&gt;md_data;\n\n        /* Skip self-references */\n        /* Check if already loaded */\n        mod = modlist_lookup2(modname, verinfo);\n        if (mod) {\n            lfdep = mod-&gt;container;\n            lfdep-&gt;refs++;\n            linker_file_add_dependancy(lf, lfdep);\n            continue;\n        }\n        /* Load the dependency */\n        error = linker_load_module(NULL, modname, lf, verinfo, NULL);\n    }\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/kld/#module-unloading","title":"Module Unloading","text":""},{"location":"sys/kern/kld/#unload-flow","title":"Unload Flow","text":"<p>The <code>sys_kldunload()</code> syscall (<code>kern_linker.c:833</code>) unloads a module:</p> <pre><code>int sys_kldunload(struct sysmsg *sysmsg, const struct kldunload_args *uap)\n{\n    if (securelevel &gt; 0 || kernel_mem_readonly)\n        return EPERM;\n\n    lockmgr(&amp;kld_lock, LK_EXCLUSIVE);\n    lf = linker_find_file_by_id(uap-&gt;fileid);\n    if (lf) {\n        if (lf-&gt;userrefs == 0) {\n            /* kernel-loaded, cannot unload */\n            error = EBUSY;\n        } else {\n            lf-&gt;userrefs--;\n            error = linker_file_unload(lf);\n            if (error)\n                lf-&gt;userrefs++;  /* restore on failure */\n        }\n    }\n    lockmgr(&amp;kld_lock, LK_RELEASE);\n    return error;\n}\n</code></pre> <p>The <code>linker_file_unload()</code> function (<code>kern_linker.c:477</code>):</p> <ol> <li>Check reference count - If refs &gt; 1, just decrement</li> <li>Notify modules - Send MOD_UNLOAD event (may veto)</li> <li>Run SYSUNINITs - In reverse order</li> <li>Unregister sysctls</li> <li>Unload dependencies - Recursively</li> <li>Free resources - Call class unload, free memory</li> </ol> <pre><code>int linker_file_unload(linker_file_t file)\n{\n    if (securelevel &gt; 0 || kernel_mem_readonly)\n        return EPERM;\n\n    /* Just drop reference if not last */\n    if (file-&gt;refs &gt; 1) {\n        file-&gt;refs--;\n        return 0;\n    }\n\n    /* Notify modules - they can veto */\n    for (mod = TAILQ_FIRST(&amp;file-&gt;modules); mod; mod = next) {\n        next = module_getfnext(mod);\n        if ((error = module_unload(mod)) != 0)\n            return error;  /* vetoed */\n        module_release(mod);\n    }\n\n    /* Run SYSUNINITs */\n    if (file-&gt;flags &amp; LINKER_FILE_LINKED) {\n        linker_file_sysuninit(file);\n        linker_file_unregister_sysctls(file);\n    }\n\n    /* Unload dependencies */\n    for (i = 0; i &lt; file-&gt;ndeps; i++)\n        linker_file_unload(file-&gt;deps[i]);\n\n    /* Class-specific cleanup */\n    file-&gt;ops-&gt;unload(file);\n    kfree(file, M_LINKER);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/kld/#sysinit-in-modules","title":"SYSINIT in Modules","text":"<p>Modules can include SYSINIT entries that run when the module loads:</p> <pre><code>static void linker_file_sysinit(linker_file_t lf)\n{\n    /* Find sysinit_set in the module */\n    if (linker_file_lookup_set(lf, \"sysinit_set\", &amp;start, &amp;stop, NULL) != 0)\n        return;\n\n    /* Sort by subsystem and order (bubble sort) */\n    for (sipp = start; sipp &lt; stop; sipp++) {\n        for (xipp = sipp + 1; xipp &lt; stop; xipp++) {\n            if ((*sipp)-&gt;subsystem &gt; (*xipp)-&gt;subsystem ||\n                ((*sipp)-&gt;subsystem == (*xipp)-&gt;subsystem &amp;&amp;\n                 (*sipp)-&gt;order &gt; (*xipp)-&gt;order)) {\n                save = *sipp; *sipp = *xipp; *xipp = save;\n            }\n        }\n    }\n\n    /* Execute in order */\n    for (sipp = start; sipp &lt; stop; sipp++) {\n        if ((*sipp)-&gt;subsystem != SI_SPECIAL_DUMMY)\n            (*((*sipp)-&gt;func))((*sipp)-&gt;udata);\n    }\n}\n</code></pre> <p>SYSUNINITs run in reverse order during unload.</p>"},{"location":"sys/kern/kld/#kld-syscalls","title":"KLD Syscalls","text":"Syscall Description <code>kldload(2)</code> Load a kernel module <code>kldunload(2)</code> Unload a kernel module <code>kldfind(2)</code> Find a loaded module by name <code>kldnext(2)</code> Iterate over loaded modules <code>kldstat(2)</code> Get module statistics <code>kldfirstmod(2)</code> Get first module in a file <code>kldsym(2)</code> Look up a symbol <p>Module information syscalls:</p> Syscall Description <code>modnext(2)</code> Iterate over registered modules <code>modfnext(2)</code> Next module in same file <code>modstat(2)</code> Get module statistics <code>modfind(2)</code> Find module by name"},{"location":"sys/kern/kld/#ddb-integration","title":"DDB Integration","text":"<p>The linker provides helpers for the kernel debugger (DDB) to look up symbols across all loaded modules (<code>kern_linker.c:718</code>):</p> <pre><code>int linker_ddb_lookup(const char *symstr, c_linker_sym_t *sym)\n{\n    TAILQ_FOREACH(lf, &amp;linker_files, link) {\n        if (lf-&gt;ops-&gt;lookup_symbol(lf, symstr, sym) == 0)\n            return 0;\n    }\n    return ENOENT;\n}\n\nint linker_ddb_search_symbol(caddr_t value, c_linker_sym_t *sym, long *diffp)\n{\n    /* Find closest symbol to address across all files */\n    TAILQ_FOREACH(lf, &amp;linker_files, link) {\n        if (lf-&gt;ops-&gt;search_symbol(lf, value, &amp;es, &amp;diff) == 0) {\n            if (es != NULL &amp;&amp; diff &lt; bestdiff) {\n                best = es;\n                bestdiff = diff;\n            }\n        }\n    }\n    /* ... */\n}\n</code></pre>"},{"location":"sys/kern/kld/#security","title":"Security","text":"<p>Module loading is restricted by:</p> <ol> <li>Securelevel - Loading blocked if <code>securelevel &gt; 0</code></li> <li>kernel_mem_readonly - Loading blocked if set</li> <li>Capabilities - Requires <code>SYSCAP_NOKLD</code> capability</li> </ol>"},{"location":"sys/kern/kld/#see-also","title":"See Also","text":"<ul> <li>initialization.md - Kernel boot and SYSINIT</li> <li>syscalls.md - Syscall registration for modules</li> <li>devices.md - Device driver modules</li> </ul>"},{"location":"sys/kern/lwkt/","title":"LWKT Threading","text":"<p>LWKT (Lightweight Kernel Threading) is DragonFly BSD's unique message-passing based concurrency model. It is the architectural foundation that distinguishes DragonFly from traditional BSD systems and is essential to understand before exploring any other kernel subsystem.</p>"},{"location":"sys/kern/lwkt/#overview","title":"Overview","text":"<p>LWKT implements a message-passing threading model designed for multiprocessor scalability. Instead of relying primarily on locks to protect shared data, DragonFly uses:</p> <ul> <li>Message passing between threads via message ports</li> <li>Serializing tokens that can be held across blocking operations</li> <li>Per-CPU thread schedulers that minimize cross-CPU synchronization</li> <li>Inter-processor interrupt queues (IPIQs) for cross-CPU communication</li> </ul>"},{"location":"sys/kern/lwkt/#why-lwkt-exists","title":"Why LWKT Exists","text":"<p>Traditional BSD kernels use pervasive locking (mutexes, spinlocks, read-write locks) to protect shared data structures in multiprocessor environments. This approach suffers from:</p> <ul> <li>Lock contention \u2014 Multiple CPUs waiting for the same lock</li> <li>Cache-line bouncing \u2014 Locks ping-pong between CPU caches</li> <li>Priority inversion \u2014 Lower-priority threads holding locks needed by higher-priority threads</li> <li>Deadlock potential \u2014 Complex lock ordering requirements</li> </ul> <p>DragonFly's LWKT addresses these issues by:</p> <ul> <li>Minimizing shared state \u2014 Each CPU has its own scheduler and thread queues</li> <li>Using message passing \u2014 Threads communicate via asynchronous messages instead of shared memory</li> <li>Allowing tokens across sleeps \u2014 Tokens don't have the strict semantics of traditional locks</li> <li>Deferring to thread owner \u2014 Operations on a thread are sent as messages to its owning CPU</li> </ul>"},{"location":"sys/kern/lwkt/#where-lwkt-fits-in-the-architecture","title":"Where LWKT Fits in the Architecture","text":"<p>LWKT is the lowest-level threading abstraction in DragonFly. It sits below:</p> <ul> <li>Process/LWP management (<code>kern_proc.c</code>, <code>kern_fork.c</code>, etc.)</li> <li>CPU scheduling policies (<code>usched_*.c</code>)</li> <li>All kernel subsystems (VFS, VM, networking, etc.)</li> </ul> <p>Everything in the kernel runs in the context of an LWKT thread. Even interrupt handlers run as threads in DragonFly.</p>"},{"location":"sys/kern/lwkt/#key-concepts","title":"Key Concepts","text":""},{"location":"sys/kern/lwkt/#threads-vs-processes","title":"Threads vs Processes","text":"<p>In DragonFly:</p> <ul> <li>Thread (<code>struct thread</code>) \u2014 The basic unit of execution in LWKT</li> <li>LWP (<code>struct lwp</code>) \u2014 Light Weight Process, represents a user-level thread of execution</li> <li>Process (<code>struct proc</code>) \u2014 A collection of LWPs sharing an address space and resources</li> </ul> <p>An LWKT thread may be:</p> <ul> <li>A kernel thread (no associated LWP or process)</li> <li>A user thread (has an associated LWP and process)</li> </ul> <p>All execution happens via LWKT threads. User threads enter the kernel via system calls, traps, or signals and execute in kernel mode using their LWKT thread context.</p>"},{"location":"sys/kern/lwkt/#message-passing","title":"Message Passing","text":"<p>Threads communicate via message ports (<code>lwkt_port</code>). Each thread has an embedded message port (<code>td_msgport</code>).</p> <p>Synchronous messaging: <pre><code>// Send message, block until reply\nlwkt_sendmsg(target_port, &amp;msg);\n</code></pre></p> <p>Asynchronous messaging: <pre><code>// Send message, don't wait for reply\nlwkt_sendmsg_async(target_port, &amp;msg);\n// ... do other work ...\n// Later, check for reply\nlwkt_waitmsg(&amp;msg, 0);\n</code></pre></p> <p>Messages (<code>struct lwkt_msg</code>) contain:</p> <ul> <li>Target and reply ports</li> <li>Result fields (error code, return value)</li> <li>State flags (DONE, REPLY, QUEUED, SYNC, etc.)</li> </ul>"},{"location":"sys/kern/lwkt/#tokens-serialization-without-traditional-locking","title":"Tokens: Serialization Without Traditional Locking","text":"<p>Tokens (<code>struct lwkt_token</code>) are DragonFly's primary synchronization primitive. They differ fundamentally from traditional locks:</p> <p>Traditional locks (mutexes, spinlocks): - Must be released before blocking - Strict acquire/release semantics - Can deadlock if ordering is incorrect - Cause cache-line bouncing</p> <p>DragonFly tokens: - Can be held across blocking operations - Automatically released on sleep, reacquired on wakeup - Cannot deadlock regardless of acquisition order - Serialization only effective while thread is running</p> <p>Example: <pre><code>lwkt_gettoken(&amp;mp-&gt;mnt_token);  // Acquire token\n\n// Can safely sleep here!\n// Token is temporarily released, reacquired on wakeup\ntsleep(wchan, 0, \"wait\", 0);\n\n// Still holding token after wakeup\nlwkt_reltoken(&amp;mp-&gt;mnt_token);\n</code></pre></p> <p>The key insight: tokens provide logical serialization rather than physical lock-holding. If you block, another thread may run and access the same data, but it will also hold the token, maintaining serialization.</p>"},{"location":"sys/kern/lwkt/#token-types","title":"Token Types","text":"<p>Tokens support two acquisition modes:</p> <ul> <li>Exclusive \u2014 Only one thread at a time (TOK_EXCLUSIVE bit set)</li> <li>Shared \u2014 Multiple threads simultaneously (reference count in <code>t_count</code>)</li> </ul> <p>Multiple exclusive acquisitions by the same thread are allowed and tracked.</p>"},{"location":"sys/kern/lwkt/#per-cpu-scheduling","title":"Per-CPU Scheduling","text":"<p>Each CPU has its own thread scheduler:</p> <ul> <li>Thread queues are per-CPU (<code>gd_tdrunq</code>, <code>gd_tdallq</code>)</li> <li>Switching threads on the same CPU requires only a critical section</li> <li>No locks or cross-CPU synchronization for local scheduling</li> </ul> <p>To schedule a thread on another CPU, use IPIQs (see below).</p>"},{"location":"sys/kern/lwkt/#ipiqs-inter-processor-interrupt-queues","title":"IPIQs: Inter-Processor Interrupt Queues","text":"<p>When one CPU needs to operate on a thread owned by another CPU, it sends a message via an IPIQ (<code>struct lwkt_ipiq</code>):</p> <ul> <li>Lock-free circular buffer (FIFO)</li> <li>Source CPU writes functions to execute</li> <li>Target CPU processes them in interrupt context</li> <li>Used for cross-CPU thread migration, scheduling, etc.</li> </ul> <p>Example: To schedule a thread on CPU 1 from CPU 0: 1. CPU 0 writes a scheduling function to CPU 1's IPIQ 2. CPU 0 sends an inter-processor interrupt (IPI) to CPU 1 3. CPU 1 handles the IPI, processes the IPIQ entry 4. CPU 1 adds the thread to its local run queue</p>"},{"location":"sys/kern/lwkt/#critical-sections","title":"Critical Sections","text":"<p>Critical sections prevent preemption and must be used carefully:</p> <pre><code>crit_enter();\n// Cannot be preempted here\n// Keep this SHORT!\ncrit_exit();\n</code></pre> <p>Critical sections do not prevent interrupts, but LWKT threads (including interrupt threads) will not preempt code in a critical section on the same CPU.</p>"},{"location":"sys/kern/lwkt/#thread-ownership","title":"Thread Ownership","text":"<p>A thread is owned by the CPU in its <code>td_gd</code> (globaldata) field. Only the owning CPU can directly manipulate the thread. Other CPUs must use:</p> <ul> <li>IPIQs to send requests to the owning CPU</li> <li>Messaging to communicate with the thread</li> </ul> <p>This ownership model eliminates many locking requirements.</p>"},{"location":"sys/kern/lwkt/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/lwkt/#struct-thread","title":"<code>struct thread</code>","text":"<p>Defined in <code>sys/sys/thread.h</code>. Key fields:</p> <pre><code>struct thread {\n    TAILQ_ENTRY(thread) td_threadq;   // Queue linkage (run/sleep/etc.)\n    TAILQ_ENTRY(thread) td_allq;      // Link in gd_tdallq\n    lwkt_port td_msgport;             // Built-in message port\n\n    struct lwp *td_lwp;                // Associated LWP (if user thread)\n    struct proc *td_proc;              // Associated process (if user thread)\n    struct pcb *td_pcb;                // Process control block, top of kstack\n    struct globaldata *td_gd;          // Owning CPU's globaldata\n\n    const char *td_wmesg;              // Reason for blocking\n    const volatile void *td_wchan;     // Wait channel\n    int td_pri;                        // Priority (0-31, 31=highest)\n    int td_critcount;                  // Critical section nesting count\n    u_int td_flags;                    // TDF_* flags\n\n    char *td_kstack;                   // Kernel stack base\n    int td_kstack_size;                // Kernel stack size\n    char *td_sp;                       // Saved stack pointer for context switch\n\n    thread_t (*td_switch)(struct thread *); // Context switch function\n\n    lwkt_tokref_t td_toks_have;        // Tokens currently held\n    lwkt_tokref_t td_toks_stop;        // Tokens to acquire\n    struct lwkt_tokref td_toks_array[LWKT_MAXTOKENS];\n\n    char td_comm[MAXCOMLEN+1];         // Thread name\n    struct ucred *td_ucred;            // Credentials (synchronized from proc)\n\n    struct md_thread td_mach;          // Machine-dependent state\n};\n</code></pre> <p>Important fields:</p> <ul> <li><code>td_gd</code> \u2014 Identifies the owning CPU</li> <li><code>td_msgport</code> \u2014 Every thread has a built-in message port</li> <li><code>td_toks_have</code> / <code>td_toks_stop</code> \u2014 Token stack for serialization</li> <li><code>td_pri</code> \u2014 Determines scheduling priority</li> <li><code>td_critcount</code> \u2014 Critical section depth (&gt;0 means non-preemptible)</li> <li><code>td_switch</code> \u2014 Function pointer for machine-dependent context switching</li> </ul>"},{"location":"sys/kern/lwkt/#struct-lwkt_msg","title":"<code>struct lwkt_msg</code>","text":"<p>Defined in <code>sys/sys/msgport.h</code>. Messages sent between threads:</p> <pre><code>struct lwkt_msg {\n    TAILQ_ENTRY(lwkt_msg) ms_node;    // Queue linkage\n    lwkt_port_t ms_target_port;       // Current target port\n    lwkt_port_t ms_reply_port;        // Reply sent here\n\n    void (*ms_abortfn)(struct lwkt_msg *);  // Abort handler\n    int ms_flags;                     // MSGF_* flags\n    int ms_error;                     // Error code (0 = success)\n\n    union {\n        void *ms_resultp;             // Pointer result\n        int ms_result;                // Integer result\n        long ms_lresult;              // Long result\n        __int64_t ms_result64;        // 64-bit result\n        __off_t ms_offset;            // Offset result\n    } u;\n\n    void (*ms_receiptfn)(struct lwkt_msg *, lwkt_port_t);\n};\n</code></pre> <p>Message flags (<code>ms_flags</code>):</p> <ul> <li><code>MSGF_DONE</code> \u2014 Message complete</li> <li><code>MSGF_REPLY</code> \u2014 Message is a reply</li> <li><code>MSGF_QUEUED</code> \u2014 Queued on a port</li> <li><code>MSGF_SYNC</code> \u2014 Synchronous (caller blocked)</li> <li><code>MSGF_INTRANSIT</code> \u2014 Being passed via IPI</li> <li><code>MSGF_ABORTABLE</code> \u2014 Can be aborted</li> <li><code>MSGF_PRIORITY</code> \u2014 High-priority message</li> </ul>"},{"location":"sys/kern/lwkt/#struct-lwkt_port","title":"<code>struct lwkt_port</code>","text":"<p>Message ports for receiving messages:</p> <pre><code>struct lwkt_port {\n    lwkt_msg_queue mp_msgq;           // Normal priority messages\n    lwkt_msg_queue mp_msgq_prio;      // High priority messages\n    int mp_flags;                     // Port flags\n    int mp_cpuid;                     // CPU affinity\n\n    union {\n        struct spinlock spin;         // Spinlock-protected port\n        struct lwkt_serialize *serialize;  // Serializer-protected\n        void *data;                   // Or custom data\n    } mp_u;\n\n    struct thread *mpu_td;            // Owning thread (if thread port)\n\n    // Port operations (function pointers):\n    void (*mp_putport)(lwkt_port_t, lwkt_msg_t);\n    int (*mp_waitmsg)(lwkt_msg_t, int);\n    void *(*mp_waitport)(lwkt_port_t, int);\n    void (*mp_replyport)(lwkt_port_t, lwkt_msg_t);\n    int (*mp_dropmsg)(lwkt_port_t, lwkt_msg_t);\n};\n</code></pre>"},{"location":"sys/kern/lwkt/#struct-lwkt_token","title":"<code>struct lwkt_token</code>","text":"<p>Serializing tokens:</p> <pre><code>struct lwkt_token {\n    long t_count;                     // Shared count | EXCLUSIVE | EXCLREQ\n    struct lwkt_tokref *t_ref;        // Exclusive holder reference\n    long t_collisions;                // Contention counter\n    const char *t_desc;               // Descriptive name\n};\n</code></pre> <p>Token states (encoded in <code>t_count</code>):</p> <ul> <li><code>TOK_EXCLUSIVE</code> (bit 0) \u2014 Exclusively held</li> <li><code>TOK_EXCLREQ</code> (bit 1) \u2014 Exclusive request pending</li> <li>Count (bits 2+) \u2014 Number of shared holders (shifted by <code>TOK_INCR</code>)</li> </ul>"},{"location":"sys/kern/lwkt/#struct-lwkt_tokref","title":"<code>struct lwkt_tokref</code>","text":"<p>Token reference (thread's token stack entry):</p> <pre><code>struct lwkt_tokref {\n    lwkt_token_t tr_tok;              // Token being held\n    long tr_count;                    // TOK_EXCLUSIVE or 0\n    struct thread *tr_owner;          // Thread holding this ref\n};\n</code></pre> <p>Each thread has an array of <code>LWKT_MAXTOKENS</code> (32) token references, allowing nested token acquisition.</p>"},{"location":"sys/kern/lwkt/#struct-lwkt_ipiq","title":"<code>struct lwkt_ipiq</code>","text":"<p>Inter-processor interrupt queue:</p> <pre><code>struct lwkt_ipiq {\n    int ip_rindex;                    // Read index (target CPU updates)\n    int ip_xindex;                    // Completion index (target updates)\n    int ip_windex;                    // Write index (source CPU updates)\n    int ip_drain;                     // Drain source limit\n\n    struct {\n        ipifunc3_t func;              // Function to execute\n        void *arg1;                   // First argument\n        int arg2;                     // Second argument\n        char filler[32 - ...];        // Cache-line alignment\n    } ip_info[MAXCPUFIFO];            // Circular buffer (256 entries)\n};\n</code></pre> <p>Lock-free design: - Source CPU writes to <code>ip_windex</code> slot - Target CPU reads from <code>ip_rindex</code> - No locks needed due to single-writer, single-reader pattern</p>"},{"location":"sys/kern/lwkt/#key-functions","title":"Key Functions","text":""},{"location":"sys/kern/lwkt/#thread-management","title":"Thread Management","text":""},{"location":"sys/kern/lwkt/#lwkt_init_thread","title":"<code>lwkt_init_thread()</code>","text":"<p>Initialize a new LWKT thread structure.</p> <ul> <li>Purpose: Set up thread structure, allocate kernel stack, initialize message port</li> <li>Called by: <code>lwkt_alloc_thread()</code>, kernel thread creation routines</li> <li>Parameters: <code>thread_t td, void *stack, int stksize, int flags, struct globaldata *gd</code></li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_alloc_thread","title":"<code>lwkt_alloc_thread()</code>","text":"<p>Allocate and initialize a new LWKT thread.</p> <ul> <li>Purpose: Allocate memory for thread structure and kernel stack</li> <li>Returns: Initialized <code>thread_t</code></li> <li>Called by: <code>lwkt_create()</code>, process/thread creation code</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_switch","title":"<code>lwkt_switch()</code>","text":"<p>Low-level context switch between threads.</p> <ul> <li>Purpose: Save current thread context, restore next thread context</li> <li>Called by: Scheduler when switching threads</li> <li>Critical section: Must be called within a critical section</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_schedule_self","title":"<code>lwkt_schedule_self()</code>","text":"<p>Deschedule current thread (voluntary sleep).</p> <ul> <li>Purpose: Place current thread on specified queue, switch to next runnable thread</li> <li>Called by: <code>tsleep()</code>, <code>lwkt_deschedule_self()</code>, any code voluntarily blocking</li> <li>Note: Thread will resume when rescheduled by another CPU via <code>lwkt_schedule()</code></li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_schedule","title":"<code>lwkt_schedule()</code>","text":"<p>Schedule a thread for execution.</p> <ul> <li>Purpose: Add thread to its CPU's run queue</li> <li>Cross-CPU: If thread is on another CPU, uses IPIQ to send scheduling request</li> <li>Called by: Wakeup routines, thread creation, message completion</li> </ul>"},{"location":"sys/kern/lwkt/#message-passing_1","title":"Message Passing","text":""},{"location":"sys/kern/lwkt/#lwkt_sendmsg","title":"<code>lwkt_sendmsg()</code>","text":"<p>Send a synchronous message (block until reply).</p> <ul> <li>Purpose: Send message to target port, block waiting for reply</li> <li>Returns: Error code from message (<code>ms_error</code>)</li> <li>Typical use: Synchronous RPC-style operations</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_sendmsg_async","title":"<code>lwkt_sendmsg_async()</code>","text":"<p>Send an asynchronous message (don't wait).</p> <ul> <li>Purpose: Initiate message send, return immediately</li> <li>Note: Caller must later call <code>lwkt_waitmsg()</code> or check <code>MSGF_DONE</code></li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_waitmsg","title":"<code>lwkt_waitmsg()</code>","text":"<p>Wait for message completion.</p> <ul> <li>Purpose: Block until message reply is received</li> <li>Parameters: <code>lwkt_msg_t msg, int flags</code></li> <li>Returns: Error code</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_replymsg","title":"<code>lwkt_replymsg()</code>","text":"<p>Reply to a received message.</p> <ul> <li>Purpose: Send reply back to originating port</li> <li>Called by: Message handler after processing request</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_initmsg","title":"<code>lwkt_initmsg()</code>","text":"<p>Initialize a message structure.</p> <ul> <li>Purpose: Set up message for sending</li> <li>Parameters: <code>lwkt_msg_t msg, lwkt_port_t rport, int flags</code></li> </ul>"},{"location":"sys/kern/lwkt/#token-operations","title":"Token Operations","text":""},{"location":"sys/kern/lwkt/#lwkt_gettoken","title":"<code>lwkt_gettoken()</code>","text":"<p>Acquire a token (exclusive by default).</p> <ul> <li>Purpose: Serialize access to protected data</li> <li>Blocking: Spins if token unavailable, may deschedule on contention</li> <li>Can be called multiple times (token stack)</li> <li>Held across sleep: Token automatically released/reacquired</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_reltoken","title":"<code>lwkt_reltoken()</code>","text":"<p>Release a token.</p> <ul> <li>Purpose: Release the most recently acquired token</li> <li>Must match acquisition (tokens released in reverse order of acquisition)</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_gettoken_shared","title":"<code>lwkt_gettoken_shared()</code>","text":"<p>Acquire a shared (read) token.</p> <ul> <li>Purpose: Allow multiple concurrent readers</li> <li>Blocks if: Exclusive holder or exclusive request pending</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_token_pool_lookup","title":"<code>lwkt_token_pool_lookup()</code>","text":"<p>Look up a token from a token pool.</p> <ul> <li>Purpose: Get a token for a specific object from a pool of tokens</li> <li>Used by: Subsystems that need many tokens (e.g., vnodes)</li> </ul>"},{"location":"sys/kern/lwkt/#ipiq-operations","title":"IPIQ Operations","text":""},{"location":"sys/kern/lwkt/#lwkt_send_ipiq","title":"<code>lwkt_send_ipiq()</code>","text":"<p>Send a function to execute on another CPU.</p> <ul> <li>Purpose: Cross-CPU operation request</li> <li>Parameters: <code>globaldata *gd, ipifunc_t func, void *arg</code></li> <li>Non-blocking: Queues function in target CPU's IPIQ</li> <li>Returns: 0 on success, error if IPIQ full</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_process_ipiq","title":"<code>lwkt_process_ipiq()</code>","text":"<p>Process pending IPIQs.</p> <ul> <li>Purpose: Execute functions queued in local CPU's IPIQ</li> <li>Called by: IPI interrupt handler, scheduler</li> <li>Runs in interrupt context</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_synchronous_ipiq","title":"<code>lwkt_synchronous_ipiq()</code>","text":"<p>Send IPIQ and wait for completion.</p> <ul> <li>Purpose: Execute function on remote CPU, wait for it to finish</li> <li>Blocking: Waits for target CPU to process the request</li> </ul>"},{"location":"sys/kern/lwkt/#port-operations","title":"Port Operations","text":""},{"location":"sys/kern/lwkt/#lwkt_initport_thread","title":"<code>lwkt_initport_thread()</code>","text":"<p>Initialize a thread's built-in port.</p> <ul> <li>Purpose: Set up thread's message port for receiving messages</li> <li>Called by: <code>lwkt_init_thread()</code></li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_waitport","title":"<code>lwkt_waitport()</code>","text":"<p>Wait for a message to arrive on a port.</p> <ul> <li>Purpose: Block until message received</li> <li>Returns: Pointer to received message</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_getport","title":"<code>lwkt_getport()</code>","text":"<p>Get next message from port (non-blocking).</p> <ul> <li>Purpose: Dequeue message if available</li> <li>Returns: Message or NULL if queue empty</li> </ul>"},{"location":"sys/kern/lwkt/#subsystem-interactions","title":"Subsystem Interactions","text":""},{"location":"sys/kern/lwkt/#with-the-scheduler","title":"With the Scheduler","text":"<p>LWKT provides the low-level threading mechanism; the scheduler determines which thread to run:</p> <ul> <li>Scheduler calls <code>lwkt_switch()</code> to context-switch</li> <li>Threads have priorities (<code>td_pri</code>) used by scheduler</li> <li>Per-CPU run queues (<code>gd_tdrunq</code>) hold runnable threads</li> <li>Scheduler policies (<code>usched_dfly</code>, <code>usched_bsd4</code>) implement different algorithms on top of LWKT</li> </ul> <p>See Scheduling for details.</p>"},{"location":"sys/kern/lwkt/#with-processes-and-lwps","title":"With Processes and LWPs","text":"<p>User threads are LWKT threads with attached LWP and process structures:</p> <ul> <li>System calls execute in user thread's LWKT context</li> <li>Process creation (<code>fork()</code>) creates new LWKT threads</li> <li>Thread exit releases LWKT thread structure</li> </ul> <p>See Processes &amp; Threads for details.</p>"},{"location":"sys/kern/lwkt/#with-the-virtual-filesystem-vfs","title":"With the Virtual Filesystem (VFS)","text":"<p>VFS uses tokens extensively for serialization:</p> <ul> <li>Mount points have tokens (<code>mnt_token</code>)</li> <li>Vnodes use token pools</li> <li>Buffer cache operations hold tokens</li> <li>Token semantics allow sleeping during I/O</li> </ul> <p>See VFS for details.</p>"},{"location":"sys/kern/lwkt/#with-device-drivers","title":"With Device Drivers","text":"<p>Drivers often use:</p> <ul> <li>Serializers (<code>lwkt_serialize</code>) for interrupt synchronization</li> <li>Tokens for driver-internal serialization</li> <li>Message ports for async operations</li> </ul> <p>Device interrupt threads run as LWKT threads, allowing uniform scheduling.</p>"},{"location":"sys/kern/lwkt/#with-the-vm-system","title":"With the VM System","text":"<p>VM operations:</p> <ul> <li>Use tokens to protect VM objects and maps</li> <li>May block during page-ins (tokens held across sleep)</li> <li>Page daemon runs as an LWKT kernel thread</li> </ul>"},{"location":"sys/kern/lwkt/#code-flow-examples","title":"Code Flow Examples","text":""},{"location":"sys/kern/lwkt/#example-1-thread-creation-and-scheduling","title":"Example 1: Thread Creation and Scheduling","text":"<pre><code>// Create a new kernel thread\nthread_t td;\n\ntd = lwkt_alloc_thread(NULL, LWKT_THREAD_STACK, -1, TDF_MPSAFE);\nlwkt_init_thread(td, stack, stksize, 0, my_gd);\ntd-&gt;td_flags |= TDF_MPSAFE;\nbcopy(\"mythrd\", td-&gt;td_comm, sizeof(\"mythrd\"));\n\n// Set up to run a function\ncpu_set_thread_handler(td, my_thread_fn, arg);\n\n// Schedule it for execution\nlwkt_schedule(td);  // Will run on td-&gt;td_gd CPU\n</code></pre> <p>Flow: 1. Allocate thread structure 2. Initialize (stack, message port, globaldata) 3. Set up machine state to call <code>my_thread_fn</code> 4. Add to run queue via <code>lwkt_schedule()</code> 5. Scheduler eventually switches to new thread</p>"},{"location":"sys/kern/lwkt/#example-2-synchronous-message-send","title":"Example 2: Synchronous Message Send","text":"<pre><code>struct lwkt_msg msg;\n\n// Initialize message\nlwkt_initmsg(&amp;msg, &amp;my_reply_port, 0);\n\n// Send to target, block until reply\nint error = lwkt_sendmsg(target_port, &amp;msg);\n\n// Message has been processed, result in msg.ms_error or msg.u.*\nif (error == 0) {\n    result = msg.u.ms_result;\n}\n</code></pre> <p>Flow: 1. Caller initializes message with reply port 2. <code>lwkt_sendmsg()</code> calls target port's <code>mp_putport()</code> function 3. Target port queues message (or processes immediately) 4. Caller blocks waiting for reply (<code>MSGF_SYNC</code> set) 5. Target processes message, calls <code>lwkt_replymsg()</code> 6. Reply wakes up caller, caller returns with result</p>"},{"location":"sys/kern/lwkt/#example-3-token-acquisition-across-sleep","title":"Example 3: Token Acquisition Across Sleep","text":"<pre><code>// Acquire token\nlwkt_gettoken(&amp;vp-&gt;v_token);\n\n// Safe to access vnode fields here\n\n// Need to sleep waiting for I/O\ntsleep(&amp;bp-&gt;b_flags, 0, \"biowait\", 0);\n// Token is temporarily released during sleep\n// Token is reacquired before tsleep() returns\n\n// Still holding token, safe to access vnode\nlwkt_reltoken(&amp;vp-&gt;v_token);\n</code></pre> <p>Flow: 1. <code>lwkt_gettoken()</code> acquires token 2. Code accesses protected data 3. <code>tsleep()</code> deschedules thread, saves token stack 4. While asleep, another thread may acquire the same token 5. On wakeup, <code>tsleep()</code> reacquires all tokens before returning 6. Code continues with token held 7. <code>lwkt_reltoken()</code> releases token</p>"},{"location":"sys/kern/lwkt/#example-4-cross-cpu-scheduling-via-ipiq","title":"Example 4: Cross-CPU Scheduling via IPIQ","text":"<p>CPU 0 wants to schedule a thread owned by CPU 1:</p> <pre><code>// On CPU 0, scheduling thread td (owned by CPU 1)\nlwkt_schedule(td);\n</code></pre> <p>Internal flow: 1. <code>lwkt_schedule()</code> checks <code>td-&gt;td_gd</code> (= CPU 1's globaldata) 2. Sees thread is on another CPU 3. Calls <code>lwkt_send_ipiq(cpu1_gd, lwkt_schedule_remote, td)</code> 4. Writes <code>{lwkt_schedule_remote, td}</code> to CPU 1's IPIQ 5. Sends inter-processor interrupt to CPU 1 6. CPU 1 handles IPI, calls <code>lwkt_process_ipiq()</code> 7. <code>lwkt_process_ipiq()</code> executes <code>lwkt_schedule_remote(td)</code> 8. <code>lwkt_schedule_remote()</code> adds <code>td</code> to CPU 1's run queue</p>"},{"location":"sys/kern/lwkt/#traditional-bsd-vs-dragonfly-lwkt","title":"Traditional BSD vs DragonFly LWKT","text":""},{"location":"sys/kern/lwkt/#traditional-bsd-approach","title":"Traditional BSD Approach","text":"<pre><code>// Traditional: Acquire lock, access data, release lock\nmtx_lock(&amp;vp-&gt;v_lock);\n// Cannot sleep here!\n// Access v_data\nmtx_unlock(&amp;vp-&gt;v_lock);\n</code></pre> <p>Problems: - Locks must be released before sleeping - Complex code to handle sleep/wakeup with locks - Lock ordering requirements to avoid deadlock - Cache-line bouncing on multiprocessor systems</p>"},{"location":"sys/kern/lwkt/#dragonfly-lwkt-approach","title":"DragonFly LWKT Approach","text":"<pre><code>// DragonFly: Acquire token, access data, can sleep, release token\nlwkt_gettoken(&amp;vp-&gt;v_token);\n// Can sleep here!\ntsleep(wchan, 0, \"vnode\", 0);\n// Still serialized after wakeup\n// Access v_data\nlwkt_reltoken(&amp;vp-&gt;v_token);\n</code></pre> <p>Advantages: - Tokens held across sleep (simpler code) - No deadlock possibility (tokens can be acquired in any order) - Per-CPU scheduling (no global scheduler lock) - Message passing reduces shared state</p>"},{"location":"sys/kern/lwkt/#multiprocessor-scalability","title":"Multiprocessor Scalability","text":"<p>Traditional approach: - Global scheduler lock contended by all CPUs - Locks on shared data structures (e.g., vnodes, sockets) - Cache coherency overhead</p> <p>LWKT approach: - Per-CPU scheduling (no global lock) - Message passing instead of shared state - Tokens reduce contention (logical serialization) - Thread ownership eliminates many locks</p>"},{"location":"sys/kern/lwkt/#important-notes","title":"Important Notes","text":""},{"location":"sys/kern/lwkt/#token-deadlock-freedom","title":"Token Deadlock Freedom","text":"<p>Tokens cannot deadlock because:</p> <ol> <li>Held across sleep: If you block waiting for a resource, your token is released</li> <li>Any ordering: Tokens can be acquired in any order</li> <li>Priority boosting: Token contention can boost thread priority</li> </ol> <p>Traditional locks can deadlock because they must be held continuously and have strict ordering requirements.</p>"},{"location":"sys/kern/lwkt/#when-to-use-what","title":"When to Use What","text":"<ul> <li>Tokens: Subsystem-level serialization (e.g., entire mount point, vnode)</li> <li>Spinlocks: Very short critical sections, interrupt context</li> <li>Serializers: Device driver interrupt/thread synchronization</li> <li>Messages: Cross-CPU operations, async work queuing</li> </ul>"},{"location":"sys/kern/lwkt/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Critical sections must be short \u2014 Prevent preemption, don't abuse</li> <li>IPIQs are bounded \u2014 Can fill up if target CPU is busy</li> <li>Token contention \u2014 Tracked in <code>t_collisions</code>, can indicate bottleneck</li> </ul>"},{"location":"sys/kern/lwkt/#files","title":"Files","text":"<p>Key source files implementing LWKT:</p> <ul> <li><code>sys/kern/lwkt_thread.c</code> \u2014 Thread management, scheduling, context switching</li> <li><code>sys/kern/lwkt_msgport.c</code> \u2014 Message ports and message passing</li> <li><code>sys/kern/lwkt_token.c</code> \u2014 Serializing tokens</li> <li><code>sys/kern/lwkt_ipiq.c</code> \u2014 Inter-processor interrupt queues</li> <li><code>sys/kern/lwkt_serialize.c</code> \u2014 Serializer helpers (for drivers)</li> </ul> <p>Key header files:</p> <ul> <li><code>sys/sys/thread.h</code> \u2014 <code>struct thread</code>, token structures</li> <li><code>sys/sys/msgport.h</code> \u2014 <code>struct lwkt_msg</code>, <code>struct lwkt_port</code></li> <li><code>sys/sys/thread2.h</code> \u2014 Inline functions, macros</li> </ul>"},{"location":"sys/kern/lwkt/#references","title":"References","text":"<ul> <li>Synchronization \u2014 Other synchronization primitives (spinlocks, mutexes, etc.)</li> <li>Scheduling \u2014 CPU scheduling policies built on LWKT</li> <li>Processes &amp; Threads \u2014 Process and LWP management using LWKT</li> <li>VFS \u2014 Extensive use of tokens for filesystem serialization</li> <li>IPC &amp; Sockets \u2014 Message passing used in socket layer</li> </ul>"},{"location":"sys/kern/lwkt/#further-reading","title":"Further Reading","text":"<p>DragonFly's LWKT is unique among BSD systems. Understanding it is essential for kernel development. Key concepts to remember:</p> <ol> <li>Message passing over shared memory</li> <li>Tokens held across sleep</li> <li>Per-CPU scheduling</li> <li>Thread ownership by CPU</li> <li>Deadlock-free by design</li> </ol> <p>These principles enable DragonFly's superior multiprocessor scalability compared to traditional BSD kernels.</p>"},{"location":"sys/kern/memory/","title":"Memory Allocation","text":"<p>This document describes DragonFly BSD's kernel memory allocation subsystems, which provide efficient, scalable memory management for kernel code.</p>"},{"location":"sys/kern/memory/#overview","title":"Overview","text":"<p>DragonFly BSD implements a sophisticated multi-layered memory allocation system optimized for SMP scalability and low overhead. The system provides several allocators, each optimized for different use cases:</p> <ol> <li>kmalloc/kfree - General-purpose allocation API (<code>sys/kern/kern_slaballoc.c</code>)</li> <li>kmalloc_obj - Type-stable object allocator (<code>sys/kern/kern_kmalloc.c</code>)</li> <li>objcache - Per-CPU object caches with magazine layer (<code>sys/kern/kern_objcache.c</code>)</li> <li>mpipe - Pre-allocated object pools (<code>sys/kern/kern_mpipe.c</code>)</li> <li>Helper allocators - Specialized data structure allocators (alist, blist, etc.)</li> </ol> <p>The system is designed around several key principles:</p> <ul> <li>Per-CPU operation: Most allocations occur without locks, using per-CPU data structures</li> <li>Lock-free fast paths: Common allocations complete without synchronization</li> <li>Magazine/depot architecture: Multi-level caching reduces contention</li> <li>Type stability: Object allocators maintain type information for improved debuggability</li> <li>Lazy IPI: Cross-CPU operations use asynchronous IPI messages</li> </ul>"},{"location":"sys/kern/memory/#architectural-layers","title":"Architectural Layers","text":"<pre><code>flowchart TB\n    KERNEL[\"Kernel Code(drivers, filesystems, network stack, etc.)\"]\n\n    subgraph API[\"API Layer\"]\n        KMALLOC[\"kmalloc/kfree(slab API)\"]\n        KMALLOCOBJ[\"kmalloc_obj(type-stable)\"]\n        OBJCACHE[\"objcache(magazine)\"]\n    end\n\n    SLAB[\"Slab Allocator (kern_slaballoc.c)Per-CPU zones, chunk management, IPI ops\"]\n\n    KMEM[\"kmem_slab_alloc/free\"]\n\n    VM[\"VM System(vm_map, etc)\"]\n\n    KERNEL --&gt; KMALLOC\n    KERNEL --&gt; KMALLOCOBJ\n    KERNEL --&gt; OBJCACHE\n    KMALLOC --&gt; SLAB\n    KMALLOCOBJ --&gt; SLAB\n    OBJCACHE --&gt; SLAB\n    SLAB --&gt; KMEM\n    KMEM --&gt; VM\n</code></pre>"},{"location":"sys/kern/memory/#layer-responsibilities","title":"Layer Responsibilities","text":"<p>API Layer (<code>kern_kmalloc.c</code>): - Provides <code>kmalloc()</code> and <code>kfree()</code> API - Implements kmalloc_obj for type-stable allocations - Manages malloc types and statistics - Routes large allocations directly to VM</p> <p>Slab Allocator (<code>kern_slaballoc.c</code>): - Power-of-2 sized zones (8 bytes to 32KB) - Per-CPU zone management - Lock-free allocation and freeing - Cross-CPU IPI-based operations - Bitmap tracking for allocated chunks</p> <p>Object Cache (<code>kern_objcache.c</code>): - Magazine-based per-CPU caching - Depot layer for magazine exchange - Constructor/destructor support - Configurable limits and policies</p> <p>VM Interface (<code>kmem_slab_alloc/free</code>): - Direct KVA and page allocation - Wiring and mapping management - Large allocation support</p>"},{"location":"sys/kern/memory/#memory-allocation-flags","title":"Memory Allocation Flags","text":"<p>All allocation functions accept flags that control behavior:</p>"},{"location":"sys/kern/memory/#blocking-behavior","title":"Blocking Behavior","text":"Flag Description Can Fail? <code>M_WAITOK</code> Block until memory available No (panics on failure) <code>M_NOWAIT</code> Return NULL immediately if unavailable Yes <code>M_NULLOK</code> Return NULL instead of panicking Yes (with M_WAITOK) <code>M_INTWAIT</code> Like M_WAITOK for interrupt context No <code>M_INTNOWAIT</code> Like M_NOWAIT for interrupt context Yes"},{"location":"sys/kern/memory/#memory-properties","title":"Memory Properties","text":"Flag Description <code>M_ZERO</code> Zero allocated memory <code>M_USE_RESERVE</code> Can use emergency reserves <code>M_USE_INTERRUPT_RESERVE</code> Can exhaust free list (interrupt) <code>M_CACHEALIGN</code> Align to cache line boundary <code>M_POWEROF2</code> Round size to power of 2 <p>Important: The default behavior without <code>M_ZERO</code> leaves memory uninitialized for performance. In <code>INVARIANTS</code> mode, memory may be filled with patterns to detect use-after-free.</p>"},{"location":"sys/kern/memory/#kmallockfree-api","title":"kmalloc/kfree API","text":"<p>The primary allocation interface for general-purpose kernel memory.</p>"},{"location":"sys/kern/memory/#basic-usage","title":"Basic Usage","text":"<pre><code>#include &lt;sys/malloc.h&gt;\n\n/* Define a malloc type */\nMALLOC_DEFINE(M_MYSUBSYS, \"mysubsys\", \"My subsystem allocations\");\n\n/* Allocate memory */\nvoid *ptr = kmalloc(size, M_MYSUBSYS, M_WAITOK);\n\n/* Allocate and zero */\nvoid *ptr = kmalloc(size, M_MYSUBSYS, M_WAITOK | M_ZERO);\n\n/* Non-blocking allocation */\nvoid *ptr = kmalloc(size, M_MYSUBSYS, M_NOWAIT);\nif (ptr == NULL) {\n    /* Handle allocation failure */\n}\n\n/* Free memory */\nkfree(ptr, M_MYSUBSYS);\n</code></pre>"},{"location":"sys/kern/memory/#malloc-types","title":"Malloc Types","text":"<p>Every allocation must specify a <code>malloc_type</code> for tracking and statistics. Types are defined with:</p> <pre><code>MALLOC_DEFINE(type_name, \"short-desc\", \"Long description\");\n</code></pre> <p>Pre-defined types (in <code>sys/kern/kern_slaballoc.c</code>): - <code>M_CACHE</code> - Various dynamically allocated caches - <code>M_DEVBUF</code> - Device driver memory - <code>M_TEMP</code> - Temporary buffers - <code>M_DRM</code> - DRM subsystem allocations</p>"},{"location":"sys/kern/memory/#allocation-size-limits","title":"Allocation Size Limits","text":"<p>The slab allocator handles allocations based on size:</p> Size Range Chunk Size Zones Behavior 1-127 bytes 8 bytes 16 Slab allocation 128-255 16 bytes 8 Slab allocation 256-511 32 bytes 8 Slab allocation 512-1023 64 bytes 8 Slab allocation 1024-2047 128 bytes 8 Slab allocation 2048-4095 256 bytes 8 Slab allocation 4096-8191 512 bytes 8 Slab allocation 8192-16383 1024 bytes 8 Slab allocation 16384-32767 2048 bytes 8 Slab allocation \u2265 ZoneLimit Page-aligned - Direct VM allocation <p>Notes: - Allocations are rounded up to chunk size - Alignment equals chunk size for power-of-2 requests - Large allocations (\u2265 ZoneLimit or &gt;2 pages) bypass slab allocator - ZoneLimit is typically 32KB on systems with \u22651GB RAM</p>"},{"location":"sys/kern/memory/#zero-sized-allocations","title":"Zero-Sized Allocations","text":"<p>DragonFly allows <code>kmalloc(0, ...)</code> for compatibility (some drivers depend on this):</p> <pre><code>void *ptr = kmalloc(0, M_TEMP, M_WAITOK);\n/* Returns special ZERO_LENGTH_PTR (-8), not NULL */\nkfree(ptr, M_TEMP);  /* Safe to free */\n</code></pre>"},{"location":"sys/kern/memory/#string-allocation","title":"String Allocation","text":"<p>Convenience functions for string duplication:</p> <pre><code>/* Duplicate a string */\nchar *str = kstrdup(original, M_TEMP);\nkfree(str, M_TEMP);\n\n/* Duplicate with length limit */\nchar *str = kstrndup(original, maxlen, M_TEMP);\n</code></pre>"},{"location":"sys/kern/memory/#reallocation","title":"Reallocation","text":"<pre><code>/* Resize allocation - may move data */\nvoid *new_ptr = krealloc(old_ptr, new_size, M_TEMP, M_WAITOK);\n\n/* If new size fits in same zone, returns same pointer */\n/* Otherwise allocates new memory and copies data */\n</code></pre> <p>Note: <code>krealloc()</code> does not support <code>M_ZERO</code> flag.</p>"},{"location":"sys/kern/memory/#memory-limits","title":"Memory Limits","text":"<p>Each malloc type has an associated limit (typically 10% of kernel memory):</p> <pre><code>/* Check current limit */\nlong limit = kmalloc_limit(M_MYSUBSYS);\n\n/* Raise limit (never shrinks) */\nkmalloc_raise_limit(M_MYSUBSYS, new_limit_bytes);\n\n/* Set to unlimited */\nkmalloc_set_unlimited(M_MYSUBSYS);\n</code></pre> <p>When a limit is exceeded: - <code>M_WAITOK</code> allocations panic - <code>M_NULLOK</code> allocations return NULL - Prevents runaway memory usage</p>"},{"location":"sys/kern/memory/#dynamic-type-creation","title":"Dynamic Type Creation","text":"<p>For dynamically loaded modules or runtime-created subsystems:</p> <pre><code>struct malloc_type *my_type = NULL;\n\n/* Create malloc type */\nkmalloc_create(&amp;my_type, \"runtime-type\");\n\n/* Use it */\nvoid *ptr = kmalloc(size, my_type, M_WAITOK);\nkfree(ptr, my_type);\n\n/* Destroy when done (must have no outstanding allocations) */\nkmalloc_destroy(&amp;my_type);\n</code></pre>"},{"location":"sys/kern/memory/#type-stable-object-allocator-kmalloc_obj","title":"Type-Stable Object Allocator (kmalloc_obj)","text":"<p>For fixed-size object allocations, DragonFly provides a type-stable allocator optimized for common-sized objects. This allocator provides:</p> <ul> <li>Type stability: All objects of a type can be freed in bulk</li> <li>Per-CPU slabs: 128KB slabs with embedded object management</li> <li>Lock-free operations: Atomic operations for cross-CPU frees</li> <li>Efficient recycling: Two-tier slab management (per-CPU + global)</li> </ul>"},{"location":"sys/kern/memory/#defining-object-types","title":"Defining Object Types","text":"<pre><code>/* Define a type-stable object type */\nstatic struct malloc_type *my_obj_type;\n\nMALLOC_DEFINE_OBJ(my_obj_type, sizeof(struct my_object),\n                  \"myobj\", \"My object type\");\n</code></pre> <p>Alternatively, create at runtime:</p> <pre><code>_kmalloc_create_obj(&amp;my_obj_type, \"my_runtime_obj\",\n                    sizeof(struct my_object));\n</code></pre>"},{"location":"sys/kern/memory/#object-allocation","title":"Object Allocation","text":"<pre><code>struct my_object *obj;\n\n/* Allocate object */\nobj = kmalloc_obj(sizeof(struct my_object), my_obj_type, M_WAITOK);\n\n/* Use object */\nobj-&gt;field1 = value;\nobj-&gt;field2 = value;\n\n/* Free object */\nkfree_obj(obj, my_obj_type);\n</code></pre>"},{"location":"sys/kern/memory/#type-stable-architecture","title":"Type-Stable Architecture","text":"<p>Per-CPU Slab Management:</p> <p>Each malloc type maintains per-CPU state in <code>struct kmalloc_use</code>: - <code>active</code> - Current slab being allocated from - <code>alternate</code> - Backup slab (hot spare) - Per-CPU statistics (<code>memuse</code>, <code>inuse</code>, etc.)</p> <p>Slab Structure (128KB each):</p> <pre><code>flowchart TB\n    subgraph SLAB[\"kmalloc_slab (128KB)\"]\n        HEADER[\"kmalloc_slab (header)\"]\n        FOBJS[\"fobjs[4096] - Circular buffer of free object pointers\u2022 findex: next free index\u2022 nindex: next allocation index\u2022 maxobjs: total capacity\"]\n        OBJECTS[\"Objects (allocated from end of slab)[ obj 1 ][ obj 2 ][ obj 3 ] ... [ obj N ]\"]\n    end\n\n    HEADER --- FOBJS\n    FOBJS --- OBJECTS\n</code></pre> <p>Slab States:</p> <ol> <li>Active/Alternate (per-CPU): Fast allocation, no locks</li> <li>Partial (global): Has free objects, used to refill per-CPU</li> <li>Full (global): All allocated, kept for type-safety</li> <li>Empty (global): All freed, eligible for recycling</li> </ol>"},{"location":"sys/kern/memory/#allocation-fast-path","title":"Allocation Fast Path","text":"<pre><code>/* From kern_kmalloc.c:kmalloc_obj_alloc() */\n1. Try current active slab's fobjs[] array\n2. If empty, swap active \u2194 alternate\n3. If both empty, rotate global partial\u2192active\n4. If no partial slabs, allocate new 128KB slab\n</code></pre> <p>Key Insight: The fobjs[] circular buffer provides O(1) alloc/free with no linked-list manipulation.</p>"},{"location":"sys/kern/memory/#free-fast-path","title":"Free Fast Path","text":"<pre><code>/* From kern_kmalloc.c:kmalloc_obj_free() */\n1. Calculate slab from object pointer: obj &amp; ~(128KB-1)\n2. Atomically store pointer in slab-&gt;fobjs[slab-&gt;findex++]\n3. Update per-CPU statistics\n4. If cross-CPU free, use atomic ops only\n</code></pre> <p>Cross-CPU Free: When freeing to remote CPU's slab, atomic increments ensure thread-safety without explicit locks.</p>"},{"location":"sys/kern/memory/#type-stability-benefits","title":"Type Stability Benefits","text":"<p>When unmounting a filesystem (e.g., tmpfs), all memory from the filesystem's malloc type can be returned:</p> <pre><code>/* During unmount */\nmalloc_uninit(tmpfs_mount_type);\n\n/* This returns ALL slabs for this type to the system */\n/* No fragmentation! All 128KB slabs are freed intact */\n</code></pre> <p>Traditional allocators would have fragmentation preventing full memory reclamation.</p>"},{"location":"sys/kern/memory/#per-cpu-slab-cache","title":"Per-CPU Slab Cache","text":"<p>To avoid expensive <code>kmem_slab_alloc()</code> calls, DragonFly maintains a per-globaldata slab cache:</p> <pre><code>/* In globaldata structure */\nstruct globaldata {\n    ...\n    kmalloc_slab_t gd_kmslab[KMGD_MAXFREESLABS];  /* 128 slabs */\n    int            gd_kmslab_avail;\n    ...\n};\n</code></pre> <p>Benefits: - Fast slab allocation from cache - Amortizes VM allocation cost - Reduces system map contention</p>"},{"location":"sys/kern/memory/#slab-allocator-internals","title":"Slab Allocator Internals","text":"<p>The slab allocator (<code>kern_slaballoc.c</code>) provides the foundation for kmalloc operations.</p>"},{"location":"sys/kern/memory/#zone-structure","title":"Zone Structure","text":"<p>A zone represents a region of memory divided into fixed-size chunks:</p> <pre><code>struct SLZone {\n    int32_t  z_Magic;          /* Magic number validation */\n    int      z_NFree;          /* Number of free chunks */\n    int      z_NMax;           /* Total chunks in zone */\n    struct SLZone *z_Next;     /* List linkage */\n\n    int      z_BaseIndex;      /* Optimization: start search here */\n    int      z_ChunkSize;      /* Size of each chunk */\n    int      z_ZoneIndex;      /* Which zone array (by size) */\n    int      z_Cpu;            /* Owning CPU */\n    struct globaldata *z_CpuGd;/* Owning CPU's globaldata */\n\n    /* Free chunk lists */\n    struct SLChunk *z_LChunks;    /* Local free chunks (LIFO) */\n    struct SLChunk **z_LChunksp;  /* Tail pointer */\n    struct SLChunk *z_RChunks;    /* Remote free chunks (from other CPUs) */\n\n    /* Cross-CPU synchronization */\n    int      z_RSignal;        /* Signal for remote operations */\n    int      z_RCount;         /* Count of in-flight remote operations */\n\n    char     *z_BasePtr;       /* Base of chunk array */\n\n#ifdef INVARIANTS\n    uint32_t z_Bitmap[...];    /* Allocation bitmap for debugging */\n#endif\n};\n</code></pre>"},{"location":"sys/kern/memory/#per-cpu-zone-management","title":"Per-CPU Zone Management","text":"<p>Each CPU maintains separate zone lists in <code>struct SLGlobalData</code>:</p> <pre><code>struct SLGlobalData {\n    SLZoneList ZoneAry[NZONES];      /* Zones by size */\n    SLZoneList FreeZones;            /* Completely free zones */\n    SLZoneList FreeOvZones;          /* Oversized free zones */\n    int        NFreeZones;           /* Count of free zones */\n    int        JunkIndex;            /* Randomization for new zones */\n};\n</code></pre> <p>NZONES is typically 72 (16 for small + 56 for larger sizes).</p>"},{"location":"sys/kern/memory/#zone-size-calculation","title":"Zone Size Calculation","text":"<p>Zone size is determined at boot based on system memory:</p> <pre><code>/* From kmeminit() in kern_slaballoc.c */\nZoneSize = ZALLOC_MIN_ZONE_SIZE;  /* Start at 32KB */\nwhile (ZoneSize &lt; ZALLOC_MAX_ZONE_SIZE &amp;&amp; (ZoneSize &lt;&lt; 1) &lt; usesize)\n    ZoneSize &lt;&lt;= 1;\n\n/* Typically:\n * &lt; 128MB RAM: 32KB zones\n * &gt;= 1GB RAM:  128KB zones (most systems)\n */\n</code></pre> <p>Larger zones reduce per-zone overhead but increase memory slack.</p>"},{"location":"sys/kern/memory/#allocation-algorithm","title":"Allocation Algorithm","text":"<p>The <code>_kmalloc()</code> function (kern_slaballoc.c:814) implements allocation:</p> <pre><code>1. Handle special cases:\n   - Check malloc type limit\n   - Handle size == 0 (return ZERO_LENGTH_PTR)\n\n2. For small allocations (&lt; ZoneLimit):\n   a. Calculate zone index: zoneindex(&amp;size, &amp;align)\n   b. Enter critical section\n   c. Find zone with free chunks:\n      - Check tail of ZoneAry[zi] (most recently used)\n      - If no free chunks, check z_RChunks (remote frees)\n      - If still none, allocate new zone\n   d. Pop chunk from free list or use z_UIndex (never-allocated)\n   e. Update statistics\n   f. Exit critical section\n   g. Zero memory if M_ZERO requested\n\n3. For large allocations (\u2265 ZoneLimit):\n   a. Round size to page boundary\n   b. Call kmem_slab_alloc(size, PAGE_SIZE, flags)\n   c. Mark pages in kernel page table (btokup)\n   d. Return pointer\n</code></pre> <p>Critical section: Uses <code>crit_enter()/crit_exit()</code> not locks. This blocks preemption but allows interrupts.</p>"},{"location":"sys/kern/memory/#free-algorithm","title":"Free Algorithm","text":"<p>The <code>_kfree()</code> function (kern_slaballoc.c:1391) implements deallocation:</p> <pre><code>1. Handle special cases:\n   - NULL pointer (panic)\n   - ZERO_LENGTH_PTR (return immediately)\n   - Check magic number\n\n2. Determine zone or oversized:\n   kup = btokup(ptr);  /* Kernel page table lookup */\n   if (*kup &gt; 0) {\n       /* Oversized allocation */\n       size = *kup &lt;&lt; PAGE_SHIFT;\n       kmem_slab_free(ptr, size);\n       return;\n   }\n\n3. For zone allocations:\n   z = (SLZone *)((uintptr_t)ptr &amp; ZoneMask);\n\n   a. If z-&gt;z_CpuGd != mycpu:\n      /* Cross-CPU free */\n      - Atomic add to z-&gt;z_RChunks list\n      - Send IPI if zone needs reactivation\n\n   b. If local CPU:\n      - Enter critical section\n      - Add chunk to z-&gt;z_LChunks (front, LIFO)\n      - Update z-&gt;z_NFree\n      - If zone becomes fully free, move to FreeZones list\n      - Exit critical section\n</code></pre> <p>LIFO ordering: Recently freed memory is hot in cache, so reallocate it first.</p>"},{"location":"sys/kern/memory/#cross-cpu-operations","title":"Cross-CPU Operations","text":"<p>When CPU A frees memory owned by CPU B:</p> <pre><code>/* CPU A (freeing) */\n1. chunk-&gt;c_Next = z-&gt;z_RChunks;     /* Atomic */\n2. atomic_cmpset_ptr(&amp;z-&gt;z_RChunks, old, chunk);\n3. if (zone was fully allocated) {\n       lwkt_send_ipiq_passive(z-&gt;z_CpuGd, kfree_remote, z);\n   }\n\n/* CPU B (IPI handler) */\nkfree_remote(void *ptr) {\n    z = (SLZone *)ptr;\n    /* Move z-&gt;z_RChunks to z-&gt;z_LChunks */\n    clean_zone_rchunks(z);\n    /* Reactivate zone if it has free chunks */\n    if (z-&gt;z_NFree &gt; 0)\n        TAILQ_INSERT_HEAD(&amp;slgd-&gt;ZoneAry[z-&gt;z_ZoneIndex], z, z_Entry);\n}\n</code></pre> <p>Key optimizations: - Passive IPI: Low priority, batched - z_RSignal flag: Avoids IPI if zone already active - z_RCount: Prevents premature zone destruction</p>"},{"location":"sys/kern/memory/#zone-recycling","title":"Zone Recycling","text":"<p>Completely free zones are kept in the <code>FreeZones</code> list up to a threshold (<code>ZoneRelsThresh</code>, default 32). Beyond that, zones are returned to the VM system:</p> <pre><code>/* In _kmalloc() hysteresis check */\nwhile (slgd-&gt;NFreeZones &gt; ZoneRelsThresh) {\n    z = TAILQ_LAST(&amp;slgd-&gt;FreeZones, SLZoneList);\n    TAILQ_REMOVE(&amp;slgd-&gt;FreeZones, z, z_Entry);\n    --slgd-&gt;NFreeZones;\n    kmem_slab_free(z, ZoneSize);  /* May block */\n}\n</code></pre> <p>Hysteresis: Prevents thrashing by keeping a buffer of free zones.</p>"},{"location":"sys/kern/memory/#chunk-bitmap-invariants","title":"Chunk Bitmap (INVARIANTS)","text":"<p>In debug builds, zones maintain a bitmap to detect double-free and use-after-free:</p> <pre><code>#ifdef INVARIANTS\nstatic void chunk_mark_allocated(SLZone *z, void *chunk) {\n    int bitdex = ((char *)chunk - (char *)z-&gt;z_BasePtr) / z-&gt;z_ChunkSize;\n    KASSERT(!(z-&gt;z_Bitmap[bitdex &gt;&gt; 5] &amp; (1 &lt;&lt; (bitdex &amp; 31))),\n            (\"double allocation\"));\n    z-&gt;z_Bitmap[bitdex &gt;&gt; 5] |= (1 &lt;&lt; (bitdex &amp; 31));\n}\n\nstatic void chunk_mark_free(SLZone *z, void *chunk) {\n    int bitdex = ((char *)chunk - (char *)z-&gt;z_BasePtr) / z-&gt;z_ChunkSize;\n    KASSERT(z-&gt;z_Bitmap[bitdex &gt;&gt; 5] &amp; (1 &lt;&lt; (bitdex &amp; 31)),\n            (\"double free\"));\n    z-&gt;z_Bitmap[bitdex &gt;&gt; 5] &amp;= ~(1 &lt;&lt; (bitdex &amp; 31));\n}\n#endif\n</code></pre>"},{"location":"sys/kern/memory/#memory-pattern-debugging","title":"Memory Pattern Debugging","text":"<pre><code>/* In INVARIANTS mode */\nstatic int use_malloc_pattern = 0;  /* sysctl debug.use_malloc_pattern */\nstatic int use_weird_array = 0;     /* sysctl debug.use_weird_array */\n\n/* On allocation without M_ZERO */\nif (use_malloc_pattern) {\n    for (i = 0; i &lt; size; i += sizeof(int))\n        *(int *)((char *)chunk + i) = -1;  /* 0xFFFFFFFF pattern */\n}\n\n/* On free */\nif (use_weird_array) {\n    bcopy(weirdary, chunk, min(size, sizeof(weirdary)));\n    /* weirdary = { WEIRD_ADDR, ...} = { 0xdeadc0de, ... } */\n}\n</code></pre> <p>Usage: Enable via sysctl to detect: - Use of uninitialized memory (all 0xFF bytes) - Use after free (0xDEADC0DE pattern)</p>"},{"location":"sys/kern/memory/#object-cache-objcache","title":"Object Cache (objcache)","text":"<p>The object cache system (<code>kern_objcache.c</code>) provides a high-level caching layer for frequently allocated objects. It implements a magazine-based architecture similar to Solaris/Illumos slab allocator.</p>"},{"location":"sys/kern/memory/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph PERCPU[\"Per-CPU Layer (lock-free)\"]\n        subgraph CPU0[\"CPU 0\"]\n            L0[\"loadedmagazine\"]\n            P0[\"previousmagazine\"]\n        end\n        subgraph CPU1[\"CPU 1\"]\n            L1[\"loadedmagazine\"]\n            P1[\"previousmagazine\"]\n        end\n        subgraph CPUN[\"CPU N\"]\n            LN[\"loadedmagazine\"]\n            PN[\"previousmagazine\"]\n        end\n    end\n\n    subgraph DEPOT[\"Depot Layer (locked)Cluster 0 Depot (spinlock protected)\"]\n        FULL[\"Full Magazines\"]\n        EMPTY[\"Empty Magazines\"]\n        STATS[\"unallocated_objects: 1024cluster_limit: 2048\"]\n    end\n\n    subgraph BACKEND[\"Backend Allocator\"]\n        ALLOC[\"objcache_malloc_alloc() / objcache_malloc_free()(or custom allocator)\"]\n    end\n\n    PERCPU --&gt; DEPOT\n    DEPOT --&gt; BACKEND\n</code></pre>"},{"location":"sys/kern/memory/#key-data-structures","title":"Key Data Structures","text":"<pre><code>/* Magazine - array of object pointers */\nstruct magazine {\n    int rounds;              /* Current number of objects */\n    int capacity;            /* Maximum capacity */\n    SLIST_ENTRY(magazine) nextmagazine;\n    void *objects[];         /* Flexible array of object pointers */\n};\n\n/* Per-CPU cache state */\nstruct percpu_objcache {\n    struct magazine *loaded_magazine;    /* Active magazine */\n    struct magazine *previous_magazine;  /* Backup magazine */\n\n    /* Statistics */\n    u_long gets_cumulative;\n    u_long gets_null;\n    u_long allocs_cumulative;  /* Backend allocations */\n    u_long puts_cumulative;\n    u_long gets_exhausted;     /* Hit limit */\n};\n\n/* Depot (shared between CPUs) */\nstruct magazinedepot {\n    struct magazinelist fullmagazines;\n    struct magazinelist emptymagazines;\n    int magcapacity;           /* Objects per magazine */\n    struct spinlock spin;      /* Protects depot */\n\n    int unallocated_objects;   /* Remaining quota */\n    int cluster_limit;         /* Total object limit */\n    int waiting;               /* Waiters for objects */\n};\n\n/* Object cache */\nstruct objcache {\n    objcache_ctor_fn  *ctor;       /* Constructor */\n    objcache_dtor_fn  *dtor;       /* Destructor */\n    void              *privdata;   /* Private data for ctor/dtor */\n\n    objcache_alloc_fn *alloc;      /* Backend allocator */\n    objcache_free_fn  *free;       /* Backend free */\n    void              *allocator_args;\n\n    struct magazinedepot depot[MAXCLUSTERS];  /* MAXCLUSTERS=1 currently */\n    struct percpu_objcache cache_percpu[];    /* Per-CPU caches */\n};\n</code></pre>"},{"location":"sys/kern/memory/#creating-an-object-cache","title":"Creating an Object Cache","text":"<pre><code>/* Simple cache backed by kmalloc */\nstruct objcache *cache;\ncache = objcache_create_simple(M_MYTYPE, sizeof(struct my_object));\n\n/* Cache with constructor/destructor */\nstatic boolean_t my_ctor(void *obj, void *privdata, int ocflags) {\n    struct my_object *o = obj;\n    /* Initialize object fields */\n    o-&gt;refcount = 1;\n    o-&gt;lock = NULL;\n    return TRUE;  /* Success */\n}\n\nstatic void my_dtor(void *obj, void *privdata) {\n    struct my_object *o = obj;\n    /* Clean up before freeing */\n    KKASSERT(o-&gt;refcount == 0);\n}\n\ncache = objcache_create_mbacked(\n    M_MYTYPE,                    /* malloc type */\n    sizeof(struct my_object),    /* object size */\n    1000,                        /* cluster_limit (total objects) */\n    200,                         /* nom_cache (cached objects) */\n    my_ctor,                     /* constructor */\n    my_dtor,                     /* destructor */\n    NULL                         /* privdata */\n);\n</code></pre> <p>Key parameters: - <code>cluster_limit</code>: Maximum total objects (0 = unlimited) - <code>nom_cache</code>: Desired number of cached objects - Magazine capacity is calculated: <code>nom_cache / (ncpus + 1) / 2</code></p>"},{"location":"sys/kern/memory/#allocating-objects","title":"Allocating Objects","text":"<pre><code>struct my_object *obj;\n\n/* Blocking allocation */\nobj = objcache_get(cache, M_WAITOK);\n\n/* Non-blocking allocation */\nobj = objcache_get(cache, M_NOWAIT);\nif (obj == NULL) {\n    /* Handle failure */\n}\n\n/* Use object */\nobj-&gt;data = value;\n\n/* Return to cache */\nobjcache_put(cache, obj);\n</code></pre> <p>Allocation Flow:</p> <pre><code>/* From objcache_get() - kern_objcache.c:427 */\n1. crit_enter();  /* Block preemption */\n\n2. Check loaded magazine:\n   if (loadedmag-&gt;rounds &gt; 0) {\n       obj = loadedmag-&gt;objects[--loadedmag-&gt;rounds];\n       crit_exit();\n       return obj;  /* FAST PATH - no depot access */\n   }\n\n3. Check previous magazine:\n   if (previous-&gt;rounds &gt; 0) {\n       swap(loaded, previous);\n       obj = loadedmag-&gt;objects[--loadedmag-&gt;rounds];\n       crit_exit();\n       return obj;  /* Still fast, no depot */\n   }\n\n4. Both magazines empty - depot exchange:\n   spin_lock(&amp;depot-&gt;spin);\n   if (!SLIST_EMPTY(&amp;depot-&gt;fullmagazines)) {\n       /* Exchange empty for full */\n       emptymag = previous;\n       previous = loaded;\n       loaded = SLIST_FIRST(&amp;depot-&gt;fullmagazines);\n       SLIST_REMOVE_HEAD(&amp;depot-&gt;fullmagazines);\n       SLIST_INSERT_HEAD(&amp;depot-&gt;emptymagazines, emptymag);\n       spin_unlock(&amp;depot-&gt;spin);\n       goto retry;  /* Now have full magazine */\n   }\n\n5. Depot empty - backend allocation:\n   if (depot-&gt;unallocated_objects &gt; 0) {\n       --depot-&gt;unallocated_objects;\n       spin_unlock(&amp;depot-&gt;spin);\n       crit_exit();\n\n       obj = oc-&gt;alloc(oc-&gt;allocator_args, ocflags);\n       if (obj &amp;&amp; !oc-&gt;ctor(obj, oc-&gt;privdata, ocflags)) {\n           oc-&gt;free(obj, oc-&gt;allocator_args);\n           obj = NULL;\n       }\n       return obj;\n   }\n\n6. Limit exceeded:\n   if (ocflags &amp; M_WAITOK) {\n       ssleep(depot, &amp;depot-&gt;spin, 0, \"objcache_get\", 0);\n       goto retry;\n   }\n   return NULL;\n</code></pre>"},{"location":"sys/kern/memory/#returning-objects","title":"Returning Objects","text":"<pre><code>/* From objcache_put() - kern_objcache.c:620 */\n1. crit_enter();\n\n2. Check loaded magazine:\n   if (loadedmag-&gt;rounds &lt; loadedmag-&gt;capacity) {\n       loadedmag-&gt;objects[loadedmag-&gt;rounds++] = obj;\n       crit_exit();\n       return;  /* FAST PATH */\n   }\n\n3. Check previous magazine:\n   if (previous-&gt;rounds &lt; previous-&gt;capacity) {\n       swap(loaded, previous);\n       loadedmag-&gt;objects[loadedmag-&gt;rounds++] = obj;\n       crit_exit();\n       return;\n   }\n\n4. Both magazines full - depot exchange:\n   spin_lock(&amp;depot-&gt;spin);\n   if (!SLIST_EMPTY(&amp;depot-&gt;emptymagazines)) {\n       /* Exchange full for empty */\n       loadedmag = previous;\n       previous = loaded;\n       loaded = SLIST_FIRST(&amp;depot-&gt;emptymagazines);\n       SLIST_REMOVE_HEAD(&amp;depot-&gt;emptymagazines);\n\n       if (MAGAZINE_EMPTY(loadedmag))\n           SLIST_INSERT_HEAD(&amp;depot-&gt;emptymagazines, loadedmag);\n       else\n           SLIST_INSERT_HEAD(&amp;depot-&gt;fullmagazines, loadedmag);\n\n       spin_unlock(&amp;depot-&gt;spin);\n       goto retry;\n   }\n\n5. No empty magazines - free to backend:\n   ++depot-&gt;unallocated_objects;\n   spin_unlock(&amp;depot-&gt;spin);\n   crit_exit();\n\n   oc-&gt;dtor(obj, oc-&gt;privdata);\n   oc-&gt;free(obj, oc-&gt;allocator_args);\n</code></pre>"},{"location":"sys/kern/memory/#magazine-sizing","title":"Magazine Sizing","text":"<p>Magazine capacity is cache-line aligned and bounded:</p> <pre><code>/* From objcache_create() */\n#define MAGAZINE_CAPACITY_MIN  4\n#define MAGAZINE_CAPACITY_MAX  4096\n\nmag_capacity = mag_capacity_align(nom_cache / (ncpus + 1) / 2 + 1);\nif (mag_capacity &gt; MAGAZINE_CAPACITY_MAX)\n    mag_capacity = MAGAZINE_CAPACITY_MAX;\nelse if (mag_capacity &lt; MAGAZINE_CAPACITY_MIN)\n    mag_capacity = MAGAZINE_CAPACITY_MIN;\n\n/* Align to cache line */\nmag_size = __VM_CACHELINE_ALIGN(offsetof(struct magazine, objects[mag_capacity]));\nmag_capacity = (mag_size - MAGAZINE_HDRSIZE) / sizeof(void *);\n</code></pre> <p>Example: For 1000 object cache on 4-CPU system: - <code>nom_cache / (4 + 1) / 2 = 1000 / 5 / 2 = 100</code> objects per magazine - Aligns to cache line, typically 96 or 128 depending on alignment</p>"},{"location":"sys/kern/memory/#dynamic-limit-adjustment","title":"Dynamic Limit Adjustment","text":"<pre><code>/* Adjust cluster limit at runtime */\nobjcache_set_cluster_limit(cache, new_limit);\n\n/* Affects depot-&gt;unallocated_objects:\n * delta = new_limit - old_cluster_limit;\n * depot-&gt;unallocated_objects += delta;\n */\n</code></pre> <p>Use case: Dynamically grow/shrink cache based on load.</p>"},{"location":"sys/kern/memory/#destroying-an-object-cache","title":"Destroying an Object Cache","text":"<pre><code>/* All objects must be returned to cache first */\nobjcache_destroy(cache);\n\n/* Process:\n * 1. Remove from global objcache list\n * 2. Drain all depot magazines (call dtors)\n * 3. Drain all per-CPU magazines\n * 4. Free magazines and objcache structure\n */\n</code></pre>"},{"location":"sys/kern/memory/#backend-allocators","title":"Backend Allocators","text":"<p>Built-in allocators:</p> <pre><code>/* Standard kmalloc backend */\nobjcache_malloc_alloc();    /* Allocates with kmalloc */\nobjcache_malloc_free();     /* Frees with kfree */\n\n/* kmalloc with M_ZERO */\nobjcache_malloc_alloc_zero();\n\n/* No-op backend (pre-allocated objects only) */\nobjcache_nop_alloc();\nobjcache_nop_free();\n</code></pre> <p>Custom backend:</p> <pre><code>void *my_alloc(void *allocator_args, int ocflags) {\n    struct my_alloc_args *args = allocator_args;\n    /* Custom allocation logic */\n    return custom_allocate(args-&gt;param, ocflags &amp; OC_MFLAGS);\n}\n\nvoid my_free(void *obj, void *allocator_args) {\n    /* Custom free logic */\n    custom_free(obj);\n}\n\nstruct my_alloc_args alloc_args = { .param = 42 };\n\ncache = objcache_create(\"mycache\", limit, nom_cache,\n                        ctor, dtor, privdata,\n                        my_alloc, my_free, &amp;alloc_args);\n</code></pre>"},{"location":"sys/kern/memory/#objcache-statistics","title":"Objcache Statistics","text":"<p>Statistics are accessible via <code>sysctl kern.objcache.stats</code>:</p> <pre><code>struct objcache_stats {\n    char oc_name[OBJCACHE_NAMELEN];\n    u_long oc_limit;       /* cluster_limit */\n    u_long oc_requested;   /* Total get requests */\n    u_long oc_allocated;   /* Backend allocations */\n    u_long oc_exhausted;   /* Times limit was hit */\n    u_long oc_failed;      /* Failed allocations */\n    u_long oc_used;        /* Currently allocated */\n    u_long oc_cached;      /* Cached in magazines */\n};\n</code></pre> <p>Example usage: <pre><code>$ sysctl kern.objcache.stats\n# Shows statistics for all objcaches\n</code></pre></p>"},{"location":"sys/kern/memory/#magazine-advantages","title":"Magazine Advantages","text":"<ol> <li>Batch operations: Amortize depot lock acquisition over magazine capacity</li> <li>CPU cache affinity: Recently freed objects are hot in cache</li> <li>Reduced contention: Most operations are per-CPU lock-free</li> <li>Simple: No complex data structures, just object pointer arrays</li> </ol>"},{"location":"sys/kern/memory/#when-to-use-objcache","title":"When to Use Objcache","text":"<p>Use objcache when: - Allocating many same-sized objects - High allocation/free frequency - Constructor/destructor needed - Want detailed statistics - Need fine-grained limit control</p> <p>Use kmalloc_obj when: - Fixed-size objects, simple lifecycle - Want type stability (bulk free) - Less overhead than objcache - No constructor/destructor needed</p> <p>Use plain kmalloc when: - Variable sizes - Infrequent allocations - No special lifecycle requirements</p>"},{"location":"sys/kern/memory/#malloc-pipes-mpipe","title":"Malloc Pipes (mpipe)","text":"<p>Location: <code>sys/kern/kern_mpipe.c</code></p> <p>Malloc pipes provide pre-allocated object pools with automatic growth and callback support. They are primarily used for critical allocations that cannot fail, such as network packet buffers (mbufs).</p>"},{"location":"sys/kern/memory/#architecture_1","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph MPIPE[\"Malloc Pipe (mpipe)\"]\n        FREE[\"Free list (LIFO, cache-hot)obj1 \u2192 obj2 \u2192 obj3 \u2192 ...\"]\n        TOKEN[\"Allocation token (atomic)Lock-free fast path\"]\n        CALLBACK[\"Callback mechanism\u2022 MPF_CALLBACK flag\u2022 Support thread (mpipe_thread)\u2022 Pending request queue\"]\n        CTOR[\"Constructor/deconstructorCalled on alloc/free\"]\n        STATS[\"Statisticstotal, free, array size\"]\n    end\n</code></pre>"},{"location":"sys/kern/memory/#key-features","title":"Key Features","text":"<ol> <li>Pre-allocated pool: Objects allocated at initialization</li> <li>Lock-free allocation: Uses atomic token instead of spinlock</li> <li>Callback support: Can queue requests when pool exhausted</li> <li>LIFO caching: Recently freed objects are cache-hot</li> <li>Constructor/deconstructor: Called on alloc/free for initialization</li> <li>Statistics: Track usage via sysctl</li> </ol>"},{"location":"sys/kern/memory/#api","title":"API","text":""},{"location":"sys/kern/memory/#creating-a-malloc-pipe","title":"Creating a Malloc Pipe","text":"<pre><code>void mpipe_init(malloc_pipe_t mpipe, malloc_type_t type,\n                size_t bytes, int nnom, int nmax,\n                int mpflags,\n                void (*construct)(void *, void *),\n                void (*deconstruct)(void *, void *),\n                void *priv);\n\n/* Parameters:\n * mpipe:      Pointer to malloc_pipe structure\n * type:       Malloc type for accounting\n * bytes:      Size of each object\n * nnom:       Nominal number of objects (initial allocation)\n * nmax:       Maximum number of objects (0 = unlimited)\n * mpflags:    MPF_CALLBACK (enable callback support) | MPF_NOZERO (don't zero)\n * construct:  Constructor called on allocation\n * deconstruct: Deconstructor called on free\n * priv:       Private data passed to construct/deconstruct\n */\n</code></pre> <p>Example: <pre><code>#include &lt;sys/mpipe.h&gt;\n\nstruct malloc_pipe my_pipe;\n\nvoid my_construct(void *obj, void *priv) {\n    struct my_object *o = obj;\n    /* Initialize object fields */\n    o-&gt;magic = MY_MAGIC;\n    o-&gt;refcount = 1;\n}\n\nvoid my_deconstruct(void *obj, void *priv) {\n    struct my_object *o = obj;\n    /* Clean up object before returning to pool */\n    KASSERT(o-&gt;refcount == 0, (\"mpipe: object still referenced\"));\n    o-&gt;magic = 0;\n}\n\nvoid init_my_subsystem(void) {\n    mpipe_init(&amp;my_pipe, M_MYSUBSYS, sizeof(struct my_object),\n               64,      /* Start with 64 objects */\n               1024,    /* Max 1024 objects */\n               MPF_CALLBACK,  /* Enable callback support */\n               my_construct, my_deconstruct, NULL);\n}\n</code></pre></p>"},{"location":"sys/kern/memory/#allocating-from-pipe","title":"Allocating from Pipe","text":"<pre><code>void *mpipe_alloc_waitok(malloc_pipe_t mpipe);\nvoid *mpipe_alloc_nowait(malloc_pipe_t mpipe);\n\n/* mpipe_alloc_waitok():\n *  - Always succeeds (blocks if necessary)\n *  - If pool exhausted and nmax not reached, grows pool\n *  - If nmax reached and MPF_CALLBACK set, queues request\n *  - Constructor called before returning\n *\n * mpipe_alloc_nowait():\n *  - Returns NULL if pool exhausted\n *  - Never blocks\n *  - Constructor still called on success\n */\n</code></pre> <p>Example: <pre><code>struct my_object *obj;\n\n/* In process context (can sleep) */\nobj = mpipe_alloc_waitok(&amp;my_pipe);\n/* obj is never NULL */\n\n/* In interrupt context (cannot sleep) */\nobj = mpipe_alloc_nowait(&amp;my_pipe);\nif (obj == NULL) {\n    /* Handle allocation failure */\n    return ENOMEM;\n}\n\n/* Use object... */\n</code></pre></p>"},{"location":"sys/kern/memory/#freeing-to-pipe","title":"Freeing to Pipe","text":"<pre><code>void mpipe_free(malloc_pipe_t mpipe, void *obj);\n\n/* Process:\n * 1. Call deconstructor on obj\n * 2. Return obj to pipe's free list (LIFO)\n * 3. If MPF_CALLBACK and pending requests, wake support thread\n */\n</code></pre> <p>Example: <pre><code>/* Return object to pool */\nmpipe_free(&amp;my_pipe, obj);\n/* obj is back in pool, ready for reuse */\n</code></pre></p>"},{"location":"sys/kern/memory/#destroying-a-malloc-pipe","title":"Destroying a Malloc Pipe","text":"<pre><code>void mpipe_done(malloc_pipe_t mpipe);\n\n/* Cleans up malloc pipe:\n * - Frees all objects in pool\n * - Deallocates array memory\n * - Asserts that all objects were returned (ary_count == total_count)\n */\n</code></pre>"},{"location":"sys/kern/memory/#callback-mechanism","title":"Callback Mechanism","text":"<p>When <code>MPF_CALLBACK</code> is set, the malloc pipe can handle allocation requests when the pool is exhausted:</p> <pre><code>flowchart TB\n    A[\"1. mpipe_alloc_waitok() called\"]\n    B[\"2. Pool empty, nmax reached\"]\n    C[\"3. Add to pending list(mpipe-&gt;pending_list)\"]\n    D[\"4. tsleep() on mpipe\"]\n    E[\"... waiting ...\"]\n    F[\"5. Another thread calls mpipe_free()\"]\n    G[\"6. mpipe_thread woken\"]\n    H[\"7. Process pending requests\"]\n    I[\"8. Original thread woken\"]\n    J[\"9. Allocation completes\"]\n\n    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H --&gt; I --&gt; J\n</code></pre> <p>mpipe_thread (<code>kern_mpipe.c:277</code>): - Support thread for callback mechanism - Processes <code>mpipe_pending_list</code> - Wakes blocked allocators when objects become available - Only runs when MPF_CALLBACK pipes exist</p>"},{"location":"sys/kern/memory/#internal-structure","title":"Internal Structure","text":"<pre><code>struct malloc_pipe {\n    void    **array;           /* Array of object pointers */\n    int     total_count;       /* Total objects allocated */\n    int     free_count;        /* Objects in array */\n    int     ary_count;         /* Array size */\n    int     max_count;         /* nmax (0 = unlimited) */\n    int     nominal_count;     /* nnom */\n    int     mpflags;           /* MPF_* flags */\n    size_t  bytes;             /* Object size */\n    malloc_type_t type;        /* For accounting */\n    int     token;             /* Atomic allocation token */\n    void    (*construct)(void *, void *);\n    void    (*deconstruct)(void *, void *);\n    void    *priv;\n    /* ... */\n};\n</code></pre>"},{"location":"sys/kern/memory/#allocation-algorithm_1","title":"Allocation Algorithm","text":"<p>Fast path (lock-free): <pre><code>int mpipe_alloc_callback(...) {  /* kern_mpipe.c:103 */\n    /* 1. Try to get token atomically */\n    if (token_test_and_set(&amp;mpipe-&gt;token)) {\n        /* Someone else has token, slow path */\n        goto slow_path;\n    }\n\n    /* 2. We have token, check free list */\n    if (mpipe-&gt;free_count) {\n        /* 3. Pop object from array (LIFO) */\n        obj = mpipe-&gt;array[--mpipe-&gt;free_count];\n        token_clear(&amp;mpipe-&gt;token);\n\n        /* 4. Call constructor */\n        if (mpipe-&gt;construct)\n            mpipe-&gt;construct(obj, mpipe-&gt;priv);\n\n        return obj;\n    }\n\n    /* Pool empty, release token and go to slow path */\n    token_clear(&amp;mpipe-&gt;token);\n\nslow_path:\n    /* Acquire lock, check if need to grow pool... */\n}\n</code></pre></p> <p>Slow path (with lock): - Grow pool if <code>total_count &lt; max_count</code> (or <code>max_count == 0</code>) - If cannot grow and <code>MPF_CALLBACK</code> set, queue request - Otherwise block or return NULL</p>"},{"location":"sys/kern/memory/#statistics","title":"Statistics","text":"<pre><code>/* Per-pipe statistics via mpipe structure */\nstruct malloc_pipe {\n    int total_count;   /* Total objects ever allocated */\n    int free_count;    /* Objects currently in pool */\n    int ary_count;     /* Size of array */\n    /* ... */\n};\n\n/* Can be exposed via sysctl for monitoring */\n</code></pre>"},{"location":"sys/kern/memory/#usage-patterns","title":"Usage Patterns","text":"<p>Network packet buffers (mbufs): <pre><code>static struct malloc_pipe mbuf_pipe;\n\nvoid mbuf_init(void) {\n    mpipe_init(&amp;mbuf_pipe, M_MBUF, MSIZE,\n               256,     /* Start with 256 mbufs */\n               0,       /* Unlimited */\n               MPF_CALLBACK,\n               mbuf_construct, mbuf_deconstruct, NULL);\n}\n\nstruct mbuf *m_get(int how) {\n    if (how == M_WAITOK)\n        return mpipe_alloc_waitok(&amp;mbuf_pipe);\n    else\n        return mpipe_alloc_nowait(&amp;mbuf_pipe);\n}\n\nvoid m_free(struct mbuf *m) {\n    mpipe_free(&amp;mbuf_pipe, m);\n}\n</code></pre></p> <p>Temporary objects with initialization: <pre><code>static struct malloc_pipe temp_pipe;\n\nvoid temp_construct(void *obj, void *priv) {\n    struct temp_obj *t = obj;\n    spin_init(&amp;t-&gt;lock, \"tempobj\");\n    TAILQ_INIT(&amp;t-&gt;list);\n}\n\nvoid temp_deconstruct(void *obj, void *priv) {\n    struct temp_obj *t = obj;\n    spin_uninit(&amp;t-&gt;lock);\n    KASSERT(TAILQ_EMPTY(&amp;t-&gt;list), (\"temp_obj: list not empty\"));\n}\n</code></pre></p>"},{"location":"sys/kern/memory/#when-to-use-malloc-pipes","title":"When to Use Malloc Pipes","text":"<p>Use mpipe when: - Need guaranteed allocations (cannot fail) - High allocation/free frequency - Constructor/deconstructor required - Want to pre-allocate and limit resource usage - Need LIFO cache-hot behavior</p> <p>Use objcache when: - Need detailed statistics and tuning - Want dynamic magazine sizing - More complex caching strategies</p> <p>Use kmalloc when: - Simple allocations without special lifecycle - Variable sizes</p>"},{"location":"sys/kern/memory/#advantages","title":"Advantages","text":"<ol> <li>Pre-allocation: Amortize allocation cost at initialization</li> <li>Lock-free fast path: Uses atomic token, no spinlock contention</li> <li>Cache-hot: LIFO means recently freed objects are hot in CPU cache</li> <li>Simple: Straightforward LIFO array, easy to understand</li> <li>Callback mechanism: Handles exhaustion gracefully</li> </ol>"},{"location":"sys/kern/memory/#limitations","title":"Limitations","text":"<ol> <li>Fixed size: All objects same size</li> <li>Memory overhead: Pre-allocated objects consume memory even if unused</li> <li>Constructor overhead: Called on every allocation</li> <li>No depot: Unlike objcache, no sophisticated caching layers</li> </ol>"},{"location":"sys/kern/memory/#helper-allocators","title":"Helper Allocators","text":""},{"location":"sys/kern/memory/#alist-power-of-2-allocator","title":"alist: Power-of-2 Allocator","text":"<p>Location: <code>sys/kern/subr_alist.c</code></p> <p>The alist allocator manages free blocks using a radix tree and supports only power-of-2 sized allocations. It can handle unlimited address space sizes.</p> <p>Key characteristics: - Power-of-2 only: Can only allocate sizes that are powers of 2 - Power-of-2 aligned: Allocations aligned to their size - Radix tree: Efficient O(log n) operations with hinting - Unlimited size: No fixed maximum range - Bitmap-based: Uses bitmaps in tree nodes</p> <p>Data structure: <pre><code>typedef struct almeta {\n    alist_blk_t bm_bighint;  /* Biggest allocatable block in subtree */\n    u_daddr_t   bm_bitmap;   /* Bitmap of free blocks (power-of-2) */\n} almeta_t;\n\ntypedef struct alist {\n    alist_blk_t bl_radix;    /* Coverage of one meta element */\n    alist_blk_t bl_radix_mask;\n    alist_blk_t bl_radix_ext;\n    alist_blk_t bl_skip;     /* Starting skip (address offset) */\n    alist_blk_t bl_free;     /* Number of free blocks */\n    alist_blk_t bl_blocks;   /* Total blocks managed */\n    alist_blk_t bl_bighint;  /* Biggest hint */\n    almeta_t    *bl_root;    /* Root of radix tree */\n    /* ... */\n} *alist_t;\n</code></pre></p> <p>API: <pre><code>alist_t alist_create(alist_blk_t blocks, struct malloc_type *mtype);\nvoid alist_destroy(alist_t live);\nvoid alist_free(alist_t live, alist_blk_t blkno, alist_blk_t count);\nalist_blk_t alist_alloc(alist_t live, alist_blk_t count);\n</code></pre></p> <p>Example: <pre><code>/* Create alist for 1GB range (2^30 bytes, 4KB blocks = 2^18 blocks) */\nalist_t al = alist_create(262144, M_TEMP);  /* 2^18 blocks */\n\n/* Allocate 16 blocks (must be power-of-2) */\nalist_blk_t blk = alist_alloc(al, 16);\n\n/* Free the blocks */\nalist_free(al, blk, 16);\n\n/* Destroy */\nalist_destroy(al);\n</code></pre></p> <p>Use case: Managing memory ranges that require power-of-2 alignment (e.g., DMA buffers, hardware resource allocation).</p> <p>Algorithm (<code>subr_alist.c:185</code>): - Radix tree with 64-way fanout (ALIST_BMAP_RADIX = 64) - Each meta node has bitmap and bighint - Hinting accelerates allocation (bm_bighint) - Power-of-2 check: <code>count &amp; (count - 1) == 0</code></p>"},{"location":"sys/kern/memory/#blist-bitmap-block-allocator","title":"blist: Bitmap Block Allocator","text":"<p>Location: <code>sys/kern/subr_blist.c</code></p> <p>The blist allocator is a general-purpose bitmap allocator using a radix tree, primarily used for swap space allocation. Unlike alist, it supports arbitrary sizes and ranges.</p> <p>Key characteristics: - General sizes: Allocate any number of blocks - Arbitrary ranges: No power-of-2 restrictions - Fixed maximum: Cannot exceed initial size - Radix tree: Efficient with hinting - Swap allocator: Primary use case</p> <p>Data structure: <pre><code>typedef struct blmeta {\n    daddr_t bm_bighint;   /* Biggest allocatable run in subtree */\n    daddr_t bm_bitmap;    /* Bitmap of free blocks */\n} blmeta_t;\n\ntypedef struct blist {\n    daddr_t bl_blocks;    /* Total blocks managed */\n    daddr_t bl_radix;     /* Coverage of one meta element */\n    daddr_t bl_skip;      /* Starting skip */\n    daddr_t bl_free;      /* Number of free blocks */\n    daddr_t bl_rootblks;  /* daddr_t blks allocated for root */\n    blmeta_t *bl_root;    /* Root of radix tree */\n    /* ... */\n} *blist_t;\n</code></pre></p> <p>API: <pre><code>blist_t blist_create(daddr_t blocks, int flags);\nvoid blist_destroy(blist_t blist);\ndaddr_t blist_alloc(blist_t blist, daddr_t count);\nvoid blist_free(blist_t blist, daddr_t blkno, daddr_t count);\nvoid blist_resize(blist_t *pblist, daddr_t count, int freenew, int flags);\n</code></pre></p> <p>Example: <pre><code>/* Create blist for swap: 10000 blocks */\nblist_t bl = blist_create(10000, M_WAITOK);\n\n/* Allocate 100 blocks (any size) */\ndaddr_t blk = blist_alloc(bl, 100);\nif (blk == SWAPBLK_NONE) {\n    /* Allocation failed */\n}\n\n/* Free the blocks */\nblist_free(bl, blk, 100);\n\n/* Destroy */\nblist_destroy(bl);\n</code></pre></p> <p>Use case: Swap space management (<code>vm/swap_pager.c</code>).</p> <p>Algorithm (<code>subr_blist.c:276</code>): - Radix tree with BLIST_BMAP_RADIX fanout (currently 64) - Each meta has bitmap and bighint - Cursor-based allocation for sequential patterns - Supports resizing (grow or shrink)</p> <p>Differences from alist:</p> Feature alist blist Alignment Power-of-2 only Any Sizes Power-of-2 only Any Max range Unlimited (dynamic) Fixed at creation Primary use General power-of-2 ranges Swap allocation"},{"location":"sys/kern/memory/#sbuf-string-buffers","title":"sbuf: String Buffers","text":"<p>Location: <code>sys/kern/subr_sbuf.c</code></p> <p>The sbuf API provides dynamic string buffers with automatic growth, similar to C++ <code>std::string</code> or Java <code>StringBuilder</code>.</p> <p>Key features: - Dynamic growth: Automatically expands as needed - Safe: Bounds-checked operations - Printf-style: <code>sbuf_printf()</code> for formatted output - Fixed or auto: Can use fixed buffer or auto-allocate - Length tracking: Always know current length</p> <p>Data structure: <pre><code>struct sbuf {\n    char    *s_buf;       /* Buffer itself */\n    int     s_size;       /* Size of buffer */\n    int     s_len;        /* Current length */\n    int     s_flags;      /* Flags (SBUF_*) */\n    /* ... */\n};\n\n/* Flags */\n#define SBUF_FIXEDLEN   0x00000001  /* Fixed length buffer */\n#define SBUF_AUTOEXTEND 0x00000002  /* Auto-extend buffer */\n#define SBUF_OVERFLOWED 0x00000010  /* Buffer overflowed */\n#define SBUF_FINISHED   0x00000020  /* Buffer finalized */\n</code></pre></p> <p>API: <pre><code>/* Create/destroy */\nstruct sbuf *sbuf_new(struct sbuf *s, char *buf, int length, int flags);\nvoid sbuf_delete(struct sbuf *s);\n\n/* Append operations */\nint sbuf_cat(struct sbuf *s, const char *str);\nint sbuf_printf(struct sbuf *s, const char *fmt, ...);\nint sbuf_putc(struct sbuf *s, int c);\nint sbuf_bcpy(struct sbuf *s, const char *buf, size_t len);  /* Copy */\nint sbuf_bcat(struct sbuf *s, const char *buf, size_t len);  /* Append */\n\n/* Finalize and extract */\nint sbuf_finish(struct sbuf *s);  /* NUL-terminate */\nchar *sbuf_data(struct sbuf *s);\nint sbuf_len(struct sbuf *s);\nint sbuf_overflowed(struct sbuf *s);\n\n/* Clear */\nvoid sbuf_clear(struct sbuf *s);\n</code></pre></p> <p>Example - Auto-extending buffer: <pre><code>struct sbuf *sb;\n\n/* Create auto-extending buffer */\nsb = sbuf_new(NULL, NULL, 0, SBUF_AUTOEXTEND);\n\n/* Build string */\nsbuf_printf(sb, \"Process %d: \", proc-&gt;p_pid);\nsbuf_cat(sb, proc-&gt;p_comm);\nsbuf_printf(sb, \" (%d threads)\", proc-&gt;p_nthreads);\n\n/* Finalize */\nsbuf_finish(sb);\n\n/* Use the string */\nprintf(\"%s\\n\", sbuf_data(sb));\n\n/* Clean up */\nsbuf_delete(sb);\n</code></pre></p> <p>Example - Fixed buffer: <pre><code>char buffer[256];\nstruct sbuf sb;\n\n/* Use fixed buffer (no allocation) */\nsbuf_new(&amp;sb, buffer, sizeof(buffer), SBUF_FIXEDLEN);\n\nsbuf_printf(&amp;sb, \"CPU %d: %s\", cpuid, status);\nsbuf_finish(&amp;sb);\n\nif (sbuf_overflowed(&amp;sb)) {\n    printf(\"Buffer too small!\\n\");\n} else {\n    printf(\"%s\\n\", sbuf_data(&amp;sb));\n}\n\nsbuf_delete(&amp;sb);  /* Does not free buffer, just cleans up */\n</code></pre></p> <p>Use cases: - Building complex strings for sysctl output - Generating debug messages - Creating formatted data for userspace - Any string building where final size is unknown</p> <p>Performance note: Auto-extending sbufs reallocate using <code>kmalloc()</code>, which may be expensive for frequently reallocated buffers. Pre-allocate reasonable size if known.</p>"},{"location":"sys/kern/memory/#sglist-scatter-gather-lists","title":"sglist: Scatter-Gather Lists","text":"<p>Location: <code>sys/kern/subr_sglist.c</code></p> <p>The sglist API manages scatter-gather lists for DMA operations. It describes physically discontiguous memory regions as an array of <code>(address, length)</code> pairs.</p> <p>Key features: - Physical addressing: Describes physical memory ranges - DMA-friendly: Standard format for DMA engines - Bounds tracking: Knows remaining space - Utilities: Build from uio, mbufs, physical buffers</p> <p>Data structure: <pre><code>struct sglist_seg {\n    vm_paddr_t ss_paddr;  /* Physical address */\n    size_t     ss_len;    /* Length in bytes */\n};\n\nstruct sglist {\n    struct sglist_seg *sg_segs;  /* Array of segments */\n    int               sg_nseg;   /* Number of segments */\n    int               sg_maxseg; /* Maximum segments */\n    int               sg_refs;   /* Reference count */\n};\n</code></pre></p> <p>API: <pre><code>/* Create/destroy */\nstruct sglist *sglist_alloc(int nsegs, int mflags);\nstruct sglist *sglist_build(void *buf, size_t len, int mflags);\nvoid sglist_free(struct sglist *sg);\nvoid sglist_hold(struct sglist *sg);   /* Increment refcount */\nvoid sglist_release(struct sglist *sg); /* Decrement refcount */\n\n/* Reset for reuse */\nvoid sglist_reset(struct sglist *sg);\n\n/* Append segments */\nint sglist_append(struct sglist *sg, vm_paddr_t paddr, size_t len);\nint sglist_append_mbuf(struct sglist *sg, struct mbuf *m);\nint sglist_append_phys(struct sglist *sg, vm_paddr_t paddr, size_t len);\nint sglist_append_uio(struct sglist *sg, struct uio *uio);\n\n/* Query */\nint sglist_count(void *buf, size_t len);\nsize_t sglist_length(struct sglist *sg);\nint sglist_slice(struct sglist *original, struct sglist **slice,\n                 size_t offset, size_t length, int mflags);\n\n/* Join two sglists */\nint sglist_join(struct sglist *first, struct sglist *second,\n                struct sglist *out);\n</code></pre></p> <p>Example - Build sglist from buffer: <pre><code>void *buf = kmalloc(8192, M_TEMP, M_WAITOK);\nstruct sglist *sg;\n\n/* Build sglist describing physical pages of buf */\nsg = sglist_build(buf, 8192, M_WAITOK);\n\n/* Program DMA engine with sglist */\nfor (int i = 0; i &lt; sg-&gt;sg_nseg; i++) {\n    dma_program_segment(sg-&gt;sg_segs[i].ss_paddr,\n                       sg-&gt;sg_segs[i].ss_len);\n}\n\n/* Clean up */\nsglist_free(sg);\nkfree(buf, M_TEMP);\n</code></pre></p> <p>Example - Build from mbuf chain: <pre><code>struct mbuf *m = /* ... received packet ... */;\nstruct sglist *sg;\n\n/* Allocate sglist with enough segments for mbuf chain */\nint nsegs = /* estimate based on m_length(m) and page size */;\nsg = sglist_alloc(nsegs, M_WAITOK);\n\n/* Append mbuf chain to sglist */\nif (sglist_append_mbuf(sg, m) != 0) {\n    /* Not enough segments */\n    sglist_free(sg);\n    return EFBIG;\n}\n\n/* Use sglist for DMA... */\n\nsglist_free(sg);\n</code></pre></p> <p>Use cases: - Network DMA (NIC drivers) - Storage device DMA - Any hardware requiring scatter-gather I/O - Zero-copy transfers</p> <p>Implementation note (<code>subr_sglist.c:94</code>): - <code>sglist_build()</code> walks virtual address using <code>pmap_extract()</code> to find physical pages - Consecutive physical pages are coalesced into single segment - <code>sglist_append_mbuf()</code> walks mbuf chain, extracting physical addresses</p>"},{"location":"sys/kern/memory/#rbtree-red-black-tree-utilities","title":"rbtree: Red-Black Tree Utilities","text":"<p>Location: <code>sys/kern/subr_rbtree.c</code></p> <p>The rbtree module provides red-black tree helper functions, but DragonFly uses a custom implementation that is largely self-contained in header files (<code>sys/sys/tree.h</code>).</p> <p>Note: This is not a full-featured allocator, but rather utilities for tree-based data structures. Most code uses the <code>RB_*</code> macros from <code>&lt;sys/tree.h&gt;</code> directly.</p> <p>Red-black tree properties: 1. Every node is red or black 2. Root is black 3. Leaves (NIL) are black 4. Red nodes have black children 5. All paths from node to leaves have same black-height</p> <p>sys/tree.h macros: <pre><code>/* Define tree structure */\nRB_HEAD(my_tree, my_node);\n\n/* Define node structure */\nstruct my_node {\n    RB_ENTRY(my_node) entry;\n    int key;\n    /* ... data ... */\n};\n\n/* Generate tree functions */\nRB_GENERATE(my_tree, my_node, entry, my_compare);\n\n/* Use tree */\nstruct my_tree head = RB_INITIALIZER(&amp;head);\n\nRB_INSERT(my_tree, &amp;head, node);\nstruct my_node *found = RB_FIND(my_tree, &amp;head, &amp;lookup);\nRB_REMOVE(my_tree, &amp;head, node);\n\n/* Iteration */\nRB_FOREACH(node, my_tree, &amp;head) {\n    /* ... */\n}\n</code></pre></p> <p>Use cases: - Ordered key-value data structures - Interval trees - Priority queues with O(log n) operations</p> <p>Alternative: DragonFly also provides <code>&lt;sys/queue.h&gt;</code> for simpler lists/queues (TAILQ, LIST, SLIST, STAILQ).</p>"},{"location":"sys/kern/memory/#code-flow-examples","title":"Code Flow Examples","text":""},{"location":"sys/kern/memory/#example-1-simple-kmalloc-allocation","title":"Example 1: Simple kmalloc Allocation","text":"<p>Scenario: Allocate memory for a temporary buffer in process context.</p> <pre><code>void process_data(void) {\n    char *buffer;\n    size_t size = 4096;\n\n    /* Step 1: Call kmalloc */\n    buffer = kmalloc(size, M_TEMP, M_WAITOK | M_ZERO);\n    /* M_WAITOK: Can block (process context)\n     * M_ZERO: Zero-fill the buffer\n     */\n\n    /* Step 2: kmalloc() \u2192 kern_slaballoc.c:kmalloc() */\n    /* - Round size to power-of-2 zone: 4096 \u2192 zone 4096 */\n\n    /* Step 3: Get CPU-local zone structure */\n    /* - cpuid = mycpuid (no lock needed) */\n    /* - zone = &amp;SLZone[cpuid][zid] */\n\n    /* Step 4: Try to allocate from CPU-local zone */\n    /* - Check zone-&gt;z_NFree &gt; 0 */\n    /* - Pop chunk from zone-&gt;z_FreeChunk */\n    /* - Decrement zone-&gt;z_NFree */\n    /* - If M_ZERO, bzero() the buffer */\n\n    /* Step 5: Return buffer to caller */\n\n    /* Use buffer... */\n    bcopy(source, buffer, size);\n\n    /* Step 6: Free buffer */\n    kfree(buffer, M_TEMP);\n\n    /* Step 7: kfree() \u2192 kern_slaballoc.c:kfree() */\n    /* - Find zone based on pointer address */\n    /* - Determine owning CPU from chunk metadata */\n    /* - If local CPU: add to z_FreeChunk (fast) */\n    /* - If remote CPU: add to z_ReleaseChunk and send IPI */\n}\n</code></pre> <p>Flow diagram: <pre><code>kmalloc(4096, M_TEMP, M_WAITOK|M_ZERO)\n  \u2193\nFind zone: zid = 12 (4KB zone)\n  \u2193\nGet CPU-local zone: cpuid=2, zone=&amp;SLZone[2][12]\n  \u2193\nzone-&gt;z_NFree = 5 (chunks available)\n  \u2193\nPop chunk from z_FreeChunk list\n  \u2193\nz_NFree = 4\n  \u2193\nbzero(chunk, 4096)  \u2190 M_ZERO flag\n  \u2193\nReturn chunk to caller\n  \u2193\n... use memory ...\n  \u2193\nkfree(chunk, M_TEMP)\n  \u2193\nFind owning CPU: cpuid=2 (local)\n  \u2193\nAdd chunk to z_FreeChunk (LIFO)\n  \u2193\nz_NFree = 5\n</code></pre></p>"},{"location":"sys/kern/memory/#example-2-cross-cpu-free-with-ipi","title":"Example 2: Cross-CPU Free with IPI","text":"<p>Scenario: Thread on CPU 0 frees memory allocated by thread on CPU 2.</p> <pre><code>/* Thread on CPU 2 allocates */\nchar *buf = kmalloc(512, M_TEMP, M_WAITOK);\n/* Allocated from SLZone[2][zone_512] */\n\n/* Pass buf to thread on CPU 0 (via queue, etc.) */\n\n/* Thread on CPU 0 frees */\nkfree(buf, M_TEMP);\n\n/* Step 1: kfree() determines owning CPU from chunk */\n/* - Chunk metadata indicates CPU 2 */\n\n/* Step 2: Current CPU (0) != owning CPU (2) */\n/* - Cannot directly modify CPU 2's zone */\n\n/* Step 3: Add to remote CPU's release list */\n/* - zone-&gt;z_ReleaseChunk (uses lock) */\n/* - Add chunk to linked list */\n\n/* Step 4: Send IPI to CPU 2 */\n/* - lwkt_send_ipiq(2, ...) */\n\n/* Step 5: CPU 2 receives IPI */\n/* - Interrupt handler runs on CPU 2 */\n\n/* Step 6: Process release list */\n/* - Move chunks from z_ReleaseChunk to z_FreeChunk */\n/* - Update z_NFree */\n/* - Chunks now available for allocation on CPU 2 */\n</code></pre> <p>Flow diagram: <pre><code>CPU 2: kmalloc(512)\n  \u2193\nAllocate from SLZone[2][zone_512]\n  \u2193\nReturn chunk (metadata: owner=CPU2)\n  \u2193\n... pass to CPU 0 ...\n  \u2193\nCPU 0: kfree(chunk)\n  \u2193\nCheck owner: CPU 2 (remote!)\n  \u2193\nAcquire zone-&gt;z_ReleaseChunk lock\n  \u2193\nAdd chunk to z_ReleaseChunk list\n  \u2193\nRelease lock\n  \u2193\nSend IPI to CPU 2\n  \u2193\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nCPU 2: IPI arrives\n  \u2193\nslgd_alloc() or periodic check\n  \u2193\nProcess z_ReleaseChunk list\n  \u2193\nMove chunks to z_FreeChunk\n  \u2193\nz_NFree += count\n  \u2193\nChunks available for reuse on CPU 2\n</code></pre></p> <p>Why IPIs? - Maintain per-CPU zone invariants - Avoid lock contention on allocation (fast path) - Memory eventually returns to owning CPU's pool - Trade-off: Occasional IPI cost vs. lock-free allocation</p>"},{"location":"sys/kern/memory/#example-3-type-stable-object-allocation","title":"Example 3: Type-Stable Object Allocation","text":"<p>Scenario: VFS allocating vnodes with type stability.</p> <pre><code>/* Initialize malloc type for vnodes */\nMALLOC_DEFINE(M_VNODE, \"vnodes\", \"vnodes\");\n\nvoid vnode_init(void) {\n    /* Create type for vnodes */\n    kmalloc_create(&amp;M_VNODE);\n}\n\nstruct vnode *vnode_alloc(void) {\n    struct vnode *vp;\n\n    /* Step 1: Allocate from type-stable allocator */\n    vp = kmalloc_obj(sizeof(struct vnode), M_VNODE, M_WAITOK|M_ZERO);\n\n    /* Step 2: kmalloc_obj() flow */\n    /* - Get per-CPU active slab (objcache-&gt;slabdata[cpuid].active) */\n    /* - Check active-&gt;findex &lt; KMALLOC_SLAB_FOBJS */\n    /* - Pop object from fobjs[] circular buffer */\n    /* - If active slab exhausted, get new slab from partial list */\n\n    /* Step 3: Initialize vnode */\n    vp-&gt;v_type = VNON;\n    lockinit(&amp;vp-&gt;v_lock, \"vnode\", 0, LK_CANRECURSE);\n    TAILQ_INIT(&amp;vp-&gt;v_namecache);\n\n    return vp;\n}\n\nvoid vnode_free(struct vnode *vp) {\n    /* Clean up */\n    lockuninit(&amp;vp-&gt;v_lock);\n\n    /* Step 1: Free to type-stable allocator */\n    kfree_obj(vp, M_VNODE);\n\n    /* Step 2: kfree_obj() flow */\n    /* - Find slab from object address */\n    /* - Get per-CPU active slab */\n    /* - Add object to fobjs[] circular buffer */\n    /* - Update slab-&gt;findex atomically */\n    /* - Object remains in slab, not returned to system */\n}\n\nvoid vnode_unmount_all(struct mount *mp) {\n    /* On unmount: bulk destroy all vnodes for this mount */\n\n    /* Step 1: Walk vnode list, free each */\n    TAILQ_FOREACH(vp, &amp;mp-&gt;mnt_vnodelist, v_mntvnodes) {\n        kfree_obj(vp, M_VNODE);\n    }\n\n    /* Step 2: Type-stable allocator can now bulk-free slabs */\n    /* - All vnodes for mount are freed */\n    /* - Entire slabs may be empty */\n    /* - kmalloc_obj can return slabs to system */\n\n    /* Advantage: Type stability allows efficient bulk operations */\n}\n</code></pre> <p>Slab allocation flow: <pre><code>kmalloc_obj(sizeof(struct vnode), M_VNODE, ...)\n  \u2193\nGet per-CPU objcache: &amp;objcache[mycpuid]\n  \u2193\nGet active slab: objcache-&gt;slabdata[mycpuid].active\n  \u2193\nActive slab has free objects? (findex &lt; KMALLOC_SLAB_FOBJS)\n  \u2193 YES\nPop object: obj = active-&gt;fobjs[findex++]\n  \u2193\nReturn object\n  \u2193\n... use vnode ...\n  \u2193\nkfree_obj(vnode, M_VNODE)\n  \u2193\nFind slab from object address\n  \u2193\nGet per-CPU active slab\n  \u2193\nAdd to circular buffer: active-&gt;fobjs[findex++] = obj\n  \u2193\nObject back in pool (type-stable: not freed to system)\n</code></pre></p>"},{"location":"sys/kern/memory/#example-4-objcache-magazine-exchange","title":"Example 4: Objcache Magazine Exchange","text":"<p>Scenario: High-frequency allocation from objcache.</p> <pre><code>/* Create objcache for connection objects */\nstruct objcache *conn_cache;\n\nvoid conn_construct(void *obj, void *priv) {\n    struct connection *conn = obj;\n    bzero(conn, sizeof(*conn));\n    TAILQ_INIT(&amp;conn-&gt;send_queue);\n}\n\nvoid conn_destruct(void *obj, void *priv) {\n    struct connection *conn = obj;\n    KASSERT(TAILQ_EMPTY(&amp;conn-&gt;send_queue), (\"conn: queue not empty\"));\n}\n\nvoid conn_init(void) {\n    conn_cache = objcache_create(\"connections\",\n                                 1000,  /* cluster_limit */\n                                 0,     /* nom_cache (auto) */\n                                 conn_construct, conn_destruct, NULL,\n                                 objcache_malloc_alloc,\n                                 objcache_malloc_free, NULL);\n}\n\nstruct connection *conn_alloc(void) {\n    /* Step 1: Call objcache_get() */\n    return objcache_get(conn_cache, M_WAITOK);\n}\n\n/* objcache_get() flow - Fast path */\nvoid *objcache_get_fast_path(struct objcache *cache) {\n    struct objcache_cpu *cpudata;\n    struct magazine *mag;\n    void *obj;\n\n    /* Step 1: Get per-CPU data (lock-free) */\n    cpudata = &amp;cache-&gt;cpu[mycpuid];\n\n    /* Step 2: Get loaded magazine */\n    mag = cpudata-&gt;loaded_mag;\n\n    /* Step 3: Check if magazine has objects */\n    if (mag-&gt;rounds &gt; 0) {\n        /* Step 4: Pop object from magazine (lock-free) */\n        obj = mag-&gt;objects[--mag-&gt;rounds];\n\n        /* Step 5: Return object (constructor already called) */\n        return obj;\n    }\n\n    /* Magazine empty, go to slow path */\n    return objcache_get_slow_path(cache);\n}\n\n/* objcache_get() flow - Slow path (magazine empty) */\nvoid *objcache_get_slow_path(struct objcache *cache) {\n    struct objcache_cpu *cpudata = &amp;cache-&gt;cpu[mycpuid];\n    struct magazine *empty_mag, *full_mag;\n\n    /* Step 1: Loaded magazine is empty */\n    empty_mag = cpudata-&gt;loaded_mag;\n\n    /* Step 2: Check previous magazine */\n    if (cpudata-&gt;previous_mag &amp;&amp; cpudata-&gt;previous_mag-&gt;rounds &gt; 0) {\n        /* Step 3: Swap loaded &lt;-&gt; previous */\n        cpudata-&gt;loaded_mag = cpudata-&gt;previous_mag;\n        cpudata-&gt;previous_mag = empty_mag;\n\n        /* Step 4: Pop from newly loaded magazine */\n        struct magazine *mag = cpudata-&gt;loaded_mag;\n        return mag-&gt;objects[--mag-&gt;rounds];\n    }\n\n    /* Step 5: Both magazines empty, need to get from depot */\n    spin_lock(&amp;cache-&gt;depot_lock);\n\n    /* Step 6: Check depot for full magazine */\n    if (!SLIST_EMPTY(&amp;cache-&gt;depot_full)) {\n        full_mag = SLIST_FIRST(&amp;cache-&gt;depot_full);\n        SLIST_REMOVE_HEAD(&amp;cache-&gt;depot_full, link);\n        cache-&gt;depot_full_count--;\n\n        /* Step 7: Return empty magazine to depot */\n        SLIST_INSERT_HEAD(&amp;cache-&gt;depot_empty, empty_mag, link);\n        cache-&gt;depot_empty_count++;\n\n        spin_unlock(&amp;cache-&gt;depot_lock);\n\n        /* Step 8: Install full magazine as loaded */\n        cpudata-&gt;loaded_mag = full_mag;\n\n        /* Step 9: Pop object */\n        return full_mag-&gt;objects[--full_mag-&gt;rounds];\n    }\n\n    spin_unlock(&amp;cache-&gt;depot_lock);\n\n    /* Step 10: Depot empty, allocate from backend */\n    return cache-&gt;alloc(cache-&gt;allocator_args, M_WAITOK);\n}\n</code></pre> <p>Magazine exchange diagram: <pre><code>objcache_get()\n  \u2193\nCheck loaded magazine: rounds = 0 (empty)\n  \u2193\nCheck previous magazine: rounds = 16 (full)\n  \u2193\nSwap loaded \u2194 previous (lock-free)\n  \u2193\nPop from loaded: obj = loaded-&gt;objects[15]\n  \u2193\nloaded-&gt;rounds = 15\n  \u2193\nReturn object\n  \u2193\n... 15 more fast allocations from loaded ...\n  \u2193\nBoth magazines empty\n  \u2193\nAcquire depot_lock\n  \u2193\nCheck depot_full list: has magazines\n  \u2193\nPop full magazine from depot\n  \u2193\nPush empty magazine to depot\n  \u2193\nRelease depot_lock\n  \u2193\nInstall full magazine as loaded\n  \u2193\nPop object from new loaded magazine\n</code></pre></p> <p>Key insight: Most allocations are lock-free from per-CPU magazines. Only when magazines are exhausted do we acquire the depot lock.</p>"},{"location":"sys/kern/memory/#example-5-zone-recycling-and-hysteresis","title":"Example 5: Zone Recycling and Hysteresis","text":"<p>Scenario: System with low memory pressure triggers zone recycling.</p> <pre><code>/* Slab allocator monitors zone usage via:\n * - ZGlobalZone[zid].ZoneRelsThresh (hysteresis threshold)\n * - zone-&gt;z_NFree (free chunks in zone)\n * - zone-&gt;z_NMax (maximum free chunks before recycling)\n */\n\n/* Allocation increases zone pressure */\nvoid allocation_surge(void) {\n    for (int i = 0; i &lt; 1000; i++) {\n        void *p = kmalloc(1024, M_TEMP, M_WAITOK);\n        /* ... use p ... */\n    }\n    /* Many zones now have high z_NFree (lots of free chunks cached) */\n}\n\n/* Periodic zone cleanup (kern_slaballoc.c:slab_cleanup()) */\nvoid slab_cleanup(void) {\n    /* Called periodically (every few seconds) */\n\n    for (int zid = 0; zid &lt; nzones; zid++) {\n        SLGlobalZone *zglobal = &amp;ZGlobalZone[zid];\n\n        /* Step 1: Calculate zone release threshold */\n        /* Based on:\n         * - Total zone memory\n         * - System memory pressure\n         * - Hysteresis to avoid thrashing\n         */\n        int thresh = zglobal-&gt;ZoneRelsThresh;\n\n        /* Step 2: Check each CPU's zone */\n        for (int cpuid = 0; cpuid &lt; ncpus; cpuid++) {\n            SLZone *zone = &amp;SLZone[cpuid][zid];\n\n            /* Step 3: Exceeds threshold? */\n            if (zone-&gt;z_NFree &gt; thresh) {\n                /* Step 4: Recycle excess chunks */\n                int excess = zone-&gt;z_NFree - (thresh / 2);\n\n                /* Step 5: Free excess chunks back to system */\n                for (int i = 0; i &lt; excess; i++) {\n                    void *chunk = zone-&gt;z_FreeChunk;\n                    zone-&gt;z_FreeChunk = *(void **)chunk;\n                    zone-&gt;z_NFree--;\n\n                    /* Step 6: Return page to VM system */\n                    kfree_real(chunk);  /* \u2192 vm_page_free() */\n                }\n            }\n        }\n    }\n}\n\n/* Result: Zone caches are trimmed, memory returned to VM */\n</code></pre> <p>Hysteresis prevents thrashing: <pre><code>Zone free chunks over time:\n\nz_NFree\n  ^\n  |\n200 |         \u2571\u2572              \u2190 Threshold (ZoneRelsThresh)\n  |        \u2571  \u2572\n  |       \u2571    \u2572___\n100 |------\u2571------\u2572___________  \u2190 Recycle to 100\n  |     \u2571              \u2572\n  |    \u2571                \u2572\n  0 |___/________________\u2572___&gt; time\n     Alloc    Recycle    Steady state\n     surge    event\n\nWithout hysteresis:\n- Constant oscillation around threshold\n- Frequent expensive recycle operations\n\nWith hysteresis:\n- Recycle only when significantly above threshold\n- Recycle to midpoint (thresh/2)\n- Stable behavior under varying load\n</code></pre></p>"},{"location":"sys/kern/memory/#best-practices-and-guidelines","title":"Best Practices and Guidelines","text":""},{"location":"sys/kern/memory/#choosing-the-right-allocator","title":"Choosing the Right Allocator","text":"<p>Decision tree:</p> <pre><code>flowchart TB\n    Q[\"What are you allocating?\"]\n\n    Q --&gt; VAR[\"Variable sizes, infrequent\u2192 Use kmalloc/kfree\"]\n    Q --&gt; SAME[\"Same-size, high frequency, no ctor/dtor\u2192 Use kmalloc_obj/kfree_obj\"]\n    Q --&gt; CTOR[\"Same-size, high frequency, need ctor/dtor\u2192 Use objcache\"]\n    Q --&gt; PRE[\"Pre-allocated pool, guaranteed allocation\u2192 Use mpipe\"]\n    Q --&gt; POW[\"Power-of-2 ranges, alignment critical\u2192 Use alist\"]\n    Q --&gt; GEN[\"General ranges (e.g., swap blocks)\u2192 Use blist\"]\n    Q --&gt; STR[\"Building strings dynamically\u2192 Use sbuf\"]\n    Q --&gt; DMA[\"DMA scatter-gather lists\u2192 Use sglist\"]\n</code></pre> <p>Detailed guidelines:</p> Allocator Best For Avoid When kmalloc General allocations, variable sizes, infrequent High-frequency same-size objects kmalloc_obj Fixed-size, high frequency, type stability Need ctor/dtor, variable sizes objcache High frequency, ctor/dtor, statistics Simple allocations, low frequency mpipe Pre-allocated pools, guaranteed success Dynamic sizing, unpredictable counts alist Power-of-2 aligned ranges Non-power-of-2 sizes blist General block ranges (swap) Need unlimited growth sbuf Dynamic string building Fixed-size strings sglist DMA scatter-gather Non-DMA memory management"},{"location":"sys/kern/memory/#allocation-flags-m_waitok-vs-m_nowait","title":"Allocation Flags: M_WAITOK vs. M_NOWAIT","text":"<p>M_WAITOK (can sleep): <pre><code>/* Use in process context where blocking is acceptable */\nvoid process_context_function(void) {\n    void *p = kmalloc(size, M_TEMP, M_WAITOK);\n    /* p is guaranteed to be non-NULL (or panic if truly OOM) */\n\n    /* Advantages:\n     * - Simple error handling (almost always succeeds)\n     * - Can trigger VM pageout to free memory\n     * - No need to check for NULL\n     */\n}\n</code></pre></p> <p>M_NOWAIT (cannot sleep): <pre><code>/* Use in interrupt context or holding locks */\nvoid interrupt_handler(void) {\n    void *p = kmalloc(size, M_TEMP, M_NOWAIT);\n    if (p == NULL) {\n        /* Handle allocation failure */\n        return ENOMEM;\n    }\n    /* Use p... */\n}\n\nvoid holding_spinlock(void) {\n    spin_lock(&amp;lock);\n\n    void *p = kmalloc(size, M_TEMP, M_NOWAIT);\n    if (p == NULL) {\n        spin_unlock(&amp;lock);\n        return ENOMEM;\n    }\n\n    /* Use p... */\n    spin_unlock(&amp;lock);\n}\n</code></pre></p> <p>Guidelines:</p> <ol> <li>Default to M_WAITOK in process context</li> <li>Simpler code (no NULL checks)</li> <li>Better resource utilization</li> <li> <p>Kernel can free memory if needed</p> </li> <li> <p>Use M_NOWAIT when:</p> </li> <li>In interrupt context</li> <li>Holding spinlocks</li> <li>In callback functions (may be called from interrupt)</li> <li> <p>Performance-critical paths where blocking is unacceptable</p> </li> <li> <p>Always check M_NOWAIT return:    <pre><code>/* WRONG */\np = kmalloc(size, M_TEMP, M_NOWAIT);\nbcopy(src, p, size);  /* May crash if p == NULL! */\n\n/* CORRECT */\np = kmalloc(size, M_TEMP, M_NOWAIT);\nif (p == NULL)\n    return ENOMEM;\nbcopy(src, p, size);\n</code></pre></p> </li> <li> <p>M_INTWAIT: Special flag for slightly more aggressive M_NOWAIT</p> </li> <li>Will try harder than M_NOWAIT</li> <li>Still won't block</li> <li>Use in interrupt context when allocation is important</li> </ol>"},{"location":"sys/kern/memory/#memory-leak-prevention","title":"Memory Leak Prevention","text":"<p>Common patterns:</p> <ol> <li> <p>Match alloc/free:    <pre><code>/* WRONG: Leak on error path */\nint process_data(void) {\n    char *buf = kmalloc(size, M_TEMP, M_WAITOK);\n\n    if (some_error())\n        return EINVAL;  /* Leak: forgot to free buf! */\n\n    /* ... use buf ... */\n    kfree(buf, M_TEMP);\n    return 0;\n}\n\n/* CORRECT: Free on all paths */\nint process_data(void) {\n    char *buf = kmalloc(size, M_TEMP, M_WAITOK);\n    int error = 0;\n\n    if (some_error()) {\n        error = EINVAL;\n        goto cleanup;\n    }\n\n    /* ... use buf ... */\n\ncleanup:\n    kfree(buf, M_TEMP);\n    return error;\n}\n</code></pre></p> </li> <li> <p>Use correct malloc type:    <pre><code>/* WRONG: Type mismatch */\np = kmalloc(size, M_VNODE, M_WAITOK);\n/* ... */\nkfree(p, M_TEMP);  /* Statistics corrupted! */\n\n/* CORRECT */\np = kmalloc(size, M_VNODE, M_WAITOK);\n/* ... */\nkfree(p, M_VNODE);\n</code></pre></p> </li> <li> <p>Avoid double-free:    <pre><code>/* WRONG: Double free */\nkfree(p, M_TEMP);\n/* ... */\nkfree(p, M_TEMP);  /* Corruption! */\n\n/* CORRECT: NULL after free */\nkfree(p, M_TEMP);\np = NULL;\n/* Later accidental free is safe */\nif (p != NULL)\n    kfree(p, M_TEMP);\n</code></pre></p> </li> <li> <p>Reference counting:    <pre><code>struct refcounted_obj {\n    int refcount;\n    /* ... data ... */\n};\n\nvoid obj_hold(struct refcounted_obj *obj) {\n    atomic_add_int(&amp;obj-&gt;refcount, 1);\n}\n\nvoid obj_drop(struct refcounted_obj *obj) {\n    if (atomic_fetchadd_int(&amp;obj-&gt;refcount, -1) == 1) {\n        /* Last reference, free object */\n        kfree(obj, M_OBJ);\n    }\n}\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/memory/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Batch allocations:    <pre><code>/* INEFFICIENT: Many small allocations */\nfor (int i = 0; i &lt; 1000; i++) {\n    items[i] = kmalloc(sizeof(item_t), M_TEMP, M_WAITOK);\n}\n\n/* EFFICIENT: One large allocation */\nitem_t *all_items = kmalloc(1000 * sizeof(item_t), M_TEMP, M_WAITOK);\nfor (int i = 0; i &lt; 1000; i++) {\n    items[i] = &amp;all_items[i];\n}\n</code></pre></p> </li> <li> <p>Use object caches for hot paths:    <pre><code>/* INEFFICIENT: kmalloc in packet processing loop */\nvoid process_packet(struct mbuf *m) {\n    struct pkt_ctx *ctx = kmalloc(sizeof(*ctx), M_TEMP, M_NOWAIT);\n    /* ... */\n    kfree(ctx, M_TEMP);\n}\n\n/* EFFICIENT: Use objcache */\nstatic struct objcache *pkt_ctx_cache;\n\nvoid process_packet(struct mbuf *m) {\n    struct pkt_ctx *ctx = objcache_get(pkt_ctx_cache, M_NOWAIT);\n    /* ... */\n    objcache_put(pkt_ctx_cache, ctx);\n}\n</code></pre></p> </li> <li> <p>Avoid allocations in interrupt context:    <pre><code>/* POOR: Allocate in interrupt */\nvoid device_interrupt(void) {\n    struct work *w = kmalloc(sizeof(*w), M_TEMP, M_NOWAIT);\n    if (w == NULL)\n        return;  /* Drop work! */\n    queue_work(w);\n}\n\n/* BETTER: Pre-allocated pool (mpipe) */\nstatic struct malloc_pipe work_pipe;\n\nvoid device_interrupt(void) {\n    struct work *w = mpipe_alloc_nowait(&amp;work_pipe);\n    if (w == NULL)\n        return;\n    queue_work(w);\n}\n</code></pre></p> </li> <li> <p>Cache-aware allocation:    <pre><code>/* Per-CPU data structure: use kmalloc_obj for cache affinity */\nstruct per_cpu_data *pcd = kmalloc_obj(sizeof(*pcd), M_PERCPU, M_WAITOK);\n/* Likely to be allocated from local CPU zone, cache-hot */\n</code></pre></p> </li> <li> <p>Reuse objects when possible:    <pre><code>/* INEFFICIENT: Constant alloc/free */\nvoid process_loop(void) {\n    for (;;) {\n        char *buf = kmalloc(BUFSIZE, M_TEMP, M_WAITOK);\n        process(buf);\n        kfree(buf, M_TEMP);\n    }\n}\n\n/* EFFICIENT: Allocate once */\nvoid process_loop(void) {\n    char *buf = kmalloc(BUFSIZE, M_TEMP, M_WAITOK);\n    for (;;) {\n        process(buf);\n        /* Reuse buf */\n    }\n    kfree(buf, M_TEMP);\n}\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/memory/#debugging-techniques","title":"Debugging Techniques","text":"<p>Enable INVARIANTS: <pre><code>/* In kernel config */\noptions INVARIANTS\n\n/* Enables:\n * - Memory pattern checks (0xdeadc0de, 0x5a5a5a5a)\n * - Chunk bitmap verification\n * - Redzone protection\n * - Double-free detection\n */\n</code></pre></p> <p>Use vmstat -m for malloc statistics: <pre><code>$ vmstat -m\n         Type InUse MemUse HighUse Requests  Size(s)\n       vnode   1234   123K    234K     5678  128,256\n        temp    567    67K     89K    12345  16,32,64,128,256\n       mbuf    8901   890K   1234K   567890  256\n# Shows:\n# - InUse: Currently allocated objects\n# - MemUse: Current memory usage\n# - HighUse: Peak memory usage\n# - Requests: Total allocation requests\n# - Size(s): Zones used\n</code></pre></p> <p>Check objcache statistics: <pre><code>$ sysctl kern.objcache.stats\n# Shows per-cache statistics:\n# - oc_limit: Cluster limit\n# - oc_requested: Total get requests\n# - oc_allocated: Backend allocations\n# - oc_exhausted: Times limit was hit\n# - oc_failed: Failed allocations\n# - oc_used: Currently allocated\n# - oc_cached: Cached in magazines\n</code></pre></p> <p>Memory leak detection: <pre><code>/* Before suspected leak point */\nvmstat -m &gt; before.txt\n\n/* Run code that might leak */\ntest_suspected_leak();\n\n/* After */\nvmstat -m &gt; after.txt\n\n/* Compare */\ndiff before.txt after.txt\n# Look for types with increased InUse counts\n</code></pre></p> <p>Kernel malloc tracing: <pre><code>/* Enable malloc tracing (if compiled with MALLOC_PROFILE) */\nsysctl debug.malloc.trace=1\n\n/* Generate trace */\n/* ... run workload ... */\n\n/* Analyze trace output in kernel log */\ndmesg | grep malloc\n</code></pre></p> <p>KTR (Kernel Trace) for detailed flow: <pre><code>/* In kernel config */\noptions KTR\noptions KTR_MEMORY\n\n/* At runtime */\nsysctl debug.ktr.entries=16384\nsysctl debug.ktr.mask=0x800  /* KTR_MEMORY */\n\n/* Generate trace */\n/* ... run workload ... */\n\n/* Dump trace */\nsysctl debug.ktr.dump\n</code></pre></p> <p>Common memory corruption patterns: <pre><code>/* Use-after-free: INVARIANTS detects 0xdeadc0de pattern */\nkfree(p, M_TEMP);\np-&gt;field = 123;  /* INVARIANTS panic: 0xdeadc0de detected */\n\n/* Buffer overflow: Watch for panics about corrupted chunks */\nchar *buf = kmalloc(10, M_TEMP, M_WAITOK);\nstrcpy(buf, \"this_string_is_too_long\");  /* Corruption! */\n\n/* Double free: INVARIANTS detects chunk already free */\nkfree(p, M_TEMP);\nkfree(p, M_TEMP);  /* INVARIANTS panic: chunk already free */\n</code></pre></p>"},{"location":"sys/kern/memory/#security-considerations","title":"Security Considerations","text":"<ol> <li> <p>Zero sensitive data:    <pre><code>/* Always use M_ZERO for sensitive data */\nstruct password *pwd = kmalloc(sizeof(*pwd), M_SECURE, M_WAITOK|M_ZERO);\n\n/* After use, explicit zero before free */\nbzero(pwd, sizeof(*pwd));\nkfree(pwd, M_SECURE);\n</code></pre></p> </li> <li> <p>Avoid uninitialized data leaks:    <pre><code>/* UNSAFE: May leak kernel data to userspace */\nstruct userdata *ud = kmalloc(sizeof(*ud), M_TEMP, M_WAITOK);\nud-&gt;field1 = value1;\n/* ud-&gt;field2 uninitialized! */\ncopyout(ud, userptr, sizeof(*ud));  /* Leak! */\n\n/* SAFE: Zero or initialize all fields */\nstruct userdata *ud = kmalloc(sizeof(*ud), M_TEMP, M_WAITOK|M_ZERO);\nud-&gt;field1 = value1;\nud-&gt;field2 = value2;\ncopyout(ud, userptr, sizeof(*ud));\n</code></pre></p> </li> <li> <p>Validate sizes from userspace:    <pre><code>/* UNSAFE: User can cause huge allocation */\nsize_t size;\ncopyin(userptr, &amp;size, sizeof(size));\nvoid *buf = kmalloc(size, M_TEMP, M_WAITOK);  /* Danger! */\n\n/* SAFE: Validate and limit size */\nsize_t size;\ncopyin(userptr, &amp;size, sizeof(size));\nif (size &gt; MAX_USER_ALLOC) {\n    return EINVAL;\n}\nvoid *buf = kmalloc(size, M_TEMP, M_WAITOK);\n</code></pre></p> </li> <li> <p>Use M_NULLOK when appropriate:    <pre><code>/* Allow NULL return instead of panic on exhaustion */\nvoid *p = kmalloc(huge_size, M_TEMP, M_WAITOK|M_NULLOK);\nif (p == NULL) {\n    /* Handle gracefully instead of panic */\n    return ENOMEM;\n}\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/memory/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li> <p>Allocating with locks held:    <pre><code>/* WRONG: May deadlock */\nspin_lock(&amp;lock);\np = kmalloc(size, M_TEMP, M_WAITOK);  /* Can't sleep with spinlock! */\nspin_unlock(&amp;lock);\n\n/* CORRECT: Allocate before lock or use M_NOWAIT */\np = kmalloc(size, M_TEMP, M_WAITOK);\nspin_lock(&amp;lock);\n/* ... use p ... */\nspin_unlock(&amp;lock);\n</code></pre></p> </li> <li> <p>Ignoring M_NOWAIT failures:    <pre><code>/* WRONG: No NULL check */\np = kmalloc(size, M_TEMP, M_NOWAIT);\nbcopy(src, p, size);  /* Crash if p == NULL! */\n\n/* CORRECT */\np = kmalloc(size, M_TEMP, M_NOWAIT);\nif (p == NULL)\n    return ENOMEM;\nbcopy(src, p, size);\n</code></pre></p> </li> <li> <p>Type-stable allocator misconceptions:    <pre><code>/* WRONG: Expecting memory to be returned to VM immediately */\nkfree_obj(obj, M_TYPE);\n/* Memory stays in type-stable slab, not freed to VM! */\n\n/* This is intentional: Type stability keeps memory allocated\n * for fast reuse. Use kmalloc if you need memory returned\n * to system immediately.\n */\n</code></pre></p> </li> <li> <p>Objcache magazine sizing:    <pre><code>/* POOR: Cluster limit too small */\ncache = objcache_create(\"foo\", 10, 0, ...);  /* Only 10 objects max */\n/* Under load, will hit limit and fall back to backend frequently */\n\n/* BETTER: Size based on expected load */\ncache = objcache_create(\"foo\", 1000, 0, ...);  /* Allow 1000 cached */\n</code></pre></p> </li> <li> <p>Mixing allocators:    <pre><code>/* WRONG: Allocate with one, free with another */\np = kmalloc_obj(size, M_TYPE, M_WAITOK);\nkfree(p, M_TYPE);  /* Should use kfree_obj! */\n\n/* CORRECT */\np = kmalloc_obj(size, M_TYPE, M_WAITOK);\nkfree_obj(p, M_TYPE);\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/memory/#summary","title":"Summary","text":"<p>DragonFly's memory allocation system provides a sophisticated, multi-layered architecture optimized for different use cases:</p> <ul> <li>kmalloc/kfree: General-purpose, power-of-2 zones, per-CPU optimization</li> <li>kmalloc_obj/kfree_obj: Type-stable, 128KB slabs, bulk operations</li> <li>objcache: Magazine/depot caching, constructor/destructor support</li> <li>mpipe: Pre-allocated pools, guaranteed allocation</li> <li>Helper allocators: alist, blist, sbuf, sglist for specialized needs</li> </ul> <p>Key design principles: 1. Lock-free fast paths: Per-CPU zones, atomic operations 2. Cache affinity: LIFO behavior, per-CPU allocation 3. Hysteresis: Prevent thrashing in zone recycling 4. Layered architecture: Choose the right tool for the job 5. Type stability: Enable efficient bulk operations</p> <p>The system balances performance, memory efficiency, and simplicity, providing appropriate allocators for different allocation patterns from high-frequency small objects to dynamic string building and DMA scatter-gather lists.</p>"},{"location":"sys/kern/newbus/","title":"NewBus Framework","text":"<p>The NewBus framework is DragonFly BSD's device driver infrastructure, providing a unified, object-oriented approach to device management. It handles device enumeration, driver binding, resource allocation, and interrupt management.</p> <p>Source files: - <code>sys/kern/subr_bus.c</code> - Core NewBus implementation (~3,900 lines) - <code>sys/kern/subr_autoconf.c</code> - Auto-configuration hooks - <code>sys/kern/device_if.m</code> - Device interface methods - <code>sys/kern/bus_if.m</code> - Bus interface methods - <code>sys/sys/bus.h</code> - Public data structures and macros - <code>sys/sys/bus_private.h</code> - Internal structures - <code>sys/sys/kobj.h</code> - Kernel object system (method dispatch)</p>"},{"location":"sys/kern/newbus/#overview","title":"Overview","text":"<p>NewBus derives from FreeBSD's NewBus but includes several DragonFly-specific extensions:</p> <ol> <li>CPU Affinity - Interrupt resources include CPU binding</li> <li>Asynchronous Probing - Devices can probe/attach in parallel threads</li> <li>Global Probe Priority - Fine-grained control over probe ordering</li> <li>LWKT Integration - Uses LWKT threads for async operations</li> <li>Serializer Support - Interrupt handlers can use LWKT serializers</li> <li>Threaded Interrupts Only - No fast interrupts; all are threaded</li> </ol>"},{"location":"sys/kern/newbus/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/newbus/#device_t-struct-bsd_device","title":"device_t (struct bsd_device)","text":"<p>The fundamental device representation:</p> <pre><code>struct bsd_device {\n    KOBJ_FIELDS;                        /* kobj ops table */\n\n    /* Device hierarchy */\n    TAILQ_ENTRY(bsd_device) link;       /* list of devices in parent */\n    TAILQ_ENTRY(bsd_device) devlink;    /* global device list */\n    device_t        parent;\n    device_list_t   children;           /* subordinate devices */\n\n    /* Device details */\n    driver_t       *driver;\n    devclass_t      devclass;\n    int             unit;\n    char           *nameunit;           /* e.g., \"em0\" */\n    char           *desc;               /* driver description */\n    int             busy;               /* device_busy() count */\n    device_state_t  state;\n    uint32_t        devflags;           /* API-level flags */\n    u_short         flags;              /* internal flags */\n    u_char          order;              /* attachment order */\n    void           *ivars;              /* bus-specific instance vars */\n    void           *softc;              /* driver-specific data */\n\n    struct sysctl_ctx_list sysctl_ctx;\n    struct sysctl_oid *sysctl_tree;\n};\n</code></pre> <p>Defined in <code>sys/sys/bus_private.h:97-139</code>.</p>"},{"location":"sys/kern/newbus/#device-flags","title":"Device Flags","text":"<p>Internal flags (<code>flags</code> field):</p> Flag Value Description <code>DF_ENABLED</code> 0x0001 Device should be probed/attached <code>DF_FIXEDCLASS</code> 0x0002 Devclass specified at creation <code>DF_WILDCARD</code> 0x0004 Unit originally wildcard (-1) <code>DF_DESCMALLOCED</code> 0x0008 Description was allocated <code>DF_QUIET</code> 0x0010 Don't print verbose attach message <code>DF_DONENOMATCH</code> 0x0020 DEVICE_NOMATCH already called <code>DF_EXTERNALSOFTC</code> 0x0040 softc not allocated by bus <code>DF_ASYNCPROBE</code> 0x0080 Can probe with its own thread <p>Defined in <code>sys/sys/bus_private.h:124-131</code>.</p>"},{"location":"sys/kern/newbus/#device_state_t","title":"device_state_t","text":"<p>Device lifecycle states:</p> <pre><code>typedef enum device_state {\n    DS_NOTPRESENT,    /* not probed or probe failed */\n    DS_ALIVE,         /* probe succeeded */\n    DS_INPROGRESS,    /* attach in progress (async) */\n    DS_ATTACHED,      /* attach method called */\n    DS_BUSY           /* device is open */\n} device_state_t;\n</code></pre> <p>Defined in <code>sys/sys/bus.h:76-82</code>.</p>"},{"location":"sys/kern/newbus/#devclass_t-struct-devclass","title":"devclass_t (struct devclass)","text":"<p>Groups devices by type (e.g., \"pci\", \"em\", \"ahci\"):</p> <pre><code>struct devclass {\n    TAILQ_ENTRY(devclass) link;\n    devclass_t      parent;         /* parent devclass hierarchy */\n    driver_list_t   drivers;        /* drivers for this bus type */\n    char           *name;\n    device_t       *devices;        /* array indexed by unit */\n    int             maxunit;        /* devices array size */\n\n    struct sysctl_ctx_list sysctl_ctx;\n    struct sysctl_oid *sysctl_tree;\n};\n</code></pre> <p>Defined in <code>sys/sys/bus_private.h:58-68</code>.</p>"},{"location":"sys/kern/newbus/#driver_t-kobj_class_t","title":"driver_t (kobj_class_t)","text":"<p>Driver definition using the kobj class system:</p> <pre><code>struct kobj_class {\n    const char     *name;       /* driver name */\n    kobj_method_t  *methods;    /* method table */\n    size_t          size;       /* softc size */\n    kobj_class_t   *baseclasses;/* inheritance */\n    u_int           refs;       /* reference count */\n    kobj_ops_t      ops;        /* compiled methods */\n    u_int           gpri;       /* global probe priority */\n};\n</code></pre> <p>Global Probe Priorities (<code>sys/sys/kobj.h:89-91</code>):</p> Priority Value Description <code>KOBJ_GPRI_ACPI</code> 0x00FF ACPI drivers probe first <code>KOBJ_GPRI_DEFAULT</code> 0x0000 Default priority <code>KOBJ_GPRI_LAST</code> 0x0000 Same as default <p>Defined in <code>sys/sys/kobj.h:62-73</code>.</p>"},{"location":"sys/kern/newbus/#struct-resource","title":"struct resource","text":"<p>Represents allocated system resources:</p> <pre><code>struct resource {\n    TAILQ_ENTRY(resource) r_link;\n    LIST_ENTRY(resource)  r_sharelink;\n    LIST_HEAD(, resource) *r_sharehead;\n    u_long          r_start;        /* first index */\n    u_long          r_end;          /* last index (inclusive) */\n    u_int           r_flags;\n    void           *r_virtual;      /* virtual address */\n    bus_space_tag_t r_bustag;\n    bus_space_handle_t r_bushandle;\n    device_t        r_dev;          /* owning device */\n    struct rman    *r_rm;           /* resource manager */\n    int             r_rid;          /* resource ID */\n};\n</code></pre> <p>Resource Flags (<code>sys/sys/rman.h:45-52</code>):</p> Flag Value Description <code>RF_ALLOCATED</code> 0x0001 Resource reserved <code>RF_ACTIVE</code> 0x0002 Resource activated <code>RF_SHAREABLE</code> 0x0004 Permits sharing <code>RF_TIMESHARE</code> 0x0008 Permits time-division sharing <code>RF_WANTED</code> 0x0010 Someone waiting <code>RF_FIRSTSHARE</code> 0x0020 First in sharing list <code>RF_PREFETCHABLE</code> 0x0040 Memory is prefetchable <code>RF_OPTIONAL</code> 0x0080 For bus_alloc_resources() <p>Defined in <code>sys/sys/rman.h:98-111</code>.</p>"},{"location":"sys/kern/newbus/#resource-types","title":"Resource Types","text":"<pre><code>#define SYS_RES_IRQ     1   /* interrupt lines */\n#define SYS_RES_DRQ     2   /* ISA DMA lines */\n#define SYS_RES_MEMORY  3   /* I/O memory */\n#define SYS_RES_IOPORT  4   /* I/O ports */\n</code></pre> <p>Defined in <code>sys/sys/bus_resource.h:41-44</code>.</p>"},{"location":"sys/kern/newbus/#struct-resource_list_entry","title":"struct resource_list_entry","text":"<p>Bus-specific resource tracking:</p> <pre><code>struct resource_list_entry {\n    SLIST_ENTRY(resource_list_entry) link;\n    int             type;       /* SYS_RES_* */\n    int             rid;        /* resource identifier */\n    struct resource *res;       /* actual resource */\n    u_long          start;\n    u_long          end;\n    u_long          count;\n    int             cpuid;      /* CPU affinity (DragonFly) */\n};\n</code></pre> <p>Defined in <code>sys/sys/bus.h:136-146</code>.</p>"},{"location":"sys/kern/newbus/#device-interface-methods","title":"Device Interface Methods","text":"<p>Core methods that drivers implement (from <code>sys/kern/device_if.m</code>):</p> Method Purpose <code>DEVICE_PROBE</code> Check if driver supports device; return priority <code>DEVICE_IDENTIFY</code> Add child devices to bus (static method) <code>DEVICE_ATTACH</code> Initialize hardware and allocate resources <code>DEVICE_DETACH</code> Remove device and free resources <code>DEVICE_SHUTDOWN</code> Prepare for system shutdown <code>DEVICE_SUSPEND</code> Save state before power management suspend <code>DEVICE_RESUME</code> Restore state after resume <code>DEVICE_QUIESCE</code> FreeBSD compat: prepare for detach <code>DEVICE_REGISTER</code> Return device registration data"},{"location":"sys/kern/newbus/#probe-return-values","title":"Probe Return Values","text":"<p>Drivers return priority values from <code>DEVICE_PROBE</code>:</p> Value Constant Description 0 <code>BUS_PROBE_SPECIFIC</code> Only this driver can use device 0 <code>BUS_PROBE_VENDOR</code> Vendor-supplied driver 0 <code>BUS_PROBE_DEFAULT</code> Standard driver 0 <code>BUS_PROBE_LOW_PRIORITY</code> Lower priority match 0 <code>BUS_PROBE_GENERIC</code> Generic fallback 0 <code>BUS_PROBE_HOOVER</code> Catch-all device <p>Note: In DragonFly, these are all defined as 0. Use <code>gpri</code> for priority ordering instead.</p> <p>See <code>sys/sys/bus.h:476-489</code>.</p>"},{"location":"sys/kern/newbus/#bus-interface-methods","title":"Bus Interface Methods","text":"<p>Methods for bus drivers to manage children (from <code>sys/kern/bus_if.m</code>):</p> Method Purpose <code>BUS_PRINT_CHILD</code> Print device attachment info <code>BUS_PROBE_NOMATCH</code> Called when no driver matches <code>BUS_READ_IVAR</code> / <code>BUS_WRITE_IVAR</code> Read/write instance variables <code>BUS_CHILD_DETACHED</code> Notification of child detach <code>BUS_DRIVER_ADDED</code> New driver added notification <code>BUS_ADD_CHILD</code> Create child device <code>BUS_ALLOC_RESOURCE</code> Allocate system resource <code>BUS_ACTIVATE_RESOURCE</code> Activate resource <code>BUS_DEACTIVATE_RESOURCE</code> Deactivate resource <code>BUS_RELEASE_RESOURCE</code> Free resource <code>BUS_SETUP_INTR</code> Set up interrupt handler <code>BUS_TEARDOWN_INTR</code> Remove interrupt handler <code>BUS_ENABLE_INTR</code> / <code>BUS_DISABLE_INTR</code> Enable/disable interrupt <code>BUS_SET_RESOURCE</code> Set resource range <code>BUS_GET_RESOURCE</code> Get resource range <code>BUS_DELETE_RESOURCE</code> Delete resource <code>BUS_GET_RESOURCE_LIST</code> Get resource list <code>BUS_CHILD_PRESENT</code> Check if child hardware present <code>BUS_CHILD_PNPINFO_STR</code> Get PnP info string <code>BUS_CHILD_LOCATION_STR</code> Get location string <code>BUS_CONFIG_INTR</code> Configure interrupt trigger/polarity <code>BUS_GET_DMA_TAG</code> Get DMA tag"},{"location":"sys/kern/newbus/#device-lifecycle","title":"Device Lifecycle","text":""},{"location":"sys/kern/newbus/#device-creation","title":"Device Creation","text":"<p>device_add_child() - <code>sys/kern/subr_bus.c:1240-1244</code></p> <p>Creates a new device as a child of an existing device:</p> <pre><code>device_t device_add_child(device_t dev, const char *name, int unit);\n</code></pre> <ul> <li><code>name</code> - Driver name to use (NULL for any)</li> <li><code>unit</code> - Unit number (-1 for auto-assignment)</li> </ul> <p>device_add_child_ordered() - <code>sys/kern/subr_bus.c:1246-1281</code></p> <p>Creates a child with explicit attachment order:</p> <pre><code>device_t device_add_child_ordered(device_t dev, int order,\n                                  const char *name, int unit);\n</code></pre> <p>Lower order values are probed first.</p> <p>make_device() - <code>sys/kern/subr_bus.c:1174-1225</code></p> <p>Internal function that: 1. Allocates <code>struct bsd_device</code> 2. Initializes kobj with null_class 3. Sets initial state to <code>DS_NOTPRESENT</code> 4. Adds to global <code>bus_data_devices</code> list</p>"},{"location":"sys/kern/newbus/#probe-and-attach-flow","title":"Probe and Attach Flow","text":"<p>device_probe_and_attach() - <code>sys/kern/subr_bus.c:1961-2013</code></p> <p>Main entry point for device attachment:</p> <ol> <li>Check if already alive</li> <li>Check if enabled</li> <li>Call <code>device_probe_child()</code></li> <li>On probe failure, call <code>BUS_PROBE_NOMATCH</code></li> <li>Print device info if verbose</li> <li>Either attach async or sync</li> </ol> <p>device_probe_child() - <code>sys/kern/subr_bus.c:1403-1489</code></p> <p>Finds the best matching driver:</p> <ol> <li>Get parent's devclass</li> <li>Iterate through devclass hierarchy</li> <li>For each driver:</li> <li>Set driver on device</li> <li>Call <code>DEVICE_PROBE()</code></li> <li>Track best match by priority</li> <li>Set device to best driver</li> <li>Change state to <code>DS_ALIVE</code></li> </ol> <p>device_doattach() - <code>sys/kern/subr_bus.c:2098-2124</code></p> <p>Performs actual attachment:</p> <ol> <li>Initialize sysctl tree</li> <li>Call <code>DEVICE_ATTACH()</code></li> <li>On success: set state to <code>DS_ATTACHED</code>, notify devctl</li> <li>On failure: restore to <code>DS_NOTPRESENT</code></li> </ol>"},{"location":"sys/kern/newbus/#asynchronous-attachment-dragonfly-extension","title":"Asynchronous Attachment (DragonFly Extension)","text":"<p>Enabled by <code>kern.do_async_attach</code> tunable.</p> <p>device_attach_async() - <code>sys/kern/subr_bus.c:2071-2093</code></p> <pre><code>static void\ndevice_attach_async(device_t dev)\n{\n    atomic_add_int(&amp;numasyncthreads, 1);\n    lwkt_create(device_attach_thread, dev, &amp;td, NULL,\n                0, 0, \"%s\", (dev-&gt;desc ? dev-&gt;desc : \"devattach\"));\n}\n</code></pre> <p>Devices must set <code>DF_ASYNCPROBE</code> flag to use async attach.</p>"},{"location":"sys/kern/newbus/#detach","title":"Detach","text":"<p>device_detach() - <code>sys/kern/subr_bus.c:2126-2152</code></p> <ol> <li>Check if busy</li> <li>Call <code>DEVICE_DETACH()</code></li> <li>Notify devctl</li> <li>Call <code>BUS_CHILD_DETACHED()</code></li> <li>Remove from devclass if not fixed</li> <li>Set state to <code>DS_NOTPRESENT</code></li> <li>Clear driver</li> </ol>"},{"location":"sys/kern/newbus/#resource-allocation","title":"Resource Allocation","text":""},{"location":"sys/kern/newbus/#allocating-resources","title":"Allocating Resources","text":"<p>bus_alloc_resource() - <code>sys/kern/subr_bus.c:3290-3298</code></p> <pre><code>struct resource *\nbus_alloc_resource(device_t dev, int type, int *rid,\n                   u_long start, u_long end, u_long count, u_int flags);\n</code></pre> <ul> <li><code>type</code> - <code>SYS_RES_IRQ</code>, <code>SYS_RES_MEMORY</code>, <code>SYS_RES_IOPORT</code>, <code>SYS_RES_DRQ</code></li> <li><code>rid</code> - Resource ID (in/out)</li> <li><code>start</code>, <code>end</code> - Requested range (0, ~0 for any)</li> <li><code>count</code> - Number of units</li> <li><code>flags</code> - <code>RF_*</code> flags</li> </ul> <p>bus_alloc_resource_any() - <code>sys/sys/bus.h:345-349</code> (inline)</p> <p>Convenience wrapper:</p> <pre><code>static __inline struct resource *\nbus_alloc_resource_any(device_t dev, int type, int *rid, u_int flags)\n{\n    return (bus_alloc_resource(dev, type, rid, 0ul, ~0ul, 1, flags));\n}\n</code></pre> <p>bus_alloc_legacy_irq_resource() - <code>sys/kern/subr_bus.c:3300-3307</code></p> <p>DragonFly-specific: Allocates IRQ with CPU affinity:</p> <pre><code>struct resource *\nbus_alloc_legacy_irq_resource(device_t dev, int *rid, u_long irq, u_int flags);\n</code></pre>"},{"location":"sys/kern/newbus/#activating-resources","title":"Activating Resources","text":"<p>bus_activate_resource() - <code>sys/kern/subr_bus.c:3309-3315</code></p> <p>Maps the resource (for memory/I/O port) or enables it (for IRQ).</p> <p>bus_deactivate_resource() - <code>sys/kern/subr_bus.c:3317-3323</code></p> <p>Unmaps or disables the resource.</p>"},{"location":"sys/kern/newbus/#releasing-resources","title":"Releasing Resources","text":"<p>bus_release_resource() - <code>sys/kern/subr_bus.c:3325-3331</code></p> <p>Frees a previously allocated resource.</p>"},{"location":"sys/kern/newbus/#resource-list-management","title":"Resource List Management","text":"<p>Buses use resource lists to track child resources:</p> Function Purpose <code>resource_list_init()</code> Initialize list <code>resource_list_free()</code> Free list and all entries <code>resource_list_add()</code> Add resource to list <code>resource_list_find()</code> Find resource by type/rid <code>resource_list_delete()</code> Remove resource from list <code>resource_list_alloc()</code> Allocate from list <code>resource_list_release()</code> Release resource <p>See <code>sys/kern/subr_bus.c:2592-2746</code>.</p>"},{"location":"sys/kern/newbus/#interrupt-handling","title":"Interrupt Handling","text":""},{"location":"sys/kern/newbus/#interrupt-flags","title":"Interrupt Flags","text":"<pre><code>#define INTR_HIFREQ     0x0040  /* high frequency interrupt */\n#define INTR_CLOCK      0x0080  /* clock interrupt */\n#define INTR_EXCL       0x0100  /* exclusive, non-shared */\n#define INTR_MPSAFE     0x0200  /* handler is MP-safe */\n#define INTR_NOENTROPY  0x0400  /* don't add to entropy pool */\n#define INTR_NOPOLL     0x0800  /* cannot be polled */\n</code></pre> <p>Note: <code>INTR_FAST</code> is no longer supported - all device interrupts are threaded.</p> <p>Defined in <code>sys/sys/bus.h:109-114</code>.</p>"},{"location":"sys/kern/newbus/#interrupt-trigger-and-polarity","title":"Interrupt Trigger and Polarity","text":"<pre><code>enum intr_trigger {\n    INTR_TRIGGER_CONFORM = 0,\n    INTR_TRIGGER_EDGE = 1,\n    INTR_TRIGGER_LEVEL = 2\n};\n\nenum intr_polarity {\n    INTR_POLARITY_CONFORM = 0,\n    INTR_POLARITY_HIGH = 1,\n    INTR_POLARITY_LOW = 2\n};\n</code></pre> <p>Defined in <code>sys/sys/bus.h:116-126</code>.</p>"},{"location":"sys/kern/newbus/#setting-up-interrupts","title":"Setting Up Interrupts","text":"<p>bus_setup_intr() - <code>sys/kern/subr_bus.c:3344-3351</code></p> <pre><code>int bus_setup_intr(device_t dev, struct resource *r, int flags,\n                   driver_intr_t handler, void *arg, void **cookiep,\n                   lwkt_serialize_t serializer);\n</code></pre> <ul> <li><code>r</code> - IRQ resource from <code>bus_alloc_resource()</code></li> <li><code>flags</code> - <code>INTR_*</code> flags</li> <li><code>handler</code> - Interrupt handler function</li> <li><code>arg</code> - Argument passed to handler</li> <li><code>cookiep</code> - Returns cookie for teardown</li> <li><code>serializer</code> - Optional LWKT serializer (DragonFly extension)</li> </ul> <p>bus_setup_intr_descr() - <code>sys/kern/subr_bus.c:3333-3342</code></p> <p>Same as above but with description string for debugging.</p>"},{"location":"sys/kern/newbus/#tearing-down-interrupts","title":"Tearing Down Interrupts","text":"<p>bus_teardown_intr() - <code>sys/kern/subr_bus.c:3353-3359</code></p> <pre><code>int bus_teardown_intr(device_t dev, struct resource *r, void *cookie);\n</code></pre>"},{"location":"sys/kern/newbus/#enablingdisabling-interrupts","title":"Enabling/Disabling Interrupts","text":"<p>bus_enable_intr() - <code>sys/kern/subr_bus.c:3361-3366</code> bus_disable_intr() - <code>sys/kern/subr_bus.c:3368-3375</code></p>"},{"location":"sys/kern/newbus/#devclass-management","title":"Devclass Management","text":""},{"location":"sys/kern/newbus/#findingcreating-devclasses","title":"Finding/Creating Devclasses","text":"<p>devclass_find() - <code>sys/kern/subr_bus.c:789-793</code></p> <p>Finds an existing devclass by name.</p> <p>devclass_create() - <code>sys/kern/subr_bus.c:783-787</code></p> <p>Finds or creates a devclass.</p> <p>devclass_find_internal() - <code>sys/kern/subr_bus.c:738-781</code></p> <p>Internal function that: 1. Searches global <code>devclasses</code> list 2. Creates if not found and <code>create</code> is true 3. Handles parent devclass for inheritance</p>"},{"location":"sys/kern/newbus/#driver-registration","title":"Driver Registration","text":"<p>devclass_add_driver() - <code>sys/kern/subr_bus.c:805-850</code></p> <p>Adds a driver to a devclass:</p> <ol> <li>Instantiate driver kobj class</li> <li>Create devclass for driver name</li> <li>Add to devclass driver list</li> <li>Call <code>BUS_DRIVER_ADDED</code> for existing buses</li> </ol> <p>devclass_delete_driver() - <code>sys/kern/subr_bus.c:852-906</code></p> <p>Removes a driver:</p> <ol> <li>Find driver link</li> <li>Detach all devices using driver</li> <li>Remove from driver list</li> <li>Uninstantiate kobj class</li> </ol>"},{"location":"sys/kern/newbus/#generic-bus-methods","title":"Generic Bus Methods","text":"<p>Standard implementations for bus drivers in <code>sys/kern/subr_bus.c</code>:</p> Function Line Purpose <code>bus_generic_identify</code> 2786 Add child with driver name <code>bus_generic_identify_sameunit</code> 2795 Add child with parent's unit <code>bus_generic_probe</code> 2807 Call DEVICE_IDENTIFY for all drivers <code>bus_generic_attach</code> 2840 Probe/attach all children <code>bus_generic_attach_gpri</code> 2852 Attach with specific priority <code>bus_generic_detach</code> 2864 Detach all children <code>bus_generic_shutdown</code> 2880 Shutdown all children <code>bus_generic_suspend</code> 2891 Suspend all children (with rollback) <code>bus_generic_resume</code> 2910 Resume all children <code>bus_generic_print_child</code> 2958 Print header + footer <code>bus_generic_driver_added</code> 3004 Try attaching unprobed children <code>bus_generic_setup_intr</code> 3016 Propagate to parent <code>bus_generic_teardown_intr</code> 3030 Propagate to parent <code>bus_generic_alloc_resource</code> 3068 Propagate to parent <code>bus_generic_release_resource</code> 3080 Propagate to parent"},{"location":"sys/kern/newbus/#configuration-resource-hints","title":"Configuration Resource Hints","text":"<p>Support for device hints from <code>/boot/loader.conf</code>:</p> <p>resource_int_value() - <code>sys/kern/subr_bus.c:2325-2342</code> resource_long_value() - <code>sys/kern/subr_bus.c:2344-2362</code> resource_string_value() - <code>sys/kern/subr_bus.c:2364-2397</code></p> <pre><code>int resource_int_value(const char *name, int unit,\n                       const char *resname, int *result);\n</code></pre> <p>resource_kenv() - <code>sys/kern/subr_bus.c:2298-2323</code></p> <p>Supports both DragonFly and FreeBSD hint formats: - DragonFly: <code>deviceN.property</code> - FreeBSD: <code>hint.device.N.property</code></p> <p>resource_disabled() - <code>sys/kern/subr_bus.c:3818-3827</code></p> <pre><code>int resource_disabled(const char *name, int unit);\n</code></pre> <p>Checks if <code>deviceN.disabled=1</code> is set.</p>"},{"location":"sys/kern/newbus/#auto-configuration-hooks","title":"Auto-Configuration Hooks","text":"<p>For drivers that need interrupt-driven configuration:</p>"},{"location":"sys/kern/newbus/#struct-intr_config_hook","title":"struct intr_config_hook","text":"<pre><code>struct intr_config_hook {\n    TAILQ_ENTRY(intr_config_hook) ich_links;\n    void    (*ich_func)(void *);\n    void    *ich_arg;\n    const char *ich_desc;\n    int     ich_order;\n    int     ich_ran;\n};\n</code></pre> <p>Defined in <code>sys/sys/kernel.h:475-482</code>.</p>"},{"location":"sys/kern/newbus/#api-functions","title":"API Functions","text":"<p>config_intrhook_establish() - <code>sys/kern/subr_autoconf.c:136-177</code></p> <p>Registers hook for post-interrupt configuration: - Hooks ordered by <code>ich_order</code> - If called after boot, runs immediately</p> <p>config_intrhook_disestablish() - <code>sys/kern/subr_autoconf.c:179-201</code></p> <p>Removes hook and wakes up waiters.</p> <p>run_interrupt_driven_config_hooks() - <code>sys/kern/subr_autoconf.c:64-127</code></p> <p>Called at <code>SI_SUB_INT_CONFIG_HOOKS</code>: - Runs each hook function - Waits for hooks to complete (with timeout warnings) - USB hack: waits extra 5 seconds for USB devices</p>"},{"location":"sys/kern/newbus/#root-bus","title":"Root Bus","text":"<p>The root bus is the top of the device tree:</p> <pre><code>static driver_t root_driver = {\n    \"root\",\n    root_methods,\n    1,  /* no softc */\n};\n\ndevice_t    root_bus;\ndevclass_t  root_devclass;\n</code></pre> <p>Defined in <code>sys/kern/subr_bus.c:3523-3560</code>.</p> <p>root_bus_configure() - <code>sys/kern/subr_bus.c:3562-3603</code></p> <p>Called during boot to: 1. Call <code>bus_generic_probe()</code> for root bus 2. Probe and attach children (typically nexus) 3. Wait for async attaches</p>"},{"location":"sys/kern/newbus/#devctl-device","title":"devctl Device","text":"<p>Character device <code>/dev/devctl</code> for userland notification:</p> <p>Events Sent: - <code>devadded()</code> - Device attached successfully - <code>devremoved()</code> - Device about to detach - <code>devnomatch()</code> - No driver found</p> <p>devctl_notify() - <code>sys/kern/subr_bus.c:529-559</code></p> <p>Standard notification format: <pre><code>!system=&lt;system&gt; subsystem=&lt;subsystem&gt; type=&lt;type&gt; [data]\n</code></pre></p> <p>See <code>sys/kern/subr_bus.c:250-721</code>.</p>"},{"location":"sys/kern/newbus/#sysctl-interface","title":"Sysctl Interface","text":"<p>hw.bus.info - Returns bus generation count hw.bus.devices - Returns device tree</p> <p><code>struct u_device</code> (<code>sys/sys/bus.h:87-100</code>) exported to userspace.</p> <p>See <code>sys/kern/subr_bus.c:3843-3907</code>.</p>"},{"location":"sys/kern/newbus/#driver-definition-macros","title":"Driver Definition Macros","text":"<p>DRIVER_MODULE - <code>sys/sys/bus.h:518-538</code></p> <pre><code>#define DRIVER_MODULE(name, busname, driver, devclass, evh, arg)    \\\n    DRIVER_MODULE_ORDERED(name, busname, driver, &amp;devclass, evh, arg,\\\n                          SI_ORDER_MIDDLE)\n</code></pre> <p>DEVMETHOD / DEVMETHOD_END - <code>sys/sys/bus.h:494-495</code></p> <pre><code>#define DEVMETHOD       KOBJMETHOD\n#define DEVMETHOD_END   KOBJMETHOD_END\n</code></pre>"},{"location":"sys/kern/newbus/#bus-space-access","title":"Bus Space Access","text":"<p>Shorthand macros for <code>bus_space_*</code> functions:</p> <pre><code>bus_read_1(r, o)    /* read 1 byte */\nbus_read_2(r, o)    /* read 2 bytes */\nbus_read_4(r, o)    /* read 4 bytes */\nbus_write_1(r, o, v)\nbus_write_2(r, o, v)\nbus_write_4(r, o, v)\nbus_read_region_N()\nbus_write_region_N()\nbus_set_region_N()\nbus_copy_region_N()\nbus_barrier()\n</code></pre> <p>See <code>sys/sys/bus.h:563-692</code>.</p>"},{"location":"sys/kern/newbus/#example-simple-pci-driver","title":"Example: Simple PCI Driver","text":"<pre><code>#include &lt;sys/param.h&gt;\n#include &lt;sys/kernel.h&gt;\n#include &lt;sys/module.h&gt;\n#include &lt;sys/bus.h&gt;\n#include &lt;bus/pci/pcivar.h&gt;\n\nstatic int\nmydev_probe(device_t dev)\n{\n    if (pci_get_vendor(dev) == 0x1234 &amp;&amp;\n        pci_get_device(dev) == 0x5678) {\n        device_set_desc(dev, \"My Device\");\n        return BUS_PROBE_DEFAULT;\n    }\n    return ENXIO;\n}\n\nstatic int\nmydev_attach(device_t dev)\n{\n    struct mydev_softc *sc = device_get_softc(dev);\n    int rid;\n\n    /* Allocate BAR0 */\n    rid = PCIR_BAR(0);\n    sc-&gt;mem_res = bus_alloc_resource_any(dev, SYS_RES_MEMORY,\n                                         &amp;rid, RF_ACTIVE);\n    if (sc-&gt;mem_res == NULL)\n        return ENXIO;\n\n    /* Allocate interrupt */\n    rid = 0;\n    sc-&gt;irq_res = bus_alloc_resource_any(dev, SYS_RES_IRQ,\n                                         &amp;rid, RF_ACTIVE | RF_SHAREABLE);\n    if (sc-&gt;irq_res == NULL) {\n        bus_release_resource(dev, SYS_RES_MEMORY, PCIR_BAR(0), sc-&gt;mem_res);\n        return ENXIO;\n    }\n\n    /* Setup interrupt handler */\n    bus_setup_intr(dev, sc-&gt;irq_res, INTR_MPSAFE,\n                   mydev_intr, sc, &amp;sc-&gt;irq_cookie, NULL);\n\n    return 0;\n}\n\nstatic int\nmydev_detach(device_t dev)\n{\n    struct mydev_softc *sc = device_get_softc(dev);\n\n    bus_teardown_intr(dev, sc-&gt;irq_res, sc-&gt;irq_cookie);\n    bus_release_resource(dev, SYS_RES_IRQ, 0, sc-&gt;irq_res);\n    bus_release_resource(dev, SYS_RES_MEMORY, PCIR_BAR(0), sc-&gt;mem_res);\n\n    return 0;\n}\n\nstatic device_method_t mydev_methods[] = {\n    DEVMETHOD(device_probe,     mydev_probe),\n    DEVMETHOD(device_attach,    mydev_attach),\n    DEVMETHOD(device_detach,    mydev_detach),\n    DEVMETHOD_END\n};\n\nstatic driver_t mydev_driver = {\n    \"mydev\",\n    mydev_methods,\n    sizeof(struct mydev_softc)\n};\n\nstatic devclass_t mydev_devclass;\n\nDRIVER_MODULE(mydev, pci, mydev_driver, mydev_devclass, NULL, NULL);\n</code></pre>"},{"location":"sys/kern/newbus/#cross-references","title":"Cross-References","text":"<ul> <li>Device Framework - Character/block device layer</li> <li>Resource Management - rman and DMA</li> <li>LWKT - Threading and serializers</li> <li>Synchronization - Locking primitives</li> </ul>"},{"location":"sys/kern/processes/","title":"Process and Thread Management","text":"<p>This document describes the complete lifecycle of processes and threads in DragonFly BSD, from creation through execution to termination and cleanup.</p>"},{"location":"sys/kern/processes/#overview","title":"Overview","text":"<p>DragonFly BSD has a three-level threading model:</p> <ol> <li>Processes (<code>struct proc</code>) - Traditional BSD process containers</li> <li>LWPs (Light Weight Processes, <code>struct lwp</code>) - Kernel-visible threads</li> <li>LWKT threads (<code>struct thread</code>) - Lightweight kernel threads</li> </ol> <p>User processes contain one or more LWPs, each with an associated LWKT thread. Pure kernel threads exist as LWKT threads without an associated process or LWP.</p> <p>Key source files: - <code>kern_proc.c</code> - Process table management, PID allocation, lookups - <code>kern_fork.c</code> - Process/thread creation (fork, rfork, lwp_create) - <code>kern_exec.c</code> - Program execution (execve) - <code>kern_exit.c</code> - Process termination and cleanup - <code>kern_kthread.c</code> - Kernel thread creation</p>"},{"location":"sys/kern/processes/#process-table-and-pid-management","title":"Process Table and PID Management","text":""},{"location":"sys/kern/processes/#data-structures","title":"Data Structures","text":"<p>The kernel maintains several global lists for process tracking:</p> <pre><code>// Active processes - hash table with 256 buckets\n#define ALLPROC_HSIZE 256\nstatic struct procglob allproc_hash[ALLPROC_HSIZE];\n\n// Zombie processes - single list\nstatic struct procglob zombproc_list;\n\n// PID domain tracking (prevents rapid reuse)\n#define PIDDOM_DELAY 10\nstatic pid_t pid_doms[PIDDOM_DELAY];\n</code></pre> <p>Each <code>procglob</code> entry contains: - <code>allproc</code> - List head for active or zombie processes - <code>lock</code> - Spinlock protecting the list</p>"},{"location":"sys/kern/processes/#pid-allocation","title":"PID Allocation","text":"<p>PID allocation (<code>kern_proc.c:proc_getnewpid()</code>) includes security features to prevent predictability:</p> <ol> <li>Random offset - Start PID counter at random value modulo PID_MAX</li> <li>Anti-reuse - Track last 10 PIDs in <code>pid_doms[]</code>, prevent reuse for ~10 seconds</li> <li>Hash bucket optimization - Increment by ALLPROC_HSIZE (256) to keep process in same hash bucket for better cache locality</li> </ol> <pre><code>newpid += ALLPROC_HSIZE;  // Stay in same hash bucket\nif (newpid &gt;= PID_MAX) {\n    newpid = newpid % ALLPROC_HSIZE;\n    newpid += ALLPROC_HSIZE;\n}\n</code></pre> <p>The algorithm checks: - PID not already in use (<code>pfind()</code>) - PID not recently freed (within PIDDOM_DELAY seconds) - PID not in session/process group use</p>"},{"location":"sys/kern/processes/#process-reference-counting","title":"Process Reference Counting","text":"<p>Processes use atomic reference counting via <code>p_lock</code> with multiple flag bits:</p> <pre><code>#define PLOCK_MASK     0x1fffffff  // Reference count\n#define PLOCK_WAITING  0x20000000  // Someone waiting for lock\n#define PLOCK_ZOMB     0x40000000  // Zombie being reaped\n#define PLOCK_WAITRES  0x80000000  // Waiting for resources\n</code></pre> <p>Reference count macros: - <code>PHOLD(p)</code> / <code>PRELE(p)</code> - General purpose reference counting - <code>PHOLDZOMB(p)</code> / <code>PRELEZOMB(p)</code> - Exclusive zombie reaping (sets PLOCK_ZOMB) - <code>PWAITRES(p)</code> / <code>PWAKEUP(p)</code> - Resource wait coordination</p> <p>The reference count prevents a process structure from being freed while in use. Zombie processes remain in the table with p_stat=SZOMB until the parent reaps them via wait().</p>"},{"location":"sys/kern/processes/#process-lookup-functions","title":"Process Lookup Functions","text":"<pre><code>struct proc *pfind(pid_t pid);           // Find active process\nstruct proc *zpfind(pid_t pid);          // Find zombie process\nstruct proc *pgfind(pid_t pgid);         // Find process group leader\nstruct lwp *lwpfind(struct proc *p, lwpid_t lwpid);  // Find LWP by ID\n</code></pre> <p>All lookup functions: - Acquire the appropriate hash bucket spinlock - Return process/LWP with reference count held (PHOLD) - Caller must release with PRELE when done</p>"},{"location":"sys/kern/processes/#process-iteration","title":"Process Iteration","text":"<pre><code>int allproc_scan(int (*callback)(struct proc *, void *), void *data, int flags);\nint zombproc_scan(int (*callback)(struct proc *, void *), void *data, int flags);\n</code></pre> <p>Flags control behavior: - <code>PFSCAN_NOBRK</code> - Don't break on non-zero callback return - <code>PFSCAN_LOCKED</code> - Keep process locked during callback (PHOLD)</p> <p>These functions are used by system utilities (ps, top) and kernel subsystems that need to enumerate all processes.</p>"},{"location":"sys/kern/processes/#process-creation-fork","title":"Process Creation: fork()","text":""},{"location":"sys/kern/processes/#system-call-entry-points","title":"System Call Entry Points","text":"<pre><code>int sys_fork(struct sysmsg *);          // Traditional fork\nint sys_vfork(struct sysmsg *);         // vfork (shared memory)\nint sys_rfork(struct sysmsg *, struct rfork_args *);  // Extended fork\n</code></pre> <p>All fork variants funnel through <code>fork1()</code> with different flag combinations:</p> <p>RFORK flags: - <code>RFPROC</code> - Create new process (required) - <code>RFMEM</code> - Share address space (for vfork or threads) - <code>RFFDG</code> - Share file descriptor table - <code>RFCFDG</code> - Close file descriptors (exclusive with RFFDG) - <code>RFSIGSHARE</code> - Share signal handlers - <code>RFTHREAD</code> - Create thread (LWP) in current process - <code>RFPPWAIT</code> - Parent sleeps until child releases vmspace (vfork) - <code>RFENVG</code> - Create new environment group - <code>RFCENVG</code> - Close environment variables</p>"},{"location":"sys/kern/processes/#fork1-overview","title":"fork1() Overview","text":"<p>The main fork implementation (<code>kern_fork.c:fork1()</code>) performs these steps:</p> <ol> <li>Preparation</li> <li>Validate flags (RFPROC required, mutual exclusions)</li> <li>Check resource limits (RLIMIT_NPROC)</li> <li> <p>Account for new process in uid structure</p> </li> <li> <p>Process structure allocation</p> </li> <li>Allocate <code>struct proc</code> via <code>kmalloc()</code></li> <li>Initialize reference count, locks, lists</li> <li>Insert into <code>allproc</code> hash table</li> <li> <p>Allocate PID via <code>proc_getnewpid()</code></p> </li> <li> <p>Process setup</p> </li> <li>Copy or share credentials based on flags</li> <li>Copy or share file descriptor table</li> <li>Copy or share signal handler table</li> <li> <p>Set process state to SIDL (intermediate)</p> </li> <li> <p>LWP creation (two-phase)</p> </li> <li><code>lwp_fork1()</code> - Allocate LWP structure, prepare for vm_fork</li> <li><code>vm_fork()</code> - Handle address space (copy-on-write or share)</li> <li> <p><code>lwp_fork2()</code> - Finalize LWP, insert into process</p> </li> <li> <p>Finalization</p> </li> <li>Call <code>at_fork()</code> registered callbacks</li> <li>Set up parent/child relationship</li> <li> <p>For RFPPWAIT (vfork): parent sleeps until child execs or exits</p> </li> <li> <p>Activation</p> </li> <li><code>start_forked_proc()</code> transitions child from SIDL to SACTIVE</li> <li>Child LWP becomes schedulable</li> </ol>"},{"location":"sys/kern/processes/#two-phase-lwp-creation","title":"Two-Phase LWP Creation","text":"<p>The fork process splits LWP creation around <code>vm_fork()</code>:</p> <p>Phase 1: lwp_fork1() (<code>kern_fork.c:lwp_fork1()</code>) - Allocates <code>struct lwp</code> - Allocates kernel stack via <code>lwkt_alloc_thread()</code> - Initializes basic fields (proc pointer, flags) - Does not insert into p_lwp_tree yet</p> <p>VM Fork: vm_fork() (in vm subsystem) - Sets up address space for child process - Creates COW (copy-on-write) mappings for non-shared pages - For RFMEM: shares vmspace directly (vfork, threads) - Handles special mappings (shared memory, mmap)</p> <p>Phase 2: lwp_fork2() (<code>kern_fork.c:lwp_fork2()</code>) - Completes machine-dependent setup via <code>cpu_lwp_fork()</code> - Sets initial register state (return value, stack pointer) - Inserts LWP into <code>p_lwp_tree</code> (RB tree indexed by lwpid) - Sets LWP state to LSSTOP (not running yet)</p> <p>This two-phase design ensures the LWP is not visible to the rest of the system (via p_lwp_tree) until the address space is properly set up.</p>"},{"location":"sys/kern/processes/#starting-the-forked-process","title":"Starting the Forked Process","text":"<pre><code>void start_forked_proc(struct lwp *parent, struct proc *child)\n</code></pre> <p>This function transitions the child process to runnable state:</p> <ol> <li>Acquire process token</li> <li>Transition from SIDL to SACTIVE state</li> <li>Transition LWP from LSSTOP to LSRUN</li> <li>Schedule the child's thread via <code>lwkt_schedule()</code></li> <li>Child will return from fork with retval=0 (vs parent getting child PID)</li> </ol>"},{"location":"sys/kern/processes/#vfork-coordination","title":"vfork() Coordination","text":"<p>When <code>RFPPWAIT</code> is set (vfork):</p> <ol> <li>Parent process blocks in <code>fork1()</code> after starting child</li> <li>Parent waits on <code>p_ppwait_cv</code> condition variable</li> <li>Child signals parent when:</li> <li>Child calls exec (in <code>kern_exec.c:exec_new_vmspace()</code>)</li> <li>Child exits (in <code>kern_exit.c:exit1()</code>)</li> <li>Parent wakes up and continues</li> </ol> <p>This ensures the parent doesn't modify shared memory while the child is using it.</p>"},{"location":"sys/kern/processes/#thread-creation-lwp_create","title":"Thread Creation: lwp_create()","text":"<p>User threads (pthreads) are created via:</p> <pre><code>int sys_lwp_create(struct lwp_params *params);\n</code></pre> <p>This creates a new LWP within the current process:</p> <ol> <li>Calls <code>lwp_create1()</code> to allocate LWP structure</li> <li>Does not call <code>vm_fork()</code> - address space is shared</li> <li>Sets up new stack region within shared address space</li> <li>Sets entry point to user-specified function</li> <li>New thread runs in parallel with existing LWPs</li> </ol> <p>LWP creation is much lighter than process creation since: - No new process structure - No PID allocation - No credential/fd/signal copying - No COW setup (memory is shared)</p>"},{"location":"sys/kern/processes/#process-execution-execve","title":"Process Execution: execve()","text":""},{"location":"sys/kern/processes/#system-call-entry","title":"System Call Entry","text":"<pre><code>int sys_execve(struct sysmsg *, struct execve_args *);\n</code></pre> <p>Arguments: - <code>fname</code> - Path to executable - <code>argv</code> - Argument vector (NULL-terminated) - <code>envv</code> - Environment vector (NULL-terminated)</p>"},{"location":"sys/kern/processes/#execution-overview","title":"Execution Overview","text":"<p>The <code>kern_execve()</code> function (<code>kern_exec.c</code>) replaces the current process's address space and execution context with a new program:</p> <ol> <li>Path resolution - Lookup executable via <code>nlookup()</code></li> <li>Permission checks - Verify execute permission, handle setuid/setgid</li> <li>Image identification - Try each image activator to identify format</li> <li>Point of no return - Destroy old address space</li> <li>New address space setup - Load program, build stack</li> <li>State reset - Close files, reset signals, update credentials</li> <li>Entry - Set PC to program entry point and return to usermode</li> </ol>"},{"location":"sys/kern/processes/#preventing-concurrent-execution","title":"Preventing Concurrent Execution","text":"<p>Multi-threaded processes must not exec concurrently:</p> <pre><code>if (p-&gt;p_flags &amp; P_INEXEC) {\n    return EBUSY;  // Another thread is already in exec\n}\np-&gt;p_flags |= P_INEXEC;\n</code></pre> <p>The <code>P_INEXEC</code> flag remains set until exec completes or fails.</p>"},{"location":"sys/kern/processes/#image-activators","title":"Image Activators","text":"<p>The kernel tries each registered image activator in turn:</p> <pre><code>const struct execsw execsw[] = {\n    { exec_elf_imgact, \"ELF\" },\n    { exec_resident_imgact, \"resident\" },\n    { exec_script_imgact, \"#!\" },\n    { NULL, NULL }\n};\n</code></pre> <p>Each activator's function signature:</p> <pre><code>int (*ex_imgact)(struct image_params *imgp);\n</code></pre> <p>Image activator process: 1. Read first page of executable (4KB) 2. Call each activator's <code>ex_imgact()</code> function 3. Activator inspects headers (ELF magic, #! shebang, etc.) 4. Return <code>-1</code> if not recognized, <code>0</code> if accepted, error code if failed 5. First activator to accept the image handles execution</p>"},{"location":"sys/kern/processes/#script-execution","title":"Script Execution (#!)","text":"<p>The script activator (<code>exec_script_imgact()</code>) handles interpreter files:</p> <pre><code>#!/usr/bin/interpreter [args]\nprogram content...\n</code></pre> <p>When detected: 1. Extract interpreter path and optional arguments from first line 2. Construct new argv: <code>[interpreter, args..., scriptname, original_argv...]</code> 3. Recursively call <code>kern_execve()</code> with interpreter as executable 4. Recursion depth limited to prevent infinite loops</p>"},{"location":"sys/kern/processes/#point-of-no-return","title":"Point of No Return","text":"<pre><code>error = exec_new_vmspace(imgp, stack);\n</code></pre> <p>This function (<code>kern_exec.c:exec_new_vmspace()</code>) destroys the old address space:</p> <ol> <li>Create new vmspace structure</li> <li>If process was vfork'd (P_PPWAIT set):</li> <li>Signal parent to wake up via <code>wakeup(p-&gt;p_pptr)</code></li> <li>Drop shared vmspace reference</li> <li>Destroy old vmspace</li> <li>Attach new vmspace to process</li> <li>Cannot fail after this point - process has no valid address space to return to</li> </ol>"},{"location":"sys/kern/processes/#building-the-new-stack","title":"Building the New Stack","text":"<pre><code>error = exec_copyout_strings(imgp, &amp;stack_base);\n</code></pre> <p>This function constructs the user stack in this order (growing downward):</p> <pre><code>block-beta\n    columns 1\n\n    block:high[\"High addresses\"]\n        envp_strings[\"envp strings\"]\n    end\n    block:argv[\"\"]\n        argv_strings[\"argv strings\"]\n    end\n    block:exec[\"\"]\n        exec_path[\"exec path\"]\n    end\n    block:pad[\"\"]\n        padding[\"padding (for alignment)\"]\n    end\n    block:auxv[\"\"]\n        auxv_vector[\"auxv vector (ELF only)\"]\n    end\n    block:envp_ptrs[\"\"]\n        env_null[\"NULL\"]\n        envp_n[\"envp[n-1] (pointers to env strings)\"]\n        envp_dots[\"...\"]\n        envp_0[\"envp[0]\"]\n    end\n    block:argv_ptrs[\"\"]\n        argv_null[\"NULL\"]\n        argv_n[\"argv[n-1] (pointers to arg strings)\"]\n        argv_dots[\"...\"]\n        argv_0[\"argv[0]\"]\n    end\n    block:argc_block[\"\"]\n        argc[\"argc (argument count)\"]\n    end\n    block:low[\"Low addresses (stack pointer at program entry)\"]\n    end\n</code></pre> <p>Stack gap randomization: - Controlled by <code>exec.stackgap_random</code> sysctl - Randomly subtracts 0 to stackgap_random bytes from stack pointer - Helps prevent return-to-libc attacks</p> <p>Auxiliary vector (auxv): ELF programs receive metadata through auxv entries: - <code>AT_PHDR</code> - Address of program header table - <code>AT_PHENT</code> - Size of program header entry - <code>AT_PHNUM</code> - Number of program headers - <code>AT_PAGESZ</code> - System page size - <code>AT_BASE</code> - Interpreter base address (for dynamic executables) - <code>AT_ENTRY</code> - Program entry point - <code>AT_EXECPATH</code> - Full path to executable</p>"},{"location":"sys/kern/processes/#credential-handling-setuidsetgid","title":"Credential Handling (setuid/setgid)","text":"<p>If the executable has setuid or setgid bits:</p> <pre><code>if (attr.va_mode &amp; (S_ISUID | S_ISGID)) {\n    p-&gt;p_flags |= P_SUGID;  // Mark process as tainted\n    // Update credentials\n    if (attr.va_mode &amp; S_ISUID)\n        change_euid(attr.va_uid);\n    if (attr.va_mode &amp; S_ISGID)\n        change_egid(attr.va_gid);\n}\n</code></pre> <p>P_SUGID security restrictions: - Prevents ptrace() attachment - Prevents core dumps to user directories - Restricts /proc access - Cleared on subsequent exec of non-setuid binary</p>"},{"location":"sys/kern/processes/#file-descriptor-cleanup","title":"File Descriptor Cleanup","text":"<p>During exec, the kernel closes file descriptors marked close-on-exec:</p> <pre><code>fdcloseexec(p);  // Close all FDs with FD_CLOEXEC set\n</code></pre> <p>This is commonly used for: - Pipe file descriptors in shell pipelines - Internal file descriptors that shouldn't be inherited - Library handles that need to be reopened</p>"},{"location":"sys/kern/processes/#signal-handler-reset","title":"Signal Handler Reset","text":"<pre><code>execsigs(p);\n</code></pre> <p>This function resets all signal dispositions: - Signals set to SIG_IGN remain ignored - Signals set to SIG_DFL remain default - Caught signals (custom handlers) reset to SIG_DFL</p> <p>Since the old address space is gone, signal handler function pointers are no longer valid.</p>"},{"location":"sys/kern/processes/#entry-to-new-program","title":"Entry to New Program","text":"<p>After all setup completes:</p> <ol> <li><code>exec_setregs()</code> sets up initial CPU state:</li> <li>Program counter (PC) \u2192 entry point address</li> <li>Stack pointer (SP) \u2192 top of prepared stack</li> <li>Argument registers \u2192 argc, argv (architecture-dependent)</li> <li>Exec system call returns to user mode</li> <li>CPU state causes execution to begin at program entry point</li> </ol> <p>For dynamically linked ELF programs, entry point is actually the dynamic linker (ld-elf.so), which: - Loads shared libraries - Performs relocations - Eventually transfers control to program's actual <code>_start()</code></p>"},{"location":"sys/kern/processes/#process-termination-exit","title":"Process Termination: exit()","text":""},{"location":"sys/kern/processes/#system-call-entry_1","title":"System Call Entry","text":"<pre><code>int sys_exit(struct sysmsg *, struct exit_args *);\nint sys_exit_group(struct sysmsg *, struct exit_group_args *);  // Linux compat\n</code></pre> <p>Both funnel through <code>exit1()</code> with a status code.</p>"},{"location":"sys/kern/processes/#exit1-overview","title":"exit1() Overview","text":"<p>The main exit implementation (<code>kern_exit.c:exit1()</code>) performs these steps:</p> <ol> <li>Exit coordination - Set P_WEXIT flag to prevent concurrent exits</li> <li>Stop other threads - Call <code>killalllwps()</code> to terminate all other LWPs</li> <li>Accounting - Record CPU time and resource usage</li> <li>Cleanup - Close files, release resources, detach from process groups</li> <li>Zombie transition - Move from allproc to zombproc list</li> <li>Parent notification - Send SIGCHLD to parent</li> <li>Thread termination - Current LWP calls <code>lwp_exit()</code> and never returns</li> </ol>"},{"location":"sys/kern/processes/#preventing-concurrent-exit","title":"Preventing Concurrent Exit","text":"<pre><code>if (p-&gt;p_flags &amp; P_WEXIT) {\n    lwkt_reltoken(&amp;p-&gt;p_token);\n    return;  // Already exiting\n}\np-&gt;p_flags |= P_WEXIT;\n</code></pre> <p>Only the first thread to set <code>P_WEXIT</code> performs the full exit sequence. Other threads in <code>exit1()</code> simply return and get cleaned up by <code>killalllwps()</code>.</p>"},{"location":"sys/kern/processes/#killing-all-lwps","title":"Killing All LWPs","text":"<pre><code>killalllwps(int signo);\n</code></pre> <p>This function (<code>kern_exit.c</code>) terminates all other LWPs in the process:</p> <ol> <li>Acquire proc token</li> <li>Iterate through <code>p_lwp_tree</code> (all LWPs in process)</li> <li>For each other LWP:</li> <li>Set LWP state to LSZOMB</li> <li>Post signal if specified</li> <li>Issue <code>lwkt_deschedule()</code> to remove from scheduler</li> <li>Wait for all LWPs to finish deschedule</li> <li>Reap zombie LWPs</li> </ol> <p>LWP reaper mechanism: - Dead LWPs are added to <code>deadlwp_list</code> - <code>deadlwp_task</code> taskqueue entry processes the list - Each dead LWP's kernel stack is freed - LWP structure itself is freed</p> <p>This asynchronous reaping avoids freeing the kernel stack that might still be in use.</p>"},{"location":"sys/kern/processes/#process-cleanup-sequence","title":"Process Cleanup Sequence","text":"<p>After killing all LWPs, <code>exit1()</code> performs:</p> <ol> <li>Session/Process Group Cleanup</li> <li>If session leader: terminate controlling terminal</li> <li>If process group leader: send SIGHUP to foreground processes</li> <li> <p>Remove from process group</p> </li> <li> <p>File Descriptor Cleanup <pre><code>fdfree(p, td);  // Close all open files\n</code></pre></p> </li> <li>Closes all file descriptors</li> <li>Releases file descriptor table</li> <li> <p>Decrements reference counts on file structures</p> </li> <li> <p>Virtual Memory Cleanup <pre><code>if (!--vmspace-&gt;vm_refcnt) {\n    vmspace_dtor(vmspace);  // Last reference, free vmspace\n}\n</code></pre></p> </li> <li>Decrements vmspace reference count</li> <li> <p>If last reference: unmaps all regions, frees page tables</p> </li> <li> <p>IPC Cleanup</p> </li> <li>Remove from System V semaphore undo list</li> <li> <p>Detach from shared memory segments</p> </li> <li> <p>Credential Cleanup <pre><code>crfree(p-&gt;p_ucred);  // Release credential structure\n</code></pre></p> </li> <li> <p>Timer Cleanup</p> </li> <li>Stop ITIMER_REAL, ITIMER_VIRTUAL, ITIMER_PROF timers</li> <li>Remove from timer queues</li> </ol>"},{"location":"sys/kern/processes/#zombie-transition","title":"Zombie Transition","text":"<pre><code>proc_move_allproc_zombie(struct proc *p);\n</code></pre> <p>This function (<code>kern_proc.c</code>) atomically moves the process from active to zombie state:</p> <ol> <li>Remove from <code>allproc_hash[bucket]</code></li> <li>Add to <code>zombproc_list</code></li> <li>Set <code>p-&gt;p_stat = SZOMB</code></li> <li>Leave <code>p_nthreads</code> at 1 (decrement happens after parent reaps)</li> </ol> <p>Zombie state characteristics: - Process structure remains allocated - PID remains allocated (in <code>pid_doms[]</code> anti-reuse list) - Exit status preserved in <code>p-&gt;p_xstat</code> - All other resources freed</p>"},{"location":"sys/kern/processes/#parent-notification","title":"Parent Notification","text":"<p>After transition to zombie:</p> <ol> <li>Increment parent's <code>p_waitgen</code> counter (wake optimization)</li> <li>Send SIGCHLD to parent via <code>ksignal(p-&gt;p_pptr, SIGCHLD)</code></li> <li>Wakeup any threads in <code>wait()</code> via <code>wakeup(p-&gt;p_pptr)</code></li> </ol> <p>The <code>p_waitgen</code> counter allows wait() to quickly detect if any child has changed state without scanning the entire child list.</p>"},{"location":"sys/kern/processes/#orphan-handling","title":"Orphan Handling","text":"<p>If parent has already exited, child is reparented:</p> <pre><code>proc_reparent(struct proc *child, struct proc *parent);\n</code></pre> <p>Reparenting rules: 1. If parent is init (PID 1): child becomes init's child 2. If process has a registered reaper: child moves to reaper 3. Otherwise: child becomes init's child</p> <p>The reaper mechanism allows creating \"sub-init\" processes for containers or process supervision.</p>"},{"location":"sys/kern/processes/#thread-exit","title":"Thread Exit","text":"<p>Finally, the exiting thread calls:</p> <pre><code>lwp_exit(int masterexit, void (*exitfunc)(void *), void *exitarg);\n</code></pre> <p>This function: 1. Sets <code>lp-&gt;lwp_stat = LSZOMB</code> 2. Calls exit function if provided (for cleanup) 3. Calls <code>cpu_lwp_exit()</code> to release machine-dependent resources 4. Calls <code>lwkt_exit()</code> which:    - Deschedules thread    - Marks stack for deferred free    - Switches to new thread    - Never returns</p>"},{"location":"sys/kern/processes/#parent-wait-and-reaping","title":"Parent Wait and Reaping","text":"<p>Parent processes retrieve exit status via wait():</p> <pre><code>int sys_wait4(struct sysmsg *, struct wait4_args *);\n</code></pre> <p>The <code>kern_wait()</code> function (<code>kern_exit.c</code>) performs:</p> <ol> <li> <p>Wait loop optimization <pre><code>int waitgen = p-&gt;p_waitgen;\nwhile (/* no matching child */) {\n    tsleep_interlock(&amp;waitgen, PCATCH);\n    if (waitgen == p-&gt;p_waitgen)\n        tsleep(p, PCATCH | PINTERLOCKED, \"wait\", 0);\n    waitgen = p-&gt;p_waitgen;\n}\n</code></pre>    The waitgen counter avoids spurious wakeups - only sleep if counter hasn't changed.</p> </li> <li> <p>Child scan</p> </li> <li>Iterate through <code>p-&gt;p_children</code> list</li> <li>Match on PID (if specified) or any child (PID -1)</li> <li> <p>Match on process group (if negative PID)</p> </li> <li> <p>Zombie reaping <pre><code>PHOLDZOMB(child);  // Exclusive access, sets PLOCK_ZOMB\n</code></pre></p> </li> <li>Acquire exclusive zombie lock (prevents concurrent wait by other threads)</li> <li>Copy exit status from <code>p-&gt;p_xstat</code></li> <li>Copy resource usage from <code>p-&gt;p_ru</code></li> <li>Remove from parent's <code>p_children</code> list</li> <li> <p>Call <code>proc_finish(child)</code> to free process structure</p> </li> <li> <p>Process finish <pre><code>proc_finish(struct proc *p);\n</code></pre></p> </li> <li>Remove from zombie list</li> <li>Decrement PID domain reference in <code>pid_doms[]</code></li> <li>Free process structure</li> <li>PID becomes available for reuse after PIDDOM_DELAY</li> </ol>"},{"location":"sys/kern/processes/#wait-options","title":"Wait Options","text":"<p>The wait4() system call supports various options:</p> <ul> <li><code>WNOHANG</code> - Return immediately if no child has exited</li> <li><code>WUNTRACED</code> - Also return for stopped children (job control)</li> <li><code>WCONTINUED</code> - Also return for continued children (SIGCONT)</li> <li><code>WLINUXCLONE</code> - Linux compatibility for thread waiting</li> </ul> <p>Without WNOHANG, wait() sleeps until a child changes state or a signal arrives.</p>"},{"location":"sys/kern/processes/#kernel-threads","title":"Kernel Threads","text":""},{"location":"sys/kern/processes/#overview_1","title":"Overview","text":"<p>Kernel threads are lightweight threads that run entirely in kernel mode without an associated user process or LWP. They are used for:</p> <ul> <li>Asynchronous I/O completion</li> <li>Device driver background tasks</li> <li>Network stack processing</li> <li>Virtual memory pageout daemon</li> <li>System maintenance tasks</li> </ul>"},{"location":"sys/kern/processes/#creating-kernel-threads","title":"Creating Kernel Threads","text":"<pre><code>int kthread_create(void (*func)(void *), void *arg,\n                   struct thread **tdp, const char *fmt, ...);\n</code></pre> <p>Parameters: - <code>func</code> - Thread entry point function - <code>arg</code> - Argument passed to function - <code>tdp</code> - Returns pointer to created thread (can be NULL) - <code>fmt</code> - Printf-style format for thread name (appears in ps)</p> <p>Variants:</p> <pre><code>// Create but don't schedule immediately\nint kthread_alloc(void (*func)(void *), void *arg,\n                  struct thread **tdp, const char *fmt, ...);\n\n// Pin to specific CPU\nint kthread_create_cpu(void (*func)(void *), void *arg,\n                       struct thread **tdp, int cpu, const char *fmt, ...);\n</code></pre>"},{"location":"sys/kern/processes/#thread-creation-process","title":"Thread Creation Process","text":"<p>Internal function <code>_kthread_create()</code> (<code>kern_kthread.c</code>):</p> <ol> <li>Allocate thread structure <pre><code>td = lwkt_alloc_thread(NULL, LWKT_THREAD_STACK, cpu, flags);\n</code></pre></li> <li>Allocates <code>struct thread</code></li> <li>Allocates kernel stack (typically 16KB)</li> <li> <p>CPU parameter pins thread to specific CPU, or -1 for any CPU</p> </li> <li> <p>Set up execution context <pre><code>cpu_set_thread_handler(td, kthread_exit, func, arg);\n</code></pre></p> </li> <li>Sets stack pointer to top of kernel stack</li> <li>Sets up return chain: <code>func</code> returns to <code>kthread_exit</code></li> <li> <p>Saves argument for function</p> </li> <li> <p>Initialize thread metadata</p> </li> <li>Copy name to <code>td-&gt;td_comm</code> (visible in ps, top)</li> <li>Share credentials with proc0 (kernel process)</li> <li> <p>Set <code>td-&gt;td_proc = NULL</code> to mark as pure kernel thread</p> </li> <li> <p>Schedule thread <pre><code>lwkt_schedule(td);\n</code></pre></p> </li> <li>Only if <code>schedule_now</code> parameter is true</li> <li>Thread becomes runnable on target CPU</li> <li>Scheduler will run thread when appropriate</li> </ol>"},{"location":"sys/kern/processes/#kernel-thread-lifecycle","title":"Kernel Thread Lifecycle","text":"<p>Entry: - Thread starts executing at <code>func</code> - Runs at elevated privilege (kernel mode) - Has full access to kernel memory and data structures</p> <p>Execution: - Thread can sleep, waiting for events - Thread can acquire locks and access shared data - Thread should check for termination requests</p> <p>Exit: - Thread returns from <code>func</code>, which chains to <code>kthread_exit()</code> - <code>kthread_exit()</code> calls <code>lwkt_exit()</code>:   - Marks thread as exiting   - Deschedules from runqueue   - Marks stack for deferred free   - Switches to another thread   - Never returns</p>"},{"location":"sys/kern/processes/#kernel-process-creation","title":"Kernel Process Creation","text":"<p>The <code>kproc_start()</code> function creates kernel threads via SYSINIT:</p> <pre><code>struct kproc_desc {\n    char *arg0;                        // Thread name\n    void (*func)(void);                // Entry point\n    struct thread **global_threadpp;   // Where to store thread pointer\n};\n\nvoid kproc_start(const void *udata);\n</code></pre> <p>Usage:</p> <pre><code>static struct thread *mythread;\n\nstatic struct kproc_desc my_kproc = {\n    \"mythread\",\n    my_thread_func,\n    &amp;mythread\n};\nSYSINIT(my_init, SI_SUB_KTHREAD_IDLE, SI_ORDER_ANY, kproc_start, &amp;my_kproc);\n</code></pre> <p>This creates the kernel thread during system initialization, after core kernel threads are running.</p>"},{"location":"sys/kern/processes/#kernel-thread-suspension","title":"Kernel Thread Suspension","text":"<p>Kernel threads can voluntarily suspend:</p> <pre><code>int suspend_kproc(struct thread *td, int timo);\nvoid kproc_suspend_loop(void);\n</code></pre> <p>Suspend mechanism:</p> <ol> <li>External code calls <code>suspend_kproc(td, timeout)</code></li> <li>Sets <code>TDF_MP_STOPREQ</code> flag on target thread</li> <li>Wakes target thread (if sleeping)</li> <li>Waits for thread to acknowledge suspension</li> </ol> <p>Thread cooperation:</p> <p>The kernel thread periodically calls: <pre><code>kproc_suspend_loop();  // Check if someone wants us to suspend\n</code></pre></p> <p>If <code>TDF_MP_STOPREQ</code> is set: - Clear request flag - Sleep until <code>TDF_MP_WAKEREQ</code> is set - Clear wake flag and continue</p> <p>This allows kernel threads to be paused for maintenance operations or shutdown.</p>"},{"location":"sys/kern/processes/#process-states-and-transitions","title":"Process States and Transitions","text":""},{"location":"sys/kern/processes/#process-states-p_stat","title":"Process States (p_stat)","text":"<pre><code>#define SIDL    1   // Process being created\n#define SACTIVE 2   // Process is active\n#define SSTOP   3   // Process stopped (job control)\n#define SZOMB   4   // Process is zombie (awaiting reaping)\n</code></pre> <p>State transitions:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; SIDL : fork1()\n    SIDL --&gt; SACTIVE : start_forked_proc()\n    SACTIVE --&gt; SZOMB : exit1()\n    SACTIVE --&gt; SSTOP : via signal\n    SSTOP --&gt; SZOMB : exit1()\n    SZOMB --&gt; [*] : wait4()\n</code></pre>"},{"location":"sys/kern/processes/#lwp-states-lwp_stat","title":"LWP States (lwp_stat)","text":"<pre><code>#define LSRUN    1  // Runnable (on or waiting for CPU)\n#define LSSTOP   2  // Stopped (job control, debugging)\n#define LSSLEEP  3  // Sleeping (waiting for event)\n#define LSZOMB   4  // Zombie (terminated but not reaped)\n#define LSSUSPENDED 5  // Suspended (special conditions)\n</code></pre> <p>Typical LWP lifecycle:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; LSSTOP : lwp_fork2()\n    LSSTOP --&gt; LSRUN : scheduler\n    LSRUN --&gt; LSSLEEP : sleep event\n    LSSLEEP --&gt; LSRUN : wakeup\n    LSRUN --&gt; LSZOMB : lwp_exit()\n    LSSLEEP --&gt; LSZOMB : lwp_exit()\n    LSZOMB --&gt; [*] : deferred free\n</code></pre>"},{"location":"sys/kern/processes/#thread-states-td_flags","title":"Thread States (td_flags)","text":"<p>LWKT threads have numerous flags in <code>td_flags</code>:</p> <pre><code>#define TDF_RUNNING      0x0001  // Thread on CPU\n#define TDF_RUNQ         0x0002  // Thread on runqueue\n#define TDF_TSLEEP       0x0004  // Thread in tsleep\n#define TDF_EXITING      0x0010  // Thread exiting\n#define TDF_SINTR        0x0020  // Sleep is interruptible\n#define TDF_TIMEOUT      0x0040  // Timeout in progress\n// ... many more\n</code></pre> <p>Threads transition between runqueue, running, sleeping states based on scheduler and synchronization events.</p>"},{"location":"sys/kern/processes/#process-relationships","title":"Process Relationships","text":""},{"location":"sys/kern/processes/#parent-child-relationships","title":"Parent-Child Relationships","text":"<p>Each process maintains:</p> <pre><code>struct proc {\n    struct proc *p_pptr;           // Parent process\n    struct proclist p_children;    // List of child processes\n    struct sibling_entry p_sibling; // Sibling list entry\n    // ...\n};\n</code></pre> <p>Invariants: - Every process except proc0 has a parent - A process's children are on its <code>p_children</code> list - Siblings are linked through <code>p_sibling</code></p>"},{"location":"sys/kern/processes/#process-groups-and-sessions","title":"Process Groups and Sessions","text":"<pre><code>struct proc {\n    struct pgrp *p_pgrp;          // Process group\n    pid_t p_pgid;                 // Process group ID\n    // ...\n};\n\nstruct pgrp {\n    struct proclist pg_members;    // Members of this group\n    struct session *pg_session;    // Session containing this group\n    pid_t pg_id;                   // Process group ID\n};\n\nstruct session {\n    struct pgrp_list s_groups;     // Groups in this session\n    struct vnode *s_ttyvp;         // Controlling terminal\n    pid_t s_sid;                   // Session ID\n};\n</code></pre> <p>Hierarchy:</p> <pre><code>flowchart TD\n    Session[\"Session\"]\n    PG1[\"Process Group 1\"]\n    PG2[\"Process Group 2\"]\n    PA[\"Process A (leader)\"]\n    PB[\"Process B\"]\n    PC[\"Process C\"]\n    PD[\"Process D (leader)\"]\n    PE[\"Process E\"]\n\n    Session --&gt; PG1\n    Session --&gt; PG2\n    PG1 --&gt; PA\n    PG1 --&gt; PB\n    PG1 --&gt; PC\n    PG2 --&gt; PD\n    PG2 --&gt; PE\n</code></pre> <p>Purpose: - Sessions - Isolate groups of related processes (typically one per login) - Process Groups - Job control (foreground/background jobs in shell) - Controlling Terminal - Associated with session, receives signals (SIGINT, SIGQUIT)</p> <p>Process group leaders (PID == PGID) and session leaders (PID == SID) have special responsibilities: - Session leader death causes SIGHUP to all processes in session - Process group leader death can cause controlling terminal loss</p>"},{"location":"sys/kern/processes/#reaper-hierarchy","title":"Reaper Hierarchy","text":"<pre><code>struct proc {\n    struct sysreaper *p_reaper;    // Our reaper (or NULL)\n    // ...\n};\n\nstruct sysreaper {\n    struct proc *p;                // Reaper process\n    int refs;                      // Reference count\n    struct sysreaper *parent;      // Parent reaper\n};\n</code></pre> <p>Reapers provide a mechanism for process supervision:</p> <ul> <li>When a process exits, orphaned children are reparented to the reaper</li> <li>If no reaper, children go to init (PID 1)</li> <li>Allows containers or supervision trees without requiring PID 1</li> </ul> <p>Example hierarchy:</p> <pre><code>flowchart TD\n    init[\"init (PID 1)\"]\n    reaper[\"reaper_daemon(reaper for container)\"]\n    cp1[\"container_process_1\"]\n    cp2[\"container_process_2\"]\n    child[\"child_process\"]\n\n    init --&gt; reaper\n    reaper --&gt; cp1\n    reaper --&gt; cp2\n    cp2 --&gt; child\n</code></pre> <p>If <code>container_process_2</code> exits, <code>child_process</code> is reparented to <code>reaper_daemon</code>, not init.</p>"},{"location":"sys/kern/processes/#process-trees-and-debugging","title":"Process Trees and Debugging","text":""},{"location":"sys/kern/processes/#proc-filesystem","title":"/proc Filesystem","text":"<p>The process filesystem exposes process information:</p> <ul> <li><code>/proc/&lt;pid&gt;/status</code> - Process status</li> <li><code>/proc/&lt;pid&gt;/mem</code> - Process memory (for debugging)</li> <li><code>/proc/&lt;pid&gt;/map</code> - Memory mappings</li> <li><code>/proc/&lt;pid&gt;/cmdline</code> - Command line arguments</li> </ul>"},{"location":"sys/kern/processes/#ptrace-system-call","title":"ptrace() System Call","text":"<pre><code>int sys_ptrace(struct sysmsg *, struct ptrace_args *);\n</code></pre> <p>The ptrace system call allows one process to control another:</p> <p>Operations: - <code>PT_TRACE_ME</code> - Mark process as traceable - <code>PT_ATTACH</code> - Attach to process as debugger - <code>PT_DETACH</code> - Detach from traced process - <code>PT_CONTINUE</code> - Resume execution - <code>PT_STEP</code> - Single-step execution - <code>PT_READ_I/PT_WRITE_I</code> - Read/write instruction space - <code>PT_READ_D/PT_WRITE_D</code> - Read/write data space - <code>PT_GETREGS/PT_SETREGS</code> - Read/write registers</p> <p>Security restrictions: - Cannot trace processes with P_SUGID flag (setuid/setgid) - Cannot trace processes owned by other users (unless root) - Traced process stops on signals for debugger examination</p> <p>Debuggers (gdb, lldb) use ptrace to: - Set breakpoints (write INT3 instruction) - Read/modify memory and registers - Single-step through code - Examine process state after crashes</p>"},{"location":"sys/kern/processes/#resource-limits","title":"Resource Limits","text":"<p>Each process has resource limits inherited from parent:</p> <pre><code>struct proc {\n    struct plimit *p_limit;  // Resource limits\n    struct pstats *p_stats;  // Statistics and timers\n    // ...\n};\n\nstruct plimit {\n    struct rlimit pl_rlimit[RLIM_NLIMITS];\n};\n\nstruct rlimit {\n    rlim_t rlim_cur;  // Current (soft) limit\n    rlim_t rlim_max;  // Maximum (hard) limit\n};\n</code></pre> <p>Standard limits: - <code>RLIMIT_CPU</code> - CPU time (seconds) - <code>RLIMIT_FSIZE</code> - Maximum file size - <code>RLIMIT_DATA</code> - Data segment size - <code>RLIMIT_STACK</code> - Stack size - <code>RLIMIT_CORE</code> - Core file size - <code>RLIMIT_RSS</code> - Resident set size - <code>RLIMIT_MEMLOCK</code> - Locked memory - <code>RLIMIT_NPROC</code> - Number of processes per uid - <code>RLIMIT_NOFILE</code> - Number of open files - <code>RLIMIT_SBSIZE</code> - Socket buffer size</p> <p>Exceeded soft limits typically cause signals (SIGXCPU, SIGXFSZ). Hard limits cannot be exceeded.</p>"},{"location":"sys/kern/processes/#process-accounting","title":"Process Accounting","text":""},{"location":"sys/kern/processes/#resource-usage","title":"Resource Usage","text":"<pre><code>struct rusage {\n    struct timeval ru_utime;   // User CPU time\n    struct timeval ru_stime;   // System CPU time\n    long ru_maxrss;            // Max resident set size\n    long ru_ixrss;             // Shared memory size\n    long ru_idrss;             // Unshared data size\n    long ru_isrss;             // Unshared stack size\n    long ru_minflt;            // Page faults (no I/O)\n    long ru_majflt;            // Page faults (I/O)\n    long ru_nswap;             // Swaps\n    long ru_inblock;           // Block input operations\n    long ru_oublock;           // Block output operations\n    long ru_msgsnd;            // Messages sent\n    long ru_msgrcv;            // Messages received\n    long ru_nsignals;          // Signals received\n    long ru_nvcsw;             // Voluntary context switches\n    long ru_nivcsw;            // Involuntary context switches\n};\n</code></pre> <p>Accumulated statistics are returned to parent via wait4() and can be queried via getrusage().</p>"},{"location":"sys/kern/processes/#time-accounting","title":"Time Accounting","text":"<p>Processes track time in multiple ways:</p> <pre><code>struct pstats {\n    struct timeval p_start;    // Process start time\n    struct rusage p_ru;        // Self resource usage\n    struct rusage p_cru;       // Cumulative child usage\n};\n</code></pre> <p>Time types: - Real time - Wall clock time since start - User time - CPU time in user mode - System time - CPU time in kernel mode</p> <p>Tracked per-process and accumulated across all children.</p>"},{"location":"sys/kern/processes/#summary","title":"Summary","text":"<p>Process and thread management in DragonFly BSD implements a three-level model:</p> <ol> <li>Processes provide resource containers with credentials, address spaces, and file descriptors</li> <li>LWPs provide user-visible threads within processes</li> <li>LWKT threads provide the low-level scheduling primitive</li> </ol> <p>The lifecycle follows: - Creation via fork1() with two-phase LWP initialization around vm_fork() - Execution via kern_execve() with image activators and point-of-no-return design - Termination via exit1() with comprehensive cleanup and zombie transition - Reaping via kern_wait() with waitgen optimization and zombie cleanup</p> <p>Key design features: - PID anti-reuse protection via pid_doms[] array - Reference counting with PHOLD/PRELE and zombie-specific PHOLDZOMB/PRELEZOMB - P_WEXIT coordination to prevent concurrent exit - Two-phase LWP creation to hide partially-initialized state - P_INEXEC flag to serialize exec in multi-threaded processes - Waitgen optimization to avoid spurious wakeups in wait() - Reaper hierarchy for flexible orphan handling</p> <p>All code locations follow the <code>~/s/dragonfly/sys/kern/</code> directory structure as documented.</p>"},{"location":"sys/kern/random/","title":"Random Number Generation","text":"<p>The kernel's random number generation subsystem provides cryptographically secure pseudo-random numbers for security-sensitive operations such as key generation, address space randomization, and network protocol parameters.</p> <p>DragonFly BSD uses a hybrid design combining multiple entropy sources and cryptographic algorithms to ensure unpredictability even under adversarial conditions.</p> <p>Source files:</p> <ul> <li><code>sys/kern/kern_nrandom.c</code> - Main random subsystem (966 lines)</li> <li><code>sys/kern/subr_csprng.c</code> - CSPRNG (Fortuna-based) implementation (286 lines)</li> <li><code>sys/sys/random.h</code> - Public kernel API (123 lines)</li> <li><code>sys/sys/csprng.h</code> - CSPRNG state structures (53 lines)</li> <li><code>sys/sys/ibaa.h</code> - IBAA/L15 state definitions (27 lines)</li> </ul>"},{"location":"sys/kern/random/#overview","title":"Overview","text":""},{"location":"sys/kern/random/#design-goals","title":"Design Goals","text":"Goal Implementation Unpredictability Multiple entropy sources Forward secrecy Automatic rekeying Recovery from compromise Continuous entropy mixing Performance Per-CPU generators Availability Always returns data"},{"location":"sys/kern/random/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Sources[\"Entropy Sources\"]\n        KB[\"Keyboard\"]\n        INT[\"Interrupts\"]\n        TSC[\"TSC timing\"]\n        RD[\"RDRAND\"]\n        HW[\"Hardware RNG\"]\n        STATS[\"System stats\"]\n    end\n\n    subgraph Processing[\"Per-CPU State\"]\n        IBAA[\"IBAA CSPRNG\"]\n        L15[\"L15 Stream\"]\n        FORT[\"Fortuna/ChaCha20\"]\n    end\n\n    subgraph Output[\"Output\"]\n        DEV_RAND[\"/dev/random\"]\n        DEV_URAND[\"/dev/urandom\"]\n        KAPI[\"Kernel APIs\"]\n    end\n\n    KB --&gt; Processing\n    INT --&gt; Processing\n    TSC --&gt; Processing\n    RD --&gt; Processing\n    HW --&gt; Processing\n    STATS --&gt; Processing\n\n    IBAA --&gt; DEV_RAND\n    L15 --&gt; DEV_RAND\n    FORT --&gt; DEV_RAND\n    IBAA --&gt; DEV_URAND\n    L15 --&gt; DEV_URAND\n    FORT --&gt; DEV_URAND\n    IBAA --&gt; KAPI\n    L15 --&gt; KAPI\n    FORT --&gt; KAPI\n</code></pre>"},{"location":"sys/kern/random/#random-modes","title":"Random Modes","text":"<p>The output mode is configurable via sysctl:</p> <pre><code>/* sys/kern/kern_nrandom.c:466 */\nstatic int rand_mode = 2;  /* default: mixed */\n\nSYSCTL_PROC(_kern, OID_AUTO, rand_mode, ...);\n</code></pre> Mode Name Description 0 <code>csprng</code> Only use Fortuna/ChaCha20 1 <code>ibaa</code> Only use IBAA CSPRNG 2 <code>mixed</code> XOR both outputs (default) <p>The mixed mode provides defense in depth: even if one algorithm is compromised, the other provides protection.</p>"},{"location":"sys/kern/random/#entropy-sources","title":"Entropy Sources","text":""},{"location":"sys/kern/random/#source-identifiers","title":"Source Identifiers","text":"<pre><code>/* sys/sys/random.h:77 */\n#define RAND_SRC_UNKNOWN    0x0000\n#define RAND_SRC_SEEDING    0x0001\n#define RAND_SRC_TIMING     0x0002\n#define RAND_SRC_INTR       0x0003\n#define RAND_SRC_RDRAND     0x0004\n#define RAND_SRC_PADLOCK    0x0005\n#define RAND_SRC_GLXSB      0x0006\n#define RAND_SRC_HIFN       0x0007\n#define RAND_SRC_UBSEC      0x0008\n#define RAND_SRC_SAFE       0x0009\n#define RAND_SRC_VIRTIO     0x000a\n#define RAND_SRC_THREAD1    0x000b\n#define RAND_SRC_THREAD2    0x000c\n#define RAND_SRC_THREAD3    0x000d\n#define RAND_SRC_TPM        0x000e\n\n#define RAND_SRC_MASK       0x00FF\n#define RAND_SRCF_PCPU      0x0100  /* Per-CPU source */\n</code></pre>"},{"location":"sys/kern/random/#keyboard-events","title":"Keyboard Events","text":"<p>Keyboard scan codes provide timing-based entropy:</p> <pre><code>/* sys/kern/kern_nrandom.c:567 */\nvoid\nadd_keyboard_randomness(u_char scancode)\n{\n    struct csprng_state *state;\n\n    state = iterate_csprng_state(1);\n    if (state) {\n        spin_lock(&amp;state-&gt;spin);\n        L15_Vector(&amp;state-&gt;l15,\n                   (const LByteType *)&amp;scancode, sizeof(scancode));\n        ++state-&gt;nrandevents;\n        ++state-&gt;nrandseed;\n        spin_unlock(&amp;state-&gt;spin);\n        add_interrupt_randomness(0);\n    }\n}\n</code></pre>"},{"location":"sys/kern/random/#interrupt-timing","title":"Interrupt Timing","text":"<p>High-frequency timer (TSC) sampled at interrupt time:</p> <pre><code>/* sys/kern/kern_nrandom.c:590 */\nvoid\nadd_interrupt_randomness(int intr)\n{\n    if (tsc_present) {\n        rand_thread_value = (rand_thread_value &lt;&lt; 4) ^ 1 ^\n            ((int)rdtsc() % 151);\n    }\n    ++rand_thread_value;  /* ~1 bit of entropy */\n}\n</code></pre> <p>The TSC provides sub-microsecond timing variations that are difficult to predict.</p>"},{"location":"sys/kern/random/#hardware-rng-rdrand","title":"Hardware RNG (RDRAND)","text":"<p>Intel RDRAND instruction provides hardware-generated random numbers:</p> <pre><code>/* Called from various drivers with RAND_SRC_RDRAND */\nadd_buffer_randomness_src(buf, bytes, RAND_SRC_RDRAND | RAND_SRCF_PCPU);\n</code></pre>"},{"location":"sys/kern/random/#system-statistics","title":"System Statistics","text":"<p>The random thread periodically harvests system statistics:</p> <pre><code>/* sys/kern/kern_nrandom.c:886 */\nadd_buffer_randomness_state(state,\n                            (void *)&amp;rgd-&gt;gd_cnt,\n                            sizeof(rgd-&gt;gd_cnt),\n                            RAND_SRC_THREAD2);\nadd_buffer_randomness_state(state,\n                            (void *)&amp;rgd-&gt;gd_vmtotal,\n                            sizeof(rgd-&gt;gd_vmtotal),\n                            RAND_SRC_THREAD3);\n</code></pre>"},{"location":"sys/kern/random/#cryptographic-primitives","title":"Cryptographic Primitives","text":""},{"location":"sys/kern/random/#ibaa-bob-jenkins-csprng","title":"IBAA (Bob Jenkins' CSPRNG)","text":"<p>IBAA is a cryptographically secure PRNG with 256 32-bit words of state:</p> <pre><code>/* sys/sys/ibaa.h:8 */\nstruct ibaa_state {\n    uint32_t    IBAA_memory[SIZE];     /* 256 words of state */\n    uint32_t    IBAA_results[SIZE];    /* 256 words of output */\n    uint32_t    IBAA_aa;               /* accumulator */\n    uint32_t    IBAA_bb;               /* previous result */\n    uint32_t    IBAA_counter;          /* counter */\n    int         IBAA_byte_index;       /* output position */\n    int         memIndex;              /* seed position */\n};\n</code></pre> <p>Core algorithm:</p> <pre><code>/* sys/kern/kern_nrandom.c:217 */\nstatic void IBAA(u4 *m, u4 *r, u4 *aa, u4 *bb, u4 *counter)\n{\n    u4 a, b, x, y, i;\n\n    a = *aa;\n    b = *bb + *counter;\n    ++*counter;\n    for (i = 0; i &lt; SIZE; ++i) {\n        x = m[i];\n        a = barrel(a) + m[ind(i + (SIZE / 2))];  /* set a */\n        m[i] = y = m[ind(x)] + a + b;            /* set m */\n        r[i] = b = m[ind(y &gt;&gt; ALPHA)] + x;       /* set r */\n    }\n    *bb = b; *aa = a;\n}\n</code></pre>"},{"location":"sys/kern/random/#l15-stream-cipher","title":"L15 Stream Cipher","text":"<p>L15 is a lightweight stream cipher providing additional mixing:</p> <pre><code>/* sys/sys/ibaa.h:18 */\nstruct l15_state {\n    uint8_t     L15_x;\n    uint8_t     L15_y;\n    uint8_t     L15_start_x;\n    uint8_t     L15_state[L15_STATE_SIZE];  /* 256 bytes */\n    uint8_t     stateIndex;\n};\n</code></pre> <p>Output generation:</p> <pre><code>/* sys/kern/kern_nrandom.c:431 */\nstatic LByteType\nL15_Byte(struct l15_state *l15)\n{\n    LByteType z;\n\n    L15_Swap(l15, l15-&gt;L15_state[l15-&gt;L15_x], l15-&gt;L15_y);\n    z = (l15-&gt;L15_state[l15-&gt;L15_x++] + l15-&gt;L15_state[l15-&gt;L15_y--]);\n    if (l15-&gt;L15_x == l15-&gt;L15_start_x) {\n        --l15-&gt;L15_y;\n    }\n    return (l15-&gt;L15_state[z]);\n}\n</code></pre>"},{"location":"sys/kern/random/#fortuna-chacha20-based","title":"Fortuna (ChaCha20-based)","text":"<p>The main CSPRNG uses a Fortuna-like design with ChaCha20:</p> <pre><code>/* sys/sys/csprng.h:23 */\nstruct csprng_state {\n    uint8_t         key[SHA256_DIGEST_LENGTH];  /* 32-byte key */\n    uint64_t        reseed_cnt;                  /* reseed counter */\n    struct chacha_ctx cipher_ctx;                /* ChaCha20 context */\n    struct csprng_pool pool[32];                 /* 32 entropy pools */\n    uint8_t         src_pool_idx[256];           /* per-source pool index */\n    struct spinlock spin;                        /* per-CPU lock */\n    /* ... plus IBAA and L15 states ... */\n};\n</code></pre>"},{"location":"sys/kern/random/#entropy-pools","title":"Entropy Pools","text":"<p>32 pools with exponentially increasing reseed intervals:</p> <pre><code>/* sys/kern/subr_csprng.c:16 */\nstruct csprng_pool {\n    uint64_t    bytes;          /* entropy bytes accumulated */\n    SHA256_CTX  hash_ctx;       /* running SHA-256 hash */\n};\n</code></pre> <p>Pool <code>i</code> contributes to reseed when <code>reseed_cnt % 2^i == 0</code>:</p> <ul> <li>Pool 0: Every reseed</li> <li>Pool 1: Every 2nd reseed</li> <li>Pool 2: Every 4th reseed</li> <li>Pool 31: Every 2^31 reseeds</li> </ul> <p>This design (from Fortuna) ensures that even if an attacker knows the generator state, they cannot predict past outputs beyond a certain window.</p>"},{"location":"sys/kern/random/#reseeding","title":"Reseeding","text":"<pre><code>/* sys/kern/subr_csprng.c:174 */\nstatic int\ncsprng_reseed(struct csprng_state *state)\n{\n    /* Require minimum entropy in pool 0 */\n    if (state-&gt;pool[0].bytes &lt; MIN_POOL_SIZE) {\n        ++state-&gt;failed_reseeds;\n        return 1;\n    }\n\n    SHA256_Init(&amp;hash_ctx);\n    SHA256_Update(&amp;hash_ctx, state-&gt;key, sizeof(state-&gt;key));\n\n    state-&gt;reseed_cnt++;\n\n    /* Gather entropy from eligible pools */\n    for (i = 0; i &lt; 32; i++) {\n        if ((state-&gt;reseed_cnt % (1 &lt;&lt; i)) != 0)\n            break;\n\n        pool = &amp;state-&gt;pool[i];\n        SHA256_Final(digest, &amp;pool-&gt;hash_ctx);\n        csprng_pool_init(pool, digest, sizeof(digest));\n        SHA256_Update(&amp;hash_ctx, digest, sizeof(digest));\n    }\n\n    SHA256_Final(state-&gt;key, &amp;hash_ctx);\n    chacha_keysetup(&amp;state-&gt;cipher_ctx, state-&gt;key, 8*sizeof(state-&gt;key));\n\n    /* Reset counter (IV) */\n    bzero(counter, sizeof(counter));\n    chacha_ivsetup(&amp;state-&gt;cipher_ctx, NULL, counter);\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/random/#random-generation","title":"Random Generation","text":"<pre><code>/* sys/kern/subr_csprng.c:126 */\nint\ncsprng_get_random(struct csprng_state *state, uint8_t *out, int bytes,\n                  int flags)\n{\n    /* Check for reseed interval */\n    if (ratecheck(&amp;state-&gt;last_reseed, &amp;csprng_reseed_interval)) {\n        csprng_reseed(state);\n    }\n\n    /* Block if insufficient entropy (unless unlimited) */\n    if ((flags &amp; CSPRNG_UNLIMITED) == 0 &amp;&amp; state-&gt;reseed_cnt == 0) {\n        ssleep(state, &amp;state-&gt;spin, 0, \"csprngrsd\", 0);\n    }\n\n    while (bytes &gt; 0) {\n        /* Limit output per key to 2^20 bytes */\n        cnt = (bytes &gt; (1 &lt;&lt; 20)) ? (1 &lt;&lt; 20) : bytes;\n\n        /* Generate random bytes */\n        chacha_encrypt_bytes(&amp;state-&gt;cipher_ctx, NULL, out, cnt);\n\n        /* Rekey after each output block */\n        chacha_encrypt_bytes(&amp;state-&gt;cipher_ctx, NULL, state-&gt;key,\n                             sizeof(state-&gt;key));\n        chacha_keysetup(&amp;state-&gt;cipher_ctx, state-&gt;key,\n                        8 * sizeof(state-&gt;key));\n\n        out += cnt;\n        bytes -= cnt;\n    }\n\n    return total_bytes;\n}\n</code></pre> <p>Key properties: - Automatic rekeying after each output limits damage from state compromise - Maximum 2^20 bytes per key prevents cryptanalytic attacks - Forward secrecy: old outputs cannot be recovered after rekeying</p>"},{"location":"sys/kern/random/#per-cpu-architecture","title":"Per-CPU Architecture","text":""},{"location":"sys/kern/random/#state-distribution","title":"State Distribution","text":"<p>Each CPU has its own random state to avoid lock contention:</p> <pre><code>/* sys/kern/kern_nrandom.c:173 */\nstatic struct csprng_state *csprng_pcpu;\n\n/* sys/kern/kern_nrandom.c:697 */\nstate = &amp;csprng_pcpu[mycpu-&gt;gd_cpuid];\n</code></pre>"},{"location":"sys/kern/random/#entropy-distribution","title":"Entropy Distribution","text":"<p>Entropy is distributed across CPUs in round-robin fashion:</p> <pre><code>/* sys/kern/kern_nrandom.c:181 */\nstatic struct csprng_state *\niterate_csprng_state(int bytes)\n{\n    static unsigned int csprng_iterator;\n    unsigned int n;\n\n    if (csprng_pcpu) {\n        n = csprng_iterator++ % ncpus;\n        return &amp;csprng_pcpu[n];\n    }\n    return NULL;\n}\n</code></pre>"},{"location":"sys/kern/random/#cross-cpu-mixing","title":"Cross-CPU Mixing","text":"<p>CPUs are chained together to spread entropy:</p> <pre><code>/* sys/kern/kern_nrandom.c:898 */\n/* In rand_thread_loop: read from current CPU, feed to next */\nread_random(buf, sizeof(buf), 1);\n</code></pre> <p>This ensures: 1. No CPU is starved of entropy 2. Entropy from any source eventually affects all CPUs 3. CPUs diverge quickly from similar initial states</p>"},{"location":"sys/kern/random/#kernel-api","title":"Kernel API","text":""},{"location":"sys/kern/random/#reading-random-data","title":"Reading Random Data","text":"<pre><code>/* sys/sys/random.h:112 */\nu_int read_random(void *buf, u_int size, int unlimited);\n</code></pre> <p>Primary interface for kernel random data:</p> <p>Parameters:</p> Parameter Description <code>buf</code> Output buffer <code>size</code> Number of bytes requested <code>unlimited</code> If non-zero, never block (for /dev/urandom) <p>Returns: Number of bytes written (may be less than requested).</p> <p>Implementation:</p> <pre><code>/* sys/kern/kern_nrandom.c:687 */\nu_int\nread_random(void *buf, u_int nbytes, int unlimited)\n{\n    struct csprng_state *state;\n    int i, j;\n\n    state = &amp;csprng_pcpu[mycpu-&gt;gd_cpuid];\n\n    spin_lock(&amp;state-&gt;spin);\n    if (rand_mode == 0) {\n        /* Only CSPRNG */\n        i = csprng_get_random(state, buf, nbytes,\n                              unlimited ? CSPRNG_UNLIMITED : 0);\n    } else if (rand_mode == 1) {\n        /* Only IBAA */\n        for (i = 0; i &lt; nbytes; i++)\n            ((u_char *)buf)[i] = IBAA_Byte(&amp;state-&gt;ibaa);\n    } else {\n        /* Mix CSPRNG and IBAA */\n        i = csprng_get_random(state, buf, nbytes,\n                              unlimited ? CSPRNG_UNLIMITED : 0);\n        for (j = 0; j &lt; i; j++)\n            ((u_char *)buf)[j] ^= IBAA_Byte(&amp;state-&gt;ibaa);\n    }\n    spin_unlock(&amp;state-&gt;spin);\n\n    add_interrupt_randomness(0);\n    return (i &gt; 0) ? i : 0;\n}\n</code></pre>"},{"location":"sys/kern/random/#adding-entropy","title":"Adding Entropy","text":"<pre><code>/* sys/sys/random.h:109 */\nint add_buffer_randomness(const char *buf, int bytes);\nint add_buffer_randomness_src(const char *buf, int bytes, int srcid);\n</code></pre> <p>Add entropy from any source:</p> <pre><code>/* sys/kern/kern_nrandom.c:641 */\nint\nadd_buffer_randomness(const char *buf, int bytes)\n{\n    struct csprng_state *state;\n\n    state = iterate_csprng_state(bytes);\n    return add_buffer_randomness_state(state, buf, bytes, RAND_SRC_UNKNOWN);\n}\n</code></pre>"},{"location":"sys/kern/random/#interrupt-and-keyboard-sources","title":"Interrupt and Keyboard Sources","text":"<pre><code>/* sys/sys/random.h:107 */\nvoid add_keyboard_randomness(u_char scancode);\nvoid add_interrupt_randomness(int intr);\n</code></pre> <p>Specialized interfaces for common entropy sources.</p>"},{"location":"sys/kern/random/#system-call-getrandom","title":"System Call: getrandom()","text":"<pre><code>/* sys/kern/kern_nrandom.c:748 */\nint\nsys_getrandom(struct sysmsg *sysmsg, const struct getrandom_args *uap)\n{\n    char buf[256];\n    ssize_t bytes = (ssize_t)uap-&gt;len;\n\n    while (r &lt; bytes) {\n        n = sizeof(buf);\n        if (n &gt; bytes - r)\n            n = bytes - r;\n        read_random(buf, n, 1);\n        error = copyout(buf, (char *)uap-&gt;buf + r, n);\n        if (error)\n            break;\n        r += n;\n        lwkt_user_yield();\n\n        /* Check for signals periodically */\n        if (++sigcnt == 128) {\n            sigcnt = 0;\n            if (CURSIG_NOBLOCK(curthread-&gt;td_lwp) != 0) {\n                error = EINTR;\n                break;\n            }\n        }\n    }\n    return error;\n}\n</code></pre> <p>User space interface: <pre><code>ssize_t getrandom(void *buf, size_t buflen, unsigned int flags);\n</code></pre></p> <p>Flags: <pre><code>#define GRND_RANDOM     0x0001  /* Use /dev/random (blocking) */\n#define GRND_NONBLOCK   0x0002  /* Don't block */\n#define GRND_INSECURE   0x0004  /* Allow before full initialization */\n</code></pre></p>"},{"location":"sys/kern/random/#device-nodes","title":"Device Nodes","text":""},{"location":"sys/kern/random/#devrandom","title":"/dev/random","text":"<p>Blocking interface - may block if insufficient entropy is available:</p> <pre><code>/* In device read handler */\nread_random(buf, count, 0);  /* unlimited = 0 */\n</code></pre>"},{"location":"sys/kern/random/#devurandom","title":"/dev/urandom","text":"<p>Non-blocking interface - always returns data:</p> <pre><code>/* In device read handler */\nread_random(buf, count, 1);  /* unlimited = 1 */\n</code></pre> <p>Both use the same underlying generator; the difference is only in blocking behavior when the generator hasn't been seeded.</p>"},{"location":"sys/kern/random/#initialization","title":"Initialization","text":""},{"location":"sys/kern/random/#boot-sequence","title":"Boot Sequence","text":"<pre><code>/* sys/kern/kern_nrandom.c:485 */\nvoid\nrand_initialize(void)\n{\n    /* Called twice:\n     * 1. Early boot (ncpus == 1)\n     * 2. After SMP initialization\n     */\n\n    /* Allocate per-CPU state */\n    if (ncpus == 1) {\n        csprng_pcpu = &amp;csprng_boot;\n    } else {\n        csprng_pcpu = kmalloc(ncpus * sizeof(*csprng_pcpu),\n                              M_NRANDOM, M_WAITOK | M_ZERO);\n    }\n\n    for (n = 0; n &lt; ncpus; ++n) {\n        state = &amp;csprng_pcpu[n];\n\n        /* Initialize CSPRNG */\n        csprng_init(state);\n\n        /* Initialize IBAA */\n        IBAA_Init(&amp;state-&gt;ibaa);\n\n        /* Initialize L15 with nanosecond timing */\n        nanouptime(&amp;now);\n        L15_Init(&amp;state-&gt;l15, (const LByteType *)&amp;now.tv_nsec,\n                 sizeof(now.tv_nsec));\n\n        /* Seed with timing data */\n        for (i = 0; i &lt; (SIZE / 2); ++i) {\n            nanotime(&amp;now);\n            add_buffer_randomness_state(state, &amp;now.tv_nsec, ...);\n            nanouptime(&amp;now);\n            add_buffer_randomness_state(state, &amp;now.tv_nsec, ...);\n        }\n\n        /* Chain to next CPU for divergence */\n        read_random(buf, sizeof(buf), 1);\n    }\n}\n\nSYSINIT(rand1, SI_BOOT2_POST_SMP, SI_ORDER_SECOND, rand_initialize, 0);\n</code></pre>"},{"location":"sys/kern/random/#random-thread","title":"Random Thread","text":"<p>A dedicated thread provides continuous entropy gathering:</p> <pre><code>/* sys/kern/kern_nrandom.c:843 */\nstatic void\nrand_thread_loop(void *dummy)\n{\n    for (;;) {\n        wcpu = (wcpu + 1) % ncpus;\n        state = &amp;csprng_pcpu[wcpu];\n\n        /* Harvest timing entropy */\n        NANOUP_EVENT(&amp;last, state);\n\n        /* Calculate variable sleep interval (0.1-0.2 seconds) */\n        count = muldivu64(sys_cputimer-&gt;freq, count + 256, 256 * 10);\n        if (time_uptime &lt; 120)\n            count = count / 10 + 1;  /* Faster during early boot */\n\n        /* Harvest system statistics */\n        add_buffer_randomness_state(state, &amp;rgd-&gt;gd_cnt, ...);\n        add_buffer_randomness_state(state, &amp;rgd-&gt;gd_vmtotal, ...);\n\n        /* Cross-CPU mixing */\n        read_random(buf, sizeof(buf), 1);\n\n        tsleep(rand_td, 0, \"rwait\", 0);\n    }\n}\n\nSYSINIT(rand2, SI_SUB_HELPER_THREADS, SI_ORDER_ANY, rand_thread_init, 0);\n</code></pre>"},{"location":"sys/kern/random/#sysctl-interface","title":"Sysctl Interface","text":""},{"location":"sys/kern/random/#reading-random-data_1","title":"Reading Random Data","text":"<pre><code>sysctl kern.random\n</code></pre> <p>Returns up to 1MB of random data via sysctl:</p> <pre><code>/* sys/kern/kern_nrandom.c:724 */\nstatic int\nsysctl_kern_random(SYSCTL_HANDLER_ARGS)\n{\n    char buf[256];\n    size_t n = req-&gt;oldlen;\n    if (n &gt; 1024 * 1024)\n        n = 1024 * 1024;\n\n    while (n &gt; 0) {\n        r = (n &gt; sizeof(buf)) ? sizeof(buf) : n;\n        read_random(buf, r, 1);\n        error = SYSCTL_OUT(req, buf, r);\n        if (error)\n            break;\n        n -= r;\n    }\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/random/#configuring-mode","title":"Configuring Mode","text":"<pre><code>sysctl kern.rand_mode=mixed    # Use both generators (default)\nsysctl kern.rand_mode=csprng   # Only ChaCha20/Fortuna\nsysctl kern.rand_mode=ibaa     # Only IBAA\n</code></pre>"},{"location":"sys/kern/random/#security-considerations","title":"Security Considerations","text":""},{"location":"sys/kern/random/#entropy-estimation","title":"Entropy Estimation","text":"<p>The system does not track precise entropy estimates. Instead, it relies on:</p> <ol> <li>Multiple independent entropy sources</li> <li>Conservative pool sizing (MIN_POOL_SIZE = 64 + 32 bytes)</li> <li>Cryptographic mixing that preserves entropy</li> </ol>"},{"location":"sys/kern/random/#boot-time-entropy","title":"Boot-Time Entropy","text":"<p>Early boot is the most vulnerable period. The system mitigates this by:</p> <ol> <li>Using high-resolution timing (nanoseconds)</li> <li>Sampling multiple times during initialization</li> <li>Running the random thread faster during first 2 minutes</li> <li>Accepting hardware RNG input when available</li> </ol>"},{"location":"sys/kern/random/#state-compromise-recovery","title":"State Compromise Recovery","text":"<p>The Fortuna design ensures recovery from state compromise:</p> <ol> <li>Higher-numbered pools accumulate entropy over longer periods</li> <li>After compromise, pools 1-31 eventually provide fresh entropy</li> <li>Automatic rekeying limits the impact of key compromise</li> </ol>"},{"location":"sys/kern/random/#forward-secrecy","title":"Forward Secrecy","text":"<p>Output cannot be used to recover previous outputs because:</p> <ol> <li>ChaCha20 key is replaced after each output</li> <li>New key is generated from current state</li> <li>Old key is overwritten in memory</li> </ol>"},{"location":"sys/kern/random/#performance","title":"Performance","text":""},{"location":"sys/kern/random/#typical-performance","title":"Typical Performance","text":"<p>On modern hardware, <code>read_random()</code> achieves: - ~100-200 MB/s for large requests - Minimal overhead for small requests (&lt; 1KB)</p>"},{"location":"sys/kern/random/#optimization-techniques","title":"Optimization Techniques","text":"<ol> <li>Per-CPU state - Eliminates cross-CPU lock contention</li> <li>Batch processing - ChaCha20 generates 64 bytes at a time</li> <li>Lazy reseeding - Rate-limited to avoid overhead</li> <li>Hardware acceleration - Uses RDRAND when available</li> </ol>"},{"location":"sys/kern/random/#see-also","title":"See Also","text":"<ul> <li>Security - Kernel security framework</li> <li>Synchronization - Spinlock primitives</li> <li>Time - Timekeeping subsystem</li> <li>Sysctl - System control interface</li> </ul>"},{"location":"sys/kern/resources/","title":"Process Resources and Credentials","text":"<p>Source files: <code>kern_descrip.c</code>, <code>kern_plimit.c</code>, <code>kern_resource.c</code>, <code>kern_prot.c</code></p> <p>This document covers the management of process resources, limits, credentials, and file descriptors in DragonFly BSD. These subsystems provide resource accounting, access control, and per-process resource tracking.</p>"},{"location":"sys/kern/resources/#overview","title":"Overview","text":"<p>Process resource management encompasses four major areas:</p> <ol> <li>File descriptors (<code>kern_descrip.c</code>) \u2014 per-process file descriptor tables with thread-local caching</li> <li>Resource limits (<code>kern_plimit.c</code>) \u2014 copy-on-write limit structures (RLIMIT_* values)</li> <li>Resource accounting (<code>kern_resource.c</code>) \u2014 priority management, CPU usage tracking, per-user accounting</li> <li>Credentials (<code>kern_prot.c</code>) \u2014 UID/GID management with atomic copy-on-write semantics</li> </ol> <p>All four interact closely: file descriptors are subject to resource limits (RLIMIT_NOFILE), credentials control access to resources, and resource accounting tracks consumption per user and process.</p>"},{"location":"sys/kern/resources/#file-descriptors-kern_descripc","title":"File Descriptors (<code>kern_descrip.c</code>)","text":""},{"location":"sys/kern/resources/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/resources/#struct-filedesc","title":"<code>struct filedesc</code>","text":"<p>The per-process file descriptor table:</p> <pre><code>struct filedesc {\n    struct file **fd_files;        /* File pointer array */\n    uint32_t fd_cmask;             /* umask for open() */\n    int fd_lastfile;               /* High water mark */\n    int fd_freefile;               /* Hint for next free fd */\n    int fd_nfiles;                 /* Total slots allocated */\n    int fd_refcnt;                 /* Reference count */\n    struct uidinfo *fd_uinfo;      /* Per-user accounting */\n};\n</code></pre> <p>Key points: - <code>fd_files[]</code> is dynamically resized as file descriptors are allocated - Binary tree structure tracks free slots for efficient allocation - Each process owns one <code>filedesc</code>, inherited from parent on fork()</p>"},{"location":"sys/kern/resources/#thread-local-caching-td_fdcache","title":"Thread-local Caching (<code>td_fdcache</code>)","text":"<p>Each thread maintains a small cache to avoid expensive reference counting:</p> <pre><code>struct thread {\n    struct file *td_fdcache[NFDCACHE];  /* Cached file pointers */\n    // NFDCACHE = 16\n};\n</code></pre> <p>Cache modes (stored in low bits of pointer): - Mode 0: Available slot, no reference held - Mode 1: Locked (transitional state during lookup) - Mode 2: Borrowed reference (can reuse without atomic inc/dec)</p> <p>The cache dramatically improves performance for frequently used file descriptors (e.g., stdin/stdout/stderr).</p>"},{"location":"sys/kern/resources/#file-descriptor-allocation","title":"File Descriptor Allocation","text":""},{"location":"sys/kern/resources/#binary-tree-algorithm-fdalloc_locked","title":"Binary Tree Algorithm (<code>fdalloc_locked</code>)","text":"<p>Source: <code>kern_descrip.c:839-1064</code></p> <p>File descriptor slots are organized as a binary tree for O(log n) allocation:</p> <pre><code>flowchart TB\n    HINT[\"fd_freefile (hint)\"]\n    LEFT[\"left subtree\"]\n    RIGHT[\"right subtree\"]\n\n    HINT --&gt; LEFT\n    HINT --&gt; RIGHT\n</code></pre> <p>Key functions: - <code>right_subtree_size(fd, nfiles)</code> \u2014 size of right subtree at fd - <code>right_ancestor(fd, nfiles)</code> \u2014 next right ancestor in tree - <code>left_ancestor(fd, nfiles)</code> \u2014 next left ancestor in tree</p> <p>Allocation logic: 1. Start at <code>fd_freefile</code> (last known free) 2. If slot free, allocate immediately 3. Otherwise, traverse tree using right_ancestor/left_ancestor 4. On failure, grow <code>fd_files[]</code> array</p>"},{"location":"sys/kern/resources/#growing-the-descriptor-table","title":"Growing the Descriptor Table","text":"<p>Source: <code>kern_descrip.c:1066-1227</code></p> <p>When the table is full: 1. Calculate new size: <code>nfiles + max(nfiles/8, 15) + 3</code> 2. Allocate new array (M_FILEDESC) 3. Copy old entries 4. Free old array 5. Update <code>fd_nfiles</code></p> <p>The growth strategy balances memory overhead with reallocation frequency.</p>"},{"location":"sys/kern/resources/#file-descriptor-operations","title":"File Descriptor Operations","text":""},{"location":"sys/kern/resources/#fgetreadfgetwritefget","title":"<code>fgetread</code>/<code>fgetwrite</code>/<code>fget</code>","text":"<p>Source: <code>kern_descrip.c:1492-1650</code></p> <p>Retrieve file pointer for descriptor <code>fd</code>:</p> <ol> <li>Check cache: Look in <code>td_fdcache[]</code> first</li> <li>Fallback: If not cached, search <code>fd_files[]</code></li> <li>Validate: Check bounds, NULL, and file type (read/write)</li> <li>Reference: Increment <code>f_count</code> (borrowed ref avoids this)</li> <li>Cache: Store in <code>td_fdcache[]</code> as mode 2 (borrowed)</li> </ol> <p>Borrowed references: Cache entries hold refs without atomic ops, dramatically improving hot-path performance.</p>"},{"location":"sys/kern/resources/#fdrop-release-file-reference","title":"<code>fdrop</code> \u2014 Release File Reference","text":"<p>Source: <code>kern_descrip.c:2128-2217</code></p> <p>Decrement <code>f_count</code>; if zero, close file:</p> <ol> <li>Atomic decrement of <code>f_count</code></li> <li>If non-zero, return</li> <li>If zero:</li> <li>Call <code>fo_close()</code> (file operations close)</li> <li>Call <code>fdrevoke()</code> to invalidate all cached refs</li> <li>Free file structure</li> </ol> <p>Locking: Uses per-file <code>f_spin</code> spinlock for atomicity.</p>"},{"location":"sys/kern/resources/#dupdup2fcntlf_dupfd","title":"<code>dup</code>/<code>dup2</code>/<code>fcntl(F_DUPFD)</code>","text":"<p>Source: <code>kern_descrip.c:299-447</code></p> <p>Duplicate file descriptor:</p> <ul> <li><code>dup(old)</code> \u2014 allocate lowest free fd, copy file pointer</li> <li><code>dup2(old, new)</code> \u2014 force specific fd, close <code>new</code> if open</li> <li><code>fcntl(F_DUPFD, minfd)</code> \u2014 allocate fd &gt;= minfd</li> </ul> <p>Flags: - <code>DUP_FIXED</code> \u2014 dup2 behavior (specific fd) - <code>DUP_VARIABLE</code> \u2014 dup behavior (any free fd) - <code>DUP_CLOEXEC</code> \u2014 set close-on-exec flag</p> <p>Cache invalidation: <code>fclearcache()</code> clears all thread caches when closing or revoking.</p>"},{"location":"sys/kern/resources/#file-descriptor-limits","title":"File Descriptor Limits","text":"<p>Source: <code>kern_descrip.c:1229-1333</code></p> <p>Two limits apply:</p> <ol> <li>Per-process: <code>RLIMIT_NOFILE</code> (default 1024, hard max 1,048,576)</li> <li>Per-user: <code>maxfilesperuser</code> (default 80% of system max)</li> </ol> <p>Enforcement: - <code>fdalloc()</code> checks both limits before allocation - <code>chgproccnt()</code> updates per-user <code>ui_proccnt</code> counter (via uidinfo) - <code>chgopenfiles()</code> updates per-user <code>ui_openfiles</code> counter</p> <p>System-wide limit: <code>maxfiles</code> (global sysctl, default computed from RAM).</p>"},{"location":"sys/kern/resources/#file-descriptor-revocation-fdrevoke","title":"File Descriptor Revocation (<code>fdrevoke</code>)","text":"<p>Source: <code>kern_descrip.c:2219-2339</code></p> <p>Invalidate all references to a file:</p> <ol> <li>Mark file with <code>FREVOKED</code> flag</li> <li>Iterate all processes' file descriptor tables</li> <li>For each match, set <code>FREVOKED</code> in <code>fd_files[]</code></li> <li>Call <code>fclearcache()</code> to purge thread caches</li> <li>Wake sleeping threads (select/poll)</li> </ol> <p>Use case: Device revocation (e.g., USB device unplugged).</p>"},{"location":"sys/kern/resources/#resource-limits-kern_plimitc","title":"Resource Limits (<code>kern_plimit.c</code>)","text":""},{"location":"sys/kern/resources/#data-structures_1","title":"Data Structures","text":""},{"location":"sys/kern/resources/#struct-plimit","title":"<code>struct plimit</code>","text":"<p>Process resource limits:</p> <pre><code>struct plimit {\n    struct rlimit pl_rlimit[RLIM_NLIMITS]; /* Limit array */\n    int pl_refcnt;                          /* Reference count */\n    uint32_t pl_flags;                      /* Flags */\n};\n\n#define PLIMITF_EXCLUSIVE  0x00000001  /* Private copy (multi-threaded) */\n</code></pre> <p>Limit types (subset of <code>RLIM_NLIMITS</code>): - <code>RLIMIT_CPU</code> \u2014 CPU seconds (enforced in <code>kern_clock.c</code>) - <code>RLIMIT_DATA</code> \u2014 Data segment size - <code>RLIMIT_STACK</code> \u2014 Stack size - <code>RLIMIT_CORE</code> \u2014 Core dump size - <code>RLIMIT_NOFILE</code> \u2014 Open files per process - <code>RLIMIT_VMEM</code> \u2014 Virtual memory - <code>RLIMIT_NPROC</code> \u2014 Processes per user - <code>RLIMIT_SBSIZE</code> \u2014 Socket buffer space per user</p> <p>Each limit has soft (<code>rlim_cur</code>) and hard (<code>rlim_max</code>) values.</p>"},{"location":"sys/kern/resources/#copy-on-write-semantics","title":"Copy-on-Write Semantics","text":"<p>Source: <code>kern_plimit.c:105-212</code></p> <p>Process limits are shared across fork() using reference counting:</p> <pre><code>   parent fork \u2192 child\n      \u2193              \u2193\n   plimit \u2190 pl_refcnt = 2\n</code></pre> <p><code>plimit_fork(struct plimit *olimit)</code></p> <p>Called during fork(): 1. If <code>PLIMITF_EXCLUSIVE</code> set \u2192 allocate private copy 2. Otherwise \u2192 increment <code>pl_refcnt</code>, share with child</p> <p><code>plimit_lwp_fork(struct plimit *olimit)</code></p> <p>Called when creating LWPs (multi-threaded process): 1. Always allocate private copy 2. Set <code>PLIMITF_EXCLUSIVE</code> flag 3. Return new exclusive limit structure</p> <p>Rationale: Multi-threaded processes need private limits to avoid races when multiple LWPs modify limits simultaneously.</p>"},{"location":"sys/kern/resources/#limit-modification","title":"Limit Modification","text":""},{"location":"sys/kern/resources/#plimit_modify","title":"<code>plimit_modify</code>","text":"<p>Source: <code>kern_plimit.c:233-289</code></p> <p>Atomically modify a limit:</p> <ol> <li>If <code>pl_refcnt &gt; 1</code> \u2192 allocate private copy (copy-on-write)</li> <li>Set <code>PLIMITF_EXCLUSIVE</code> if process has LWPs</li> <li>Update limit value</li> <li>Release old limit structure</li> </ol> <p>Locking: Uses process token (<code>&amp;p-&gt;p_token</code>) for atomicity.</p>"},{"location":"sys/kern/resources/#dosetrlimit","title":"<code>dosetrlimit</code>","text":"<p>Source: <code>kern_plimit.c:291-444</code></p> <p>System call handler for <code>setrlimit()</code>:</p> <ol> <li>Validate new limits (soft \u2264 hard)</li> <li>Check permissions (non-root can only lower hard limit)</li> <li>Call <code>plimit_modify()</code> to update</li> <li>Special handling:</li> <li><code>RLIMIT_NOFILE</code> \u2014 update <code>maxfilesperproc</code> cache</li> <li><code>RLIMIT_STACK</code> \u2014 call <code>vm_map_growstack()</code> to adjust stack</li> <li><code>RLIMIT_CPU</code> \u2014 update <code>p_cpulimit</code> (in microseconds)</li> </ol> <p>Permission check: Raising hard limits requires superuser privilege.</p>"},{"location":"sys/kern/resources/#fork-depth-adjustment-plimit_getadjvalue","title":"Fork Depth Adjustment (<code>plimit_getadjvalue</code>)","text":"<p>Source: <code>kern_plimit.c:446-493</code></p> <p>Adjust limits based on chroot depth:</p> <pre><code>static uint64_t plimit_getadjvalue(uint64_t v) {\n    int depth = chroot_visible_vnodes.depth;\n    v -= v * depth * 10 / 100;  /* 10% per chroot level */\n    // Max 50% reduction\n}\n</code></pre> <p>Rationale: Nested chroot environments (e.g., jails within jails) get progressively reduced limits to prevent resource exhaustion.</p>"},{"location":"sys/kern/resources/#cpu-limit-enforcement","title":"CPU Limit Enforcement","text":"<p>The CPU limit is enforced in <code>kern_clock.c:statclock()</code>:</p> <ol> <li>Each clock tick, check <code>p-&gt;p_cpulimit</code></li> <li>If exceeded, send <code>SIGXCPU</code> signal</li> <li>If grace period exceeded, send <code>SIGKILL</code></li> </ol> <p>The limit is stored in microseconds (<code>p_cpulimit</code>) for efficient comparison.</p>"},{"location":"sys/kern/resources/#resource-accounting-kern_resourcec","title":"Resource Accounting (<code>kern_resource.c</code>)","text":""},{"location":"sys/kern/resources/#priority-management","title":"Priority Management","text":""},{"location":"sys/kern/resources/#three-priority-types","title":"Three Priority Types","text":"<ol> <li>Nice priority (<code>p_nice</code>) \u2014 CPU scheduling priority (-20 to +20)</li> <li>I/O priority (<code>p_ionice</code>) \u2014 Disk I/O priority (0 to 20)</li> <li>Real-time priority (<code>lwp_rtprio</code>) \u2014 Real-time scheduling class</li> </ol> <p>Each type is independent and affects different schedulers.</p>"},{"location":"sys/kern/resources/#getprioritysetpriority","title":"<code>getpriority</code>/<code>setpriority</code>","text":"<p>Source: <code>kern_resource.c:84-244</code></p> <p>Adjust nice value:</p> <pre><code>int setpriority(int which, id_t who, int prio) {\n    // which: PRIO_PROCESS, PRIO_PGRP, PRIO_USER\n    // prio: PRIO_MIN (-20) to PRIO_MAX (+20)\n}\n</code></pre> <p>Effects: 1. Update <code>p-&gt;p_nice</code> 2. Call <code>p-&gt;p_usched-&gt;resetpriority(lp)</code> for each LWP 3. Reschedule threads with new priority</p> <p>Permission: Non-root can only increase nice (lower priority).</p>"},{"location":"sys/kern/resources/#ioprio_getioprio_set","title":"<code>ioprio_get</code>/<code>ioprio_set</code>","text":"<p>Source: <code>kern_resource.c:246-386</code></p> <p>Adjust I/O priority:</p> <pre><code>int ioprio_set(int which, int who, int prio) {\n    // prio: IOPRIO_MIN (0) to IOPRIO_MAX (20)\n}\n</code></pre> <p>Effects: 1. Update <code>p-&gt;p_ionice</code> 2. Affects disk scheduler (dsched) I/O ordering</p> <p>Use case: Deprioritize background tasks (e.g., backups).</p>"},{"location":"sys/kern/resources/#rtpriolwp_rtprio","title":"<code>rtprio</code>/<code>lwp_rtprio</code>","text":"<p>Source: <code>kern_resource.c:388-594</code></p> <p>Real-time priority control:</p> <pre><code>struct rtprio {\n    uint16_t type;   /* RTP_PRIO_REALTIME, NORMAL, IDLE, FIFO */\n    uint16_t prio;   /* 0-31 */\n};\n</code></pre> <p>Classes: - <code>RTP_PRIO_REALTIME</code> \u2014 Hard real-time (requires <code>SYSCAP_NOSCHED</code>) - <code>RTP_PRIO_FIFO</code> \u2014 FIFO scheduling - <code>RTP_PRIO_NORMAL</code> \u2014 Time-sharing - <code>RTP_PRIO_IDLE</code> \u2014 Run only when idle</p> <p>Permission: Real-time classes require <code>SYSCAP_NOSCHED</code> capability.</p>"},{"location":"sys/kern/resources/#cpu-time-accounting","title":"CPU Time Accounting","text":""},{"location":"sys/kern/resources/#calcru-calculate-resource-usage","title":"<code>calcru</code> \u2014 Calculate Resource Usage","text":"<p>Source: <code>kern_resource.c:655-743</code></p> <p>Convert tick counters to timeval:</p> <pre><code>void calcru(struct lwp *lp, struct timeval *up, struct timeval *sp) {\n    // Convert td_uticks \u2192 up (user time)\n    // Convert td_sticks \u2192 sp (system time)\n}\n</code></pre> <p>Tick sources: - <code>td_uticks</code> \u2014 User-mode microseconds (updated in statclock) - <code>td_sticks</code> \u2014 Kernel-mode microseconds (updated in statclock) - <code>td_iticks</code> \u2014 Interrupt microseconds</p> <p>Algorithm: 1. Read tick counters atomically 2. Convert ticks to timeval using <code>sys_cputimer-&gt;freq</code> 3. Handle wraparound and monotonicity</p>"},{"location":"sys/kern/resources/#calcru_proc-aggregate-process-usage","title":"<code>calcru_proc</code> \u2014 Aggregate Process Usage","text":"<p>Source: <code>kern_resource.c:745-819</code></p> <p>Sum all LWP statistics into process rusage:</p> <pre><code>void calcru_proc(struct proc *p, struct rusage *ru) {\n    FOREACH_LWP_IN_PROC(lp, p) {\n        calcru(lp, &amp;utv, &amp;stv);\n        timeradd(&amp;ru-&gt;ru_utime, &amp;utv);\n        timeradd(&amp;ru-&gt;ru_stime, &amp;stv);\n    }\n}\n</code></pre> <p>Aggregated fields: - <code>ru_utime</code> \u2014 Total user CPU time - <code>ru_stime</code> \u2014 Total system CPU time - <code>ru_minflt</code> \u2014 Minor page faults - <code>ru_majflt</code> \u2014 Major page faults - <code>ru_inblock</code> \u2014 Block input operations - <code>ru_oublock</code> \u2014 Block output operations</p>"},{"location":"sys/kern/resources/#getrusage-system-call","title":"<code>getrusage</code> System Call","text":"<p>Source: <code>kern_resource.c:821-922</code></p> <p>Retrieve resource usage:</p> <pre><code>int getrusage(int who, struct rusage *rusage) {\n    // who: RUSAGE_SELF, RUSAGE_CHILDREN\n}\n</code></pre> <p>RUSAGE_SELF: Returns current process usage (via <code>calcru_proc</code>).</p> <p>RUSAGE_CHILDREN: Returns accumulated child usage from <code>p-&gt;p_cru</code> (updated in <code>kern_exit.c:wait1()</code> when reaping children).</p>"},{"location":"sys/kern/resources/#per-user-resource-tracking-uidinfo","title":"Per-User Resource Tracking (<code>uidinfo</code>)","text":""},{"location":"sys/kern/resources/#struct-uidinfo","title":"<code>struct uidinfo</code>","text":"<p>Source: <code>kern_resource.c:66-82</code></p> <p>Per-user accounting structure:</p> <pre><code>struct uidinfo {\n    uid_t ui_uid;           /* User ID */\n    int ui_ref;             /* Reference count */\n    int ui_proccnt;         /* Process count */\n    int ui_openfiles;       /* Open file count */\n    int ui_sbsize;          /* Socket buffer bytes */\n    // Hashed in uidinfo_hash\n};\n</code></pre> <p>Use cases: - Enforce <code>RLIMIT_NPROC</code> (processes per user) - Enforce <code>maxfilesperuser</code> (open files per user) - Enforce <code>RLIMIT_SBSIZE</code> (socket buffer space per user)</p>"},{"location":"sys/kern/resources/#chgproccntchgsbsize","title":"<code>chgproccnt</code>/<code>chgsbsize</code>","text":"<p>Source: <code>kern_resource.c:1038-1103</code></p> <p>Atomically adjust per-user counters:</p> <pre><code>int chgproccnt(struct uidinfo *uip, int diff, int max) {\n    // Returns 0 if within limit, 1 if exceeded\n}\n\nint chgsbsize(struct uidinfo *uip, int *hiwat, int to, int max) {\n    // Adjust socket buffer size tracking\n}\n</code></pre> <p>Atomicity: Uses <code>atomic_fetchadd_int()</code> for lock-free updates.</p> <p>Callers: - <code>fork()</code> \u2192 <code>chgproccnt(uip, 1, maxproc)</code> - <code>exit()</code> \u2192 <code>chgproccnt(uip, -1, 0)</code> - <code>socket()</code> \u2192 <code>chgsbsize()</code> for buffer allocation</p>"},{"location":"sys/kern/resources/#credentials-kern_protc","title":"Credentials (<code>kern_prot.c</code>)","text":""},{"location":"sys/kern/resources/#data-structures_2","title":"Data Structures","text":""},{"location":"sys/kern/resources/#struct-ucred","title":"<code>struct ucred</code>","text":"<p>Process credentials:</p> <pre><code>struct ucred {\n    int cr_ref;                /* Reference count */\n    uid_t cr_uid;              /* Effective user ID */\n    uid_t cr_ruid;             /* Real user ID */\n    uid_t cr_svuid;            /* Saved user ID */\n    gid_t cr_gid;              /* Effective group ID */\n    gid_t cr_rgid;             /* Real group ID */\n    gid_t cr_svgid;            /* Saved group ID */\n    gid_t cr_groups[NGROUPS];  /* Supplementary groups */\n    int cr_ngroups;            /* Group count */\n    struct prison *cr_prison;  /* Jail info */\n    // ... capabilities, labels, etc.\n};\n</code></pre> <p>Special credentials: - <code>NOCRED</code> \u2014 No credential (internal use) - <code>FSCRED</code> \u2014 Filesystem credential (never freed)</p>"},{"location":"sys/kern/resources/#atomic-copy-on-write-cratom","title":"Atomic Copy-on-Write (<code>cratom</code>)","text":"<p>Source: <code>kern_prot.c:113-177</code></p> <p>Ensure exclusive credential before modification:</p> <pre><code>struct ucred *cratom(struct ucred **cr) {\n    if ((*cr)-&gt;cr_ref &gt; 1) {\n        // Shared \u2192 allocate private copy\n        struct ucred *ncr = crdup(*cr);\n        crfree(*cr);\n        *cr = ncr;\n    }\n    return *cr;\n}\n</code></pre> <p>Why copy-on-write? - Credentials are shared across fork() and threads - Modification requires atomicity (no races) - Copy-on-write avoids unnecessary duplication</p>"},{"location":"sys/kern/resources/#cratom_proc","title":"<code>cratom_proc</code>","text":"<p>Source: <code>kern_prot.c:179-209</code></p> <p>Process-level atomization:</p> <pre><code>struct ucred *cratom_proc(struct proc *p) {\n    p-&gt;p_ucred = cratom(&amp;p-&gt;p_ucred);\n    return p-&gt;p_ucred;\n}\n</code></pre> <p>Locking: Uses process token (<code>&amp;p-&gt;p_token</code>) for atomicity.</p>"},{"location":"sys/kern/resources/#uidgid-system-calls","title":"UID/GID System Calls","text":""},{"location":"sys/kern/resources/#getuidgeteuidgetgidgetegid","title":"<code>getuid</code>/<code>geteuid</code>/<code>getgid</code>/<code>getegid</code>","text":"<p>Source: <code>kern_prot.c:211-279</code></p> <p>Retrieve credentials:</p> <pre><code>uid_t getuid(void)  { return curthread-&gt;td_ucred-&gt;cr_ruid; }\nuid_t geteuid(void) { return curthread-&gt;td_ucred-&gt;cr_uid; }\n</code></pre> <p>Thread vs. process credentials: - Threads cache <code>td_ucred</code> (pointer to <code>p-&gt;p_ucred</code>) - Always consistent due to copy-on-write semantics</p>"},{"location":"sys/kern/resources/#setuidseteuid","title":"<code>setuid</code>/<code>seteuid</code>","text":"<p>Source: <code>kern_prot.c:281-457</code></p> <p>Change user ID:</p> <pre><code>int setuid(uid_t uid) {\n    // POSIX_APPENDIX_B_4_2_2 semantics\n}\n</code></pre> <p>Semantics (POSIX_APPENDIX_B_4_2_2):</p> <p>For non-superuser: - Can set euid to ruid or svuid only</p> <p>For superuser: - Sets all three: ruid, euid, svuid</p> <p>Implementation: 1. Call <code>cratom_proc()</code> to get exclusive credential 2. Update uid fields 3. Call <code>change_euid()</code> to transfer uidinfo 4. Mark process as <code>P_SUGID</code> (tainted)</p>"},{"location":"sys/kern/resources/#change_euidchange_ruid","title":"<code>change_euid</code>/<code>change_ruid</code>","text":"<p>Source: <code>kern_prot.c:459-550</code></p> <p>Helper functions to change UID with uidinfo transfer:</p> <pre><code>void change_euid(uid_t euid) {\n    struct uidinfo *new_uip = uifind(euid);\n    uireplace(&amp;p-&gt;p_ucred-&gt;cr_uidinfo, new_uip);\n    // Transfer lock file ownership, etc.\n}\n</code></pre> <p>uidinfo management: - <code>uifind(uid)</code> \u2014 Lookup or create uidinfo (refcounted) - <code>uireplace()</code> \u2014 Atomically swap uidinfo pointers - <code>uihold()</code>/<code>uidrop()</code> \u2014 Reference counting</p> <p>Lock file adjustment: <code>lf_count_adjust()</code> transfers file lock ownership.</p>"},{"location":"sys/kern/resources/#setreuidsetregid","title":"<code>setreuid</code>/<code>setregid</code>","text":"<p>Source: <code>kern_prot.c:552-751</code></p> <p>Set real and effective IDs simultaneously:</p> <pre><code>int setreuid(uid_t ruid, uid_t euid) {\n    // Allows swapping ruid \u2194 euid (for setuid programs)\n}\n</code></pre> <p>Use case: Temporarily drop privileges (swap euid and ruid), perform operation, then restore.</p>"},{"location":"sys/kern/resources/#setresuidsetresgid","title":"<code>setresuid</code>/<code>setresgid</code>","text":"<p>Source: <code>kern_prot.c:753-988</code></p> <p>Set real, effective, and saved IDs:</p> <pre><code>int setresuid(uid_t ruid, uid_t euid, uid_t suid) {\n    // Fine-grained control over all three IDs\n}\n</code></pre> <p>Advantage: Provides explicit control over saved UID (needed for some security models).</p>"},{"location":"sys/kern/resources/#setgroupsgetgroups","title":"<code>setgroups</code>/<code>getgroups</code>","text":"<p>Source: <code>kern_prot.c:990-1120</code></p> <p>Manage supplementary groups:</p> <pre><code>int setgroups(int ngroups, gid_t *groups) {\n    // Requires superuser privilege\n}\n</code></pre> <p>Limit: <code>NGROUPS</code> (typically 16-64 depending on configuration).</p>"},{"location":"sys/kern/resources/#permission-checking","title":"Permission Checking","text":""},{"location":"sys/kern/resources/#p_trespass","title":"<code>p_trespass</code>","text":"<p>Source: <code>kern_prot.c:1256-1332</code></p> <p>Check if process A can signal/ptrace process B:</p> <pre><code>int p_trespass(struct ucred *cr1, struct ucred *cr2) {\n    // Returns 0 if allowed, errno otherwise\n}\n</code></pre> <p>Rules: 1. Root can always signal others 2. Same uid \u2192 allowed 3. Same ruid and target not setuid \u2192 allowed 4. Otherwise \u2192 EPERM</p> <p>Security: Prevents unprivileged processes from interfering with setuid processes.</p>"},{"location":"sys/kern/resources/#processsessiongroup-ids","title":"Process/Session/Group IDs","text":""},{"location":"sys/kern/resources/#getpidgetppid","title":"<code>getpid</code>/<code>getppid</code>","text":"<p>Source: <code>kern_prot.c:1415-1478</code></p> <p>Retrieve process/parent IDs:</p> <pre><code>pid_t getpid(void)  { return curproc-&gt;p_pid; }\npid_t getppid(void) { return curproc-&gt;p_pptr-&gt;p_pid; }\n</code></pre>"},{"location":"sys/kern/resources/#setsid-create-new-session","title":"<code>setsid</code> \u2014 Create New Session","text":"<p>Source: <code>kern_prot.c:1532-1597</code></p> <p>Become session leader:</p> <pre><code>pid_t setsid(void) {\n    // Create new session and process group\n}\n</code></pre> <p>Effects: 1. Allocate new session ID (equal to pid) 2. Allocate new process group ID (equal to pid) 3. Detach from controlling terminal 4. Become session leader</p> <p>Restrictions: Cannot be called by process group leader.</p>"},{"location":"sys/kern/resources/#setpgid-set-process-group","title":"<code>setpgid</code> \u2014 Set Process Group","text":"<p>Source: <code>kern_prot.c:1599-1749</code></p> <p>Change process group membership:</p> <pre><code>int setpgid(pid_t pid, pid_t pgid) {\n    // Move process to different process group\n}\n</code></pre> <p>Restrictions: 1. Can only set own pgid or child's pgid before exec 2. Target must be in same session 3. Cannot move across session boundaries</p>"},{"location":"sys/kern/resources/#credential-tainting-setsugid","title":"Credential Tainting (<code>setsugid</code>)","text":"<p>Source: <code>kern_prot.c:1751-1784</code></p> <p>Mark process as tainted (changed credentials):</p> <pre><code>void setsugid(void) {\n    p-&gt;p_flags |= P_SUGID;\n}\n</code></pre> <p>Effects: - Disables core dumps (security) - Disables ptrace attachment - Prevents privilege escalation exploits</p> <p>Callers: - <code>setuid()</code>, <code>setgid()</code> \u2014 after changing credentials - <code>execve()</code> \u2014 when executing setuid binary</p>"},{"location":"sys/kern/resources/#interactions-between-subsystems","title":"Interactions Between Subsystems","text":""},{"location":"sys/kern/resources/#file-descriptors-resource-limits","title":"File Descriptors \u2192 Resource Limits","text":"<p><code>fdalloc()</code> checks <code>RLIMIT_NOFILE</code> before allocating:</p> <pre><code>if (fd &gt;= p-&gt;p_rlimit[RLIMIT_NOFILE].rlim_cur)\n    return EMFILE;\n</code></pre>"},{"location":"sys/kern/resources/#file-descriptors-credentials","title":"File Descriptors \u2192 Credentials","text":"<p>File operations use <code>td_ucred</code> for permission checks:</p> <pre><code>int error = VOP_READ(vp, uio, 0, td-&gt;td_ucred);\n</code></pre>"},{"location":"sys/kern/resources/#resource-limits-fork","title":"Resource Limits \u2192 Fork","text":"<p><code>fork()</code> checks <code>RLIMIT_NPROC</code> before creating child:</p> <pre><code>if (chgproccnt(uip, 1, p-&gt;p_rlimit[RLIMIT_NPROC].rlim_cur) &gt; 0)\n    return EAGAIN;\n</code></pre>"},{"location":"sys/kern/resources/#credentials-uidinfo","title":"Credentials \u2192 uidinfo","text":"<p>Changing UID transfers uidinfo:</p> <pre><code>change_euid(new_uid) {\n    uireplace(&amp;cr-&gt;cr_uidinfo, uifind(new_uid));\n}\n</code></pre> <p>This updates per-user resource tracking atomically.</p>"},{"location":"sys/kern/resources/#key-design-principles","title":"Key Design Principles","text":""},{"location":"sys/kern/resources/#1-copy-on-write-for-sharing","title":"1. Copy-on-Write for Sharing","text":"<p>Both <code>plimit</code> and <code>ucred</code> use reference counting with copy-on-write:</p> <ul> <li>Cheap sharing across fork()</li> <li>Atomic modification when needed</li> <li>No races in multi-threaded processes</li> </ul>"},{"location":"sys/kern/resources/#2-thread-local-caching","title":"2. Thread-Local Caching","text":"<p>File descriptor cache (<code>td_fdcache</code>) avoids expensive atomic operations:</p> <ul> <li>16-entry cache per thread</li> <li>Borrowed references (mode 2) skip refcount ops</li> <li>Cache coherency via <code>fclearcache()</code></li> </ul>"},{"location":"sys/kern/resources/#3-per-user-resource-tracking","title":"3. Per-User Resource Tracking","text":"<p><code>uidinfo</code> provides global accounting per user:</p> <ul> <li>Prevents single user from exhausting system resources</li> <li>Enforced atomically with <code>chgproccnt</code>/<code>chgsbsize</code></li> <li>Hashed for efficient lookup</li> </ul>"},{"location":"sys/kern/resources/#4-binary-tree-allocation","title":"4. Binary Tree Allocation","text":"<p>File descriptor allocation uses binary tree traversal:</p> <ul> <li>O(log n) allocation even with sparse tables</li> <li>Efficient reuse of low-numbered descriptors</li> <li>Minimizes table growth</li> </ul>"},{"location":"sys/kern/resources/#5-limit-enforcement-points","title":"5. Limit Enforcement Points","text":"<p>Resource limits are enforced at allocation points:</p> <ul> <li><code>RLIMIT_NOFILE</code> \u2014 in <code>fdalloc()</code></li> <li><code>RLIMIT_NPROC</code> \u2014 in <code>fork()</code></li> <li><code>RLIMIT_CPU</code> \u2014 in <code>statclock()</code> (kern_clock.c)</li> <li><code>RLIMIT_STACK</code> \u2014 in <code>vm_map_growstack()</code> (sys/vm)</li> </ul>"},{"location":"sys/kern/resources/#summary","title":"Summary","text":"<p>Process resource management in DragonFly BSD provides:</p> <ol> <li>Efficient file descriptor management with thread-local caching and binary tree allocation</li> <li>Copy-on-write resource limits shared across fork() with atomic modification</li> <li>Multi-level priority control (nice, ionice, rtprio) for CPU and I/O scheduling</li> <li>Atomic credential management with POSIX semantics and per-user accounting</li> <li>System-wide resource tracking preventing exhaustion attacks</li> </ol> <p>The design emphasizes: - Atomicity through copy-on-write and tokens - Performance through caching and lock-free algorithms - Isolation through per-user limits and permission checks - Scalability through efficient data structures (binary tree, hash tables)</p> <p>These subsystems interact closely with process lifecycle (fork/exec/exit), virtual filesystem (file operations), and virtual memory (stack limits), forming the foundation of resource management in the kernel.</p>"},{"location":"sys/kern/scheduling/","title":"CPU Scheduling","text":""},{"location":"sys/kern/scheduling/#overview","title":"Overview","text":"<p>DragonFly's CPU scheduling system consists of two distinct layers:</p> <ol> <li>LWKT (Light Weight Kernel Threads) Layer - Low-level thread scheduling that handles all kernel threads and provides the foundation for userland scheduling</li> <li>User Scheduler Layer - Pluggable schedulers that implement policies for userland process scheduling</li> </ol> <p>This document focuses on the user scheduler layer and the sleep/wakeup synchronization primitives that coordinate thread blocking and resumption.</p> <p>Key source files: - <code>kern_sched.c</code> - POSIX real-time scheduling support (ksched) - <code>kern_synch.c</code> - Sleep/wakeup, tsleep/wakeup infrastructure - <code>kern_usched.c</code> - User scheduler registration and management - <code>usched_bsd4.c</code> - BSD4 scheduler (original DragonFly) - <code>usched_dfly.c</code> - DFLY scheduler (message-based, default) - <code>usched_dummy.c</code> - Dummy scheduler (for testing/reference)</p>"},{"location":"sys/kern/scheduling/#architecture","title":"Architecture","text":""},{"location":"sys/kern/scheduling/#two-layer-design","title":"Two-Layer Design","text":"<pre><code>flowchart TB\n    USERLAND[\"Userland Processes (LWPs)\"]\n\n    subgraph USCHED[\"User Scheduler Layer (pluggable)\"]\n        U1[\"usched_bsd4 / usched_dfly\"]\n        U2[\"Run queues per scheduler\"]\n        U3[\"CPU affinity management\"]\n        U4[\"Priority calculations\"]\n    end\n\n    subgraph LWKT[\"LWKT Scheduler (per-CPU)\"]\n        L1[\"All kernel threads\"]\n        L2[\"Thread preemption\"]\n        L3[\"Context switching\"]\n    end\n\n    USERLAND --&gt; USCHED\n    USCHED --&gt; LWKT\n</code></pre>"},{"location":"sys/kern/scheduling/#pluggable-user-schedulers","title":"Pluggable User Schedulers","text":"<p>DragonFly allows multiple user schedulers to coexist. Each process is assigned a scheduler (<code>p-&gt;p_usched</code>), and the system provides a common interface (<code>struct usched</code>) that all schedulers must implement.</p>"},{"location":"sys/kern/scheduling/#sleepwakeup-infrastructure","title":"Sleep/Wakeup Infrastructure","text":""},{"location":"sys/kern/scheduling/#overview-kern_synchc","title":"Overview (kern_synch.c)","text":"<p>The sleep/wakeup mechanism is DragonFly's primary thread synchronization primitive, allowing threads to block waiting for events and be awakened when those events occur.</p> <p>Core functions: - <code>tsleep()</code> - Sleep on an identifier (ident) with timeout and signal handling - <code>wakeup()</code> - Wake all threads sleeping on an identifier - <code>wakeup_one()</code> - Wake one thread sleeping on an identifier - <code>tsleep_interlock()</code> - Prepare to sleep without blocking yet - <code>ssleep()</code>, <code>lksleep()</code>, <code>mtxsleep()</code>, <code>zsleep()</code> - Variants that atomically release locks</p>"},{"location":"sys/kern/scheduling/#sleep-queue-hash-table","title":"Sleep Queue Hash Table","text":"<p>Data structure: <code>struct tslpque</code> (kern_synch.c:68)</p> <pre><code>struct tslpque {\n    TAILQ_HEAD(, thread)  queue;\n    const volatile void   *ident0;\n    const volatile void   *ident1;\n    const volatile void   *ident2;\n    const volatile void   *ident3;\n};\n</code></pre> <p>Each CPU maintains its own hash table of sleep queues (<code>gd-&gt;gd_tsleep_hash</code>). The hash is computed from the sleep identifier (typically an address):</p> <pre><code>#define LOOKUP(x)  ((((uintptr_t)(x) + ((uintptr_t)(x) &gt;&gt; 18)) ^ \\\n                     LOOKUP_PRIME) % slpque_tablesize)\n#define TCHASHSHIFT(x)  ((x) &gt;&gt; 4)\n</code></pre> <p>The global <code>slpque_cpumasks[]</code> array tracks which CPUs have threads sleeping on each hash bucket, enabling efficient cross-CPU wakeups.</p>"},{"location":"sys/kern/scheduling/#tsleep-flow","title":"tsleep() Flow","text":"<p>Function: <code>tsleep()</code> (kern_synch.c:512)</p> <pre><code>1. Check for delayed wakeups (TDF_DELAYED_WAKEUP)\n2. Handle early boot / panic case (just yield briefly)\n3. Enter critical section\n4. Interlock with sleep queue (if not already done via PINTERLOCKED)\n5. Handle process state (SCORE for coredump)\n6. Check for pending signals (if PCATCH set)\n   - Early return with EINTR/ERESTART if signal pending\n7. Set LWP_SINTR flag (if PCATCH) to allow signal wakeup\n8. Release from user scheduler (p_usched-&gt;release_curproc)\n9. Verify still on sleep queue (race detection)\n10. Deschedule from LWKT (lwkt_deschedule_self)\n11. Set TDF_TSLEEP_DESCHEDULED flag\n12. Setup timeout callout (if timo != 0)\n13. Set lp-&gt;lwp_stat = LSSLEEP\n14. lwkt_switch() - actually sleep\n15. [WOKEN UP]\n16. Cancel timeout (if set)\n17. Remove from sleep queue\n18. Check for signals again (if PCATCH)\n19. Clear LWP_SINTR flag\n20. Set lp-&gt;lwp_stat = LSRUN\n21. Exit critical section\n22. Return error code (0, EWOULDBLOCK, EINTR, ERESTART)\n</code></pre> <p>Key points: - The entire sleep setup runs in a critical section to prevent migration - <code>tsleep_interlock()</code> can be called beforehand to set up the sleep queue while still holding locks - Timeouts are handled by <code>endtsleep()</code> callback - Signal delivery can interrupt sleep (if PCATCH set)</p>"},{"location":"sys/kern/scheduling/#tsleep_interlock-pattern","title":"tsleep_interlock() Pattern","text":"<p>Function: <code>tsleep_interlock()</code> (kern_synch.c:451)</p> <p>This function enables a common synchronization pattern:</p> <pre><code>// Critical pattern:\nmutex_lock(&amp;lock);\n// Check condition\nif (!condition_met) {\n    tsleep_interlock(&amp;condition_ident, 0);  // Setup sleep\n    mutex_unlock(&amp;lock);                     // Release lock\n    tsleep(&amp;condition_ident, PINTERLOCKED, \"wait\", 0);\n    mutex_lock(&amp;lock);\n}\nmutex_unlock(&amp;lock);\n</code></pre> <p>The <code>PINTERLOCKED</code> flag tells <code>tsleep()</code> that the sleep queue interlocking has already been done, preventing races between the lock release and the sleep.</p>"},{"location":"sys/kern/scheduling/#wakeup-mechanism","title":"Wakeup Mechanism","text":"<p>Function: <code>_wakeup()</code> (kern_synch.c:999)</p> <p>Wakeup searches the sleep queue hash bucket for matching threads:</p> <pre><code>1. Enter critical section\n2. Compute hash bucket from ident\n3. Scan local CPU's sleep queue for matching threads\n   - Match on: td-&gt;td_wchan == ident &amp;&amp; td-&gt;td_wdomain == domain\n4. For each match:\n   - Remove from sleep queue (_tsleep_remove)\n   - Set td-&gt;td_wakefromcpu (for scheduler affinity)\n   - Schedule thread (lwkt_schedule) if TDF_TSLEEP_DESCHEDULED\n   - If PWAKEUP_ONE flag, stop after first wakeup\n5. Clean up queue tracking (ident0-3, cpumask bit)\n6. Send IPIs to other CPUs with matching threads\n   - Check slpque_cpumasks[cid] for remote CPUs\n   - Send _wakeup IPI with ident and domain\n7. Exit critical section\n</code></pre> <p>Wakeup variants: - <code>wakeup(ident)</code> - Wake all threads on all CPUs - <code>wakeup_one(ident)</code> - Wake one thread on any CPU - <code>wakeup_mycpu(ident)</code> - Wake threads on current CPU only - <code>wakeup_domain(ident, domain)</code> - Wake threads in specific domain</p>"},{"location":"sys/kern/scheduling/#delayed-wakeup-optimization","title":"Delayed Wakeup Optimization","text":"<p>Functions: <code>wakeup_start_delayed()</code>, <code>wakeup_end_delayed()</code> (kern_synch.c:1266, 1276)</p> <p>For code that performs many wakeups in quick succession, delayed wakeups batch them:</p> <pre><code>wakeup_start_delayed();\n// Multiple wakeups are queued in gd-&gt;gd_delayed_wakeup[0..1]\nwakeup(ident1);\nwakeup(ident2);\n// ...\nwakeup_end_delayed();  // Actually issue the wakeups\n</code></pre> <p>This reduces IPI traffic when many wakeups occur close together.</p>"},{"location":"sys/kern/scheduling/#sleep-queue-domains-pdomain_","title":"Sleep Queue Domains (PDOMAIN_*)","text":"<p>Sleep identifiers can be partitioned into domains to prevent false wakeups:</p> <ul> <li><code>PDOMAIN_UMTX</code> - User mutex domain</li> <li>Default domain (0) - General purpose</li> </ul> <p>Threads sleep and wake within their domain, preventing cross-contamination.</p>"},{"location":"sys/kern/scheduling/#posix-real-time-scheduling-kern_schedc","title":"POSIX Real-Time Scheduling (kern_sched.c)","text":""},{"location":"sys/kern/scheduling/#overview_1","title":"Overview","text":"<p>The <code>ksched</code> module provides POSIX.1b real-time scheduling extensions: - <code>SCHED_FIFO</code> - First-in-first-out realtime - <code>SCHED_RR</code> - Round-robin realtime - <code>SCHED_OTHER</code> - Standard timesharing</p> <p>Key functions: - <code>ksched_setscheduler()</code> - Set scheduling policy and priority - <code>ksched_getscheduler()</code> - Get current scheduling policy - <code>ksched_setparam()</code> / <code>ksched_getparam()</code> - Get/set sched parameters - <code>ksched_yield()</code> - Voluntarily yield CPU - <code>ksched_get_priority_max()</code> / <code>_min()</code> - Query priority ranges - <code>ksched_rr_get_interval()</code> - Get round-robin interval</p>"},{"location":"sys/kern/scheduling/#priority-mapping","title":"Priority Mapping","text":"<p>POSIX requires higher numbers = higher priority, but DragonFly's internal rtprio uses lower numbers = higher priority. The ksched module performs the inversion:</p> <pre><code>#define p4prio_to_rtpprio(P)  (RTP_PRIO_MAX - (P))\n#define rtpprio_to_p4prio(P)  (RTP_PRIO_MAX - (P))\n\n#define P1B_PRIO_MIN  rtpprio_to_p4prio(RTP_PRIO_MAX)\n#define P1B_PRIO_MAX  rtpprio_to_p4prio(RTP_PRIO_MIN)\n</code></pre>"},{"location":"sys/kern/scheduling/#real-time-priority-types","title":"Real-Time Priority Types","text":"<p>Stored in <code>lp-&gt;lwp_rtprio</code>:</p> <ul> <li><code>RTP_PRIO_FIFO</code> - SCHED_FIFO: runs until blocked or preempted by higher priority</li> <li><code>RTP_PRIO_REALTIME</code> - SCHED_RR: round-robin with other same-priority RT threads</li> <li><code>RTP_PRIO_NORMAL</code> - SCHED_OTHER: standard timesharing</li> <li><code>RTP_PRIO_IDLE</code> - Idle priority</li> </ul> <p>Real-time threads (<code>RTP_PRIO_FIFO</code> and <code>RTP_PRIO_REALTIME</code>) always run before normal priority threads.</p>"},{"location":"sys/kern/scheduling/#user-scheduler-management-kern_uschedc","title":"User Scheduler Management (kern_usched.c)","text":""},{"location":"sys/kern/scheduling/#scheduler-registration","title":"Scheduler Registration","text":"<p>Function: <code>usched_ctl()</code> (kern_usched.c:96)</p> <p>Schedulers register/unregister with the system:</p> <pre><code>struct usched {\n    TAILQ_ENTRY(usched) entry;\n    const char *name;\n    const char *desc;\n    void (*usched_register)(void);\n    void (*usched_unregister)(void);\n    void (*acquire_curproc)(struct lwp *);\n    void (*release_curproc)(struct lwp *);\n    void (*setrunqueue)(struct lwp *);\n    void (*schedulerclock)(struct lwp *, sysclock_t, sysclock_t);\n    void (*recalculate)(struct lwp *);\n    void (*resetpriority)(struct lwp *);\n    void (*forking)(struct lwp *parent, struct lwp *child);\n    void (*exiting)(struct lwp *, struct proc *);\n    void (*uload_update)(struct lwp *);\n    void (*setcpumask)(struct lwp *, cpumask_t);\n    void (*yield)(struct lwp *);\n    void (*changedcpu)(struct lwp *);\n};\n</code></pre> <p>Built-in schedulers: - <code>usched_bsd4</code> - Original DragonFly scheduler - <code>usched_dfly</code> - New message-based scheduler (default) - <code>usched_dummy</code> - Minimal reference implementation</p>"},{"location":"sys/kern/scheduling/#scheduler-selection","title":"Scheduler Selection","text":"<p>Function: <code>usched_init()</code> (kern_usched.c:59)</p> <p>At boot, the system selects the default scheduler based on <code>kern.user_scheduler</code> environment variable: - <code>\"dfly\"</code> \u2192 usched_dfly (default) - <code>\"bsd4\"</code> \u2192 usched_bsd4 - <code>\"dummy\"</code> \u2192 usched_dummy</p> <p>Each process inherits its scheduler from its parent on fork. The scheduler can be changed via <code>usched_set(2)</code> syscall.</p>"},{"location":"sys/kern/scheduling/#cpu-affinity-management","title":"CPU Affinity Management","text":"<p>Functions: <code>sys_lwp_setaffinity()</code>, <code>sys_lwp_getaffinity()</code> (kern_usched.c:411, 363)</p> <p>DragonFly allows per-LWP CPU affinity masks (<code>lp-&gt;lwp_cpumask</code>):</p> <pre><code>// Set affinity\ncpumask_t mask;\nCPUMASK_ASSBIT(mask, target_cpu);\nlwp_setaffinity(pid, tid, &amp;mask);\n\n// Get affinity  \nlwp_getaffinity(pid, tid, &amp;mask);\n</code></pre> <p>When an LWP's affinity is changed: 1. Update <code>lp-&gt;lwp_cpumask</code> 2. If current CPU not in new mask, call <code>lwkt_migratecpu()</code> to move thread 3. Call <code>p_usched-&gt;changedcpu(lp)</code> to notify scheduler</p>"},{"location":"sys/kern/scheduling/#usched_set-syscall","title":"usched_set() Syscall","text":"<p>Function: <code>sys_usched_set()</code> (kern_usched.c:184)</p> <p>Commands: - <code>USCHED_SET_SCHEDULER</code> - Change process's scheduler - <code>USCHED_SET_CPU</code> - Pin LWP to specific CPU - <code>USCHED_GET_CPU</code> - Get current CPU - <code>USCHED_ADD_CPU</code> - Add CPU to affinity mask - <code>USCHED_DEL_CPU</code> - Remove CPU from affinity mask - <code>USCHED_SET_CPUMASK</code> - Set full affinity mask - <code>USCHED_GET_CPUMASK</code> - Get affinity mask</p>"},{"location":"sys/kern/scheduling/#scheduler-clock","title":"Scheduler Clock","text":"<p>Function: <code>usched_schedulerclock()</code> (kern_usched.c:152)</p> <p>Called from the system's scheduler clock (hardclock) on each CPU at <code>ESTCPUFREQ</code> (typically 10 Hz). Each registered scheduler's <code>schedulerclock()</code> method is invoked to: - Update per-thread statistics (estcpu, pctcpu) - Detect if round-robin interval expired - Request reschedules as needed</p>"},{"location":"sys/kern/scheduling/#bsd4-scheduler-usched_bsd4c","title":"BSD4 Scheduler (usched_bsd4.c)","text":""},{"location":"sys/kern/scheduling/#overview_2","title":"Overview","text":"<p>The BSD4 scheduler is a traditional BSD-style scheduler with: - 32 run queues per priority class (realtime, normal, idle) - Priority calculated from nice value and CPU usage (estcpu) - Round-robin within each queue - Simple CPU load balancing</p> <p>Priority classes: - Realtime: 0-127 (maps to 32 queues via priorities/4) - Normal: 128-255 - Idle: 256-383 - Thread: 384-511 (kernel threads)</p>"},{"location":"sys/kern/scheduling/#data-structures","title":"Data Structures","text":"<p>Per-CPU state: <code>struct usched_bsd4_pcpu</code> (usched_bsd4.c:131)</p> <pre><code>struct usched_bsd4_pcpu {\n    struct thread  *helper_thread;\n    short          rrcount;        // Round-robin counter\n    short          upri;           // User priority of current process\n    struct lwp     *uschedcp;      // Current scheduled LWP\n    struct lwp     *old_uschedcp;  // Previous LWP\n    cpu_node_t     *cpunode;       // CPU topology node\n};\n</code></pre> <p>Global run queues: - <code>bsd4_queues[32]</code> - Normal priority queues - <code>bsd4_rtqueues[32]</code> - Realtime priority queues - <code>bsd4_idqueues[32]</code> - Idle priority queues - <code>bsd4_queuebits</code>, <code>bsd4_rtqueuebits</code>, <code>bsd4_idqueuebits</code> - Bitmasks indicating non-empty queues</p>"},{"location":"sys/kern/scheduling/#priority-calculation","title":"Priority Calculation","text":"<p>Function: <code>bsd4_resetpriority()</code></p> <p>The normal priority calculation considers: 1. Base priority from nice value (<code>lp-&gt;lwp_rtprio.prio</code>) 2. CPU usage (estcpu): <code>lp-&gt;lwp_estcpu</code> 3. Penalty for batch processes</p> <pre><code>// Simplified priority formula\npri = PRIBASE_NORMAL + (nice * NICEPPQ) + (estcpu / ESTCPUPPQ)\nlwp_priority = min(pri, MAXPRI-1)\n</code></pre> <p>estcpu (estimated CPU usage) is a decay-average: - Incremented on each scheduler clock tick when running - Decayed by factor over time - Used to penalize CPU-bound processes relative to I/O-bound</p>"},{"location":"sys/kern/scheduling/#run-queue-management","title":"Run Queue Management","text":"<p>Function: <code>bsd4_setrunqueue_locked()</code></p> <p>When placing an LWP on the run queue:</p> <pre><code>1. Determine queue index from priority\n   - Realtime/Idle: direct mapping\n   - Normal: (priority - PRIBASE_NORMAL) / PPQ\n2. Add to tail of appropriate queue (FIFO within priority)\n3. Set corresponding bit in queuebits\n4. Increment bsd4_runqcount\n5. Set LWP_MP_ONRUNQ flag\n</code></pre> <p>Function: <code>bsd4_chooseproc_locked()</code></p> <p>Selecting next LWP to run:</p> <pre><code>1. Check realtime queues first (highest priority)\n   - Find first set bit in bsd4_rtqueuebits\n   - Take head of that queue\n2. If no realtime, check normal queues\n   - Find first set bit in bsd4_queuebits  \n   - Take head of that queue\n3. If no normal, check idle queues\n   - Find first set bit in bsd4_idqueuebits\n   - Take head of that queue\n4. Return selected LWP (or NULL if all empty)\n</code></pre>"},{"location":"sys/kern/scheduling/#cpu-selection-heuristics","title":"CPU Selection Heuristics","text":"<p>Function: <code>bsd4_setrunqueue()</code></p> <p>When scheduling an LWP, BSD4 tries to place it intelligently:</p> <ol> <li>Check for free CPUs (not running user processes)</li> <li>Prefer CPUs in same CPU package (cache coherency)</li> <li> <p>Use topology information (<code>cpunode</code>)</p> </li> <li> <p>Check running CPUs</p> </li> <li>Find CPU running lower-priority LWP</li> <li> <p>Use upri (user priority) comparison</p> </li> <li> <p>Round-robin if all busy</p> </li> <li> <p>Use <code>bsd4_scancpu</code> to distribute load</p> </li> <li> <p>Send wakeup IPI to selected CPU if necessary</p> </li> </ol>"},{"location":"sys/kern/scheduling/#acquirerelease-curproc","title":"Acquire/Release Curproc","text":"<p>Function: <code>bsd4_acquire_curproc()</code> (usched_bsd4.c:330)</p> <p>When returning to userland:</p> <pre><code>1. Remove from tsleep queue if necessary\n2. Recalculate estcpu\n3. Handle user_resched request (release and reselect)\n4. Loop until we become dd-&gt;uschedcp:\n   - Try to steal current designation\n   - Or place on runqueue and switch away\n5. Mark CPU as running user process\n</code></pre> <p>Function: <code>bsd4_release_curproc()</code></p> <p>When entering kernel:</p> <pre><code>1. Clear dd-&gt;uschedcp\n2. Call bsd4_select_curproc() to pick new LWP\n3. Mark CPU as not running user process if no LWP selected\n</code></pre>"},{"location":"sys/kern/scheduling/#scheduler-clock_1","title":"Scheduler Clock","text":"<p>Function: <code>bsd4_schedulerclock()</code></p> <p>Called at ESTCPUFREQ for running LWP:</p> <pre><code>1. Increment estcpu (CPU usage accounting)\n2. Increment rrcount (round-robin counter)\n3. If rrcount &gt;= rrinterval:\n   - Reset rrcount\n   - Request user reschedule (need_user_resched)\n   - Triggers round-robin rotation\n</code></pre>"},{"location":"sys/kern/scheduling/#dfly-scheduler-usched_dflyc","title":"DFLY Scheduler (usched_dfly.c)","text":""},{"location":"sys/kern/scheduling/#overview_3","title":"Overview","text":"<p>The DFLY scheduler is DragonFly's modern, message-based scheduler featuring: - Per-CPU run queues (no global lock on fast path) - Sophisticated load balancing with topology awareness - IPC (Inter-Process Communication) affinity detection - NUMA awareness - Proactive load rebalancing</p> <p>Key advantages: - Better scalability on many-CPU systems - Reduced lock contention (per-CPU spinlocks) - Smarter CPU selection for IPC-heavy workloads - Topology-aware scheduling (cores, packages, NUMA nodes)</p>"},{"location":"sys/kern/scheduling/#data-structures_1","title":"Data Structures","text":"<p>Per-CPU state: <code>struct usched_dfly_pcpu</code> (sys/usched_dfly.h)</p> <pre><code>struct usched_dfly_pcpu {\n    struct spinlock spin;             // Per-CPU lock\n    struct thread   *helper_thread;   // Rebalancing helper\n    u_short         scancpu;          // Next CPU to scan\n    u_short         cpuid;\n    u_short         upri;             // Highest user priority\n    u_short         ucount;           // User thread count\n    u_short         uload;            // Load metric\n    int             rrcount;          // Round-robin counter\n    struct lwp      *uschedcp;        // Current user LWP\n    struct lwp      *old_uschedcp;    \n    cpu_node_t      *cpunode;         // Topology node\n\n    // Run queues (32 per priority class)\n    struct lwp_queue queues[NQS];\n    struct lwp_queue rtqueues[NQS];\n    struct lwp_queue idqueues[NQS];\n    u_int32_t       queuebits;\n    u_int32_t       rtqueuebits;\n    u_int32_t       idqueuebits;\n    u_int32_t       runqcount;\n\n    // IPC affinity tracking\n    cpumask_t       ipimask;          // CPUs to send IPI\n};\n</code></pre>"},{"location":"sys/kern/scheduling/#priority-calculation_1","title":"Priority Calculation","text":"<p>Similar to BSD4, but with additional fairness tuning:</p> <pre><code>pri = PRIBASE_NORMAL + \n      (nice * NICEPPQ) + \n      (estcpu / ESTCPUPPQ) +\n      batch_penalty\n</code></pre> <p>The DFLY scheduler uses more sophisticated estcpu decay and better handles bursty workloads.</p>"},{"location":"sys/kern/scheduling/#cpu-selection-algorithm","title":"CPU Selection Algorithm","text":"<p>Function: <code>dfly_choose_best_queue()</code></p> <p>The heart of DFLY scheduling. Uses a weighted scoring system:</p> <pre><code>For each potential target CPU:\n    score = 0\n\n    // Weight1: Prefer keeping thread on current CPU\n    if (cpu == lp-&gt;lwp_thread-&gt;td_gd-&gt;gd_cpuid)\n        score -= weight1\n\n    // Weight2: IPC affinity (wakefromcpu)\n    // Prefer scheduling near the CPU that last woke us\n    if (topology_allows_ipc_optimization(cpu, wakefromcpu))\n        score -= weight2\n\n    // Weight3: Queue length penalty\n    score += dd-&gt;runqcount * weight3\n\n    // Weight4: Availability (other CPU has lower priority thread)\n    if (dd-&gt;upri &gt; our_priority)\n        score -= weight4\n\n    // Weight5: NUMA node memory weighting\n    score += numa_memory_weight(cpu) * weight5\n\n    // Weight6/7: Transfer hysteresis for stability\n\nSelect CPU with lowest (best) score\n</code></pre> <p>Default weights (usched_dfly.c:280-286): - <code>weight1 = 30</code> - Affinity to current CPU - <code>weight2 = 180</code> - IPC locality (strongest) - <code>weight3 = 10</code> - Queue length - <code>weight4 = 120</code> - CPU availability - <code>weight5 = 50</code> - NUMA preference - <code>weight6 = 0</code> - Rebalance hysteresis - <code>weight7 = -100</code> - Idle pull hysteresis</p>"},{"location":"sys/kern/scheduling/#ipc-affinity-detection","title":"IPC Affinity Detection","text":"<p>Key insight: When thread A wakes thread B, they likely have a producer-consumer relationship. Scheduling B near A reduces cache misses and IPC latency.</p> <p>Tracked via: - <code>td-&gt;td_wakefromcpu</code> - CPU that last woke this thread (set in wakeup) - weight2 heuristic advantages scheduling on nearby CPUs</p> <p>The topology-aware logic considers: - Same logical CPU (hyperthreading sibling) - very strong affinity - Same physical package - strong affinity - Same NUMA node - moderate affinity - Different NUMA nodes - no affinity</p>"},{"location":"sys/kern/scheduling/#load-rebalancing","title":"Load Rebalancing","text":"<p>Function: <code>dfly_choose_worst_queue()</code></p> <p>The helper thread (<code>dfly_pcpu[cpu].helper_thread</code>) periodically rebalances:</p> <pre><code>1. Identify overloaded CPU (worst_queue)\n   - High runqcount relative to others\n\n2. Identify underloaded CPU (best_queue)\n   - Low/zero runqcount\n\n3. Transfer LWP from worst to best\n   - Call dfly_changeqcpu_locked()\n   - Move LWP between per-CPU queues\n\n4. Send IPI to target CPU to schedule the LWP\n</code></pre> <p>Rebalancing features (controlled by <code>usched_dfly_features</code>): - <code>0x01</code> - Idle CPU pulling (default on) - <code>0x02</code> - Proactive pushing (default on) - <code>0x04</code> - Rebalancing rover (default on) - <code>0x08</code> - More aggressive pushing (default on)</p>"},{"location":"sys/kern/scheduling/#acquirerelease-curproc_1","title":"Acquire/Release Curproc","text":"<p>Function: <code>dfly_acquire_curproc()</code> (usched_dfly.c:325)</p> <pre><code>1. Quick path: if already uschedcp and no resched needed, return\n2. Remove from tsleep queue if needed\n3. Recalculate estcpu\n4. Handle user_resched: release and reselect\n5. Loop until dd-&gt;uschedcp == lp:\n   - Check if outcast (CPU affinity violation)\n     - If so, migrate to best CPU via dfly_changeqcpu_locked()\n   - Try to become uschedcp\n   - Or place on runqueue and switch away\n</code></pre> <p>Function: <code>dfly_release_curproc()</code></p> <pre><code>1. Acquire per-CPU spinlock\n2. If we are dd-&gt;uschedcp:\n   - Call dfly_select_curproc() to pick new LWP\n   - Consider local runqueue first\n   - May pull from other CPUs if idle\n3. Release spinlock\n</code></pre>"},{"location":"sys/kern/scheduling/#per-cpu-run-queues","title":"Per-CPU Run Queues","text":"<p>Unlike BSD4's global queues, DFLY maintains separate 32-queue arrays on each CPU. This eliminates global lock contention but requires inter-CPU coordination for load balancing.</p> <p>Trade-off: - Pro: Much better scalability, less contention - Con: Requires active rebalancing to prevent imbalance</p> <p>The helper threads and IPC affinity heuristics work together to keep the system balanced without needing a global view.</p>"},{"location":"sys/kern/scheduling/#fork-behavior","title":"Fork Behavior","text":"<p>Function: <code>dfly_forking()</code></p> <p>When a process forks: - Feature <code>0x20</code> (default): Choose best CPU for child based on IPC affinity - Feature <code>0x40</code>: Keep child on current CPU - Feature <code>0x80</code>: Random CPU assignment</p> <p>The default (<code>0x20</code>) recognizes that fork is often followed by exec (in child) or wait (in parent), creating an IPC relationship. The scheduler tries to place the child near the parent for efficient cache sharing.</p>"},{"location":"sys/kern/scheduling/#dummy-scheduler-usched_dummyc","title":"Dummy Scheduler (usched_dummy.c)","text":""},{"location":"sys/kern/scheduling/#purpose","title":"Purpose","text":"<p>The dummy scheduler is a minimal reference implementation demonstrating the scheduler API. It's not suitable for production but useful for: - Understanding the scheduler interface - Testing scheduler infrastructure - Prototyping new scheduler ideas</p>"},{"location":"sys/kern/scheduling/#design","title":"Design","text":"<ul> <li>Single global run queue (<code>dummy_runq</code>)</li> <li>Global spinlock (<code>dummy_spin</code>)</li> <li>No sophisticated CPU selection</li> <li>No priority calculations</li> <li>Simple FIFO scheduling</li> </ul> <p>Key characteristics: - Acquires first available CPU - Helper thread per CPU to accept work - Round-robin at fixed interval - No load balancing heuristics</p> <p>This simplicity makes it easy to understand the flow of <code>acquire_curproc</code>, <code>release_curproc</code>, <code>setrunqueue</code>, etc., without the complexity of real scheduling policies.</p>"},{"location":"sys/kern/scheduling/#scheduler-clock-and-statistics","title":"Scheduler Clock and Statistics","text":""},{"location":"sys/kern/scheduling/#schedcpu-callout","title":"schedcpu() Callout","text":"<p>Function: <code>schedcpu()</code> (kern_synch.c:203)</p> <p>Called once per second on each CPU to update statistics:</p> <pre><code>1. Scan all processes (allproc_scan):\n   - Increment p_swtime (swap time)\n   - For each LWP:\n     - Increment lwp_slptime if sleeping\n     - Recalculate estcpu (if active or slptime &lt; 2)\n     - Decay pctcpu (percentage CPU)\n     - Call p_usched-&gt;recalculate(lp)\n\n2. Check CPU resource limits (schedcpu_resource):\n   - Sum td_sticks + td_uticks for all threads\n   - Call plimit_testcpulimit()\n   - Send SIGXCPU or kill if limit exceeded\n\n3. Wakeup &amp;lbolt and lbolt_syncer (on CPU 0)\n4. Reschedule callout for next second\n</code></pre>"},{"location":"sys/kern/scheduling/#load-average-calculation","title":"Load Average Calculation","text":"<p>Function: <code>loadav()</code> (kern_synch.c:1405)</p> <p>Called every 5 seconds (with randomization) to compute load averages:</p> <pre><code>1. Scan all LWPs (alllwp_scan)\n   - Count runnable LWPs (LSRUN state, not blocked)\n   - Store count in gd-&gt;gd_loadav_nrunnable\n\n2. On CPU 0:\n   - Sum counts from all CPUs\n   - Update averunnable.ldavg[0..2] (1, 5, 15 minute averages)\n   - Use exponential decay with FSCALE fixed-point math\n</code></pre> <p>The load average represents the average number of runnable threads over different time periods, a key system health metric.</p>"},{"location":"sys/kern/scheduling/#cpu-usage-tracking-estcpu-pctcpu","title":"CPU Usage Tracking (estcpu, pctcpu)","text":"<p>estcpu - Estimated CPU usage: - Incremented each scheduler clock tick when LWP is running - Decayed over time (typically 8/10 per second) - Used to calculate dynamic priority - Reset to parent's estcpu on fork (with optional bias)</p> <p>pctcpu - Percentage CPU: - Short-term CPU usage metric (over last second) - Used by ps(1) to display %CPU - Decayed more rapidly than estcpu - Updated by <code>updatepcpu()</code> when sampled</p>"},{"location":"sys/kern/scheduling/#priority-and-scheduling-classes","title":"Priority and Scheduling Classes","text":""},{"location":"sys/kern/scheduling/#priority-ranges","title":"Priority Ranges","text":"<p>DragonFly uses a unified priority space:</p> <pre><code>0-127:     Realtime (highest)\n128-255:   Normal (timesharing)\n256-383:   Idle\n384-511:   Kernel threads\n512+:      Special/NULL\n</code></pre> <p>Internal representation: - <code>lp-&gt;lwp_priority</code> - Calculated scheduling priority (0-511) - <code>lp-&gt;lwp_rtprio.type</code> - Scheduling class (REALTIME, NORMAL, IDLE, etc.) - <code>lp-&gt;lwp_rtprio.prio</code> - Priority within class - <code>td-&gt;td_upri</code> - LWKT priority (negated for proper ordering)</p>"},{"location":"sys/kern/scheduling/#priority-inversion","title":"Priority Inversion","text":"<p>LWKT priorities have inverted sense (lower number = higher priority) compared to user priorities (higher number = higher priority):</p> <pre><code>td-&gt;td_upri = -lp-&gt;lwp_priority;\n</code></pre> <p>This allows LWKT's queue ordering to work correctly.</p>"},{"location":"sys/kern/scheduling/#real-time-scheduling","title":"Real-time Scheduling","text":"<p>Real-time threads (FIFO and RR) have strict priority: - Always run before normal/idle threads - FIFO runs until it blocks or is preempted by higher-priority RT thread - RR is preempted after a quantum (round-robin interval) by same-priority RT threads</p> <p>Caution: Real-time threads can starve normal threads. Use carefully.</p>"},{"location":"sys/kern/scheduling/#nice-value","title":"Nice Value","text":"<p>The traditional Unix nice value (-20 to +19): - Stored in <code>lp-&gt;lwp_rtprio.prio</code> for NORMAL class - Lower nice = higher priority - Maps to priority via: <code>pri += nice * NICEPPQ</code> - Set via <code>setpriority(2)</code> syscall</p>"},{"location":"sys/kern/scheduling/#best-practices","title":"Best Practices","text":""},{"location":"sys/kern/scheduling/#choosing-a-scheduler","title":"Choosing a Scheduler","text":"<p>Use DFLY (default) when: - Many CPUs (&gt;= 8) - IPC-heavy workloads (e.g., build systems) - NUMA systems - Need good scalability</p> <p>Use BSD4 when: - Few CPUs (&lt;= 4) - Simple workloads - Debugging scheduler issues (simpler code) - Prefer traditional BSD behavior</p>"},{"location":"sys/kern/scheduling/#setting-priorities","title":"Setting Priorities","text":"<p>Real-time priorities: - Use sparingly - can starve normal processes - Suitable for hard real-time control tasks - Ensure RT tasks yield or block regularly - Test thoroughly under load</p> <p>Nice values: - Adjust nice for batch jobs (<code>nice +10</code>) - Use negative nice for interactive/important tasks (requires privilege) - Typical range: -5 to +10</p>"},{"location":"sys/kern/scheduling/#cpu-affinity","title":"CPU Affinity","text":"<p>When to use: - Threads with shared data (keep on nearby CPUs) - Real-time tasks (eliminate migration latency) - NUMA systems (pin to node with memory)</p> <p>When NOT to use: - General workloads (scheduler does better job) - Short-lived processes - When load distribution is important</p>"},{"location":"sys/kern/scheduling/#tunables-and-sysctls","title":"Tunables and Sysctls","text":""},{"location":"sys/kern/scheduling/#bsd4-scheduler","title":"BSD4 Scheduler","text":"<ul> <li><code>kern.usched_bsd4_rrinterval</code> - Round-robin interval (default: 10)</li> <li><code>kern.usched_bsd4_decay</code> - estcpu decay rate (default: 8)</li> <li><code>kern.usched_bsd4_batch_time</code> - Batch process threshold</li> <li><code>kern.usched_bsd4_upri_affinity</code> - Affinity threshold</li> <li><code>debug.bsd4_scdebug</code> - Debug PID</li> </ul>"},{"location":"sys/kern/scheduling/#dfly-scheduler","title":"DFLY Scheduler","text":"<ul> <li><code>kern.usched_dfly_weight1</code> - Current CPU affinity (default: 30)</li> <li><code>kern.usched_dfly_weight2</code> - IPC locality (default: 180)</li> <li><code>kern.usched_dfly_weight3</code> - Queue length penalty (default: 10)</li> <li><code>kern.usched_dfly_weight4</code> - CPU availability (default: 120)</li> <li><code>kern.usched_dfly_weight5</code> - NUMA memory (default: 50)</li> <li><code>kern.usched_dfly_features</code> - Feature flags (default: 0x2f)</li> <li><code>kern.usched_dfly_rrinterval</code> - Round-robin interval (default: 10)</li> <li><code>kern.usched_dfly_decay</code> - estcpu decay (default: 8)</li> <li><code>kern.usched_dfly_forkbias</code> - Fork estcpu bias (default: 1)</li> </ul>"},{"location":"sys/kern/scheduling/#general-scheduling","title":"General Scheduling","text":"<ul> <li><code>kern.pctcpu_decay</code> - pctcpu decay rate (default: 10)</li> <li><code>kern.fscale</code> - Fixed-point scale factor (FSCALE = 2048)</li> <li><code>kern.slpque_tablesize</code> - Sleep queue hash table size</li> </ul>"},{"location":"sys/kern/scheduling/#summary","title":"Summary","text":"<p>DragonFly's scheduling system is a sophisticated two-layer design:</p> <ol> <li>Sleep/wakeup provides efficient thread blocking and synchronization</li> <li>Pluggable user schedulers implement diverse scheduling policies</li> <li>BSD4 offers traditional simplicity for smaller systems</li> <li>DFLY provides advanced scalability and topology awareness for modern hardware</li> <li>Extensive tunables allow customization for specific workloads</li> </ol> <p>The system balances: - Responsiveness vs. overhead - Cache affinity vs. load balance - Simplicity vs. scalability</p> <p>Understanding the scheduler is crucial for: - Performance tuning - Real-time system design - Debugging scheduling issues - Kernel development</p> <p>Key takeaways: - Start with DFLY defaults on multi-CPU systems - Use BSD4 for simplicity on small systems - Reserve realtime priorities for critical tasks - Let the scheduler manage CPU affinity for most workloads - Monitor context switches and load average - Tune weights cautiously based on specific problems</p> <p>The DragonFly schedulers represent years of evolution and optimization, providing excellent out-of-the-box performance while remaining tunable for specialized needs.</p>"},{"location":"sys/kern/security/","title":"Security Subsystems","text":"<p>This document covers DragonFly BSD's security infrastructure: system capabilities for fine-grained privilege control, Access Control Lists (ACLs) for extended file permissions, and jails for lightweight OS-level virtualization.</p> <p>Source files: - <code>sys/kern/kern_caps.c</code> - System capability framework - <code>sys/kern/kern_acl.c</code> - Access Control List support - <code>sys/kern/kern_jail.c</code> - Jail (container) implementation</p>"},{"location":"sys/kern/security/#system-capabilities-syscap","title":"System Capabilities (syscap)","text":"<p>DragonFly implements a capability-based restriction system that allows processes to voluntarily drop privileges. Unlike traditional UNIX privilege models where root has full access, capabilities provide fine-grained control over specific operations.</p>"},{"location":"sys/kern/security/#design-philosophy","title":"Design Philosophy","text":"<p>Capabilities in DragonFly are implemented as restrictions (negatives) rather than allowances. A process starts with full capabilities and can only drop them, never gain them back. This provides a one-way security ratchet.</p> <p>Key properties: - Capabilities cannot be regained once dropped - Children inherit parent restrictions (for EXEC/ALL modes) - Restrictions are stored in credential structures - Operations use <code>caps_priv_check()</code> to verify permissions</p>"},{"location":"sys/kern/security/#capability-storage","title":"Capability Storage","text":"<p>Capabilities are stored as a bitmask in credentials (<code>sys/caps.h:61</code>):</p> <pre><code>typedef __uint64_t __syscapelm_t;\n\ntypedef struct syscaps {\n    __syscapelm_t caps[__SYSCAP_NUMELMS];  /* 8 elements, 256 caps */\n} __syscaps_t;\n</code></pre> <p>Each capability uses 2 bits, supporting four states:</p> Value Constant Meaning 0 <code>__SYSCAP_NONE</code> No restriction 1 <code>__SYSCAP_SELF</code> Restricted for this process 2 <code>__SYSCAP_EXEC</code> Restricted after exec 3 <code>__SYSCAP_ALL</code> Restricted for process and all children"},{"location":"sys/kern/security/#capability-groups","title":"Capability Groups","text":"<p>Capabilities are organized into 16 groups (<code>sys/caps.h:130</code>):</p> Group Meta Capability Description 0 <code>SYSCAP_ANY</code> Base group, controls other groups 0 <code>SYSCAP_RESTRICTEDROOT</code> Dangerous root operations 0 <code>SYSCAP_SENSITIVEROOT</code> Sensitive root operations 1 (RESTRICTEDROOT children) Driver, mlock, kld, reboot 2 (SENSITIVEROOT children) Process control, sysctl, scheduling 3 <code>SYSCAP_NOEXEC</code> SUID/SGID execution 4 <code>SYSCAP_NOCRED</code> Credential changes 5 <code>SYSCAP_NOJAIL</code> Jail operations 6 <code>SYSCAP_NONET</code> Basic network operations 7 <code>SYSCAP_NONET_SENSITIVE</code> Sensitive network operations 8 <code>SYSCAP_NOVFS</code> VFS operations 9 <code>SYSCAP_NOVFS_SENSITIVE</code> Sensitive VFS operations 10 <code>SYSCAP_NOMOUNT</code> Mount operations"},{"location":"sys/kern/security/#selected-capabilities","title":"Selected Capabilities","text":"<p>Group 1 - Restricted Root (dangerous operations): <pre><code>SYSCAP_NODRIVER       /* Device driver access */\nSYSCAP_NOVM_MLOCK     /* Memory locking */\nSYSCAP_NOKLD          /* Kernel module loading */\nSYSCAP_NOREBOOT       /* System reboot/shutdown */\nSYSCAP_NOACCT         /* Process accounting */\n</code></pre></p> <p>Group 4 - Credential Operations: <pre><code>SYSCAP_NOCRED_SETUID    /* setuid() */\nSYSCAP_NOCRED_SETGID    /* setgid() */\nSYSCAP_NOCRED_SETEUID   /* seteuid() */\nSYSCAP_NOCRED_SETGROUPS /* setgroups() */\n</code></pre></p> <p>Group 6/7 - Network Operations: <pre><code>SYSCAP_NONET_RESPORT    /* Bind to reserved ports */\nSYSCAP_NONET_RAW        /* Create raw sockets */\nSYSCAP_NONET_IFCONFIG   /* Interface configuration */\nSYSCAP_NONET_ROUTE      /* Routing table manipulation */\n</code></pre></p>"},{"location":"sys/kern/security/#system-calls","title":"System Calls","text":"<p>syscap_get() - Query capability status (<code>kern_caps.c:77</code>):</p> <pre><code>int sys_syscap_get(struct sysmsg *sysmsg, const struct syscap_get_args *uap)\n{\n    int cap = uap-&gt;cap &amp; ~__SYSCAP_XFLAGS;\n\n    /* Can query parent's capabilities */\n    if (uap-&gt;cap &amp; __SYSCAP_INPARENT) {\n        pp = pfind(curproc-&gt;p_ppid);\n        cred = pp-&gt;p_ucred;\n    } else {\n        cred = curthread-&gt;td_ucred;\n    }\n\n    /* Return capability bits */\n    res = (cred-&gt;cr_caps.caps[__SYSCAP_INDEX(cap)] &gt;&gt;\n           __SYSCAP_SHIFT(cap)) &amp; __SYSCAP_BITS_MASK;\n    sysmsg-&gt;sysmsg_result = res;\n}\n</code></pre> <p>syscap_set() - Set capability restrictions (<code>kern_caps.c:141</code>):</p> <pre><code>int sys_syscap_set(struct sysmsg *sysmsg, const struct syscap_set_args *uap)\n{\n    int cap = uap-&gt;cap &amp; ~__SYSCAP_XFLAGS;\n    int flags = uap-&gt;flags;\n\n    /* Can only add restrictions, never remove */\n    if (res != (res | flags)) {\n        cred = cratom_proc(pp);  /* Copy-on-write credential */\n\n        /* Set SYSCAP_ANY bits indicating deviation from root */\n        atomic_set_64(&amp;cred-&gt;cr_caps.caps[0], anymask);\n\n        /* Set the actual capability restriction */\n        atomic_set_64(&amp;cred-&gt;cr_caps.caps[__SYSCAP_INDEX(cap)],\n                      (flags &lt;&lt; __SYSCAP_SHIFT(cap)));\n    }\n}\n</code></pre>"},{"location":"sys/kern/security/#capability-inheritance","title":"Capability Inheritance","text":"<p>On exec(), EXEC bits shift into SELF bits (<code>kern_caps.c:225</code>):</p> <pre><code>void caps_exec(struct proc *p)\n{\n    for (i = 0; i &lt; __SYSCAP_NUMELMS; ++i) {\n        elm = cred-&gt;cr_caps.caps[i];\n        /* EXEC bits (even positions) shift into SELF bits (odd positions) */\n        elm = ((elm &amp; __SYSCAP_EXECMASK) &gt;&gt; 1) |\n              (elm &amp; __SYSCAP_EXECMASK);\n        cred-&gt;cr_caps.caps[i] = elm;\n    }\n}\n</code></pre> <p>This ensures that restrictions set for \"after exec\" take effect.</p>"},{"location":"sys/kern/security/#privilege-checking","title":"Privilege Checking","text":"<p>The primary check function (<code>kern_caps.c:310</code>):</p> <pre><code>int caps_priv_check(struct ucred *cred, int cap)\n{\n    /* NULL credential handling */\n    if (cred == NULL) {\n        if (cap &amp; __SYSCAP_NULLCRED)\n            return 0;\n        return EPERM;\n    }\n\n    /* UID must be 0 unless NOROOTTEST is set */\n    if (cred-&gt;cr_uid != 0 &amp;&amp; (cap &amp; __SYSCAP_NOROOTTEST) == 0)\n        return EPERM;\n\n    /* Check capability restriction */\n    res = caps_check_cred(cred, cap);\n\n    /* Also check group capability if specified */\n    if (cap &amp; __SYSCAP_GROUP_MASK) {\n        cap = (cap &amp; __SYSCAP_GROUP_MASK) &gt;&gt; __SYSCAP_GROUP_SHIFT;\n        res |= caps_check_cred(cred, cap);\n    }\n\n    if (res &amp; __SYSCAP_SELF)\n        return EPERM;\n\n    /* Finally check jail restrictions */\n    return prison_priv_check(cred, cap);\n}\n</code></pre>"},{"location":"sys/kern/security/#access-control-lists-acls","title":"Access Control Lists (ACLs)","text":"<p>ACLs extend traditional UNIX permissions to support fine-grained access control. DragonFly implements POSIX.1e ACLs, allowing permissions for specific users and groups beyond the owner/group/other model.</p>"},{"location":"sys/kern/security/#acl-structure","title":"ACL Structure","text":"<p>ACL entries define access for specific subjects (<code>sys/acl.h:54</code>):</p> <pre><code>struct acl_entry {\n    acl_tag_t   ae_tag;    /* Entry type */\n    uid_t       ae_id;     /* User/group ID */\n    acl_perm_t  ae_perm;   /* Permissions */\n};\n\nstruct acl {\n    int             acl_cnt;                    /* Number of entries */\n    struct acl_entry acl_entry[ACL_MAX_ENTRIES]; /* Up to 32 entries */\n};\n</code></pre>"},{"location":"sys/kern/security/#entry-types","title":"Entry Types","text":"Tag Description <code>ACL_USER_OBJ</code> File owner permissions <code>ACL_USER</code> Specific user permissions <code>ACL_GROUP_OBJ</code> File group permissions <code>ACL_GROUP</code> Specific group permissions <code>ACL_MASK</code> Maximum permissions for users/groups <code>ACL_OTHER</code> Everyone else"},{"location":"sys/kern/security/#acl-types","title":"ACL Types","text":"Type Description <code>ACL_TYPE_ACCESS</code> Access permissions <code>ACL_TYPE_DEFAULT</code> Default ACL for directories <code>ACL_TYPE_AFS</code> AFS ACL <code>ACL_TYPE_CODA</code> Coda ACL <code>ACL_TYPE_NTFS</code> NTFS ACL"},{"location":"sys/kern/security/#permission-bits","title":"Permission Bits","text":"<pre><code>#define ACL_PERM_READ   0x0004\n#define ACL_PERM_WRITE  0x0002\n#define ACL_PERM_EXEC   0x0001\n#define ACL_PERM_NONE   0x0000\n</code></pre>"},{"location":"sys/kern/security/#system-call-interface","title":"System Call Interface","text":"<p>The kernel provides low-level ACL operations (<code>kern_acl.c</code>):</p> <p>Get ACL (<code>kern_acl.c:148</code>): <pre><code>int sys___acl_get_file(struct sysmsg *sysmsg,\n                       const struct __acl_get_file_args *uap)\n{\n    error = nlookup_init(&amp;nd, uap-&gt;path, UIO_USERSPACE, NLC_FOLLOW);\n    error = nlookup(&amp;nd);\n    error = cache_vref(&amp;nd.nl_nch, nd.nl_cred, &amp;vp);\n\n    error = vacl_get_acl(vp, uap-&gt;type, uap-&gt;aclp);\n}\n</code></pre></p> <p>Set ACL (<code>kern_acl.c:66</code>): <pre><code>static int vacl_set_acl(struct vnode *vp, acl_type_t type, struct acl *aclp)\n{\n    error = copyin(aclp, &amp;inkernacl, sizeof(struct acl));\n\n    vn_lock(vp, LK_EXCLUSIVE | LK_RETRY);\n    error = VOP_SETACL(vp, type, &amp;inkernacl, ucred);\n    vn_unlock(vp);\n}\n</code></pre></p>"},{"location":"sys/kern/security/#acl-system-calls","title":"ACL System Calls","text":"Syscall Description <code>__acl_get_file</code> Get ACL by path <code>__acl_set_file</code> Set ACL by path <code>__acl_get_fd</code> Get ACL by file descriptor <code>__acl_set_fd</code> Set ACL by file descriptor <code>__acl_delete_file</code> Delete ACL by path <code>__acl_delete_fd</code> Delete ACL by file descriptor <code>__acl_aclcheck_file</code> Validate ACL by path <code>__acl_aclcheck_fd</code> Validate ACL by file descriptor"},{"location":"sys/kern/security/#vfs-integration","title":"VFS Integration","text":"<p>ACL operations are delegated to the filesystem via VOP:</p> <ul> <li><code>VOP_GETACL(vp, type, aclp, cred)</code> - Retrieve ACL</li> <li><code>VOP_SETACL(vp, type, aclp, cred)</code> - Set ACL  </li> <li><code>VOP_ACLCHECK(vp, type, aclp, cred)</code> - Validate ACL</li> </ul> <p>Not all filesystems support ACLs. UFS and HAMMER2 provide ACL support.</p>"},{"location":"sys/kern/security/#jails","title":"Jails","text":"<p>Jails provide lightweight OS-level virtualization, isolating processes in a restricted environment with its own hostname, IP addresses, and filesystem root. Originally from FreeBSD, DragonFly's implementation adds capability-based restrictions.</p>"},{"location":"sys/kern/security/#prison-structure","title":"Prison Structure","text":"<p>Each jail is represented by a prison structure (<code>sys/jail.h:113</code>):</p> <pre><code>struct prison {\n    LIST_ENTRY(prison) pr_list;       /* Global prison list */\n    int         pr_id;                /* Unique jail ID */\n    int         pr_ref;               /* Reference count */\n    struct nchandle pr_root;          /* Jail root directory */\n    char        pr_host[MAXHOSTNAMELEN]; /* Hostname */\n    SLIST_HEAD(, jail_ip_storage) pr_ips; /* IP addresses */\n\n    /* IP address caches for fast lookup */\n    struct sockaddr_in  *local_ip4;   /* Loopback IPv4 */\n    struct sockaddr_in  *nonlocal_ip4; /* Public IPv4 */\n    struct sockaddr_in6 *local_ip6;   /* Loopback IPv6 */\n    struct sockaddr_in6 *nonlocal_ip6; /* Public IPv6 */\n\n    void        *pr_linux;            /* Linux emulation data */\n    int         pr_securelevel;       /* Security level */\n    struct varsymset pr_varsymset;    /* Variable symbols */\n\n    struct sysctl_ctx_list *pr_sysctl_ctx;\n    struct sysctl_oid *pr_sysctl_tree;\n\n    prison_cap_t pr_caps;             /* Jail capabilities */\n};\n</code></pre>"},{"location":"sys/kern/security/#jail-capabilities","title":"Jail Capabilities","text":"<p>Jails have their own capability system (<code>sys/jail.h:68</code>):</p> Capability Description <code>PRISON_CAP_SYS_SET_HOSTNAME</code> Can set hostname <code>PRISON_CAP_SYS_SYSVIPC</code> Can use System V IPC <code>PRISON_CAP_NET_UNIXIPROUTE</code> Restricted to UNIX/IP/route sockets <code>PRISON_CAP_NET_RAW_SOCKETS</code> Can create raw sockets <code>PRISON_CAP_NET_LISTEN_OVERRIDE</code> Can override host wildcard listen <code>PRISON_CAP_VFS_CHFLAGS</code> Can modify file flags <code>PRISON_CAP_VFS_MOUNT_NULLFS</code> Can mount nullfs <code>PRISON_CAP_VFS_MOUNT_DEVFS</code> Can mount devfs <code>PRISON_CAP_VFS_MOUNT_TMPFS</code> Can mount tmpfs <code>PRISON_CAP_VFS_MOUNT_PROCFS</code> Can mount procfs <code>PRISON_CAP_VFS_MOUNT_FUSEFS</code> Can mount FUSE"},{"location":"sys/kern/security/#creating-a-jail","title":"Creating a Jail","text":"<p>The <code>jail()</code> system call creates and enters a jail (<code>kern_jail.c:254</code>):</p> <pre><code>int sys_jail(struct sysmsg *sysmsg, const struct jail_args *uap)\n{\n    /* Requires SYSCAP_NOJAIL_CREATE capability */\n    error = caps_priv_check_self(SYSCAP_NOJAIL_CREATE);\n\n    pr = kmalloc(sizeof(*pr), M_PRISON, M_WAITOK | M_ZERO);\n    SLIST_INIT(&amp;pr-&gt;pr_ips);\n\n    /* Handle jail version */\n    switch (jversion) {\n    case 0:  /* Single IPv4 jail */\n        ...\n        break;\n    case 1:  /* Multi-IP jail (DragonFly extension) */\n        for (i = 0; i &lt; j.n_ips; i++) {\n            /* Copy IP addresses */\n        }\n        break;\n    }\n\n    /* Copy hostname and set default capabilities */\n    copyinstr(j.hostname, &amp;pr-&gt;pr_host, ...);\n    pr-&gt;pr_caps = prison_default_caps;\n\n    /* Setup jail root and attach */\n    error = kern_jail(pr, &amp;j);\n}\n</code></pre>"},{"location":"sys/kern/security/#jail-attachment","title":"Jail Attachment","text":"<p>Attaching to a jail chroots and sets restrictions (<code>kern_jail.c:136</code>):</p> <pre><code>static int kern_jail_attach(int jid)\n{\n    pr = prison_find(jid);\n\n    /* Chroot to jail's root directory */\n    error = kern_chroot(&amp;pr-&gt;pr_root);\n\n    /* Set jail reference and mark process as jailed */\n    prison_hold(pr);\n    cr = cratom_proc(p);\n    cr-&gt;cr_prison = pr;\n    p-&gt;p_flags |= P_JAILED;\n\n    /* Apply restricted root capabilities */\n    caps_set_locked(p, SYSCAP_RESTRICTEDROOT, __SYSCAP_ALL);\n}\n</code></pre>"},{"location":"sys/kern/security/#ip-address-handling","title":"IP Address Handling","text":"<p>Jails can have multiple IP addresses. The framework provides address translation for loopback addresses (<code>kern_jail.c:492</code>):</p> <pre><code>int prison_local_ip(struct thread *td, struct sockaddr *ip)\n{\n    pr = td-&gt;td_ucred-&gt;cr_prison;\n\n    /* Convert jail's loopback IP back to 127.0.0.1 */\n    if (ip-&gt;sa_family == AF_INET &amp;&amp; pr-&gt;local_ip4 &amp;&amp;\n        pr-&gt;local_ip4-&gt;sin_addr.s_addr == ip4-&gt;sin_addr.s_addr) {\n        ip4-&gt;sin_addr.s_addr = htonl(INADDR_LOOPBACK);\n    }\n}\n</code></pre> <p>This allows programs in jails to use 127.0.0.1 while actually using a different loopback address (e.g., 127.0.0.2).</p>"},{"location":"sys/kern/security/#privilege-checking-in-jails","title":"Privilege Checking in Jails","text":"<p>Jail privilege checks filter syscap checks (<code>kern_jail.c:846</code>):</p> <pre><code>int prison_priv_check(struct ucred *cred, int cap)\n{\n    if (!jailed(cred))\n        return 0;  /* Not in jail, allow */\n\n    switch (cap &amp; ~__SYSCAP_XFLAGS) {\n    case SYSCAP_RESTRICTEDROOT:\n        return EPERM;  /* Always denied in jails */\n\n    case SYSCAP_NOJAIL_CREATE:\n    case SYSCAP_NOJAIL_ATTACH:\n        return EPERM;  /* No nested jails */\n\n    case SYSCAP_NONET_RAW:\n        /* Conditional based on jail capability */\n        if (PRISON_CAP_ISSET(pr-&gt;pr_caps, PRISON_CAP_NET_RAW_SOCKETS))\n            return 0;\n        return EPERM;\n\n    case SYSCAP_NOMOUNT_NULLFS:\n        if (PRISON_CAP_ISSET(pr-&gt;pr_caps, PRISON_CAP_VFS_MOUNT_NULLFS))\n            return 0;\n        return EPERM;\n\n    /* Most operations allowed in jails */\n    default:\n        return EPERM;\n    }\n}\n</code></pre>"},{"location":"sys/kern/security/#sysctl-interface","title":"Sysctl Interface","text":"<p>Jails are configurable via sysctl:</p> <p>Global defaults (<code>jail.defaults.*</code>): <pre><code>jail.defaults.set_hostname_allowed\njail.defaults.socket_unixiproute_only\njail.defaults.sysvipc_allowed\njail.defaults.chflags_allowed\njail.defaults.allow_raw_sockets\njail.defaults.vfs_mount_nullfs\njail.defaults.vfs_mount_tmpfs\n...\n</code></pre></p> <p>Per-jail settings (<code>jail.&lt;id&gt;.*</code>): <pre><code>jail.1.sys_set_hostname\njail.1.sys_sysvipc\njail.1.net_raw_sockets\njail.1.vfs_mount_nullfs\n...\n</code></pre></p> <p>Monitoring: <pre><code>jail.list      - List active jails (id hostname path IPs)\njail.jailed    - Is current process jailed?\n</code></pre></p>"},{"location":"sys/kern/security/#jail-lifecycle","title":"Jail Lifecycle","text":"<pre><code>flowchart TD\n    subgraph jail_syscall[\"jail() syscall\"]\n        J1[\"1. Allocate prison structure\"]\n        J2[\"2. Parse jail version (v0 single IP, v1 multi-IP)\"]\n        J3[\"3. Copy hostname and IP addresses\"]\n        J4[\"4. Setup root namecache reference\"]\n        J5[\"5. Assign unique jail ID\"]\n        J6[\"6. Create per-jail sysctl tree\"]\n        J7[\"7. Call kern_jail_attach()\"]\n        J1 --&gt; J2 --&gt; J3 --&gt; J4 --&gt; J5 --&gt; J6 --&gt; J7\n    end\n\n    subgraph kern_attach[\"kern_jail_attach()\"]\n        A1[\"1. Find prison by ID\"]\n        A2[\"2. chroot to prison root\"]\n        A3[\"3. Hold prison reference\"]\n        A4[\"4. Set cr_prison in credentials\"]\n        A5[\"5. Set P_JAILED flag\"]\n        A6[\"6. Apply SYSCAP_RESTRICTEDROOT restrictions\"]\n        A1 --&gt; A2 --&gt; A3 --&gt; A4 --&gt; A5 --&gt; A6\n    end\n\n    subgraph jailed_run[\"Jailed Process Runs\"]\n        R1[\"Filesystem confined to jail root\"]\n        R2[\"IP addresses restricted to jail IPs\"]\n        R3[\"Syscap checks filtered through prison_priv_check()\"]\n        R4[\"Per-jail capabilities control allowed operations\"]\n    end\n\n    subgraph prison_free[\"prison_free()\"]\n        F1[\"1. Decrement reference count\"]\n        F2[\"2. On last reference:\"]\n        F3[\"- Remove from global list\"]\n        F4[\"- Free IP address list\"]\n        F5[\"- Free Linux emulation data\"]\n        F6[\"- Clean varsymset\"]\n        F7[\"- Free sysctl tree\"]\n        F8[\"- Drop root namecache reference\"]\n        F9[\"- Free prison structure\"]\n        F1 --&gt; F2 --&gt; F3 --&gt; F4 --&gt; F5 --&gt; F6 --&gt; F7 --&gt; F8 --&gt; F9\n    end\n\n    jail_syscall --&gt; kern_attach --&gt; jailed_run --&gt; prison_free\n</code></pre>"},{"location":"sys/kern/security/#security-considerations","title":"Security Considerations","text":"<ol> <li>No Nested Jails - Processes in jails cannot create new jails</li> <li>Automatic Restrictions - Jailed processes get <code>SYSCAP_RESTRICTEDROOT</code></li> <li>IP Isolation - Jails can only bind/connect to assigned IPs</li> <li>Filesystem Isolation - chroot provides filesystem boundary</li> <li>Capability-Based Control - Fine-grained permission via <code>pr_caps</code></li> </ol>"},{"location":"sys/kern/security/#integration-points","title":"Integration Points","text":""},{"location":"sys/kern/security/#credential-structure","title":"Credential Structure","text":"<p>Security information is stored in credentials:</p> <pre><code>struct ucred {\n    uid_t       cr_uid;       /* Effective user ID */\n    gid_t       cr_gid;       /* Effective group ID */\n    uid_t       cr_ruid;      /* Real user ID */\n    gid_t       cr_rgid;      /* Real group ID */\n    struct prison *cr_prison;  /* Jail reference (or NULL) */\n    __syscaps_t cr_caps;       /* Capability restrictions */\n    ...\n};\n</code></pre>"},{"location":"sys/kern/security/#standard-privilege-check-pattern","title":"Standard Privilege Check Pattern","text":"<pre><code>int some_privileged_operation(struct thread *td)\n{\n    int error;\n\n    /* Check specific capability */\n    error = caps_priv_check_self(SYSCAP_SOME_OPERATION);\n    if (error)\n        return error;\n\n    /* Proceed with operation */\n    ...\n}\n</code></pre> <p>The check automatically considers: 1. UID (must be 0 unless NOROOTTEST) 2. Capability restrictions in credentials 3. Group capability restrictions 4. Jail-specific restrictions</p>"},{"location":"sys/kern/security/#see-also","title":"See Also","text":"<ul> <li>Processes - Process and credential management</li> <li>System Calls - System call implementation</li> <li>VFS Operations - ACL integration with filesystems</li> <li>Sysctl Framework - Jail sysctl interface</li> </ul>"},{"location":"sys/kern/shutdown/","title":"System Shutdown and Panic Handling","text":"<p>The DragonFly BSD kernel provides a comprehensive shutdown subsystem that handles orderly system shutdown, reboot operations, kernel panic handling, and crash dump generation. This functionality is implemented in <code>sys/kern/kern_shutdown.c</code>.</p>"},{"location":"sys/kern/shutdown/#overview","title":"Overview","text":"<p>The shutdown subsystem provides:</p> <ul> <li>Clean shutdown - Orderly termination with filesystem sync and unmount</li> <li>Reboot handling - System restart with various options</li> <li>Panic processing - Kernel crash handling and diagnostics</li> <li>Crash dumps - Memory dump to disk for post-mortem analysis</li> <li>Event handlers - Extensible shutdown hook mechanism</li> </ul> <pre><code>flowchart TD\n    subgraph entry[\"Entry Points\"]\n        SN[\"shutdown_nice()\"]\n        DIRECT[\"Direct boot() call\"]\n    end\n\n    SN --&gt;|\"init exists\"| SIGINT[\"Signal init(8)with SIGINT\"]\n    SN --&gt;|\"no init\"| BOOT\n\n    SIGINT --&gt; CLEAN[\"init performsclean shutdown\"]\n    CLEAN --&gt; REBOOT[\"sys_reboot()\"]\n    REBOOT --&gt; BOOT[\"boot()\"]\n    DIRECT --&gt; BOOT\n\n    BOOT --&gt; PRESYNC[\"pre_sync events\"]\n    BOOT --&gt; SYNC[\"sync disks&amp; unmount\"]\n    BOOT --&gt; POSTSYNC[\"post_sync events\"]\n\n    PRESYNC --&gt; FINAL[\"shutdown_final\"]\n    SYNC --&gt; FINAL\n    POSTSYNC --&gt; FINAL\n\n    FINAL --&gt; POWEROFF[\"poweroff\"]\n    FINAL --&gt; HALT[\"halt\"]\n    FINAL --&gt; RESET[\"reset\"]\n</code></pre>"},{"location":"sys/kern/shutdown/#shutdown-events","title":"Shutdown Events","text":"<p>The shutdown subsystem uses event handlers to coordinate shutdown activities. Components register handlers that are invoked at specific phases.</p>"},{"location":"sys/kern/shutdown/#event-types","title":"Event Types","text":"Event Priority Constants Purpose <code>shutdown_pre_sync</code> <code>SHUTDOWN_PRI_*</code> Before filesystem sync <code>shutdown_post_sync</code> <code>SHUTDOWN_PRI_*</code> After sync, before halt <code>shutdown_final</code> <code>SHUTDOWN_PRI_*</code> Final shutdown operations"},{"location":"sys/kern/shutdown/#priority-levels","title":"Priority Levels","text":"<pre><code>#define SHUTDOWN_PRI_FIRST      0\n#define SHUTDOWN_PRI_DEFAULT    10000\n#define SHUTDOWN_PRI_LAST       20000\n</code></pre>"},{"location":"sys/kern/shutdown/#built-in-final-handlers","title":"Built-in Final Handlers","text":"<p>Registered at initialization (<code>kern_shutdown.c:163-170</code>):</p> Handler Priority Purpose <code>poweroff_wait()</code> FIRST Delay before poweroff <code>shutdown_halt()</code> LAST + 100 Handle halt request <code>shutdown_panic()</code> LAST + 100 Handle panic reboot delay <code>shutdown_reset()</code> LAST + 200 Perform actual reset"},{"location":"sys/kern/shutdown/#registering-shutdown-handlers","title":"Registering Shutdown Handlers","text":"<pre><code>#include &lt;sys/eventhandler.h&gt;\n\n/* Register pre-sync handler */\nEVENTHANDLER_REGISTER(shutdown_pre_sync, my_pre_sync,\n                      NULL, SHUTDOWN_PRI_DEFAULT);\n\n/* Register post-sync handler */\nEVENTHANDLER_REGISTER(shutdown_post_sync, my_cleanup,\n                      NULL, SHUTDOWN_PRI_DEFAULT);\n\n/* Register final handler */\nEVENTHANDLER_REGISTER(shutdown_final, my_final,\n                      NULL, SHUTDOWN_PRI_DEFAULT);\n</code></pre>"},{"location":"sys/kern/shutdown/#the-boot-function","title":"The boot() Function","text":"<p>The central shutdown function that orchestrates the entire process (<code>kern_shutdown.c:250-409</code>).</p>"},{"location":"sys/kern/shutdown/#boot-flags-rb_","title":"Boot Flags (RB_*)","text":"Flag Value Description <code>RB_AUTOBOOT</code> 0 Normal reboot <code>RB_HALT</code> 0x0008 Halt instead of reboot <code>RB_NOSYNC</code> 0x0004 Don't sync filesystems <code>RB_DUMP</code> 0x0100 Dump memory before reboot <code>RB_POWEROFF</code> 0x4000 Power off instead of reboot <code>RB_SINGLE</code> 0x0002 Reboot to single user"},{"location":"sys/kern/shutdown/#shutdown-sequence","title":"Shutdown Sequence","text":"<ol> <li>CPU Migration - Move to BSP (CPU 0) for ACPI compatibility</li> <li>Pre-sync Events - Invoke <code>shutdown_pre_sync</code> handlers</li> <li>Process Cleanup - Remove filesystem references from processes</li> <li>Filesystem Sync - Sync all dirty buffers to disk</li> <li>Filesystem Unmount - Unmount all filesystems</li> <li>Print Uptime - Display system uptime</li> <li>Crash Dump - Generate dump if requested</li> <li>Post-sync Events - Invoke <code>shutdown_post_sync</code> handlers</li> <li>Final Events - Invoke <code>shutdown_final</code> handlers (halt/reset)</li> </ol>"},{"location":"sys/kern/shutdown/#process-cleanup","title":"Process Cleanup","text":"<p>During shutdown, <code>shutdown_cleanup_proc()</code> removes filesystem references (<code>kern_shutdown.c:566-610</code>):</p> <pre><code>static void\nshutdown_cleanup_proc(struct proc *p)\n{\n    /* Close all open files */\n    kern_closefrom(0);\n\n    /* Release current directory */\n    if (fdp-&gt;fd_cdir) {\n        cache_drop(&amp;fdp-&gt;fd_ncdir);\n        vrele(fdp-&gt;fd_cdir);\n        fdp-&gt;fd_cdir = NULL;\n    }\n\n    /* Release root directory */\n    if (fdp-&gt;fd_rdir) { ... }\n\n    /* Release jail directory */\n    if (fdp-&gt;fd_jdir) { ... }\n\n    /* Exit vkernel if present */\n    if (p-&gt;p_vkernel)\n        vkernel_exit(p);\n\n    /* Release text vnode */\n    if (p-&gt;p_textvp) { ... }\n\n    /* Remove user address space */\n    vm_map_remove(&amp;vm-&gt;vm_map,\n                  VM_MIN_USER_ADDRESS,\n                  VM_MAX_USER_ADDRESS);\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#filesystem-sync","title":"Filesystem Sync","text":"<p>The boot sequence performs thorough filesystem synchronization (<code>kern_shutdown.c:306-385</code>).</p>"},{"location":"sys/kern/shutdown/#sync-algorithm","title":"Sync Algorithm","text":"<pre><code>flowchart TD\n    START[\"sys_sync()\"] --&gt; COUNT[\"Count busy bufs\"]\n    COUNT --&gt; CHECK{nbusy == 0?}\n\n    CHECK --&gt;|Yes| ZINC[\"zcount++\"]\n    CHECK --&gt;|No| ITER{iter &gt; 5?}\n\n    ZINC --&gt; ZCHECK{zcount &gt;= 3?}\n    ZCHECK --&gt;|Yes| DONE[\"Done\"]\n    ZCHECK --&gt;|No| RESET[\"Reset zcount\"]\n    RESET --&gt; COUNT\n\n    ITER --&gt;|Yes| BIOSYNC[\"bio_ops_sync()\"]\n    ITER --&gt;|No| COUNT\n    BIOSYNC --&gt; COUNT\n</code></pre>"},{"location":"sys/kern/shutdown/#two-pass-buffer-counting","title":"Two-Pass Buffer Counting","text":"<p>Pass 1 (<code>shutdown_busycount1</code>) - Count all busy/dirty buffers except TMPFS:</p> <pre><code>static int\nshutdown_busycount1(struct buf *bp, void *info)\n{\n    /* Skip TMPFS buffers */\n    if (vp-&gt;v_tag == VT_TMPFS)\n        return 0;\n\n    /* Count locked buffers */\n    if ((bp-&gt;b_flags &amp; B_INVAL) == 0 &amp;&amp; BUF_LOCKINUSE(bp))\n        return 1;\n\n    /* Count dirty buffers */\n    if ((bp-&gt;b_flags &amp; (B_DELWRI | B_INVAL)) == B_DELWRI)\n        return 1;\n\n    return 0;\n}\n</code></pre> <p>Pass 2 (<code>shutdown_busycount2</code>) - Stricter filtering after initial sync:</p> <ul> <li>Ignores TMPFS, NFS, MFS, SMBFS</li> <li>Only counts buffers with active write I/O</li> <li>Reports stuck buffers with mount point information</li> </ul>"},{"location":"sys/kern/shutdown/#unmount-behavior","title":"Unmount Behavior","text":"<p>After successful sync (3 consecutive zero counts), filesystems are unmounted:</p> <pre><code>if (zcount &gt;= 3) {\n    kprintf(\"done\\n\");\n    if (panicstr == NULL)\n        vfs_unmountall(1);  /* Force unmount */\n} else {\n    kprintf(\"giving up on %d buffers\\n\", nbusy);\n    /* Skip unmount to force fsck on reboot */\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#kernel-panic-handling","title":"Kernel Panic Handling","text":"<p>The <code>panic()</code> function handles unrecoverable kernel errors (<code>kern_shutdown.c:751-889</code>).</p>"},{"location":"sys/kern/shutdown/#panic-flow","title":"Panic Flow","text":"<pre><code>flowchart TD\n    PANIC[\"panic(\"message\")\"] --&gt; ACQUIRE[\"Acquire panicCPU ownership\"]\n    ACQUIRE --&gt;|\"Secondary CPUs\"| SPIN[\"deschedule and spin\"]\n    ACQUIRE --&gt; RELEASE[\"Release tokens&amp; spinlocks\"]\n\n    RELEASE --&gt; FORMAT[\"Format &amp; printpanic message\"]\n    FORMAT --&gt; SAVE[\"Save contextfor debugger\"]\n    SAVE --&gt; DEBUGCHECK{\"debugger_on_panic?\"}\n\n    DEBUGCHECK --&gt;|Yes| DDB[\"Enter DDBdebugger\"]\n    DEBUGCHECK --&gt;|No| STOP[\"Stop other CPUsboot(RB_DUMP)\"]\n</code></pre>"},{"location":"sys/kern/shutdown/#multi-cpu-panic-coordination","title":"Multi-CPU Panic Coordination","text":"<p>Only one CPU can handle a panic; others are blocked (<code>kern_shutdown.c:777-811</code>):</p> <pre><code>for (;;) {\n    globaldata_t xgd = panic_cpu_gd;\n\n    /* Someone else has panic CPU - block this CPU */\n    if (xgd &amp;&amp; xgd != gd) {\n        crit_enter();\n        ++mycpu-&gt;gd_trap_nesting_level;\n        kprintf(\"SECONDARY PANIC ON CPU %d\\n\", mycpu-&gt;gd_cpuid);\n        for (;;) {\n            lwkt_deschedule_self(td);\n            lwkt_switch();\n        }\n    }\n\n    /* Reentrant panic on same CPU */\n    if (xgd &amp;&amp; xgd == gd)\n        break;\n\n    /* Try to claim panic CPU */\n    if (atomic_cmpset_ptr(&amp;panic_cpu_gd, NULL, gd))\n        break;\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#panic-state-recovery","title":"Panic State Recovery","text":"<p>Before proceeding, panic releases locks to prevent deadlocks:</p> <pre><code>/* Save token state for debugger */\nbcopy(td-&gt;td_toks_array, panic_tokens, sizeof(panic_tokens));\npanic_tokens_count = td-&gt;td_toks_stop - &amp;td-&gt;td_toks_base;\n\n/* Release all tokens */\nlwkt_relalltokens(td);\ntd-&gt;td_toks_stop = &amp;td-&gt;td_toks_base;\n\n/* Clear spinlock count */\nif (gd-&gt;gd_spinlocks)\n    kprintf(\"panic with %d spinlocks held\\n\", gd-&gt;gd_spinlocks);\ngd-&gt;gd_spinlocks = 0;\n</code></pre>"},{"location":"sys/kern/shutdown/#panic-notification","title":"Panic Notification","text":"<p>External panic notifiers can be registered for hardware signaling:</p> <pre><code>static struct panicerinfo *panic_notifier;\n\nint\nset_panic_notifier(struct panicerinfo *info)\n{\n    if (info == NULL)\n        panic_notifier = NULL;\n    else if (panic_notifier != NULL)\n        return 1;  /* Already registered */\n    else\n        panic_notifier = info;\n    return 0;\n}\n</code></pre> <p>Used by ACPI PV-panic and GPIO error LED support.</p>"},{"location":"sys/kern/shutdown/#crash-dump-system","title":"Crash Dump System","text":""},{"location":"sys/kern/shutdown/#dump-device-configuration","title":"Dump Device Configuration","text":"<p>The dump device is configured at boot or via sysctl:</p> <pre><code>/* Boot-time configuration via loader tunable */\nTUNABLE_STR_FETCH(\"dumpdev\", path, MNAMELEN);\ndev = kgetdiskbyname(path);\n\n/* Runtime configuration via sysctl */\nSYSCTL_PROC(_kern, KERN_DUMPDEV, dumpdev, ...);\n</code></pre>"},{"location":"sys/kern/shutdown/#dumper-registration","title":"Dumper Registration","text":"<p>Disk drivers register dump handlers (<code>kern_shutdown.c:947-961</code>):</p> <pre><code>struct dumperinfo {\n    dumper_t    *dumper;    /* Dump function */\n    void        *priv;      /* Driver private data */\n    u_int       blocksize;  /* Dump block size */\n    off_t       mediaoffset;/* Offset on media */\n    off_t       mediasize;  /* Size of dump area */\n};\n\nint\nset_dumper(struct dumperinfo *di)\n{\n    if (di == NULL) {\n        bzero(&amp;dumper, sizeof(dumper));\n        return 0;\n    }\n    if (dumper.dumper != NULL)\n        return EBUSY;\n    dumper = *di;\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#dump-header","title":"Dump Header","text":"<p>Each dump starts with a standardized header (<code>kern_shutdown.c:630-647</code>):</p> <pre><code>struct kerneldumpheader {\n    char        magic[20];          /* Dump format magic */\n    char        architecture[12];   /* Machine architecture */\n    uint32_t    version;            /* Header version */\n    uint32_t    architectureversion;/* Arch-specific version */\n    uint64_t    dumplength;         /* Total dump size */\n    uint64_t    dumptime;           /* Time of dump */\n    uint32_t    blocksize;          /* Block size */\n    char        hostname[64];       /* System hostname */\n    char        versionstring[192]; /* Kernel version */\n    char        panicstring[175];   /* Panic message */\n    uint8_t     parity;             /* Header checksum */\n};\n</code></pre>"},{"location":"sys/kern/shutdown/#generating-dumps","title":"Generating Dumps","text":"<pre><code>void\ndumpsys(void)\n{\n    /* vkernels don't support dumps */\n#if defined(_KERNEL_VIRTUAL)\n    return;\n#endif\n\n    if (dumper.dumper != NULL &amp;&amp; !dumping) {\n        dumping++;\n        md_dumpsys(&amp;dumper);  /* Machine-dependent dump */\n    }\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#dump-cpu-reactivation","title":"Dump CPU Reactivation","text":"<p>For interactive dumps, other CPUs can be reactivated (<code>kern_shutdown.c:995-1012</code>):</p> <pre><code>void\ndump_reactivate_cpus(void)\n{\n    dump_stop_usertds = 1;\n    need_user_resched();\n\n    /* Send IPI to all CPUs */\n    for (cpu = 0; cpu &lt; ncpus; cpu++) {\n        gd = globaldata_find(cpu);\n        seq = lwkt_send_ipiq(gd, need_user_resched_remote, NULL);\n        lwkt_wait_ipiq(gd, seq);\n    }\n\n    restart_cpus(stopped_cpus);\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#system-calls-and-user-interface","title":"System Calls and User Interface","text":""},{"location":"sys/kern/shutdown/#sys_reboot","title":"sys_reboot()","text":"<p>The reboot system call (<code>kern_shutdown.c:181-193</code>):</p> <pre><code>int\nsys_reboot(struct sysmsg *sysmsg, const struct reboot_args *uap)\n{\n    int error;\n\n    /* Check for SYSCAP_NOREBOOT capability */\n    if ((error = caps_priv_check_self(SYSCAP_NOREBOOT)))\n        return error;\n\n    get_mplock();\n    boot(uap-&gt;opt);\n    rel_mplock();\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#shutdown_nice","title":"shutdown_nice()","text":"<p>Graceful shutdown via init(8) (<code>kern_shutdown.c:200-213</code>):</p> <pre><code>void\nshutdown_nice(int howto)\n{\n    shutdown_howto = howto;\n\n    if (initproc != NULL) {\n        /* Signal init to perform clean shutdown */\n        ksignal(initproc, SIGINT);\n    } else {\n        /* No init - direct reboot without sync */\n        boot(RB_NOSYNC);\n    }\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#shutdown_kproc","title":"shutdown_kproc()","text":"<p>Stop system processes cleanly (<code>kern_shutdown.c:921-945</code>):</p> <pre><code>void\nshutdown_kproc(void *arg, int howto)\n{\n    struct thread *td = arg;\n    int error;\n\n    if (panicstr)\n        return;  /* Skip during panic */\n\n    kprintf(\"Waiting for system process `%s' to stop...\",\n            td-&gt;td_proc-&gt;p_comm);\n\n    error = suspend_kproc(td, kproc_shutdown_wait * hz);\n\n    if (error == EWOULDBLOCK)\n        kprintf(\"timed out\\n\");\n    else\n        kprintf(\"stopped\\n\");\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#sysctl-variables","title":"Sysctl Variables","text":"Sysctl Default Description <code>debug.debugger_on_panic</code> 1 Enter DDB on panic <code>debug.trace_on_panic</code> 0/1 Print backtrace on panic <code>kern.sync_on_panic</code> 0 Sync disks before panic reboot <code>machdep.do_dump</code> 1 Attempt crash dump <code>kern.shutdown.poweroff_delay</code> 5000 Delay before poweroff (ms) <code>kern.shutdown.kproc_shutdown_wait</code> 60 Kproc stop timeout (sec) <code>debug.bootverbose</code> 0 Verbose boot messages"},{"location":"sys/kern/shutdown/#global-state-variables","title":"Global State Variables","text":"Variable Type Description <code>panicstr</code> <code>const char *</code> First panic message (NULL if not panicked) <code>dumping</code> <code>int</code> Non-zero while dumping <code>panic_cpu_gd</code> <code>globaldata_t</code> CPU handling panic <code>cold</code> <code>int</code> Non-zero during early boot <code>dumpdev</code> <code>cdev_t</code> Configured dump device <code>dumplo64</code> <code>u_int64_t</code> Dump offset on device <code>waittime</code> <code>int</code> Sync wait status (-1 = not started)"},{"location":"sys/kern/shutdown/#compile-time-options","title":"Compile-Time Options","text":"Option Default Description <code>PANIC_REBOOT_WAIT_TIME</code> 15 Seconds before auto-reboot after panic <code>POWEROFF_DELAY</code> 5000 Milliseconds before poweroff <code>DDB</code> - Enable kernel debugger <code>DDB_UNATTENDED</code> - Don't enter debugger on panic <code>DDB_TRACE</code> - Auto-print backtrace on panic <code>WDOG_DISABLE_ON_PANIC</code> - Disable watchdog on panic <code>ERROR_LED_ON_PANIC</code> - Light GPIO error LED on panic"},{"location":"sys/kern/shutdown/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/kern/shutdown/#bsp-migration","title":"BSP Migration","text":"<p>Shutdown always migrates to CPU 0 (BSP) for ACPI compatibility:</p> <pre><code>if (panicstr == NULL &amp;&amp; mycpu-&gt;gd_cpuid != 0) {\n    kprintf(\"Switching to cpu #0 for shutdown\\n\");\n    lwkt_setcpu_self(globaldata_find(0));\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#token-preservation","title":"Token Preservation","text":"<p>During panic, token state is preserved for debugging:</p> <pre><code>bcopy(td-&gt;td_toks_array, panic_tokens, sizeof(panic_tokens));\npanic_tokens_count = td-&gt;td_toks_stop - &amp;td-&gt;td_toks_base;\n</code></pre>"},{"location":"sys/kern/shutdown/#vkernel-support","title":"vkernel Support","text":"<p>Virtual kernels handle shutdown differently:</p> <ul> <li>Clean up vkernel state in <code>shutdown_cleanup_proc()</code></li> <li>Dumps are not supported in vkernels</li> </ul>"},{"location":"sys/kern/shutdown/#priority-elevation","title":"Priority Elevation","text":"<p>The shutdown thread runs at maximum priority:</p> <pre><code>if (curthread-&gt;td_release)\n    curthread-&gt;td_release(curthread);\nlwkt_setpri_self(TDPRI_MAX);\n</code></pre>"},{"location":"sys/kern/shutdown/#example-registering-a-shutdown-handler","title":"Example: Registering a Shutdown Handler","text":"<pre><code>#include &lt;sys/eventhandler.h&gt;\n\n/* Device-specific cleanup before sync */\nstatic void\nmydev_shutdown_presync(void *arg, int howto)\n{\n    struct mydev_softc *sc = arg;\n\n    /* Flush device caches */\n    mydev_flush_cache(sc);\n\n    /* Disable DMA */\n    mydev_disable_dma(sc);\n}\n\n/* Device-specific cleanup after sync */\nstatic void\nmydev_shutdown_postsync(void *arg, int howto)\n{\n    struct mydev_softc *sc = arg;\n\n    /* Power down device */\n    if (howto &amp; RB_POWEROFF)\n        mydev_poweroff(sc);\n}\n\n/* Registration during attach */\nstatic int\nmydev_attach(device_t dev)\n{\n    struct mydev_softc *sc = device_get_softc(dev);\n\n    sc-&gt;shutdown_tag1 = EVENTHANDLER_REGISTER(shutdown_pre_sync,\n        mydev_shutdown_presync, sc, SHUTDOWN_PRI_DEFAULT);\n\n    sc-&gt;shutdown_tag2 = EVENTHANDLER_REGISTER(shutdown_post_sync,\n        mydev_shutdown_postsync, sc, SHUTDOWN_PRI_DEFAULT);\n\n    return 0;\n}\n\n/* Deregistration during detach */\nstatic int\nmydev_detach(device_t dev)\n{\n    struct mydev_softc *sc = device_get_softc(dev);\n\n    EVENTHANDLER_DEREGISTER(shutdown_pre_sync, sc-&gt;shutdown_tag1);\n    EVENTHANDLER_DEREGISTER(shutdown_post_sync, sc-&gt;shutdown_tag2);\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/shutdown/#see-also","title":"See Also","text":"<ul> <li>Event Notification (kevent) - Event handler framework</li> <li>Processes &amp; Threads - Process management</li> <li>Virtual Filesystem - Filesystem operations</li> <li>Tracing &amp; Debugging - Kernel debugging facilities</li> </ul>"},{"location":"sys/kern/signals/","title":"Signals","text":"<p>Source file: <code>kern_sig.c</code> (2,701 lines)</p> <p>This document covers signal management in DragonFly BSD, including signal generation, delivery, and handling. Signals are asynchronous notifications delivered to processes or threads to indicate events such as exceptions, terminal I/O, process control, or user-defined events.</p>"},{"location":"sys/kern/signals/#overview","title":"Overview","text":"<p>DragonFly BSD implements POSIX and BSD signal semantics with extensions for:</p> <ol> <li>Per-thread and per-process signals \u2014 Signals can target specific LWPs or the process generally</li> <li>Signal masking \u2014 Per-thread signal masks control delivery</li> <li>Signal actions \u2014 Processes can catch, ignore, or use default handling</li> <li>Process control signals \u2014 STOP/CONT signals for job control</li> <li>Core dumps \u2014 SA_CORE signals generate core files</li> <li>Real-time signals \u2014 Extended signal numbers beyond traditional POSIX</li> <li>Cross-CPU delivery \u2014 IPI-based notification for signals to remote CPUs</li> </ol> <p>Key data structures: - <code>struct sigacts</code> \u2014 Per-process signal actions and state - <code>sigset_t</code> \u2014 Bit set representing signals (32+ signals) - <code>p-&gt;p_siglist</code> \u2014 Process-wide pending signals - <code>lp-&gt;lwp_siglist</code> \u2014 Thread-specific pending signals - <code>lp-&gt;lwp_sigmask</code> \u2014 Per-thread signal mask</p>"},{"location":"sys/kern/signals/#signal-properties","title":"Signal Properties","text":""},{"location":"sys/kern/signals/#signal-categories","title":"Signal Categories","text":"<p>Source: <code>kern_sig.c:119-194</code></p> <p>Each signal has properties encoded in <code>sigproptbl[]</code>:</p> <ul> <li>SA_KILL (<code>0x01</code>) \u2014 Terminates process by default</li> <li>SA_CORE (<code>0x02</code>) \u2014 Terminates and generates core dump</li> <li>SA_STOP (<code>0x04</code>) \u2014 Stops process execution</li> <li>SA_TTYSTOP (<code>0x08</code>) \u2014 Stop from terminal (SIGTSTP/SIGTTIN/SIGTTOU)</li> <li>SA_IGNORE (<code>0x10</code>) \u2014 Ignored by default</li> <li>SA_CONT (<code>0x20</code>) \u2014 Continues stopped process</li> <li>SA_CANTMASK (<code>0x40</code>) \u2014 Cannot be blocked (SIGKILL/SIGSTOP)</li> <li>SA_CKPT (<code>0x80</code>) \u2014 Checkpoint signal (DragonFly extension)</li> </ul> <p>Examples: - <code>SIGKILL</code> \u2014 SA_KILL (cannot be caught or ignored) - <code>SIGSEGV</code> \u2014 SA_KILL | SA_CORE (core dump on segmentation fault) - <code>SIGSTOP</code> \u2014 SA_STOP | SA_CANTMASK (cannot be blocked) - <code>SIGCHLD</code> \u2014 SA_IGNORE (default is to ignore) - <code>SIGCONT</code> \u2014 SA_IGNORE | SA_CONT (resumes stopped processes)</p>"},{"location":"sys/kern/signals/#unmaskable-signals","title":"Unmaskable Signals","text":"<p>Source: <code>kern_sig.c:196, 417</code></p> <p><code>SIGKILL</code> and <code>SIGSTOP</code> cannot be masked, caught, or ignored:</p> <pre><code>sigset_t sigcantmask_mask;  // Contains SIGKILL and SIGSTOP\nSIG_CANTMASK(mask);         // Macro removes unmaskable signals\n</code></pre> <p>This ensures that processes can always be killed or stopped by administrators.</p>"},{"location":"sys/kern/signals/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/signals/#struct-sigacts-per-process-signal-actions","title":"<code>struct sigacts</code> \u2014 Per-Process Signal Actions","text":"<p>Source: <code>sys/signalvar.h:56-73</code></p> <pre><code>struct sigacts {\n    sig_t ps_sigact[_SIG_MAXSIG];      /* Signal handlers */\n    sigset_t ps_catchmask[_SIG_MAXSIG]; /* Masks during handler */\n    struct {\n        int pid;  /* Originating process PID */\n        int uid;  /* Originating process UID */\n    } ps_frominfo[_SIG_MAXSIG];\n\n    sigset_t ps_sigignore;    /* Signals set to SIG_IGN */\n    sigset_t ps_sigcatch;     /* Signals with custom handlers */\n    sigset_t ps_sigonstack;   /* Use alternate signal stack */\n    sigset_t ps_sigintr;      /* Interrupt syscalls (no SA_RESTART) */\n    sigset_t ps_sigreset;     /* Reset to SIG_DFL after catching (SA_RESETHAND) */\n    sigset_t ps_signodefer;   /* Don't mask signal during handler (SA_NODEFER) */\n    sigset_t ps_siginfo;      /* Use siginfo_t (SA_SIGINFO) */\n\n    unsigned int ps_refcnt;\n    int ps_flag;              /* PS_NOCLDSTOP, PS_NOCLDWAIT, PS_CLDSIGIGN */\n};\n</code></pre> <p>Key fields: - <code>ps_sigact[]</code> \u2014 Array of signal handlers (SIG_DFL, SIG_IGN, or function pointer) - <code>ps_catchmask[]</code> \u2014 Signals to block while handler executes - <code>ps_frominfo[]</code> \u2014 Tracks sender's PID/UID for signal provenance</p> <p>Flags: - <code>PS_NOCLDSTOP</code> \u2014 Don't notify parent of child stop/cont (SA_NOCLDSTOP) - <code>PS_NOCLDWAIT</code> \u2014 Don't create zombies, reap immediately (SA_NOCLDWAIT) - <code>PS_CLDSIGIGN</code> \u2014 SIGCHLD handler is SIG_IGN</p>"},{"location":"sys/kern/signals/#signal-masks-sigset_t","title":"Signal Masks (<code>sigset_t</code>)","text":"<p>Source: <code>sys/signalvar.h:91-150</code></p> <p>Bit set representing 32+ signals:</p> <pre><code>typedef struct {\n    unsigned int __bits[_SIG_WORDS];  /* _SIG_WORDS = 4 for 128 signals */\n} sigset_t;\n</code></pre> <p>Operations (macros): - <code>SIGADDSET(set, sig)</code> \u2014 Add signal to set - <code>SIGDELSET(set, sig)</code> \u2014 Remove signal from set - <code>SIGISMEMBER(set, sig)</code> \u2014 Test membership - <code>SIGEMPTYSET(set)</code> \u2014 Clear all signals - <code>SIGFILLSET(set)</code> \u2014 Set all signals - <code>SIGSETOR(set1, set2)</code> \u2014 Union of sets - <code>SIGSETNAND(set1, set2)</code> \u2014 Remove set2 from set1</p> <p>Atomic variants: - <code>SIGADDSET_ATOMIC(set, sig)</code> \u2014 Uses <code>atomic_set_int()</code> for SMP safety - <code>SIGDELSET_ATOMIC(set, sig)</code> \u2014 Uses <code>atomic_clear_int()</code></p> <p>Process vs. Thread signals: - <code>p-&gt;p_siglist</code> \u2014 Process-wide pending signals (any thread can handle) - <code>lp-&gt;lwp_siglist</code> \u2014 Thread-specific pending signals (only this LWP handles) - <code>lp-&gt;lwp_sigmask</code> \u2014 Per-thread signal mask</p>"},{"location":"sys/kern/signals/#signal-actions-sigaction","title":"Signal Actions (<code>sigaction</code>)","text":""},{"location":"sys/kern/signals/#setting-signal-handlers","title":"Setting Signal Handlers","text":""},{"location":"sys/kern/signals/#kern_sigaction","title":"<code>kern_sigaction</code>","text":"<p>Source: <code>kern_sig.c:248-377</code></p> <p>Set signal disposition (handler, mask, flags):</p> <pre><code>int kern_sigaction(int sig, struct sigaction *act, struct sigaction *oact);\n</code></pre> <p>Validation: 1. Check signal number (<code>1 &lt;= sig &lt; _SIG_MAXSIG</code>) 2. Reject attempts to catch <code>SIGKILL</code> or <code>SIGSTOP</code> 3. Acquire <code>p-&gt;p_token</code> for atomicity</p> <p>Installation: 1. Update <code>ps-&gt;ps_sigact[sig]</code> with handler 2. Update <code>ps-&gt;ps_catchmask[sig]</code> with additional mask 3. Set flags in various <code>ps_sig*</code> masks:    - <code>SA_ONSTACK</code> \u2192 <code>ps_sigonstack</code>    - <code>SA_RESTART</code> \u2192 clear <code>ps_sigintr</code>    - <code>SA_RESETHAND</code> \u2192 <code>ps_sigreset</code>    - <code>SA_NODEFER</code> \u2192 <code>ps_signodefer</code>    - <code>SA_SIGINFO</code> \u2192 <code>ps_siginfo</code> 4. Handle <code>SIGCHLD</code> special flags (SA_NOCLDSTOP, SA_NOCLDWAIT)</p> <p>Signal state updates: - If action is <code>SIG_IGN</code> or default-ignore \u2192 remove from pending signals - Update <code>p_sigignore</code> and <code>p_sigcatch</code> sets accordingly - Iterate all LWPs to clear pending instances</p> <p>PID 1 restriction: Process 1 (init) cannot set SA_NOCLDWAIT to prevent zombie accumulation.</p>"},{"location":"sys/kern/signals/#signal-initialization","title":"Signal Initialization","text":""},{"location":"sys/kern/signals/#siginit-process-0-setup","title":"<code>siginit</code> \u2014 Process 0 Setup","text":"<p>Source: <code>kern_sig.c:404-418</code></p> <p>Called during kernel bootstrap:</p> <ol> <li>Initialize <code>p_sigignore</code> with default-ignored signals</li> <li>Set global <code>sigcantmask_mask</code> (SIGKILL + SIGSTOP)</li> </ol>"},{"location":"sys/kern/signals/#execsigs-reset-on-exec","title":"<code>execsigs</code> \u2014 Reset on Exec","text":"<p>Source: <code>kern_sig.c:423-464</code></p> <p>Called by <code>execve()</code> to reset signal state:</p> <ol> <li>Reset all caught signals to SIG_DFL</li> <li>Clear signal stack (reset to user stack)</li> <li>Clear SA_NOCLDWAIT and SA_NOCLDSTOP flags</li> <li>Reset SIGCHLD to SIG_DFL if ignored</li> </ol> <p>Rationale: Exec'd program starts with clean signal state (caught signals don't persist across exec).</p>"},{"location":"sys/kern/signals/#signal-masking","title":"Signal Masking","text":""},{"location":"sys/kern/signals/#kern_sigprocmask","title":"<code>kern_sigprocmask</code>","text":"<p>Source: <code>kern_sig.c:472-509</code></p> <p>Modify thread's signal mask:</p> <pre><code>int kern_sigprocmask(int how, sigset_t *set, sigset_t *oset);\n</code></pre> <p>Operations: - <code>SIG_BLOCK</code> \u2014 Add signals to mask: <code>lwp_sigmask |= *set</code> - <code>SIG_UNBLOCK</code> \u2014 Remove signals from mask: <code>lwp_sigmask &amp;= ~*set</code> - <code>SIG_SETMASK</code> \u2014 Replace mask: <code>lwp_sigmask = *set</code></p> <p>Unmaskable enforcement: <code>SIG_CANTMASK()</code> macro always removes SIGKILL/SIGSTOP.</p> <p>Interlock: After <code>SIG_SETMASK</code>, calls <code>sigirefs_wait()</code> to synchronize with concurrent signal delivery (prevents races with <code>sigsuspend()</code>/<code>ppoll()</code>/<code>pselect()</code>).</p>"},{"location":"sys/kern/signals/#kern_sigpending","title":"<code>kern_sigpending</code>","text":"<p>Source: <code>kern_sig.c:541-548</code></p> <p>Return set of pending signals:</p> <pre><code>int kern_sigpending(sigset_t *set);\n</code></pre> <p>Returns union of process and thread pending signals:</p> <pre><code>*set = lwp_sigpend(lp);  // p-&gt;p_siglist | lp-&gt;lwp_siglist\n</code></pre>"},{"location":"sys/kern/signals/#kern_sigsuspend","title":"<code>kern_sigsuspend</code>","text":"<p>Source: <code>kern_sig.c:573-604</code></p> <p>Atomically change mask and sleep until signal:</p> <pre><code>int kern_sigsuspend(sigset_t *set);\n</code></pre> <p>Algorithm: 1. Save old mask in <code>lp-&gt;lwp_oldsigmask</code> 2. Set <code>LWP_OLDMASK</code> flag (indicates sigsuspend in progress) 3. Acquire <code>lp-&gt;lwp_token</code> (interlocks signal delivery) 4. Set temporary mask: <code>lp-&gt;lwp_sigmask = *set</code> 5. Release token and call <code>sigirefs_wait()</code> to synchronize 6. Sleep in <code>tsleep(ps, PCATCH, \"pause\", 0)</code> 7. Wake on signal delivery, mask restored in <code>postsig()</code></p> <p>Interlock: Holding <code>lp-&gt;lwp_token</code> during mask change ensures signal delivery sees consistent state.</p>"},{"location":"sys/kern/signals/#signal-generation","title":"Signal Generation","text":""},{"location":"sys/kern/signals/#kern_kill-send-signal-to-processthread","title":"<code>kern_kill</code> \u2014 Send Signal to Process/Thread","text":"<p>Source: <code>kern_sig.c:748-857</code></p> <p>Entry point for <code>kill()</code> and <code>lwp_kill()</code> system calls:</p> <pre><code>int kern_kill(int sig, pid_t pid, lwpt_t tid);\n</code></pre> <p>Target selection: - <code>pid &gt; 0, tid == -1</code> \u2014 Signal process (any thread) - <code>pid &gt; 0, tid &gt;= 0</code> \u2014 Signal specific thread - <code>pid == 0</code> \u2014 Signal own process group - <code>pid == -1</code> \u2014 Broadcast to all processes (privileged) - <code>pid &lt; -1</code> \u2014 Signal process group <code>-pid</code></p> <p>Permission check: - <code>CANSIGIO()</code> macro: root or matching UID - <code>p_trespass()</code> for cross-process signals</p> <p>Delivery: - Calls <code>lwpsignal(p, lp, sig)</code> with target process/thread</p>"},{"location":"sys/kern/signals/#ksignal-and-lwpsignal-core-signal-delivery","title":"<code>ksignal</code> and <code>lwpsignal</code> \u2014 Core Signal Delivery","text":"<p>Source: <code>kern_sig.c:1115-1459</code></p> <p>The heart of signal delivery:</p> <pre><code>void ksignal(struct proc *p, int sig);              // Generic process signal\nvoid lwpsignal(struct proc *p, struct lwp *lp, int sig); // Specific or auto-select LWP\n</code></pre> <p>Target selection (if <code>lp == NULL</code>): 1. Check if current preempted thread belongs to <code>p</code> and doesn't mask signal 2. Otherwise call <code>find_lwp_for_signal()</code>:    - Prefer LSRUN (running) threads    - Then LSSLEEP (sleeping with LWP_SINTR)    - Finally LSSTOP (stopped) threads 3. Returns LWP with token held, or NULL if all threads mask signal</p> <p>Signal processing:</p> <ol> <li>Ignored signals:</li> <li>If <code>SIGISMEMBER(p_sigignore, sig)</code> \u2192 discard (unless P_TRACED)</li> <li> <p>Still notify kqueue: <code>KNOTE(&amp;p-&gt;p_klist, NOTE_SIGNAL | sig)</code></p> </li> <li> <p>Continue signals (SA_CONT):</p> </li> <li>Clear all pending STOP signals: <code>SIG_STOPSIGMASK_ATOMIC(p-&gt;p_siglist)</code></li> <li> <p>If process stopped \u2192 call <code>proc_unstop()</code> \u2192 wake all threads</p> </li> <li> <p>Stop signals (SA_STOP):</p> </li> <li>Clear all pending CONT signals: <code>SIG_CONTSIGMASK_ATOMIC(p-&gt;p_siglist)</code></li> <li>TTY stop signals ignored if orphaned process group</li> <li> <p>If default action \u2192 call <code>proc_stop()</code> to stop process</p> </li> <li> <p>Process stopped (p-&gt;p_stat == SSTOP):</p> </li> <li>Add signal to pending list but don't wake (unless SIGKILL or SIGCONT)</li> <li>SIGKILL \u2192 call <code>proc_unstop()</code> to make runnable</li> <li> <p>SIGCONT \u2192 continue process and optionally notify parent</p> </li> <li> <p>Active process:</p> </li> <li>Find suitable LWP (if not already specified)</li> <li>Add signal to <code>lp-&gt;lwp_siglist</code> (thread-specific)</li> <li>Call <code>lwp_signotify(lp)</code> to wake/interrupt thread</li> </ol>"},{"location":"sys/kern/signals/#lwp_signotify-wake-thread-for-signal","title":"<code>lwp_signotify</code> \u2014 Wake Thread for Signal","text":"<p>Source: <code>kern_sig.c:1482-1548</code></p> <p>Notify LWP that signal has arrived:</p> <p>Cases: 1. Preempted on current CPU:    - Call <code>signotify()</code> (sets TDF_SIGPENDING, will check on return to userland)</p> <ol> <li>Sleeping with LWP_SINTR:</li> <li>Thread in <code>tsleep()</code> with <code>PCATCH</code> flag</li> <li>If local CPU \u2192 call <code>setrunnable(lp)</code></li> <li> <p>If remote CPU \u2192 send IPI via <code>lwkt_send_ipiq()</code></p> </li> <li> <p>Sleeping with TDF_SINTR:</p> </li> <li>Thread in <code>lwkt_sleep()</code> with <code>PCATCH</code></li> <li> <p>Same local/remote logic as above</p> </li> <li> <p>Running in userland:</p> </li> <li>Send IPI to remote CPU to knock thread into kernel</li> <li>IPI handler (<code>lwp_signotify_remote</code>) calls <code>signotify()</code></li> </ol> <p>IPI handling: IPIs forward <code>LWPHOLD()</code> reference to target CPU if thread migrates.</p>"},{"location":"sys/kern/signals/#find_lwp_for_signal","title":"<code>find_lwp_for_signal</code>","text":"<p>Source: <code>kern_sig.c:996-1095</code></p> <p>Select best LWP to receive signal:</p> <p>Priority: 1. Current preempted thread (if doesn't mask signal) \u2014 avoids context switch 2. LSRUN thread \u2014 already running, will return to userland soon 3. LSSLEEP thread with LWP_SINTR \u2014 can be interrupted 4. LSSTOP thread \u2014 stopped, will check signal when resumed</p> <p>Locking: Returns LWP with <code>lwp_token</code> held and <code>LWPHOLD()</code> reference.</p>"},{"location":"sys/kern/signals/#signal-delivery-path","title":"Signal Delivery Path","text":""},{"location":"sys/kern/signals/#process-control-signals","title":"Process Control Signals","text":""},{"location":"sys/kern/signals/#proc_stop-stop-all-threads","title":"<code>proc_stop</code> \u2014 Stop All Threads","text":"<p>Source: <code>kern_sig.c:1584-1660</code></p> <p>Called when stop signal arrives (default action):</p> <pre><code>void proc_stop(struct proc *p, int stat);  // stat = SSTOP or SCORE\n</code></pre> <p>Algorithm: 1. Set <code>p-&gt;p_stat = SSTOP</code> (or SCORE for coredump) 2. For each LWP:    - LSSTOP: Already stopped, no action    - LSSLEEP: Set <code>LWP_MP_WSTOP</code>, increment <code>p-&gt;p_nstopped</code>    - LSRUN: Call <code>lwp_signotify()</code> to interrupt 3. If all threads stopped (<code>p-&gt;p_nstopped == p-&gt;p_nthreads</code>):    - Clear <code>P_WAITED</code> flag    - Wake parent: <code>wakeup(parent)</code>    - Send <code>SIGCHLD</code> (unless PS_NOCLDSTOP set)</p> <p>SCORE state: Special state for coredump \u2014 cannot be overridden by SIGCONT until coredump completes.</p> <p>LWP_MP_WSTOP flag: Prevents sleeping threads from incrementing <code>p-&gt;p_nstopped</code> again when they reach <code>tstop()</code>.</p>"},{"location":"sys/kern/signals/#proc_unstop-resume-all-threads","title":"<code>proc_unstop</code> \u2014 Resume All Threads","text":"<p>Source: <code>kern_sig.c:1666-1728</code></p> <p>Called when SIGCONT arrives or SIGKILL overrides stop:</p> <pre><code>void proc_unstop(struct proc *p, int stat);\n</code></pre> <p>Algorithm: 1. Verify <code>p-&gt;p_stat == stat</code> (SSTOP or SCORE) 2. Set <code>p-&gt;p_stat = SACTIVE</code> 3. For each LWP:    - LSRUN: Already running (unexpected but allowed)    - LSSLEEP: Clear <code>LWP_MP_WSTOP</code>, call <code>setrunnable()</code>    - LSSTOP: Call <code>setrunnable()</code> to wake thread</p> <p>Effect: All threads transition from stopped to runnable, will return to userland or continue execution.</p>"},{"location":"sys/kern/signals/#proc_stopwait-wait-for-all-threads-to-stop","title":"<code>proc_stopwait</code> \u2014 Wait for All Threads to Stop","text":"<p>Source: <code>kern_sig.c:1731-1748</code></p> <p>Used before coredump to ensure all threads fully stopped:</p> <pre><code>while (p-&gt;p_nstopped &lt; p-&gt;p_nthreads) {\n    tsleep(&amp;p-&gt;p_nstopped, 0, \"stopwait\", 1);\n}\n</code></pre> <p>Polls until <code>p-&gt;p_nstopped</code> reaches <code>p-&gt;p_nthreads</code>.</p>"},{"location":"sys/kern/signals/#signal-dispatch","title":"Signal Dispatch","text":""},{"location":"sys/kern/signals/#issignal-check-for-pending-signal","title":"<code>issignal</code> \u2014 Check for Pending Signal","text":"<p>Source: <code>kern_sig.c:1979-2248</code></p> <p>Called via <code>CURSIG()</code> macro to check if signal should be delivered:</p> <pre><code>int issignal(struct lwp *lp, int maytrace, int *ptokp);\n</code></pre> <p>Returns: Signal number to handle, or 0 if none.</p> <p>Algorithm:</p> <ol> <li>Quick check without token:</li> <li>Compute <code>mask = lwp_sigpend(lp) &amp; ~lwp_sigmask</code></li> <li>Remove stop signals if <code>P_PPWAIT</code> (vfork parent)</li> <li> <p>If empty \u2192 return 0 (no signal)</p> </li> <li> <p>Acquire token if signal in process list:</p> </li> <li> <p>Recheck mask with token held (double-check pattern)</p> </li> <li> <p>Handle ignored signals:</p> </li> <li> <p>If <code>SIGISMEMBER(p_sigignore, sig)</code> \u2192 delete and continue</p> </li> <li> <p>Tracing (P_TRACED):</p> </li> <li>Stop process: <code>proc_stop(p, SSTOP)</code></li> <li>Call <code>tstop()</code> to block thread</li> <li>Parent can modify signal via ptrace: <code>p-&gt;p_xstat</code></li> <li> <p>If parent clears signal \u2192 continue loop</p> </li> <li> <p>Determine action:</p> </li> <li><code>SIG_DFL</code> \u2014 Default action (check properties)</li> <li><code>SIG_IGN</code> \u2014 Ignored (shouldn't happen, but continue)</li> <li> <p>Custom handler \u2014 return signal number</p> </li> <li> <p>Default action handling:</p> </li> <li>SA_CKPT: Call <code>checkpoint_signal_handler()</code> (DragonFly checkpoint)</li> <li>SA_STOP: Call <code>proc_stop()</code>, delete signal, continue loop</li> <li>SA_IGNORE: Delete signal, continue loop (e.g., SIGCONT)</li> <li> <p>SA_KILL/SA_CORE: Return signal for <code>postsig()</code> to handle</p> </li> <li> <p>Delete signal from pending lists:</p> </li> <li><code>lwp_delsig(lp, sig, haveptok)</code> removes from both <code>p_siglist</code> and <code>lwp_siglist</code></li> </ol> <p>Token management: Carefully tracks whether <code>p-&gt;p_token</code> is held via <code>haveptok</code> flag, returns ownership via <code>ptokp</code> pointer.</p>"},{"location":"sys/kern/signals/#postsig-execute-signal-action","title":"<code>postsig</code> \u2014 Execute Signal Action","text":"<p>Source: <code>kern_sig.c:2260-2360</code></p> <p>Deliver signal to handler or terminate process:</p> <pre><code>void postsig(int sig, int haveptok);\n</code></pre> <p>Called from: Trap handler or syscall return path (userret).</p> <p>Actions:</p> <ol> <li> <p>Virtual kernel: If in vkernel context, switch back to kernel context</p> </li> <li> <p>Delete signal: <code>lwp_delsig(lp, sig, haveptok)</code></p> </li> <li> <p>Notify kqueue: <code>KNOTE(&amp;p-&gt;p_klist, NOTE_SIGNAL | sig)</code></p> </li> <li> <p>Default action (SIG_DFL):</p> </li> <li>Call <code>sigexit(lp, sig)</code> to terminate process</li> <li>Generates core dump if SA_CORE property</li> <li> <p>Does not return</p> </li> <li> <p>Custom handler:</p> </li> <li>Determine mask to restore after handler:<ul> <li>If <code>LWP_OLDMASK</code> set (from sigsuspend) \u2192 use <code>lwp_oldsigmask</code></li> <li>Otherwise \u2192 use current <code>lwp_sigmask</code></li> </ul> </li> <li>Block additional signals: <code>SIGSETOR(lwp_sigmask, ps_catchmask[sig])</code></li> <li>Unless SA_NODEFER \u2192 also block signal itself</li> <li>Reset handler to SIG_DFL if SA_RESETHAND</li> <li>Call platform-specific sendsig: <code>(*sv_sendsig)(action, sig, &amp;returnmask, code)</code></li> <li>Increment <code>ru_nsignals</code> counter</li> </ol> <p>Machine-dependent sendsig: - Sets up signal trampoline on user stack - Modifies user registers to call handler - Stores return mask for <code>sigreturn()</code> syscall</p>"},{"location":"sys/kern/signals/#special-signal-handling","title":"Special Signal Handling","text":""},{"location":"sys/kern/signals/#trapsignal-synchronous-signal-from-trap","title":"<code>trapsignal</code> \u2014 Synchronous Signal from Trap","text":"<p>Source: <code>kern_sig.c:940-985</code></p> <p>Deliver signal caused by trap (e.g., SIGSEGV, SIGFPE):</p> <pre><code>void trapsignal(struct lwp *lp, int sig, u_long code);\n</code></pre> <p>Difference from regular signals: These signals MUST be delivered to the specific LWP that caused the trap (never delivered generically to process).</p> <p>Fast path: If signal is caught and not masked: - Immediately call <code>(*sv_sendsig)()</code> to invoke handler - No need to make signal pending - Update signal mask and apply SA_RESETHAND/SA_NODEFER</p> <p>Slow path: Otherwise call <code>lwpsignal(p, lp, sig)</code> to queue signal.</p> <p>Virtual kernel: If in vkernel emulation, switch back to vkernel context before delivering.</p>"},{"location":"sys/kern/signals/#sigexit-terminate-with-signal","title":"<code>sigexit</code> \u2014 Terminate with Signal","text":"<p>Source: <code>kern_sig.c:2384-2430</code></p> <p>Force process exit due to signal:</p> <pre><code>void sigexit(struct lwp *lp, int sig);\n</code></pre> <p>Steps:</p> <ol> <li> <p>Set <code>p-&gt;p_acflag |= AXSIG</code> (accounting: terminated by signal)</p> </li> <li> <p>Core dump (SA_CORE signals):</p> </li> <li>Stop all threads: <code>proc_stop(p, SCORE)</code> + <code>proc_stopwait(p)</code></li> <li>Call <code>coredump(lp, sig)</code> to write core file</li> <li>If successful \u2192 set <code>WCOREFLAG</code> in exit status</li> <li> <p>Log message: <code>\"pid %d (%s), uid %d: exited on signal %d (core dumped)\"</code></p> </li> <li> <p>Exit: Call <code>exit1(W_EXITCODE(0, sig))</code> \u2014 does not return</p> </li> </ol> <p>Sysctl: <code>kern.logsigexit</code> controls logging (default 1).</p>"},{"location":"sys/kern/signals/#coredump-generate-core-file","title":"<code>coredump</code> \u2014 Generate Core File","text":"<p>Source: <code>kern_sig.c:2515-2678</code></p> <p>Write core dump to filesystem:</p> <pre><code>static int coredump(struct lwp *lp, int sig);\n</code></pre> <p>Algorithm:</p> <ol> <li>Check permissions:</li> <li>If sugid process \u2192 check <code>kern.sugid_coredump</code> sysctl</li> <li> <p>If disabled globally \u2192 check <code>kern.coredump</code> sysctl</p> </li> <li> <p>Expand core filename:</p> </li> <li>Default: <code>\"%N.core\"</code> (process name + \".core\")</li> <li>Supports format specifiers: <code>%N</code> (name), <code>%P</code> (pid), <code>%U</code> (uid)</li> <li> <p>Example: <code>\"/cores/%U/%N-%P\"</code> \u2192 <code>/cores/1000/myapp-1234</code></p> </li> <li> <p>Create core file:</p> </li> <li>Call <code>nlookup_init_at()</code> with expanded path</li> <li>Open with <code>O_CREAT | O_TRUNC | O_NOFOLLOW</code></li> <li> <p>Prevent following symlinks (security)</p> </li> <li> <p>Write core:</p> </li> <li>Call <code>(*p-&gt;p_sysent-&gt;sv_coredump)(lp, sig, vp, limit)</code></li> <li>Platform-specific function writes ELF core format</li> <li> <p>Includes registers, memory segments, thread info</p> </li> <li> <p>Cleanup:</p> </li> <li>Close file</li> <li>Release vnode</li> </ol> <p>Security: Setuid/setgid programs don't dump core by default (prevents password/key leakage).</p>"},{"location":"sys/kern/signals/#process-group-signals","title":"Process Group Signals","text":""},{"location":"sys/kern/signals/#pgsignal-signal-process-group","title":"<code>pgsignal</code> \u2014 Signal Process Group","text":"<p>Source: <code>kern_sig.c:911-929</code></p> <p>Send signal to all members of process group:</p> <pre><code>void pgsignal(struct pgrp *pgrp, int sig, int checkctty);\n</code></pre> <p>Parameters: - <code>pgrp</code> \u2014 Process group structure - <code>sig</code> \u2014 Signal number - <code>checkctty</code> \u2014 If 1, only signal processes with controlling terminal</p> <p>Algorithm: 1. Acquire <code>pgrp-&gt;pg_lock</code> (prevents concurrent fork from missing signal) 2. Iterate <code>pgrp-&gt;pg_members</code> list 3. For each process:    - If <code>checkctty == 0</code> or <code>p-&gt;p_flags &amp; P_CONTROLT</code> \u2192 call <code>ksignal(p, sig)</code></p> <p>Locking: Process group lock ensures that processes forking during signal delivery either: - See the lock and wait \u2192 receive signal after fork completes - Complete fork before lock \u2192 child added to group and receives signal</p>"},{"location":"sys/kern/signals/#wait-for-signal","title":"Wait for Signal","text":""},{"location":"sys/kern/signals/#kern_sigtimedwait-wait-for-specific-signals","title":"<code>kern_sigtimedwait</code> \u2014 Wait for Specific Signals","text":"<p>Source: <code>kern_sig.c:1754-1866</code></p> <p>Wait for signal from specified set (with optional timeout):</p> <pre><code>static int kern_sigtimedwait(sigset_t waitset, siginfo_t *info,\n                               struct timespec *timeout);\n</code></pre> <p>Used by: <code>sigwaitinfo()</code>, <code>sigtimedwait()</code> system calls.</p> <p>Algorithm:</p> <ol> <li>Save current signal mask</li> <li>Compute timeout deadline (if specified)</li> <li>Loop:</li> <li>Check if any signal in <code>waitset</code> is pending: <code>set = lwp_sigpend(lp) &amp; waitset</code></li> <li>If signal found:<ul> <li>Temporarily fill signal mask to block all signals</li> <li>Call <code>issignal(lp, 1, NULL)</code> to process signal</li> <li>If SIGSTOP \u2192 may return 0, retry</li> <li>Delete signal from pending list</li> <li>Return signal number</li> </ul> </li> <li>If no signal and timeout expired \u2192 return EAGAIN</li> <li> <p>Otherwise:</p> <ul> <li>Block all signals in <code>waitset</code>: <code>lwp_sigmask &amp;= ~waitset</code></li> <li>Call <code>sigirefs_wait()</code> to synchronize</li> <li>Sleep: <code>tsleep(&amp;p-&gt;p_sigacts, PCATCH, \"sigwt\", hz)</code></li> <li>Wake on signal delivery (broken by <code>lwpsignal()</code>)</li> <li>Retry loop</li> </ul> </li> <li> <p>Restore original signal mask</p> </li> </ol> <p>Return: Signal number in <code>info-&gt;si_signo</code>, or error code.</p> <p>Note: Signal is consumed (removed from pending list), unlike normal signal delivery which queues for handler.</p>"},{"location":"sys/kern/signals/#kqueue-integration","title":"Kqueue Integration","text":""},{"location":"sys/kern/signals/#filt_sigattachfilt_signal","title":"<code>filt_sigattach</code>/<code>filt_signal</code>","text":"<p>Source: <code>kern_sig.c:84-89, 2562-2614</code></p> <p>Kqueue filter for signal notification:</p> <pre><code>struct filterops sig_filtops = {\n    FILTEROP_MPSAFE,\n    filt_sigattach,\n    filt_sigdetach,\n    filt_signal\n};\n</code></pre> <p>Attach: Register knote on process: <code>KNOTE_INSERT(&amp;p-&gt;p_klist, kn)</code></p> <p>Signal: When signal arrives, <code>lwpsignal()</code> calls: <pre><code>KNOTE(&amp;p-&gt;p_klist, NOTE_SIGNAL | sig);\n</code></pre></p> <p>This wakes any threads waiting in <code>kevent()</code> for signal notification.</p> <p>Use case: Alternative to <code>sigwait()</code> \u2014 allows waiting for signals via kqueue instead of signal-specific APIs.</p>"},{"location":"sys/kern/signals/#signal-interlock-mechanism","title":"Signal Interlock Mechanism","text":""},{"location":"sys/kern/signals/#sigirefs-signal-delivery-interlock","title":"<code>sigirefs</code> \u2014 Signal Delivery Interlock","text":"<p>Source: (referenced throughout <code>kern_sig.c</code>)</p> <p>Prevents races between signal delivery and operations that change the signal mask (sigsuspend, ppoll, pselect):</p> <pre><code>void sigirefs_hold(struct proc *p);     // Increment p-&gt;p_sigirefs\nvoid sigirefs_drop(struct proc *p);     // Decrement p-&gt;p_sigirefs\nvoid sigirefs_wait(struct proc *p);     // Wait for p-&gt;p_sigirefs == 0\n</code></pre> <p>Problem: Without interlock: 1. Thread calls <code>sigsuspend()</code>, changes mask, about to sleep 2. Signal arrives, sees old mask, delivered to process list instead of LWP 3. Thread sleeps, signal not delivered to correct LWP</p> <p>Solution: 1. <code>sigsuspend()</code> calls <code>sigirefs_wait()</code> before sleeping \u2192 waits for pending signal delivery to complete 2. <code>lwpsignal()</code> calls <code>sigirefs_hold()</code> before checking mask \u2192 prevents mask changes during delivery 3. After delivery, <code>sigirefs_drop()</code> releases hold</p> <p>This ensures mask changes and signal delivery don't race.</p>"},{"location":"sys/kern/signals/#syscall-signal-interruption","title":"Syscall Signal Interruption","text":""},{"location":"sys/kern/signals/#iscaught-check-for-interrupting-signal","title":"<code>iscaught</code> \u2014 Check for Interrupting Signal","text":"<p>Source: <code>kern_sig.c:1948-1962</code></p> <p>Check if system call should be interrupted:</p> <pre><code>int iscaught(struct lwp *lp);\n</code></pre> <p>Returns: - <code>EINTR</code> \u2014 Signal interrupts syscall (signal in <code>ps_sigintr</code>) - <code>ERESTART</code> \u2014 Syscall should be restarted (signal not in <code>ps_sigintr</code>, i.e., SA_RESTART) - <code>EWOULDBLOCK</code> \u2014 No signal pending</p> <p>Used by: Long-running syscalls (read, write, sleep) to check for pending signals.</p> <p>Flow: 1. Call <code>CURSIG(lp)</code> \u2192 <code>issignal()</code> 2. If signal found \u2192 check if in <code>ps_sigintr</code> set 3. Return appropriate error code 4. Syscall code checks error and either aborts or restarts</p>"},{"location":"sys/kern/signals/#dragonfly-specific-extensions","title":"DragonFly-Specific Extensions","text":""},{"location":"sys/kern/signals/#checkpoint-signals","title":"Checkpoint Signals","text":"<p>Source: <code>kern_sig.c:162-163</code></p> <p>DragonFly provides checkpoint/resume functionality via signals:</p> <ul> <li>SIGCKPT (<code>32</code>) \u2014 SA_CKPT property, triggers <code>checkpoint_signal_handler()</code></li> <li>SIGCKPTEXIT (<code>33</code>) \u2014 SA_CKPT | SA_KILL, checkpoint and exit</li> </ul> <p>Use case: Save process state to disk, allowing resume later or on different machine.</p>"},{"location":"sys/kern/signals/#cross-cpu-signal-delivery","title":"Cross-CPU Signal Delivery","text":"<p>DragonFly's LWKT threading and per-CPU design requires IPI-based signal notification:</p> <ul> <li><code>lwp_signotify()</code> checks if target LWP is on remote CPU</li> <li>Sends IPI via <code>lwkt_send_ipiq(gd, lwp_signotify_remote, lp)</code></li> <li>Remote CPU calls <code>signotify()</code> or <code>lwkt_schedule()</code> to wake thread</li> </ul> <p>Forwarding: If thread migrates to another CPU before IPI delivery, IPI is forwarded again (with LWPHOLD reference).</p>"},{"location":"sys/kern/signals/#thread-specific-signals","title":"Thread-Specific Signals","text":"<p><code>lwp_kill()</code> syscall allows targeting specific thread (LWP):</p> <pre><code>int lwp_kill(pid_t pid, lwpt_t tid, int sig);\n</code></pre> <p>Restriction: <code>tid</code> cannot be -1 (use <code>kill()</code> for process signals).</p> <p>Use case: Send signal to specific thread in multi-threaded process (e.g., sampling profiler).</p>"},{"location":"sys/kern/signals/#interaction-with-other-subsystems","title":"Interaction with Other Subsystems","text":""},{"location":"sys/kern/signals/#signal-and-fork","title":"Signal and Fork","text":"<p>Source: <code>kern_fork.c</code></p> <p>On <code>fork()</code>: 1. Child inherits <code>p_sigacts</code> (shared, refcounted) 2. Child's <code>p_siglist</code> cleared (no pending signals) 3. Child's <code>lp-&gt;lwp_siglist</code> cleared 4. Child's <code>lp-&gt;lwp_sigmask</code> inherited from parent</p> <p>On <code>rfork(RFPROC)</code>: - Same as fork</p> <p>On <code>rfork(RFTHREAD)</code>: - <code>p_sigacts</code> shared with parent - <code>lwp_sigmask</code> inherited - Signals can target specific thread</p>"},{"location":"sys/kern/signals/#signal-and-exec","title":"Signal and Exec","text":"<p>Source: <code>kern_exec.c</code>, calls <code>execsigs()</code></p> <p>On <code>execve()</code>: 1. Reset all caught signals to SIG_DFL 2. Held signals remain held (preserved in <code>lwp_sigmask</code>) 3. Reset signal stack to user stack 4. Clear SA_NOCLDWAIT / SA_NOCLDSTOP 5. Reset SIGCHLD to SIG_DFL if ignored</p> <p>Rationale: New program image shouldn't inherit old program's signal handlers.</p>"},{"location":"sys/kern/signals/#signal-and-exit","title":"Signal and Exit","text":"<p>Source: <code>kern_exit.c</code></p> <p>On <code>exit()</code>: 1. Parent receives SIGCHLD (unless PS_NOCLDSTOP or process stopped) 2. If SA_NOCLDWAIT set \u2192 child immediately reaped (no zombie) 3. Otherwise \u2192 child becomes zombie, <code>wait()</code> retrieves exit status 4. If signal caused exit \u2192 <code>p-&gt;p_xstat</code> contains signal number | WCOREFLAG</p>"},{"location":"sys/kern/signals/#signal-and-ptrace","title":"Signal and Ptrace","text":"<p>Source: <code>kern_sig.c:2060-2130</code></p> <p>When process traced (P_TRACED): 1. <code>issignal()</code> stops process on every signal 2. Parent debugger notified 3. Debugger can:    - Inspect signal number in <code>p-&gt;p_xstat</code>    - Modify signal (change <code>p-&gt;p_xstat</code>)    - Clear signal (set <code>p-&gt;p_xstat = 0</code>)    - Continue process with <code>PTRACE_CONT</code></p> <p>Use case: Debuggers intercept signals for single-stepping, breakpoints, etc.</p>"},{"location":"sys/kern/signals/#key-algorithms","title":"Key Algorithms","text":""},{"location":"sys/kern/signals/#signal-delivery-decision-tree","title":"Signal Delivery Decision Tree","text":"<pre><code>lwpsignal(p, lp, sig):\n    1. Is signal ignored (p_sigignore)?\n       \u2192 Yes: Discard (notify kqueue)\n       \u2192 No: Continue\n\n    2. Is signal a CONT (SA_CONT)?\n       \u2192 Yes: Clear pending STOP signals\n       \u2192 If process stopped: unstop process\n       \u2192 Continue\n\n    3. Is signal a STOP (SA_STOP)?\n       \u2192 Yes: Clear pending CONT signals\n       \u2192 If default action: stop process, done\n       \u2192 Otherwise: Continue\n\n    4. Is process already stopped (SSTOP)?\n       \u2192 Yes: Add signal to pending list\n       \u2192 If SIGKILL: unstop process\n       \u2192 If SIGCONT and caught: unstop process\n       \u2192 Otherwise: done\n\n    5. Find target LWP:\n       \u2192 If lp == NULL: find_lwp_for_signal()\n       \u2192 If all LWPs mask signal: deliver to process\n\n    6. Add signal to lwp_siglist\n    7. Call lwp_signotify(lp) to wake thread\n</code></pre>"},{"location":"sys/kern/signals/#signal-dispatch-loop","title":"Signal Dispatch Loop","text":"<pre><code>User process trap/syscall return:\n    while (sig = CURSIG(lp)):\n        postsig(sig):\n            if action == SIG_DFL:\n                sigexit(lp, sig)  // Does not return\n            else:\n                Setup signal trampoline\n                Call (*sv_sendsig)(action, sig, &amp;mask, code)\n                Return to userland at handler\n</code></pre> <p>Handler executes, calls <code>sigreturn()</code>, kernel restores original mask and PC, returns to interrupted code.</p>"},{"location":"sys/kern/signals/#performance-considerations","title":"Performance Considerations","text":""},{"location":"sys/kern/signals/#signal-mask-operations","title":"Signal Mask Operations","text":"<p>Signal masks use 128-bit sets (_SIG_WORDS = 4), operations are: - <code>SIGADDSET</code> / <code>SIGDELSET</code> \u2014 O(1) bit operations - <code>SIGEMPTYSET</code> / <code>SIGFILLSET</code> \u2014 O(_SIG_WORDS) loop (4 iterations) - <code>SIGSETOR</code> / <code>SIGSETNAND</code> \u2014 O(_SIG_WORDS) loop</p> <p>Optimization: Atomic variants avoid lock overhead in <code>p-&gt;p_siglist</code>: - <code>SIGADDSET_ATOMIC</code> \u2014 <code>atomic_set_int()</code> - <code>SIGDELSET_ATOMIC</code> \u2014 <code>atomic_clear_int()</code></p>"},{"location":"sys/kern/signals/#signal-delivery-fast-path","title":"Signal Delivery Fast Path","text":"<p><code>issignal()</code> has fast path without token: <pre><code>mask = lwp_sigpend(lp);\nSIGSETNAND(mask, lp-&gt;lwp_sigmask);\nif (SIGISEMPTY(mask))\n    return (0);  // No signal, no token acquired\n</code></pre></p> <p>Only acquires <code>p-&gt;p_token</code> if signal found in process list.</p>"},{"location":"sys/kern/signals/#cross-cpu-delivery","title":"Cross-CPU Delivery","text":"<p><code>lwp_signotify()</code> checks if target LWP is local before sending IPI: <pre><code>if (dtd-&gt;td_gd == mycpu) {\n    setrunnable(lp);  // Local, no IPI\n} else {\n    lwkt_send_ipiq(dtd-&gt;td_gd, lwp_signotify_remote, lp);  // Remote IPI\n}\n</code></pre></p> <p>Avoids IPI overhead for same-CPU signals.</p>"},{"location":"sys/kern/signals/#debugging-and-observability","title":"Debugging and Observability","text":""},{"location":"sys/kern/signals/#signal-logging","title":"Signal Logging","text":"<p>Sysctl: - <code>kern.logsigexit</code> \u2014 Log signal-caused exits (default 1) - <code>kern.coredump</code> \u2014 Enable/disable core dumps (default 1) - <code>kern.sugid_coredump</code> \u2014 Allow setuid/setgid core dumps (default 0)</p> <p>Ktrace: - <code>KTR_PSIG</code> \u2014 Trace signal delivery via <code>ktrace()</code> - Logs: signal number, action, mask, code</p> <p>Log format: <pre><code>pid %d (%s), uid %d: exited on signal %d (core dumped)\n</code></pre></p>"},{"location":"sys/kern/signals/#kqueue-monitoring","title":"Kqueue Monitoring","text":"<p>Register kevent to monitor process signals: <pre><code>struct kevent kev;\nEV_SET(&amp;kev, pid, EVFILT_PROC, EV_ADD, NOTE_SIGNAL, 0, NULL);\nkevent(kq, &amp;kev, 1, NULL, 0, NULL);\n</code></pre></p> <p>Wakes when any signal delivered to process.</p>"},{"location":"sys/kern/signals/#summary","title":"Summary","text":"<p>DragonFly BSD's signal implementation provides:</p> <ol> <li>POSIX compliance with extensions for per-thread signals and checkpointing</li> <li>Efficient signal delivery with fast-path checks and cross-CPU IPI notification</li> <li>Fine-grained control via SA_* flags (RESTART, RESETHAND, NODEFER, SIGINFO)</li> <li>Process control via STOP/CONT signals with accurate thread stopping</li> <li>Ptrace integration for debugging with signal interception</li> <li>Core dump generation with configurable paths and security controls</li> <li>Kqueue integration for event-based signal monitoring</li> </ol> <p>Key design principles:</p> <ul> <li>Per-thread masks allow fine-grained control in multi-threaded processes</li> <li>Copy-on-write sigacts optimize fork() performance</li> <li>Token-based locking ensures atomicity without global signal lock</li> <li>IPI-based notification supports per-CPU LWKT threading model</li> <li>Double-check pattern optimizes fast path (check without lock, recheck with lock)</li> <li>Signal interlock (<code>sigirefs</code>) prevents races with mask-changing operations</li> </ul> <p>The signal subsystem forms the foundation for: - Job control (shell background/foreground) - Process monitoring (parent notification of child events) - Exception handling (trap delivery to user handlers) - Debugging (ptrace signal interception) - Inter-process communication (asynchronous event notification)</p>"},{"location":"sys/kern/synchronization/","title":"Synchronization Primitives","text":"<p>This document describes the synchronization primitives used throughout the DragonFly BSD kernel to coordinate access to shared resources and ensure data consistency across multiple CPUs and threads.</p>"},{"location":"sys/kern/synchronization/#overview","title":"Overview","text":"<p>DragonFly BSD provides a comprehensive hierarchy of synchronization primitives, each optimized for specific use cases. Understanding when to use each primitive is critical for writing correct and performant kernel code.</p> <p>The synchronization primitives, ordered from lowest to highest level:</p> <ol> <li>Tokens - DragonFly's unique per-CPU serialization mechanism (see LWKT Threading)</li> <li>Spinlocks - Low-level busy-wait locks with shared/exclusive support</li> <li>Mutexes (mtx) - Fast persistent locks with sleep capability and async support</li> <li>Lockmgr Locks - Traditional BSD shared/exclusive locks with complex features</li> <li>Condition Variables - Thread coordination using wait/signal patterns</li> <li>Reference Counts - Atomic reference counting for object lifecycle management</li> <li>System References (sysref) - Advanced reference counting with objcache integration</li> </ol>"},{"location":"sys/kern/synchronization/#key-concepts","title":"Key Concepts","text":""},{"location":"sys/kern/synchronization/#dragonflys-unique-approach","title":"DragonFly's Unique Approach","text":"<p>DragonFly BSD differs from traditional BSD and Linux kernels in its synchronization philosophy:</p> <ul> <li>Token-based design: Most high-level kernel operations use LWKT tokens for serialization rather than traditional locks</li> <li>Per-CPU scheduling: Threads are scheduled independently on each CPU, reducing global synchronization overhead</li> <li>Reduced lock contention: By favoring tokens and per-CPU data structures, DragonFly minimizes cache line bouncing</li> </ul>"},{"location":"sys/kern/synchronization/#token-vs-lock-decision-tree","title":"Token vs. Lock Decision Tree","text":"<p>Use Tokens when: - Protecting high-level subsystem state (VFS, VM, etc.) - The critical section may block or call complex functions - Lock ordering is difficult to maintain - The code is not performance-critical</p> <p>Use Spinlocks when: - Critical section is very short (a few instructions) - Cannot sleep or call blocking functions - Need shared (reader) locks for concurrent access - Performance is critical (e.g., per-packet network processing)</p> <p>Use Mutexes when: - Need to block across long-running operations - Need asynchronous lock acquisition with callbacks - Want recursive locking capability - Need to hold lock across function calls that may block</p> <p>Use Lockmgr when: - Need complex lock operations (upgrade, downgrade, cancellation) - Interfacing with traditional BSD code - Need precise control over lock priority and behavior</p>"},{"location":"sys/kern/synchronization/#synchronization-primitives_1","title":"Synchronization Primitives","text":""},{"location":"sys/kern/synchronization/#spinlocks","title":"Spinlocks","text":"<p>Spinlocks are the lowest-level synchronization primitive, providing both exclusive and shared locking with busy-waiting.</p> <p>Source Files: - <code>sys/kern/kern_spinlock.c</code> - Implementation - <code>sys/sys/spinlock.h</code> - Data structures - <code>sys/sys/spinlock2.h</code> - Inline functions</p> <p>Data Structure:</p> <pre><code>struct spinlock {\n    int lock;        /* main spinlock */\n    int update;      /* update counter */\n};\n</code></pre> <p>Lock Field Encoding: - Bits 0-19: Reference count - Bit 31: <code>SPINLOCK_SHARED</code> flag - Bits 20-30: <code>SPINLOCK_EXCLWAIT</code> counter for exclusive waiters</p> <p>Key Functions:</p> <ul> <li><code>spin_init(struct spinlock *spin, const char *desc)</code> - Initialize spinlock</li> <li><code>spin_lock(struct spinlock *spin)</code> - Acquire exclusive lock (sys/kern/kern_spinlock.c:166)</li> <li><code>spin_unlock(struct spinlock *spin)</code> - Release exclusive lock</li> <li><code>spin_lock_shared(struct spinlock *spin)</code> - Acquire shared lock (sys/kern/kern_spinlock.c:276)</li> <li><code>spin_unlock_shared(struct spinlock *spin)</code> - Release shared lock</li> </ul> <p>Critical Implementation Details:</p> <p>Exclusive Lock Acquisition (sys/kern/kern_spinlock.c:166-256): - Increments lock with <code>atomic_fetchadd_int()</code> - On contention, sets <code>SPINLOCK_EXCLWAIT</code> to gain priority - Transfers high bits (EXCLWAIT counter) to low bits when acquiring - Uses exponential backoff to reduce cache bus traffic - Employs TSC-windowing to distribute CPU load</p> <p>Shared Lock Acquisition (sys/kern/kern_spinlock.c:276-372): - Sets <code>SPINLOCK_SHARED</code> flag when granted - Gives priority to exclusive waiters (EXCLWAIT) - Uses TSC-windowing to occasionally bypass EXCLWAIT priority to prevent starvation - No exponential backoff (would hurt shared lock performance)</p> <p>Performance Optimizations: - TSC (timestamp counter) windowing distributes shared lock attempts across CPUs - Exponential backoff prevents cache bus armageddon on multi-socket systems - Automatic downgrade to RDTSC polling in VM guests</p> <p>When to Use: - Protecting per-CPU data structures accessed from multiple contexts - Very short critical sections (microseconds) - Cannot use tokens (e.g., in interrupt context) - Need reader/writer concurrency</p> <p>Restrictions: - Cannot sleep or block - Cannot call functions that might block - Must not hold across function calls unless guaranteed not to block - Keep critical sections extremely short</p>"},{"location":"sys/kern/synchronization/#mutexes-mtx","title":"Mutexes (mtx)","text":"<p>Mutexes provide persistent locks that can be held across blocking operations, with support for asynchronous acquisition and callbacks.</p> <p>Source Files: - <code>sys/kern/kern_mutex.c</code> - Implementation (sys/kern/kern_mutex.c:1-1159) - <code>sys/sys/mutex.h</code> - Data structures - <code>sys/sys/mutex2.h</code> - Inline functions</p> <p>Data Structures:</p> <pre><code>struct mtx {\n    volatile u_int  mtx_lock;     /* lock state */\n    uint32_t        mtx_flags;    /* flags */\n    struct thread   *mtx_owner;   /* exclusive owner */\n    mtx_link_t      *mtx_exlink;  /* exclusive wait list */\n    mtx_link_t      *mtx_shlink;  /* shared wait list */\n    const char      *mtx_ident;   /* identifier */\n} __cachealign;\n\nstruct mtx_link {\n    struct mtx_link *next;\n    struct mtx_link *prev;\n    struct thread   *owner;\n    int             state;\n    void            (*callback)(struct mtx_link *, void *arg, int error);\n    void            *arg;\n};\n</code></pre> <p>Lock State Encoding: - Bit 31 (<code>MTX_EXCLUSIVE</code>): Exclusive lock flag - Bit 30 (<code>MTX_SHWANTED</code>): Shared waiters present - Bit 29 (<code>MTX_EXWANTED</code>): Exclusive waiters present - Bit 28 (<code>MTX_LINKSPIN</code>): Link list manipulation in progress - Bits 0-27: Reference count</p> <p>Key Functions:</p> <ul> <li><code>mtx_init(mtx_t *mtx, const char *ident)</code> - Initialize mutex</li> <li><code>mtx_lock(mtx_t *mtx)</code> - Acquire exclusive lock (sys/kern/kern_mutex.c:202-224)</li> <li><code>mtx_lock_sh(mtx_t *mtx)</code> - Acquire shared lock (sys/kern/kern_mutex.c:360-376)</li> <li><code>mtx_unlock(mtx_t *mtx)</code> - Release lock (sys/kern/kern_mutex.c:630-735)</li> <li><code>mtx_lock_ex_try(mtx_t *mtx)</code> - Try exclusive lock without blocking (sys/kern/kern_mutex.c:486-517)</li> <li><code>mtx_downgrade(mtx_t *mtx)</code> - Convert exclusive to shared (sys/kern/kern_mutex.c:549-583)</li> <li><code>mtx_upgrade_try(mtx_t *mtx)</code> - Try shared to exclusive upgrade (sys/kern/kern_mutex.c:596-623)</li> <li><code>mtx_spinlock(mtx_t *mtx)</code> - Acquire as spinlock (sys/kern/kern_mutex.c:381-413)</li> <li><code>mtx_lock_ex_link(mtx_t *mtx, mtx_link_t *link, int flags, int to)</code> - Async lock with callback</li> </ul> <p>Key Features:</p> <p>Exclusive Priority (sys/kern/kern_mutex.c:120-180): - Exclusive requests set <code>MTX_EXWANTED</code> to prevent new shared locks - Once EXWANTED is set, new shared requests must wait - Prevents shared lock starvation of exclusive requests</p> <p>Asynchronous Locking (sys/kern/kern_mutex.c:169-200): - Caller provides <code>mtx_link_t</code> structure with callback - If lock cannot be acquired immediately, link is queued - Callback invoked when lock is granted - Allows lock acquisition without blocking current thread</p> <p>Link Management (sys/kern/kern_mutex.c:749-876): - Exclusive and shared waiters maintained in separate circular lists - <code>MTX_LINKSPIN</code> prevents concurrent link list manipulation - Link lists allow precise wakeup control and priority ordering</p> <p>When to Use: - Protecting data structures that require blocking operations - Need to hold lock across I/O or memory allocation - Want asynchronous lock acquisition - Need recursive locking capability - Interfacing with code that may block</p> <p>Restrictions: - Heavier weight than spinlocks - Not suitable for very short critical sections - Exclusive lock holder must eventually release</p>"},{"location":"sys/kern/synchronization/#lockmgr-locks","title":"Lockmgr Locks","text":"<p>Lockmgr locks are traditional BSD shared/exclusive locks with extensive features including upgrades, downgrades, timeouts, and cancellation.</p> <p>Source Files: - <code>sys/kern/kern_lock.c</code> - Implementation (sys/kern/kern_lock.c:1-1483) - <code>sys/sys/lock.h</code> - Data structures and API</p> <p>Data Structure:</p> <pre><code>struct lock {\n    u_int           lk_flags;      /* flags */\n    int             lk_timo;       /* timeout */\n    uint64_t        lk_count;      /* state and counts */\n    const char      *lk_wmesg;     /* wait message */\n    struct thread   *lk_lockholder; /* exclusive holder */\n};\n</code></pre> <p>Count Field Encoding (64-bit): - Bit 30 (<code>LKC_EXREQ</code>): Exclusive request pending - Bit 29 (<code>LKC_SHARED</code>): Shared lock(s) granted - Bit 28 (<code>LKC_UPREQ</code>): Upgrade request pending - Bit 27 (<code>LKC_EXREQ2</code>): Multiple exclusive waiters - Bit 26 (<code>LKC_CANCEL</code>): Cancellation active - Bits 0-25 (<code>LKC_XMASK</code>): Exclusive count - Bits 32-63 (<code>LKC_SMASK</code>): Shared count (shifted by 32)</p> <p>Key Functions:</p> <ul> <li><code>lockinit(struct lock *lkp, const char *wmesg, int timo, int flags)</code> - Initialize lock</li> <li><code>lockmgr(struct lock *lkp, u_int flags)</code> - Main lock operation dispatcher</li> <li><code>lockmgr_shared(struct lock *lkp, u_int flags)</code> - Acquire shared lock (sys/kern/kern_lock.c:106-278)</li> <li><code>lockmgr_exclusive(struct lock *lkp, u_int flags)</code> - Acquire exclusive lock (sys/kern/kern_lock.c:283-489)</li> <li><code>lockmgr_upgrade(struct lock *lkp, u_int flags)</code> - Upgrade shared to exclusive (sys/kern/kern_lock.c:575-733)</li> <li><code>lockmgr_downgrade(struct lock *lkp, u_int flags)</code> - Downgrade exclusive to shared (sys/kern/kern_lock.c:497-554)</li> <li><code>lockmgr_release(struct lock *lkp, u_int flags)</code> - Release lock (sys/kern/kern_lock.c:741-966)</li> </ul> <p>Lock Operation Flags:</p> <ul> <li><code>LK_SHARED</code> - Acquire shared lock</li> <li><code>LK_EXCLUSIVE</code> - Acquire exclusive lock</li> <li><code>LK_UPGRADE</code> - Upgrade shared to exclusive (may lose lock temporarily)</li> <li><code>LK_EXCLUPGRADE</code> - Upgrade without releasing (fails if contended)</li> <li><code>LK_DOWNGRADE</code> - Downgrade exclusive to shared</li> <li><code>LK_RELEASE</code> - Release lock</li> <li><code>LK_NOWAIT</code> - Don't sleep, return EBUSY</li> <li><code>LK_CANCELABLE</code> - Lock request can be canceled</li> <li><code>LK_TIMELOCK</code> - Use lock's timeout value</li> <li><code>LK_PCATCH</code> - Catch signals during sleep</li> </ul> <p>Critical Implementation Details:</p> <p>Shared Lock Acquisition (sys/kern/kern_lock.c:106-278): - Blocks if <code>LKC_EXREQ</code> or <code>LKC_UPREQ</code> is set (unless <code>TDF_DEADLKTREAT</code>) - Increments shared count (<code>LKC_SCOUNT</code>) - Waits for <code>LKC_SHARED</code> flag to be set - If <code>undo_shreq()</code> races to zero, may grant UPREQ or EXREQ</p> <p>Exclusive Lock Acquisition (sys/kern/kern_lock.c:283-489): - Sets <code>LKC_EXREQ</code> to block new shared/upgrade requests - If can't set EXREQ (lock held exclusively), sets <code>LKC_EXREQ2</code> aggregation bit - Waits for EXREQ to be cleared (granted) - Granting thread sets <code>lk_lockholder</code> and count</p> <p>Upgrade Operations (sys/kern/kern_lock.c:575-733): - Sets <code>LKC_UPREQ</code> to request upgrade - If only holder (<code>LKC_SMASK == LKC_SCOUNT</code>), immediately converts to exclusive - Otherwise, waits for last shared release to grant upgrade - <code>LK_EXCLUPGRADE</code> fails immediately if another UPREQ or EXREQ exists - Regular <code>LK_UPGRADE</code> falls back to release+acquire if contended</p> <p>Lock Cancellation (sys/kern/kern_lock.c:977-1022): - Exclusive holder can call <code>lockmgr_cancel_beg()</code> to set <code>LKC_CANCEL</code> - All pending/future <code>LK_CANCELABLE</code> requests return <code>ENOLCK</code> - Used to abort operations on locked structures (e.g., vnode reclaim) - Cleared automatically on final release or via <code>lockmgr_cancel_end()</code></p> <p>Priority Handling: - Exclusive requests (EXREQ) have priority over new shared requests - Upgrade requests (UPREQ) have priority over exclusive requests - UPREQ &gt; EXREQ &gt; new shared locks (see <code>undo_shreq()</code> at sys/kern/kern_lock.c:1039-1091)</p> <p>When to Use: - VFS layer and vnode operations (traditional usage) - Need upgrade/downgrade without releasing - Need lock cancellation capability - Interfacing with legacy BSD code - Complex locking scenarios requiring fine control</p> <p>Restrictions: - More complex and heavier than tokens or mutexes - Upgrade operations can fail or temporarily release lock - Cannot use from interrupt context</p>"},{"location":"sys/kern/synchronization/#condition-variables","title":"Condition Variables","text":"<p>Condition variables provide wait/signal coordination between threads, allowing threads to sleep until a condition becomes true.</p> <p>Source Files: - <code>sys/kern/kern_condvar.c</code> - Implementation (sys/kern/kern_condvar.c:1-97) - <code>sys/sys/condvar.h</code> - API</p> <p>Data Structure:</p> <pre><code>struct cv {\n    struct spinlock cv_lock;    /* protects cv_waiters */\n    int             cv_waiters; /* number of waiters */\n    const char      *cv_desc;   /* description */\n};\n</code></pre> <p>Key Functions:</p> <ul> <li><code>cv_init(struct cv *c, const char *desc)</code> - Initialize condition variable (sys/kern/kern_condvar.c:6-12)</li> <li><code>cv_destroy(struct cv *c)</code> - Destroy condition variable</li> <li><code>cv_wait(struct cv *c, struct lock *lk)</code> - Wait with lockmgr lock</li> <li><code>cv_wait_sig(struct cv *c, struct lock *lk)</code> - Wait with signal catching</li> <li><code>cv_timedwait(struct cv *c, struct lock *lk, int timo)</code> - Wait with timeout</li> <li><code>cv_mtx_wait(struct cv *c, struct mtx *mtx)</code> - Wait with mutex</li> <li><code>cv_signal(struct cv *c)</code> - Wake one waiter (sys/kern/kern_condvar.c:75-90)</li> <li><code>cv_broadcast(struct cv *c)</code> - Wake all waiters</li> <li><code>cv_has_waiters(const struct cv *c)</code> - Check if waiters present</li> </ul> <p>Implementation Details:</p> <p>The implementation is deliberately simple:</p> <ol> <li><code>cv_wait()</code> atomically:</li> <li>Increments <code>cv_waiters</code> under <code>cv_lock</code> (sys/kern/kern_condvar.c:34-36)</li> <li>Calls <code>tsleep_interlock()</code> to prepare to sleep</li> <li>Releases the associated lock (lockmgr or mutex)</li> <li> <p>Sleeps on the cv address</p> </li> <li> <p><code>cv_signal()</code> atomically:</p> </li> <li>Decrements <code>cv_waiters</code> under <code>cv_lock</code></li> <li> <p>Calls <code>wakeup_one()</code> if waiters existed (sys/kern/kern_condvar.c:86-88)</p> </li> <li> <p><code>cv_broadcast()</code> atomically:</p> </li> <li>Sets <code>cv_waiters</code> to zero</li> <li>Calls <code>wakeup()</code> to wake all (sys/kern/kern_condvar.c:81-84)</li> </ol> <p>Mutex vs. Lock Versions: - <code>cv_wait(cv, lock)</code> uses <code>lksleep()</code> - releases lockmgr lock - <code>cv_mtx_wait(cv, mtx)</code> uses <code>mtxsleep()</code> - releases mutex (sys/kern/kern_condvar.c:50-72) - Both re-acquire their respective lock before returning</p> <p>When to Use: - Producer/consumer patterns - Thread coordination (wait for condition to become true) - Implementing wait queues - Event notification between threads</p> <p>Typical Pattern:</p> <pre><code>/* Waiter thread */\nmtx_lock(&amp;resource_mtx);\nwhile (!condition_is_true) {\n    cv_wait(&amp;resource_cv, &amp;resource_mtx);\n}\n/* condition is now true and lock is held */\ndo_work();\nmtx_unlock(&amp;resource_mtx);\n\n/* Signaler thread */\nmtx_lock(&amp;resource_mtx);\nmake_condition_true();\ncv_signal(&amp;resource_cv);  /* or cv_broadcast() */\nmtx_unlock(&amp;resource_mtx);\n</code></pre> <p>Restrictions: - Must hold associated lock when calling cv_wait() - Lock is automatically released during sleep - Lock is automatically re-acquired before return - Must hold lock when calling cv_signal/cv_broadcast (not strictly required but recommended)</p>"},{"location":"sys/kern/synchronization/#reference-counting","title":"Reference Counting","text":"<p>Reference counting provides atomic lifecycle management for kernel objects.</p> <p>Source Files: - <code>sys/kern/kern_refcount.c</code> - Implementation (sys/kern/kern_refcount.c:1-78) - <code>sys/sys/refcount.h</code> - Inline functions and API</p> <p>Key Functions:</p> <ul> <li><code>refcount_init(volatile u_int *countp, u_int value)</code> - Initialize counter</li> <li><code>refcount_acquire(volatile u_int *countp)</code> - Add reference (sys/sys/refcount.h:48-52)</li> <li><code>refcount_acquire_n(volatile u_int *countp, u_int n)</code> - Add n references</li> <li><code>refcount_release(volatile u_int *countp)</code> - Drop reference, returns TRUE on last (sys/sys/refcount.h:60-64)</li> <li><code>refcount_release_n(volatile u_int *countp, u_int n)</code> - Drop n references</li> <li><code>refcount_release_wakeup(volatile u_int *countp)</code> - Drop with wakeup support (sys/sys/refcount.h:86-98)</li> <li><code>refcount_wait(volatile u_int *countp, const char *wstr)</code> - Wait for count to reach zero</li> </ul> <p>Implementation Details:</p> <p>All operations use atomic instructions:</p> <ul> <li>Acquire: <code>atomic_add_acq_int(countp, 1)</code></li> <li>Release: <code>atomic_fetchadd_int(countp, -1)</code>, returns old value</li> <li>Release returns TRUE if old value was 1 (last reference)</li> </ul> <p>Waiting Support (<code>REFCNTF_WAITING</code>):</p> <ul> <li>Bit 30 of counter is <code>REFCNTF_WAITING</code> flag</li> <li><code>refcount_wait()</code> sets this flag atomically (sys/kern/kern_refcount.c:58-77)</li> <li><code>refcount_release_wakeup()</code> checks for flag and calls <code>wakeup()</code> on 1-&gt;0 transition (sys/sys/refcount.h:86-98)</li> <li>Allows threads to sleep waiting for count to drop to zero</li> </ul> <p>When to Use: - Managing object lifetimes (vnodes, vm_objects, buffers, etc.) - Shared ownership of kernel structures - Preventing premature deallocation - Lockless lifecycle management</p> <p>Typical Pattern:</p> <pre><code>/* Object creation */\nstruct myobj *obj = kmalloc(...);\nrefcount_init(&amp;obj-&gt;refcnt, 1);  /* creator holds reference */\n\n/* Share object */\nrefcount_acquire(&amp;obj-&gt;refcnt);\npass_to_another_subsystem(obj);\n\n/* Release object */\nif (refcount_release(&amp;obj-&gt;refcnt)) {\n    /* Last reference, free object */\n    cleanup_obj(obj);\n    kfree(obj);\n}\n\n/* Wait for all refs to drop (e.g., during unmount) */\nrefcount_wait(&amp;obj-&gt;refcnt, \"objwait\");\n/* Now safe to free even without lock */\n</code></pre> <p>Restrictions: - Reference count must not overflow (wraps at 2^31-1) - Caller must ensure object validity when acquiring reference - Cannot use to count references if count might be zero (race) - <code>refcount_wait()</code> should be used with <code>refcount_release_wakeup()</code></p>"},{"location":"sys/kern/synchronization/#system-reference-counting-sysref","title":"System Reference Counting (sysref)","text":"<p>System reference counting provides advanced lifecycle management integrated with the objcache allocator and cluster-wide addressing via sysids.</p> <p>Source Files: - <code>sys/kern/kern_sysref.c</code> - Implementation (sys/kern/kern_sysref.c:1-375) - <code>sys/sys/sysref2.h</code> - API and inline functions</p> <p>Data Structures:</p> <pre><code>struct sysref {\n    sysid_t             sysid;      /* cluster-wide unique ID */\n    int                 refcnt;     /* reference count */\n    u_int               flags;      /* SRF_* flags */\n    struct sysref_class *srclass;   /* class descriptor */\n    RB_ENTRY(sysref)    rbnode;     /* red-black tree node */\n};\n\nstruct sysref_class {\n    const char          *name;\n    malloc_type_t       mtype;      /* malloc type */\n    size_t              objsize;    /* object size */\n    size_t              offset;     /* sysref offset in object */\n    size_t              nom_cache;  /* nominal cache size */\n    u_int               flags;\n    objcache_t          *oc;        /* objcache */\n    struct sysref_ops   ops;        /* callbacks */\n    boolean_t           (*ctor)(void *, void *, int);\n    void                (*dtor)(void *, void *);\n};\n</code></pre> <p>Lifecycle States (refcnt):</p> <ul> <li>-0x40000000: Initialization in progress (not yet active)</li> <li>Positive: Active, each reference is +1</li> <li>-0x40000000: Termination in progress</li> <li>0: Freed (not accessible)</li> </ul> <p>Key Functions:</p> <ul> <li><code>sysref_alloc(struct sysref_class *srclass)</code> - Allocate object (sys/kern/kern_sysref.c:132-179)</li> <li><code>sysref_init(struct sysref *sr, struct sysref_class *srclass)</code> - Manual init for static objects</li> <li><code>sysref_activate(struct sysref *sr)</code> - Activate object (sys/kern/kern_sysref.c:273-286)</li> <li><code>sysref_get(struct sysref *sr)</code> - Acquire reference (inline in sys/sysref2.h)</li> <li><code>sysref_put(struct sysref *sr)</code> - Release reference (inline, calls <code>_sysref_put()</code> on special cases)</li> <li><code>sysref_lookup(sysid_t sysid)</code> - Lookup by sysid</li> </ul> <p>Implementation Details:</p> <p>Objcache Integration (sys/kern/kern_sysref.c:193-263):</p> <ul> <li>Constructor allocates sysid and inserts into per-CPU red-black tree</li> <li>Sysid embeds CPU number in low bits for locality</li> <li>If sysid not accessed, destructor just removes from tree (fast path)</li> <li>If sysid accessed (<code>SRF_SYSIDUSED</code>), destructor fully destroys via <code>objcache_dtor()</code></li> </ul> <p>Reference Count Lifecycle (sys/kern/kern_sysref.c:297-360):</p> <ol> <li>Allocation: refcnt = -0x40000000 (init in progress)</li> <li>Activation: refcnt += 0x40000001 (becomes 1, active)</li> <li>Use: refcnt incremented/decremented normally</li> <li>1-&gt;0 Transition: refcnt set to -0x40000000, <code>terminate()</code> callback invoked</li> <li>Termination: refcnt can still be modified during termination</li> <li>Final: -0x40000000 -&gt; 0, object returned to objcache</li> </ol> <p>Cluster Addressing:</p> <ul> <li>Sysid uniquely identifies object across cluster</li> <li>Per-CPU red-black tree allows O(log n) lookup</li> <li>Enables cluster-wide IPC and resource sharing (future)</li> </ul> <p>When to Use: - Managing heavyweight kernel objects (processes, vnodes, etc.) - Need cluster-wide addressing capability - Want objcache integration for performance - Object lifecycle requires termination callback</p> <p>Restrictions: - More complex than simple reference counting - Requires class descriptor setup - Termination callback must release locks - Not suitable for lightweight objects</p>"},{"location":"sys/kern/synchronization/#userland-mutex-support-umtx","title":"Userland Mutex Support (umtx)","text":"<p>Userland mutex support provides system calls for efficient user-space synchronization with kernel sleep/wakeup.</p> <p>Source Files: - <code>sys/kern/kern_umtx.c</code> - Implementation (sys/kern/kern_umtx.c:1-307)</p> <p>Key Functions:</p> <ul> <li><code>sys_umtx_sleep(const int *ptr, int value, int timeout)</code> - Sleep if *ptr == value (sys/kern/kern_umtx.c:109-241)</li> <li><code>sys_umtx_wakeup(const int *ptr, int count)</code> - Wake waiters on ptr (sys/kern/kern_umtx.c:250-306)</li> </ul> <p>Implementation Details:</p> <p>umtx_sleep() (sys/kern/kern_umtx.c:109-241):</p> <ol> <li>Translates user address to physical address via <code>uservtophys()</code></li> <li>Optionally polls for short duration (4000ns) before sleeping</li> <li>Re-checks value hasn't changed</li> <li>Sleeps on physical address with <code>PDOMAIN_UMTX</code></li> <li>Handles discontinuities (COW, paging) with retries</li> <li>Timeout capped at 2 seconds (caller must retry if needed)</li> </ol> <p>umtx_wakeup() (sys/kern/kern_umtx.c:250-306):</p> <ol> <li>Translates user address to physical address</li> <li>Calls <code>wakeup_domain()</code> to wake waiters</li> <li><code>count == 1</code> wakes one, otherwise wakes all</li> </ol> <p>Performance Optimization:</p> <ul> <li>Polls briefly with RDTSC before sleeping (avoids syscall/context switch for short locks)</li> <li>Uses physical addresses as wait channels (handles memory mapping)</li> <li>Caps timeout to avoid tracking page mapping changes</li> </ul> <p>When to Use: - Implementing pthread mutexes and condition variables in userspace - Futex-like operations (Linux compatibility) - Efficient user-space synchronization</p> <p>Restrictions: - User must handle spurious wakeups - Timeout limited to 2 seconds per call - Address must remain valid during sleep</p>"},{"location":"sys/kern/synchronization/#sleep-queues","title":"Sleep Queues","text":"<p>Sleep queues provide a FreeBSD-compatible API for thread blocking and wakeup.</p> <p>Source Files: - <code>sys/kern/subr_sleepqueue.c</code> - Implementation (sys/kern/subr_sleepqueue.c:1-400+)</p> <p>Data Structures:</p> <pre><code>struct sleepqueue_chain {\n    struct spinlock sc_spin;\n    TAILQ_HEAD(, sleepqueue_wchan) sc_wchead;\n    u_int sc_free_count;\n};\n\nstruct sleepqueue_wchan {\n    TAILQ_ENTRY(sleepqueue_wchan) wc_entry;\n    const void *wc_wchan;\n    struct sleepqueue_chain *wc_sc;\n    u_int wc_refs;\n    int wc_type;\n    u_int wc_blocked[SLEEPQ_NRQUEUES];  /* 2 queues per wchan */\n};\n</code></pre> <p>Key Functions:</p> <ul> <li><code>sleepq_lock(const void *wchan)</code> - Lock wait channel (sys/kern/subr_sleepqueue.c:176-216)</li> <li><code>sleepq_release(const void *wchan)</code> - Unlock wait channel (sys/kern/subr_sleepqueue.c:221-239)</li> <li><code>sleepq_add(const void *wchan, struct lock_object *lock, const char *wmesg, int flags, int queue)</code> - Add thread to queue (sys/kern/subr_sleepqueue.c:251-285)</li> <li><code>sleepq_wait(const void *wchan, int pri)</code> - Sleep until woken (sys/kern/subr_sleepqueue.c:386-400)</li> <li><code>sleepq_timedwait(const void *wchan, int pri)</code> - Sleep with timeout</li> <li><code>sleepq_signal(const void *wchan, int flags, int pri, int queue)</code> - Wake one thread</li> <li><code>sleepq_broadcast(const void *wchan, int flags, int pri, int queue)</code> - Wake all threads</li> </ul> <p>Implementation Details:</p> <ul> <li>Global hash table of wait channels (1024 buckets)</li> <li>Each wait channel has 2 sub-queues (typically for different priorities)</li> <li>Uses DragonFly's <code>tsleep()</code>/<code>wakeup()</code> under the hood</li> <li>Maintains blocked counts for each queue</li> <li>Reference counted wait channel structures</li> </ul> <p>When to Use: - FreeBSD compatibility (e.g., Linux KPI emulation) - Prefer native <code>tsleep()</code>/<code>wakeup()</code> for new DragonFly code - Multiple priority levels needed per wait channel</p>"},{"location":"sys/kern/synchronization/#synchronization-primitive-comparison","title":"Synchronization Primitive Comparison","text":"Primitive Can Sleep Shared Locks Recursive Async Priority Use Case Token Yes No Yes No N/A High-level subsystems (VFS, VM) Spinlock No Yes No No Excl &gt; Shared Short critical sections, per-CPU data Mutex Yes Yes Yes Yes Excl &gt; Shared Moderate critical sections, async I/O Lockmgr Yes Yes Yes No Upgrade &gt; Excl &gt; Shared VFS vnodes, complex lock operations Condvar Yes N/A N/A No N/A Thread coordination, wait/signal Refcount N/A N/A N/A No N/A Object lifecycle management Sysref N/A N/A N/A No N/A Complex object lifecycle, objcache Umtx Yes N/A N/A No N/A Userland synchronization"},{"location":"sys/kern/synchronization/#performance-characteristics","title":"Performance Characteristics","text":"<p>Tokens: - Overhead: Low for uncontended, moderate for contested - Cache impact: Low (per-CPU, no atomic ops when acquired) - Best for: High-level, infrequent operations</p> <p>Spinlocks: - Overhead: Very low - Cache impact: High under contention (busy-waiting) - Best for: Very short critical sections, interrupt context</p> <p>Mutexes: - Overhead: Low-to-moderate - Cache impact: Moderate (atomic ops, link management) - Best for: Medium critical sections with potential blocking</p> <p>Lockmgr: - Overhead: Moderate-to-high - Cache impact: Moderate - Best for: Complex locking scenarios, VFS operations</p>"},{"location":"sys/kern/synchronization/#code-examples","title":"Code Examples","text":""},{"location":"sys/kern/synchronization/#example-1-spinlock-protecting-per-packet-metadata","title":"Example 1: Spinlock Protecting Per-Packet Metadata","text":"<pre><code>struct packet_queue {\n    struct spinlock     pq_spin;\n    TAILQ_HEAD(, packet) pq_list;\n    int                 pq_count;\n};\n\nvoid\nenqueue_packet(struct packet_queue *pq, struct packet *pkt)\n{\n    spin_lock(&amp;pq-&gt;pq_spin);\n    TAILQ_INSERT_TAIL(&amp;pq-&gt;pq_list, pkt, pkt_entry);\n    pq-&gt;pq_count++;\n    spin_unlock(&amp;pq-&gt;pq_spin);\n}\n\nstruct packet *\ndequeue_packet(struct packet_queue *pq)\n{\n    struct packet *pkt;\n\n    spin_lock(&amp;pq-&gt;pq_spin);\n    pkt = TAILQ_FIRST(&amp;pq-&gt;pq_list);\n    if (pkt != NULL) {\n        TAILQ_REMOVE(&amp;pq-&gt;pq_list, pkt, pkt_entry);\n        pq-&gt;pq_count--;\n    }\n    spin_unlock(&amp;pq-&gt;pq_spin);\n\n    return pkt;\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#example-2-mutex-with-blocking-operation","title":"Example 2: Mutex with Blocking Operation","text":"<pre><code>struct device_state {\n    struct mtx          ds_mtx;\n    int                 ds_flags;\n    struct bio_queue    ds_bioq;\n};\n\nvoid\ndevice_submit_bio(struct device_state *ds, struct bio *bio)\n{\n    mtx_lock(&amp;ds-&gt;ds_mtx);\n\n    if (ds-&gt;ds_flags &amp; DSF_SUSPENDED) {\n        /* May need to block waiting for resume */\n        while (ds-&gt;ds_flags &amp; DSF_SUSPENDED) {\n            cv_mtx_wait(&amp;ds-&gt;ds_resume_cv, &amp;ds-&gt;ds_mtx);\n        }\n    }\n\n    /* Enqueue bio */\n    TAILQ_INSERT_TAIL(&amp;ds-&gt;ds_bioq, bio, bio_link);\n\n    /* Kick device (may issue I/O, which blocks) */\n    device_start_io(ds);\n\n    mtx_unlock(&amp;ds-&gt;ds_mtx);\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#example-3-condition-variable-waitsignal","title":"Example 3: Condition Variable Wait/Signal","text":"<pre><code>struct work_queue {\n    struct mtx              wq_mtx;\n    struct cv               wq_cv;\n    TAILQ_HEAD(, work_item) wq_items;\n    int                     wq_shutdown;\n};\n\n/* Worker thread */\nvoid\nworker_thread(struct work_queue *wq)\n{\n    struct work_item *item;\n\n    mtx_lock(&amp;wq-&gt;wq_mtx);\n\n    while (!wq-&gt;wq_shutdown) {\n        item = TAILQ_FIRST(&amp;wq-&gt;wq_items);\n        if (item == NULL) {\n            /* No work, sleep until signaled */\n            cv_mtx_wait(&amp;wq-&gt;wq_cv, &amp;wq-&gt;wq_mtx);\n            continue;\n        }\n\n        TAILQ_REMOVE(&amp;wq-&gt;wq_items, item, wi_entry);\n        mtx_unlock(&amp;wq-&gt;wq_mtx);\n\n        /* Process item without holding lock */\n        process_work_item(item);\n\n        mtx_lock(&amp;wq-&gt;wq_mtx);\n    }\n\n    mtx_unlock(&amp;wq-&gt;wq_mtx);\n}\n\n/* Submitter */\nvoid\nsubmit_work(struct work_queue *wq, struct work_item *item)\n{\n    mtx_lock(&amp;wq-&gt;wq_mtx);\n    TAILQ_INSERT_TAIL(&amp;wq-&gt;wq_items, item, wi_entry);\n    cv_signal(&amp;wq-&gt;wq_cv);  /* Wake one worker */\n    mtx_unlock(&amp;wq-&gt;wq_mtx);\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#example-4-reference-counting-for-object-lifecycle","title":"Example 4: Reference Counting for Object Lifecycle","text":"<pre><code>struct cached_object {\n    refcount_t          co_refs;\n    struct spinlock     co_spin;\n    void                *co_data;\n    /* ... */\n};\n\nstruct cached_object *\ncache_lookup(struct cache *cache, uint64_t key)\n{\n    struct cached_object *co;\n\n    spin_lock(&amp;cache-&gt;cache_spin);\n    co = cache_find(cache, key);\n    if (co != NULL) {\n        refcount_acquire(&amp;co-&gt;co_refs);\n    }\n    spin_unlock(&amp;cache-&gt;cache_spin);\n\n    return co;  /* caller now owns reference */\n}\n\nvoid\ncache_release(struct cached_object *co)\n{\n    if (refcount_release(&amp;co-&gt;co_refs)) {\n        /* Last reference, free object */\n        kfree(co-&gt;co_data, M_CACHE);\n        kfree(co, M_CACHE);\n    }\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#example-5-lockmgr-upgrade-operation","title":"Example 5: Lockmgr Upgrade Operation","text":"<pre><code>int\nvnode_truncate(struct vnode *vp, off_t new_size)\n{\n    int error;\n\n    /* Start with shared lock for size check */\n    lockmgr(&amp;vp-&gt;v_lock, LK_SHARED);\n\n    if (vp-&gt;v_size &lt;= new_size) {\n        /* Nothing to do */\n        lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n        return 0;\n    }\n\n    /* Need exclusive lock to modify */\n    error = lockmgr(&amp;vp-&gt;v_lock, LK_UPGRADE);\n    if (error) {\n        /* Lock was released, re-acquire exclusive */\n        return error;\n    }\n\n    /* Now have exclusive lock */\n    vnode_truncate_locked(vp, new_size);\n\n    lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#subsystem-interactions","title":"Subsystem Interactions","text":""},{"location":"sys/kern/synchronization/#relationship-to-lwkt","title":"Relationship to LWKT","text":"<p>All synchronization primitives integrate with LWKT threading:</p> <ul> <li>Critical sections: Spinlocks and tokens use <code>crit_enter()</code>/<code>crit_exit()</code> to prevent preemption</li> <li>Thread scheduling: Blocking locks deschedule via <code>lwkt_deschedule()</code></li> <li>Per-CPU data: Spinlocks often protect per-CPU structures accessed via <code>mycpu</code></li> <li>Token precedence: Tokens are typically acquired before lower-level locks</li> </ul>"},{"location":"sys/kern/synchronization/#relationship-to-sleepwakeup","title":"Relationship to Sleep/Wakeup","text":"<ul> <li>tsleep(): Used by mutexes, lockmgr, and condition variables</li> <li>tsleep_interlock(): Prepares atomic sleep without races</li> <li>wakeup(): Used by cv_signal(), cv_broadcast(), and lock releases</li> <li>Sleep domains: PDOMAIN_UMTX, PDOMAIN_FBSD0, etc. for isolation</li> </ul>"},{"location":"sys/kern/synchronization/#relationship-to-vm","title":"Relationship to VM","text":"<ul> <li>Page faults: Can occur while holding tokens or sleeping locks, but not spinlocks</li> <li>Memory allocation: <code>kmalloc()</code> may block, cannot call while holding spinlock</li> <li>Objcache: Sysref integrates with objcache for efficient allocation</li> <li>COW handling: Umtx uses physical addresses to handle COW transparently</li> </ul>"},{"location":"sys/kern/synchronization/#best-practices","title":"Best Practices","text":""},{"location":"sys/kern/synchronization/#general-guidelines","title":"General Guidelines","text":"<ol> <li>Prefer tokens for high-level code: Unless performance is critical or in interrupt context</li> <li>Keep spinlock critical sections tiny: Measure in instructions, not lines of code</li> <li>Don't hold spinlocks across function calls: Unless function is guaranteed not to block</li> <li>Use shared locks when possible: Reduces contention for read-heavy workloads</li> <li>Avoid lock nesting: If unavoidable, maintain consistent lock order</li> <li>Use condition variables for coordination: Better than polling in a loop</li> </ol>"},{"location":"sys/kern/synchronization/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Holding spinlock across blocking function: Causes panic or deadlock</li> <li>Sleeping with token but not expecting it: Other code may assume token held continuously</li> <li>Reference count overflow: Not checking for wraparound</li> <li>Lock order violation: Acquiring locks in inconsistent order causes deadlock</li> <li>Missing wakeup: Forgetting cv_signal() causes permanent sleep</li> <li>Umtx address changes: COW or munmap while threads sleeping</li> </ol>"},{"location":"sys/kern/synchronization/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>INVARIANTS kernel: Enables lock assertion checking</li> <li>witness: Tracks lock ordering violations (when enabled)</li> <li>KTR tracing: Can trace spinlock contention</li> <li>indefinite_info: Automatic warnings for locks held too long</li> <li>lock_test_mode: Special debugging mode for lockmgr</li> </ol>"},{"location":"sys/kern/synchronization/#see-also","title":"See Also","text":"<ul> <li>LWKT Threading - Thread management and tokens</li> <li>Processes - Process management</li> <li>Scheduling - Thread scheduling</li> <li>Memory Management - VM interactions</li> </ul>"},{"location":"sys/kern/synchronization/#references","title":"References","text":"<ul> <li><code>sys/kern/kern_spinlock.c</code> - Spinlock implementation</li> <li><code>sys/kern/kern_lock.c</code> - Lockmgr implementation  </li> <li><code>sys/kern/kern_mutex.c</code> - Mutex implementation</li> <li><code>sys/kern/kern_condvar.c</code> - Condition variable implementation</li> <li><code>sys/kern/kern_refcount.c</code> - Reference counting</li> <li><code>sys/kern/kern_sysref.c</code> - System reference counting</li> <li><code>sys/kern/kern_umtx.c</code> - Userland mutex support</li> <li><code>sys/kern/subr_sleepqueue.c</code> - Sleep queue implementation</li> </ul>"},{"location":"sys/kern/syscalls/","title":"System Calls","text":"<p>This document covers the system call dispatch mechanism, generic I/O syscalls, the ioctl interface, I/O multiplexing (select/poll), and process tracing (ptrace).</p>"},{"location":"sys/kern/syscalls/#overview","title":"Overview","text":"<p>System calls are the interface between user programs and the kernel. When a user program invokes a system call, the CPU transitions from user mode to kernel mode, the kernel locates the appropriate handler function, executes it, and returns the result to the user program.</p> <p>DragonFly BSD uses a table-driven syscall dispatch mechanism. Each syscall has an entry in the <code>sysent[]</code> array that specifies the handler function and argument count.</p>"},{"location":"sys/kern/syscalls/#key-source-files","title":"Key Source Files","text":"File Purpose <code>sys/sys/sysent.h</code> <code>struct sysent</code> and <code>struct sysentvec</code> definitions <code>sys/sys/sysmsg.h</code> <code>struct sysmsg</code> - syscall message structure <code>sys/kern/syscalls.c</code> Generated syscall name table <code>sys/kern/kern_syscalls.c</code> Syscall registration for modules <code>sys/kern/sys_generic.c</code> Generic I/O syscalls (read, write, ioctl, select, poll) <code>sys/kern/sys_process.c</code> ptrace implementation"},{"location":"sys/kern/syscalls/#syscall-dispatch-mechanism","title":"Syscall Dispatch Mechanism","text":""},{"location":"sys/kern/syscalls/#the-sysent-structure","title":"The sysent Structure","text":"<p>Each system call is described by a <code>struct sysent</code> entry (<code>sys/sys/sysent.h:44</code>):</p> <pre><code>struct sysent {\n    int32_t   sy_narg;    /* number of arguments */\n    uint32_t  sy_rsize;   /* sizeof(result) */\n    sy_call_t *sy_call;   /* start function */\n    sy_call_t *sy_abort;  /* abort function (only if start was async) */\n};\n</code></pre> <p>The global <code>sysent[]</code> array contains entries for all system calls. Syscall numbers index directly into this array.</p>"},{"location":"sys/kern/syscalls/#the-sysmsg-structure","title":"The sysmsg Structure","text":"<p>Every syscall handler receives a <code>struct sysmsg</code> that carries the return value back to userspace (<code>sys/sys/sysmsg.h:58</code>):</p> <pre><code>struct sysmsg {\n    union {\n        void    *resultp;       /* misc pointer data or result */\n        int     iresult;        /* standard 'int'eger result */\n        long    lresult;        /* long result */\n        size_t  szresult;       /* size_t result */\n        long    fds[2];         /* double result (e.g., pipe) */\n        __int32_t result32;     /* 32 bit result */\n        __int64_t result64;     /* 64 bit result */\n        __off_t offset;         /* off_t result */\n        register_t reg;\n    } sm_result;\n    struct trapframe *sm_frame; /* saved user context */\n    union sysunion extargs;     /* if more than 6 args */\n};\n</code></pre> <p>The union allows syscalls to return different types efficiently. Common accessor macros include: - <code>sysmsg-&gt;sysmsg_result</code> - standard int result - <code>sysmsg-&gt;sysmsg_szresult</code> - size_t result (read/write byte counts) - <code>sysmsg-&gt;sysmsg_fds</code> - dual result for syscalls like <code>pipe()</code></p>"},{"location":"sys/kern/syscalls/#syscall-handler-signature","title":"Syscall Handler Signature","text":"<p>All syscall handlers follow this signature:</p> <pre><code>int sys_xxx(struct sysmsg *sysmsg, const struct xxx_args *uap);\n</code></pre> <p>Where: - <code>sysmsg</code> carries the return value - <code>uap</code> points to the userspace arguments (already copied in) - The function returns an errno value (0 on success)</p>"},{"location":"sys/kern/syscalls/#execution-vector-sysentvec","title":"Execution Vector (sysentvec)","text":"<p>Different ABIs (native, Linux emulation, etc.) use different <code>struct sysentvec</code> configurations (<code>sys/sys/sysent.h:56</code>):</p> <pre><code>struct sysentvec {\n    int             sv_size;        /* number of entries */\n    struct sysent   *sv_table;      /* pointer to sysent */\n    int             sv_sigsize;     /* size of signal translation table */\n    int             *sv_sigtbl;     /* signal translation table */\n    int             sv_errsize;     /* size of errno translation table */\n    int             *sv_errtbl;     /* errno translation table */\n    int             (*sv_transtrap)(int, int);  /* trap translation */\n    int             (*sv_fixup)(register_t **, struct image_params *);\n    void            (*sv_sendsig)(...);         /* signal delivery */\n    char            *sv_sigcode;    /* sigtramp code */\n    int             *sv_szsigcode;  /* sigtramp size */\n    char            *sv_name;       /* ABI name */\n    int             (*sv_coredump)(...);        /* core dump function */\n    int             (*sv_imgact_try)(struct image_params *);\n    int             sv_minsigstksz; /* minimum signal stack size */\n};\n</code></pre> <p>This allows the kernel to support multiple system call ABIs simultaneously.</p>"},{"location":"sys/kern/syscalls/#generic-io-syscalls","title":"Generic I/O Syscalls","text":"<p>The generic I/O syscalls are implemented in <code>sys/kern/sys_generic.c</code>.</p>"},{"location":"sys/kern/syscalls/#read-write","title":"read / write","text":"<p>The <code>sys_read()</code> and <code>sys_write()</code> functions (<code>sys_generic.c:123</code>, <code>sys_generic.c:329</code>) build a <code>struct uio</code> describing the I/O operation and delegate to <code>kern_preadv()</code> or <code>kern_pwritev()</code>.</p> <pre><code>int sys_read(struct sysmsg *sysmsg, const struct read_args *uap)\n{\n    struct uio auio;\n    struct iovec aiov;\n\n    aiov.iov_base = uap-&gt;buf;\n    aiov.iov_len = uap-&gt;nbyte;\n    auio.uio_iov = &amp;aiov;\n    auio.uio_iovcnt = 1;\n    auio.uio_offset = -1;           /* use file position */\n    auio.uio_resid = uap-&gt;nbyte;\n    auio.uio_rw = UIO_READ;\n    auio.uio_segflg = UIO_USERSPACE;\n    auio.uio_td = curthread;\n\n    error = kern_preadv(uap-&gt;fd, &amp;auio, 0, &amp;sysmsg-&gt;sysmsg_szresult);\n    return error;\n}\n</code></pre> <p>Key points: - <code>uio_offset = -1</code> means use the file's current position - <code>uio_segflg = UIO_USERSPACE</code> indicates the buffer is in user address space - The byte count is returned via <code>sysmsg-&gt;sysmsg_szresult</code></p>"},{"location":"sys/kern/syscalls/#readv-writev-scatter-gather-io","title":"readv / writev (Scatter-Gather I/O)","text":"<p>The vectored variants (<code>sys_readv</code>, <code>sys_writev</code>) use <code>iovec_copyin()</code> to copy the iovec array from userspace, then proceed similarly:</p> <pre><code>int sys_readv(struct sysmsg *sysmsg, const struct readv_args *uap)\n{\n    struct iovec aiov[UIO_SMALLIOV], *iov = NULL;\n\n    error = iovec_copyin(uap-&gt;iovp, &amp;iov, aiov, uap-&gt;iovcnt, &amp;auio.uio_resid);\n    /* ... build uio ... */\n    error = kern_preadv(uap-&gt;fd, &amp;auio, 0, &amp;sysmsg-&gt;sysmsg_szresult);\n    iovec_free(&amp;iov, aiov);\n    return error;\n}\n</code></pre> <p>The <code>UIO_SMALLIOV</code> optimization avoids kmalloc for small iovec counts (typically 8).</p>"},{"location":"sys/kern/syscalls/#positioned-io-preadpwrite","title":"Positioned I/O (pread/pwrite)","text":"<p>DragonFly provides extended positioned I/O via <code>sys_extpread()</code> and <code>sys_extpwritev()</code>. These accept an explicit offset and flags, allowing atomic read/write at a specific file position without affecting the file pointer.</p>"},{"location":"sys/kern/syscalls/#the-file-operations-layer","title":"The File Operations Layer","text":"<p>All I/O ultimately goes through file operations (<code>fo_read</code>, <code>fo_write</code>) defined per file type. See resources.md for file descriptor management.</p>"},{"location":"sys/kern/syscalls/#the-ioctl-interface","title":"The ioctl Interface","text":""},{"location":"sys/kern/syscalls/#overview_1","title":"Overview","text":"<p>The <code>ioctl()</code> syscall provides device-specific control operations. It's implemented by <code>sys_ioctl()</code> (<code>sys_generic.c:537</code>) which delegates to <code>mapped_ioctl()</code>.</p>"},{"location":"sys/kern/syscalls/#ioctl-command-encoding","title":"ioctl Command Encoding","text":"<p>ioctl commands encode direction and size in the command number: - <code>IOC_VOID</code> - no data transfer - <code>IOC_IN</code> - data flows from user to kernel - <code>IOC_OUT</code> - data flows from kernel to user - <code>IOCPARM_LEN(cmd)</code> - extracts the data size</p>"},{"location":"sys/kern/syscalls/#mapped_ioctl","title":"mapped_ioctl()","text":"<p>The <code>mapped_ioctl()</code> function (<code>sys_generic.c:558</code>) handles:</p> <ol> <li>Command translation - For emulation layers, commands can be remapped</li> <li>Data copyin/copyout - Based on IOC_IN/IOC_OUT flags</li> <li>Built-in commands - FIONBIO, FIOASYNC, FIOCLEX, FIONCLEX</li> <li>Delegation - Calls <code>fo_ioctl()</code> for file-type-specific handling</li> </ol> <pre><code>int mapped_ioctl(int fd, u_long com, caddr_t uspc_data,\n                 struct ioctl_map *map, struct sysmsg *msg)\n{\n    /* Handle translation map if provided */\n    if (map != NULL) {\n        /* ... lookup and translate command ... */\n    }\n\n    /* Built-in commands */\n    switch (com) {\n    case FIONCLEX:  /* clear close-on-exec */\n    case FIOCLEX:   /* set close-on-exec */\n    case FIONBIO:   /* set/clear non-blocking */\n    case FIOASYNC:  /* set/clear async I/O */\n        /* ... handle directly ... */\n    }\n\n    /* Copy data in/out and call file operation */\n    size = IOCPARM_LEN(com);\n    if (com &amp; IOC_IN)\n        copyin(uspc_data, data, size);\n\n    error = fo_ioctl(fp, com, data, cred, msg);\n\n    if (com &amp; IOC_OUT)\n        copyout(data, uspc_data, size);\n}\n</code></pre>"},{"location":"sys/kern/syscalls/#mapped-ioctl-handlers","title":"Mapped ioctl Handlers","text":"<p>Subsystems can register ioctl command ranges for translation via <code>mapped_ioctl_register_handler()</code>. This is primarily used for emulation compatibility layers.</p>"},{"location":"sys/kern/syscalls/#io-multiplexing-select-and-poll","title":"I/O Multiplexing: select and poll","text":"<p>DragonFly implements <code>select()</code> and <code>poll()</code> using the kqueue infrastructure internally. This provides efficient, scalable event notification.</p>"},{"location":"sys/kern/syscalls/#select","title":"select()","text":"<p>The <code>sys_select()</code> function (<code>sys_generic.c:798</code>) and its variant <code>sys_pselect()</code> convert fd_set bitmaps into kqueue events:</p> <pre><code>int sys_select(struct sysmsg *sysmsg, const struct select_args *uap)\n{\n    /* Copy timeout if provided */\n    if (uap-&gt;tv != NULL) {\n        copyin(uap-&gt;tv, &amp;ktv, sizeof(ktv));\n        TIMEVAL_TO_TIMESPEC(&amp;ktv, &amp;kts);\n    }\n\n    error = doselect(uap-&gt;nd, uap-&gt;in, uap-&gt;ou, uap-&gt;ex, ktsp,\n                     &amp;sysmsg-&gt;sysmsg_result);\n    return error;\n}\n</code></pre> <p>The <code>doselect()</code> function (<code>sys_generic.c:1144</code>):</p> <ol> <li>Copies fd_set bitmaps from userspace</li> <li>Converts each set bit to a kevent (EVFILT_READ, EVFILT_WRITE, EVFILT_EXCEPT)</li> <li>Calls <code>kern_kevent()</code> on the per-lwp kqueue</li> <li>Converts returned events back to fd_set bitmaps</li> <li>Copies results to userspace</li> </ol> <p>Key implementation details: - Uses per-LWP kqueue (<code>lwp-&gt;lwp_kqueue</code>) for efficiency - Serial numbers (<code>lwp-&gt;lwp_kqueue_serial</code>) detect stale events - Events are registered with <code>NOTE_OLDAPI</code> for select/poll compatibility</p>"},{"location":"sys/kern/syscalls/#poll","title":"poll()","text":"<p>The <code>sys_poll()</code> function (<code>sys_generic.c:1234</code>) works similarly but uses <code>struct pollfd</code> arrays instead of bitmaps:</p> <pre><code>int sys_poll(struct sysmsg *sysmsg, const struct poll_args *uap)\n{\n    if (uap-&gt;timeout != INFTIM) {\n        ts.tv_sec = uap-&gt;timeout / 1000;\n        ts.tv_nsec = (uap-&gt;timeout % 1000) * 1000 * 1000;\n        tsp = &amp;ts;\n    }\n\n    error = dopoll(uap-&gt;nfds, uap-&gt;fds, tsp, &amp;sysmsg-&gt;sysmsg_result, 0);\n    return error;\n}\n</code></pre> <p>The <code>dopoll()</code> function (<code>sys_generic.c:1620</code>):</p> <ol> <li>Copies pollfd array from userspace</li> <li>For each pollfd, registers appropriate kevents based on <code>events</code> field</li> <li>Calls <code>kern_kevent()</code> </li> <li>Maps kevent results back to <code>revents</code> fields</li> <li>Copies pollfd array back to userspace</li> </ol>"},{"location":"sys/kern/syscalls/#ppoll-and-pselect","title":"ppoll() and pselect()","text":"<p>The <code>sys_ppoll()</code> and <code>sys_pselect()</code> variants add: - Precise timespec timeout (instead of timeval/milliseconds) - Atomic signal mask manipulation during the wait</p> <p>Signal mask handling: <pre><code>if (uap-&gt;sigmask != NULL) {\n    lp-&gt;lwp_oldsigmask = lp-&gt;lwp_sigmask;\n    lp-&gt;lwp_sigmask = sigmask;\n    /* ... do poll/select ... */\n    if (error == EINTR)\n        lp-&gt;lwp_flags |= LWP_OLDMASK;  /* restore after signal handler */\n    else\n        lp-&gt;lwp_sigmask = lp-&gt;lwp_oldsigmask;  /* restore immediately */\n}\n</code></pre></p>"},{"location":"sys/kern/syscalls/#event-flag-mapping","title":"Event Flag Mapping","text":"poll events kqueue filter Notes POLLIN, POLLRDNORM EVFILT_READ Normal read data POLLOUT, POLLWRNORM EVFILT_WRITE Write possible POLLPRI, POLLRDBAND EVFILT_EXCEPT OOB/urgent data POLLHUP EV_HUP flag Hangup detected POLLERR EV_EOF with fflags Error condition POLLNVAL EV_ERROR with EBADF Bad file descriptor"},{"location":"sys/kern/syscalls/#process-tracing-ptrace","title":"Process Tracing (ptrace)","text":"<p>The <code>ptrace()</code> syscall enables debuggers to control and inspect other processes. Implementation is in <code>sys/kern/sys_process.c</code>.</p>"},{"location":"sys/kern/syscalls/#overview_2","title":"Overview","text":"<pre><code>int sys_ptrace(struct sysmsg *sysmsg, const struct ptrace_args *uap)\n{\n    /* Copy register structures if needed */\n    switch (uap-&gt;req) {\n    case PT_SETREGS:\n        copyin(uap-&gt;addr, &amp;r.reg, sizeof(r.reg));\n        break;\n    /* ... */\n    }\n\n    error = kern_ptrace(curp, uap-&gt;req, uap-&gt;pid, addr, uap-&gt;data,\n                        &amp;sysmsg-&gt;sysmsg_result);\n\n    /* Copy results out */\n    switch (uap-&gt;req) {\n    case PT_GETREGS:\n        copyout(&amp;r.reg, uap-&gt;addr, sizeof(r.reg));\n        break;\n    /* ... */\n    }\n}\n</code></pre>"},{"location":"sys/kern/syscalls/#ptrace-requests","title":"ptrace Requests","text":"Request Description <code>PT_TRACE_ME</code> Mark self as traced (child calls this) <code>PT_ATTACH</code> Attach to existing process <code>PT_DETACH</code> Detach from traced process <code>PT_CONTINUE</code> Resume execution <code>PT_STEP</code> Single-step one instruction <code>PT_KILL</code> Kill traced process <code>PT_READ_I/D</code> Read instruction/data memory <code>PT_WRITE_I/D</code> Write instruction/data memory <code>PT_IO</code> Bulk memory read/write <code>PT_GETREGS</code> Get general registers <code>PT_SETREGS</code> Set general registers <code>PT_GETFPREGS</code> Get floating-point registers <code>PT_SETFPREGS</code> Set floating-point registers <code>PT_GETDBREGS</code> Get debug registers <code>PT_SETDBREGS</code> Set debug registers"},{"location":"sys/kern/syscalls/#security-checks","title":"Security Checks","text":"<p>The <code>kern_ptrace()</code> function (<code>sys_process.c:290</code>) enforces:</p> <ol> <li>PT_TRACE_ME - Always allowed (process traces itself)</li> <li>PT_ATTACH requires:</li> <li>Cannot trace self</li> <li>Target not already traced</li> <li>Same UID or root privileges</li> <li>Cannot trace init at securelevel &gt; 0</li> <li>Target not currently in exec</li> <li>Other requests require:</li> <li>Target is traced (<code>P_TRACED</code> flag)</li> <li>Tracer is the parent (<code>p-&gt;p_pptr == curp</code>)</li> <li>Target is stopped (<code>p-&gt;p_stat == SSTOP</code>)</li> </ol>"},{"location":"sys/kern/syscalls/#memory-access","title":"Memory Access","text":"<p>Memory read/write uses procfs infrastructure: <pre><code>case PT_READ_I:\ncase PT_READ_D:\n    iov.iov_base = &amp;tmp;\n    iov.iov_len = sizeof(int);\n    uio.uio_offset = (off_t)(uintptr_t)addr;\n    uio.uio_rw = UIO_READ;\n    error = procfs_domem(curp, lp, NULL, &amp;uio);\n    *res = tmp;\n    break;\n</code></pre></p> <p>For bulk I/O, <code>PT_IO</code> uses a <code>struct ptrace_io_desc</code>: <pre><code>struct ptrace_io_desc {\n    int     piod_op;        /* PIOD_READ_D, PIOD_WRITE_I, etc. */\n    void    *piod_offs;     /* offset in traced process */\n    void    *piod_addr;     /* buffer in tracer */\n    size_t  piod_len;       /* length */\n};\n</code></pre></p>"},{"location":"sys/kern/syscalls/#process-events-stopevent","title":"Process Events (stopevent)","text":"<p>The <code>stopevent()</code> function (<code>sys_process.c:744</code>) stops a process for procfs events, allowing debuggers to intercept specific operations:</p> <pre><code>void stopevent(struct proc *p, unsigned int event, unsigned int val)\n{\n    p-&gt;p_xstat = val;\n    p-&gt;p_stype = event;\n    p-&gt;p_step = 1;\n    wakeup(&amp;p-&gt;p_stype);  /* wake PIOCWAIT waiters */\n    tsleep(&amp;p-&gt;p_step, ...);  /* wait for PIOCCONT */\n}\n</code></pre>"},{"location":"sys/kern/syscalls/#syscall-registration-for-modules","title":"Syscall Registration for Modules","text":"<p>Kernel modules can register new system calls dynamically using the functions in <code>sys/kern/kern_syscalls.c</code>.</p>"},{"location":"sys/kern/syscalls/#registration-api","title":"Registration API","text":"<pre><code>int syscall_register(int *offset, struct sysent *new_sysent,\n                     struct sysent *old_sysent);\nint syscall_deregister(int *offset, struct sysent *old_sysent);\n</code></pre> <p>If <code>*offset == NO_SYSCALL</code>, the kernel finds an available slot (one marked with <code>sys_lkmnosys</code>). Otherwise, it uses the specified slot.</p>"},{"location":"sys/kern/syscalls/#syscall_module-macro","title":"SYSCALL_MODULE Macro","text":"<p>The <code>SYSCALL_MODULE</code> macro (<code>sys/sys/sysent.h:96</code>) simplifies module creation:</p> <pre><code>SYSCALL_MODULE(name, offset, new_sysent, evh, arg)\n</code></pre> <p>This creates the necessary module data structures and registers the module with <code>syscall_module_handler()</code> as the event handler.</p>"},{"location":"sys/kern/syscalls/#module-event-handler","title":"Module Event Handler","text":"<p>The <code>syscall_module_handler()</code> function (<code>kern_syscalls.c:80</code>) handles: - <code>MOD_LOAD</code> - Calls <code>syscall_register()</code>, stores slot in module-specific data - <code>MOD_UNLOAD</code> - Calls <code>syscall_deregister()</code> to restore original entry</p>"},{"location":"sys/kern/syscalls/#see-also","title":"See Also","text":"<ul> <li>processes.md - Process lifecycle (for ptrace context)</li> <li>signals.md - Signal handling (for ptrace signal delivery)</li> <li>resources.md - File descriptors and file operations</li> <li>ipc/sockets.md - Socket I/O operations</li> <li>kld.md - Kernel module loading (for syscall modules)</li> </ul>"},{"location":"sys/kern/sysctl/","title":"Sysctl Framework","text":"<p>The sysctl framework provides a hierarchical namespace for kernel parameters that can be queried and modified by user programs. It is the primary interface for runtime kernel configuration in DragonFly BSD.</p>"},{"location":"sys/kern/sysctl/#source-files","title":"Source Files","text":"File Lines Description <code>kern_sysctl.c</code> 1,661 Core sysctl implementation <code>kern_mib.c</code> 286 Standard MIB definitions <code>kern_posix4_mib.c</code> 107 POSIX.4 (1003.1b) MIBs <code>kern_kinfo.c</code> 333 Process/LWP info structures <code>sys/sysctl.h</code> 849 Data structures and macros"},{"location":"sys/kern/sysctl/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TD\n    subgraph tree[\"Sysctl MIB Tree\"]\n        ROOT[\"sysctl__children (root)\"]\n\n        ROOT --&gt; KERN[\"kern (CTL_KERN=1)\"]\n        ROOT --&gt; VM[\"vm (CTL_VM=2)\"]\n        ROOT --&gt; VFS[\"vfs (CTL_VFS=3)\"]\n        ROOT --&gt; NET[\"net (CTL_NET=4)\"]\n        ROOT --&gt; DEBUG[\"debug (CTL_DEBUG=5)\"]\n        ROOT --&gt; HW[\"hw (CTL_HW=6)\"]\n        ROOT --&gt; MACHDEP[\"machdep (CTL_MACHDEP=7)\"]\n        ROOT --&gt; USER[\"user (CTL_USER=8)\"]\n        ROOT --&gt; P1003[\"p1003_1b (CTL_P1003_1B=9)\"]\n        ROOT --&gt; LWKT[\"lwkt (CTL_LWKT=10)\"]\n\n        KERN --&gt; OSTYPE[\"ostype: 'DragonFly'\"]\n        KERN --&gt; OSREL[\"osrelease: '6.4'\"]\n        KERN --&gt; HOST[\"hostname (writable)\"]\n        KERN --&gt; SEC[\"securelevel (increasing only)\"]\n        KERN --&gt; MAXP[\"maxproc (read-only)\"]\n\n        DEBUG --&gt; SIZEOF[\"sizeof/\"]\n        DEBUG --&gt; KTR[\"ktr/\"]\n\n        HW --&gt; NCPU[\"ncpu: Number of CPUs\"]\n        HW --&gt; PAGESIZE[\"pagesize: PAGE_SIZE\"]\n        HW --&gt; ARCH[\"machine_arch: 'x86_64'\"]\n        HW --&gt; SENSORS[\"sensors/: Hardware sensors\"]\n\n        USER --&gt; POSIX2[\"POSIX.2 limits\"]\n        P1003 --&gt; POSIX4[\"POSIX.4 feature flags\"]\n        LWKT --&gt; LWKTSTATS[\"LWKT statistics\"]\n    end\n</code></pre>"},{"location":"sys/kern/sysctl/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/sysctl/#sysctl_oid-syssysctlh157","title":"sysctl_oid (<code>sys/sysctl.h:157</code>)","text":"<p>Each node in the MIB tree is represented by a <code>sysctl_oid</code>:</p> <pre><code>struct sysctl_oid {\n    struct sysctl_oid_list *oid_parent;  /* parent's list */\n    SLIST_ENTRY(sysctl_oid) oid_link;    /* sibling linkage */\n    int         oid_number;              /* numeric OID component */\n    int         oid_kind;                /* type + flags */\n    void       *oid_arg1;                /* data pointer or children */\n    int         oid_arg2;                /* secondary data (size, etc.) */\n    const char *oid_name;                /* string name */\n    int        (*oid_handler)(SYSCTL_HANDLER_ARGS);  /* handler function */\n    const char *oid_fmt;                 /* format string */\n    int         oid_refcnt;              /* reference count */\n    u_int       oid_running;             /* handlers currently running */\n    const char *oid_descr;               /* description */\n    struct lock oid_lock;                /* per-node lock */\n};\n</code></pre>"},{"location":"sys/kern/sysctl/#sysctl_req-syssysctlh136","title":"sysctl_req (<code>sys/sysctl.h:136</code>)","text":"<p>Request context passed to handlers:</p> <pre><code>struct sysctl_req {\n    struct thread *td;       /* requesting thread */\n    int         lock;        /* wiring state */\n    void       *oldptr;      /* output buffer (user/kernel) */\n    size_t      oldlen;      /* output buffer size */\n    size_t      oldidx;      /* current output position */\n    int       (*oldfunc)();  /* output transfer function */\n    void       *newptr;      /* input buffer */\n    size_t      newlen;      /* input buffer size */\n    size_t      newidx;      /* current input position */\n    int       (*newfunc)();  /* input transfer function */\n    size_t      validlen;    /* valid output length */\n    int         flags;       /* request flags */\n};\n</code></pre>"},{"location":"sys/kern/sysctl/#oid-types-and-flags","title":"OID Types and Flags","text":""},{"location":"sys/kern/sysctl/#type-codes-syssysctlh69","title":"Type Codes (<code>sys/sysctl.h:69</code>)","text":"Type Value Description <code>CTLTYPE_NODE</code> 0x01 Interior node with children <code>CTLTYPE_INT</code> 0x02 32-bit signed integer <code>CTLTYPE_STRING</code> 0x03 NUL-terminated string <code>CTLTYPE_S64</code> 0x04 64-bit signed integer <code>CTLTYPE_OPAQUE</code> 0x05 Opaque binary data <code>CTLTYPE_UINT</code> 0x06 32-bit unsigned integer <code>CTLTYPE_LONG</code> 0x07 Native long <code>CTLTYPE_ULONG</code> 0x08 Native unsigned long <code>CTLTYPE_U64</code> 0x09 64-bit unsigned integer <code>CTLTYPE_U8</code> 0x0a 8-bit unsigned <code>CTLTYPE_U16</code> 0x0b 16-bit unsigned <code>CTLTYPE_S8</code> 0x0c 8-bit signed <code>CTLTYPE_S16</code> 0x0d 16-bit signed <code>CTLTYPE_S32</code> 0x0e 32-bit signed <code>CTLTYPE_U32</code> 0x0f 32-bit unsigned <code>CTLTYPE_BIT32(n)</code> 0x10 Bit n in uint32_t <code>CTLTYPE_BIT64(n)</code> 0x11 Bit n in uint64_t"},{"location":"sys/kern/sysctl/#access-flags-syssysctlh92","title":"Access Flags (<code>sys/sysctl.h:92</code>)","text":"Flag Value Description <code>CTLFLAG_RD</code> 0x80000000 Readable <code>CTLFLAG_WR</code> 0x40000000 Writable <code>CTLFLAG_RW</code> (RD|WR) Read-write <code>CTLFLAG_ANYBODY</code> 0x10000000 Any user can write <code>CTLFLAG_SECURE</code> 0x08000000 securelevel &lt;= 0 to write <code>CTLFLAG_PRISON</code> 0x04000000 Jail root can modify <code>CTLFLAG_DYN</code> 0x02000000 Dynamically allocated <code>CTLFLAG_SKIP</code> 0x01000000 Skip when listing <code>CTLFLAG_DYING</code> 0x00010000 Being removed <code>CTLFLAG_SHLOCK</code> 0x00008000 Shared lock on write <code>CTLFLAG_EXLOCK</code> 0x00004000 Exclusive lock on read <code>CTLFLAG_NOLOCK</code> 0x00002000 No per-oid lock"},{"location":"sys/kern/sysctl/#defining-static-sysctls","title":"Defining Static Sysctls","text":""},{"location":"sys/kern/sysctl/#node-declaration","title":"Node Declaration","text":"<pre><code>/* Declare a node with children */\nSYSCTL_NODE(parent, nbr, name, access, handler, descr)\n\n/* Example: Create _kern_ipc node */\nSYSCTL_NODE(_kern, OID_AUTO, ipc, CTLFLAG_RW, 0, \"IPC parameters\");\n</code></pre>"},{"location":"sys/kern/sysctl/#leaf-values","title":"Leaf Values","text":"<pre><code>/* Integer (32-bit) */\nSYSCTL_INT(_kern, KERN_MAXPROC, maxproc, CTLFLAG_RD,\n    &amp;maxproc, 0, \"Maximum number of processes\");\n\n/* String */\nSYSCTL_STRING(_kern, KERN_OSTYPE, ostype, CTLFLAG_RD | CTLFLAG_NOLOCK,\n    ostype, 0, \"Operating system type\");\n\n/* Long */\nSYSCTL_LONG(_hw, OID_AUTO, physmem, CTLFLAG_RD,\n    &amp;physmem, 0, \"Physical memory\");\n\n/* Opaque structure */\nSYSCTL_STRUCT(_kern, KERN_CLOCKRATE, clockrate, CTLFLAG_RD,\n    &amp;clockrate, clockinfo, \"Clock rate info\");\n\n/* Custom handler */\nSYSCTL_PROC(_kern, KERN_HOSTNAME, hostname,\n    CTLTYPE_STRING | CTLFLAG_RW | CTLFLAG_PRISON | CTLFLAG_NOLOCK,\n    0, 0, sysctl_hostname, \"A\", \"Hostname\");\n</code></pre>"},{"location":"sys/kern/sysctl/#bit-fields","title":"Bit Fields","text":"<pre><code>/* Single bit in uint32_t */\nSYSCTL_BIT32(_debug, OID_AUTO, flag, CTLFLAG_RW,\n    &amp;debug_flags, 0, 5, \"Debug flag bit 5\");\n\n/* Single bit in uint64_t */\nSYSCTL_BIT64(_debug, OID_AUTO, flag64, CTLFLAG_RW,\n    &amp;debug_flags64, 0, 42, \"Debug flag bit 42\");\n</code></pre>"},{"location":"sys/kern/sysctl/#handler-functions","title":"Handler Functions","text":""},{"location":"sys/kern/sysctl/#standard-handlers","title":"Standard Handlers","text":"<p>The framework provides built-in handlers for common types:</p> Handler Types <code>sysctl_handle_int</code> int, uint <code>sysctl_handle_long</code> long, ulong <code>sysctl_handle_quad</code> int64, uint64 <code>sysctl_handle_8</code> int8, uint8 <code>sysctl_handle_16</code> int16, uint16 <code>sysctl_handle_32</code> int32, uint32 <code>sysctl_handle_64</code> int64, uint64 <code>sysctl_handle_string</code> strings <code>sysctl_handle_opaque</code> binary data <code>sysctl_handle_bit32</code> bit in uint32_t <code>sysctl_handle_bit64</code> bit in uint64_t"},{"location":"sys/kern/sysctl/#custom-handler-example-kern_mibc165","title":"Custom Handler Example (<code>kern_mib.c:165</code>)","text":"<pre><code>static int\nsysctl_hostname(SYSCTL_HANDLER_ARGS)\n{\n    struct proc *p = req-&gt;td-&gt;td_proc;\n    struct prison *pr = p-&gt;p_ucred-&gt;cr_prison;\n    int error;\n\n    /* Upgrade to exclusive lock for writes */\n    if (req-&gt;newptr) {\n        SYSCTL_SUNLOCK();\n        SYSCTL_XLOCK();\n    }\n\n    /* Jail-aware hostname handling */\n    if (pr) {\n        if (!PRISON_CAP_ISSET(pr-&gt;pr_caps,\n            PRISON_CAP_SYS_SET_HOSTNAME) &amp;&amp; req-&gt;newptr)\n            return EPERM;\n        error = sysctl_handle_string(oidp,\n            pr-&gt;pr_host, sizeof(pr-&gt;pr_host), req);\n    } else {\n        error = sysctl_handle_string(oidp,\n            hostname, sizeof(hostname), req);\n    }\n\n    if (req-&gt;newptr) {\n        SYSCTL_XUNLOCK();\n        SYSCTL_SLOCK();\n    }\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/sysctl/#data-transfer-macros","title":"Data Transfer Macros","text":"<pre><code>/* Read data from handler to user */\n#define SYSCTL_OUT(req, p, l)  (req-&gt;oldfunc)(req, p, l)\n\n/* Write data from user to handler */\n#define SYSCTL_IN(req, p, l)   (req-&gt;newfunc)(req, p, l)\n</code></pre>"},{"location":"sys/kern/sysctl/#dynamic-sysctl-api","title":"Dynamic Sysctl API","text":"<p>For kernel modules that need to register sysctls at runtime:</p>"},{"location":"sys/kern/sysctl/#context-management","title":"Context Management","text":"<pre><code>struct sysctl_ctx_list ctx;\n\n/* Initialize context */\nsysctl_ctx_init(&amp;ctx);\n\n/* Add sysctls to context */\nSYSCTL_ADD_INT(&amp;ctx, SYSCTL_STATIC_CHILDREN(_debug),\n    OID_AUTO, \"myvalue\", CTLFLAG_RW, &amp;myvalue, 0, \"My value\");\n\nSYSCTL_ADD_NODE(&amp;ctx, SYSCTL_STATIC_CHILDREN(_kern),\n    OID_AUTO, \"mymod\", CTLFLAG_RW, NULL, \"My module\");\n\n/* Free all sysctls in context */\nsysctl_ctx_free(&amp;ctx);\n</code></pre>"},{"location":"sys/kern/sysctl/#direct-registration-kern_sysctlc434","title":"Direct Registration (<code>kern_sysctl.c:434</code>)","text":"<pre><code>struct sysctl_oid *\nsysctl_add_oid(struct sysctl_ctx_list *clist,\n    struct sysctl_oid_list *parent,\n    int number,                    /* OID_AUTO for auto-assign */\n    const char *name,\n    int kind,                      /* type | flags */\n    void *arg1,                    /* data pointer */\n    int arg2,                      /* secondary arg (size) */\n    int (*handler)(SYSCTL_HANDLER_ARGS),\n    const char *fmt,               /* format string */\n    const char *descr);            /* description */\n</code></pre>"},{"location":"sys/kern/sysctl/#oid-removal","title":"OID Removal","text":"<pre><code>/* Remove an OID */\nint sysctl_remove_oid(struct sysctl_oid *oidp, int del, int recurse);\n\n/* Remove by name */\nint sysctl_remove_name(struct sysctl_oid *parent, const char *name,\n    int del, int recurse);\n</code></pre>"},{"location":"sys/kern/sysctl/#locking-strategy","title":"Locking Strategy","text":""},{"location":"sys/kern/sysctl/#per-cpu-shared-lock","title":"Per-CPU Shared Lock","text":"<p>The sysctl tree uses a per-CPU shared/exclusive lock design for scalability:</p> <pre><code>/* Shared lock for reads (per-CPU) */\n#define SYSCTL_SLOCK()   lockmgr(&amp;mycpu-&gt;gd_sysctllock, LK_SHARED)\n#define SYSCTL_SUNLOCK() lockmgr(&amp;mycpu-&gt;gd_sysctllock, LK_RELEASE)\n\n/* Exclusive lock for topology changes (all CPUs) */\n#define SYSCTL_XLOCK()   _sysctl_xlock()\n#define SYSCTL_XUNLOCK() _sysctl_xunlock()\n</code></pre>"},{"location":"sys/kern/sysctl/#exclusive-lock-implementation-kern_sysctlc1639","title":"Exclusive Lock Implementation (<code>kern_sysctl.c:1639</code>)","text":"<pre><code>void _sysctl_xlock(void)\n{\n    globaldata_t gd;\n    int i;\n\n    /* Lock all per-CPU locks */\n    for (i = 0; i &lt; ncpus; ++i) {\n        gd = globaldata_find(i);\n        lockmgr(&amp;gd-&gt;gd_sysctllock, LK_EXCLUSIVE);\n    }\n}\n</code></pre>"},{"location":"sys/kern/sysctl/#per-oid-locking-kern_sysctlc1458","title":"Per-OID Locking (<code>kern_sysctl.c:1458</code>)","text":"<p>Each OID has its own lock for handler serialization:</p> <pre><code>/* Default: exclusive for writes, shared for reads */\nif ((oid-&gt;oid_kind &amp; CTLFLAG_NOLOCK) == 0) {\n    lktype = (req-&gt;newptr != NULL) ? LK_EXCLUSIVE : LK_SHARED;\n    if (oid-&gt;oid_kind &amp; CTLFLAG_SHLOCK)\n        lktype = LK_SHARED;\n    if (oid-&gt;oid_kind &amp; CTLFLAG_EXLOCK)\n        lktype = LK_EXCLUSIVE;\n    lockmgr(&amp;oid-&gt;oid_lock, lktype);\n}\n</code></pre>"},{"location":"sys/kern/sysctl/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/sysctl/#sys___sysctl-kern_sysctlc1488","title":"sys___sysctl (<code>kern_sysctl.c:1488</code>)","text":"<pre><code>int sys___sysctl(struct sysmsg *sysmsg, const struct sysctl_args *uap);\n\nstruct sysctl_args {\n    int    *name;      /* MIB path (array of ints) */\n    u_int   namelen;   /* path length (max CTL_MAXNAME=12) */\n    void   *old;       /* output buffer */\n    size_t *oldlenp;   /* output size (in/out) */\n    void   *new;       /* input buffer */\n    size_t  newlen;    /* input size */\n};\n</code></pre>"},{"location":"sys/kern/sysctl/#mib-lookup-flow","title":"MIB Lookup Flow","text":"<ol> <li><code>sys___sysctl()</code> copies MIB from userspace</li> <li><code>userland_sysctl()</code> sets up request context</li> <li><code>sysctl_root()</code> traverses MIB tree</li> <li>Handler invoked with appropriate locking</li> <li>Data transferred via <code>sysctl_old_user()</code>/<code>sysctl_new_user()</code></li> </ol>"},{"location":"sys/kern/sysctl/#meta-sysctls-sysctl","title":"Meta-Sysctls (sysctl.*)","text":"<p>The <code>sysctl</code> top-level node provides tree introspection:</p> OID Function <code>sysctl.debug</code> Dump entire MIB tree <code>sysctl.name</code> Get string name from OID <code>sysctl.next</code> Get next OID in tree <code>sysctl.name2oid</code> Convert name to OID array <code>sysctl.oidfmt</code> Get OID type and format <code>sysctl.oiddescr</code> Get OID description"},{"location":"sys/kern/sysctl/#name2oid-example-kern_sysctlc799","title":"name2oid Example (<code>kern_sysctl.c:799</code>)","text":"<pre><code>static int\nname2oid(char *name, int *oid, int *len, struct sysctl_oid **oidpp)\n{\n    struct sysctl_oid_list *lsp = &amp;sysctl__children;\n    char *p;\n\n    for (*len = 0; *len &lt; CTL_MAXNAME;) {\n        p = strsep(&amp;name, \".\");\n        /* Find matching child */\n        SLIST_FOREACH(oidp, lsp, oid_link) {\n            if (strcmp(p, oidp-&gt;oid_name) == 0)\n                break;\n        }\n        if (oidp == NULL)\n            return ENOENT;\n        *oid++ = oidp-&gt;oid_number;\n        (*len)++;\n        /* Descend into children */\n        if ((oidp-&gt;oid_kind &amp; CTLTYPE) == CTLTYPE_NODE)\n            lsp = SYSCTL_CHILDREN(oidp);\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/sysctl/#standard-mibs-kern_mibc","title":"Standard MIBs (kern_mib.c)","text":""},{"location":"sys/kern/sysctl/#top-level-nodes","title":"Top-Level Nodes","text":"<pre><code>SYSCTL_NODE(, CTL_KERN,   kern,    CTLFLAG_RW, 0, \"High kernel\");\nSYSCTL_NODE(, CTL_VM,     vm,      CTLFLAG_RW, 0, \"Virtual memory\");\nSYSCTL_NODE(, CTL_VFS,    vfs,     CTLFLAG_RW, 0, \"File system\");\nSYSCTL_NODE(, CTL_NET,    net,     CTLFLAG_RW, 0, \"Network\");\nSYSCTL_NODE(, CTL_DEBUG,  debug,   CTLFLAG_RW, 0, \"Debugging\");\nSYSCTL_NODE(, CTL_HW,     hw,      CTLFLAG_RW, 0, \"Hardware\");\nSYSCTL_NODE(, CTL_MACHDEP,machdep, CTLFLAG_RW, 0, \"Machine dependent\");\nSYSCTL_NODE(, CTL_USER,   user,    CTLFLAG_RW, 0, \"User-level\");\nSYSCTL_NODE(, CTL_LWKT,   lwkt,    CTLFLAG_RW, 0, \"LWKT threads\");\n</code></pre>"},{"location":"sys/kern/sysctl/#key-kernel-parameters","title":"Key Kernel Parameters","text":"Sysctl Type Description <code>kern.ostype</code> string \"DragonFly\" <code>kern.osrelease</code> string OS version string <code>kern.version</code> string Full version/build info <code>kern.hostname</code> string System hostname <code>kern.securelevel</code> int Security level (-1 to 2) <code>kern.maxproc</code> int Max processes <code>kern.maxfiles</code> int Max open files <code>hw.ncpu</code> int Number of CPUs <code>hw.pagesize</code> int Memory page size <code>hw.physmem</code> long Physical memory"},{"location":"sys/kern/sysctl/#securelevel-kern_mibc204","title":"Securelevel (<code>kern_mib.c:204</code>)","text":"<pre><code>static int\nsysctl_kern_securelvl(SYSCTL_HANDLER_ARGS)\n{\n    int error, level;\n\n    level = securelevel;\n    error = sysctl_handle_int(oidp, &amp;level, 0, req);\n    if (error || !req-&gt;newptr)\n        return error;\n\n    /* Can only increase securelevel */\n    if (level &lt; securelevel)\n        return EPERM;\n\n    securelevel = level;\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/sysctl/#kinfo-structures-kern_kinfoc","title":"kinfo Structures (kern_kinfo.c)","text":"<p>The kinfo subsystem provides structured process information via sysctl:</p>"},{"location":"sys/kern/sysctl/#fill_kinfo_proc-kern_kinfoc116","title":"fill_kinfo_proc (<code>kern_kinfo.c:116</code>)","text":"<p>Populates <code>struct kinfo_proc</code> from a process:</p> <pre><code>void fill_kinfo_proc(struct proc *p, struct kinfo_proc *kp)\n{\n    kp-&gt;kp_paddr = (uintptr_t)p;\n    kp-&gt;kp_flags = p-&gt;p_flags;\n    kp-&gt;kp_stat = p-&gt;p_stat;\n    kp-&gt;kp_pid = p-&gt;p_pid;\n    kp-&gt;kp_ppid = p-&gt;p_pptr ? p-&gt;p_pptr-&gt;p_pid : -1;\n\n    /* Credentials */\n    if (p-&gt;p_ucred) {\n        kp-&gt;kp_uid = p-&gt;p_ucred-&gt;cr_uid;\n        kp-&gt;kp_ruid = p-&gt;p_ucred-&gt;cr_ruid;\n        /* ... */\n    }\n\n    /* VM statistics */\n    if ((vm = p-&gt;p_vmspace) != NULL) {\n        kp-&gt;kp_vm_map_size = vm-&gt;vm_map.size;\n        kp-&gt;kp_vm_rssize = vmspace_resident_count(vm);\n        /* ... */\n    }\n\n    /* Jail */\n    if (p-&gt;p_ucred &amp;&amp; jailed(p-&gt;p_ucred))\n        kp-&gt;kp_jailid = p-&gt;p_ucred-&gt;cr_prison-&gt;pr_id;\n}\n</code></pre>"},{"location":"sys/kern/sysctl/#fill_kinfo_lwp-kern_kinfoc225","title":"fill_kinfo_lwp (<code>kern_kinfo.c:225</code>)","text":"<p>Populates <code>struct kinfo_lwp</code> from an LWP:</p> <pre><code>void fill_kinfo_lwp(struct lwp *lwp, struct kinfo_lwp *kl)\n{\n    kl-&gt;kl_pid = lwp-&gt;lwp_proc-&gt;p_pid;\n    kl-&gt;kl_tid = lwp-&gt;lwp_tid;\n    kl-&gt;kl_flags = lwp-&gt;lwp_flags;\n    kl-&gt;kl_stat = lwp-&gt;lwp_stat;\n\n    /* CPU time */\n    kl-&gt;kl_uticks = lwp-&gt;lwp_thread-&gt;td_uticks;\n    kl-&gt;kl_sticks = lwp-&gt;lwp_thread-&gt;td_sticks;\n    kl-&gt;kl_cpuid = lwp-&gt;lwp_thread-&gt;td_gd-&gt;gd_cpuid;\n\n    /* Wait channel */\n    kl-&gt;kl_wchan = (uintptr_t)lwp-&gt;lwp_thread-&gt;td_wchan;\n    if (lwp-&gt;lwp_thread-&gt;td_wmesg)\n        strncpy(kl-&gt;kl_wmesg, lwp-&gt;lwp_thread-&gt;td_wmesg, WMESGLEN);\n}\n</code></pre>"},{"location":"sys/kern/sysctl/#initialization","title":"Initialization","text":""},{"location":"sys/kern/sysctl/#registration-order","title":"Registration Order","text":"<pre><code>/* Static OIDs registered at boot (kern_sysctl.c:524) */\nSYSINIT(sysctl, SI_BOOT1_POST, SI_ORDER_ANY, sysctl_register_all, 0);\n\nstatic void\nsysctl_register_all(void *arg)\n{\n    struct sysctl_oid **oidp;\n\n    SYSCTL_XLOCK();\n    SET_FOREACH(oidp, sysctl_set)\n        sysctl_register_oid(*oidp);\n    SYSCTL_XUNLOCK();\n}\n</code></pre> <p>OIDs are collected into the <code>sysctl_set</code> linker set via <code>DATA_SET()</code>.</p>"},{"location":"sys/kern/sysctl/#kernel-api","title":"Kernel API","text":""},{"location":"sys/kern/sysctl/#kernel_sysctl-kern_sysctlc1242","title":"kernel_sysctl (<code>kern_sysctl.c:1242</code>)","text":"<p>Kernel-internal sysctl access:</p> <pre><code>int kernel_sysctl(int *name, u_int namelen,\n    void *old, size_t *oldlenp,\n    void *new, size_t newlen, size_t *retval);\n</code></pre>"},{"location":"sys/kern/sysctl/#kernel_sysctlbyname-kern_sysctlc1295","title":"kernel_sysctlbyname (<code>kern_sysctl.c:1295</code>)","text":"<p>Access by string name:</p> <pre><code>int kernel_sysctlbyname(char *name,\n    void *old, size_t *oldlenp,\n    void *new, size_t newlen, size_t *retval);\n</code></pre>"},{"location":"sys/kern/sysctl/#usage-examples","title":"Usage Examples","text":""},{"location":"sys/kern/sysctl/#reading-a-sysctl-from-userspace","title":"Reading a Sysctl from Userspace","text":"<pre><code>#include &lt;sys/sysctl.h&gt;\n\nint ncpu;\nsize_t len = sizeof(ncpu);\nsysctlbyname(\"hw.ncpu\", &amp;ncpu, &amp;len, NULL, 0);\n</code></pre>"},{"location":"sys/kern/sysctl/#writing-a-sysctl","title":"Writing a Sysctl","text":"<pre><code>int new_maxproc = 2000;\nsysctlbyname(\"kern.maxprocperuid\", NULL, NULL,\n    &amp;new_maxproc, sizeof(new_maxproc));\n</code></pre>"},{"location":"sys/kern/sysctl/#listing-sysctls-sysctl-a","title":"Listing Sysctls (sysctl -a)","text":"<pre><code>int mib[2] = { CTL_SYSCTL, CTL_SYSCTL_NEXT };\nint next[CTL_MAXNAME];\nsize_t len;\n\nwhile (sysctl(mib, 2, next, &amp;len, name, namelen) == 0) {\n    /* Process 'next', then use it as new 'name' */\n}\n</code></pre>"},{"location":"sys/kern/sysctl/#see-also","title":"See Also","text":"<ul> <li>Tracing - KTR uses sysctl for enable masks</li> <li>Processes - Process info via kinfo</li> <li>Security - securelevel, jails, and sysctl access</li> </ul>"},{"location":"sys/kern/taskqueue/","title":"Task Queues","text":"<p>Task queues provide a mechanism for deferring work execution to a dedicated thread or software interrupt context. They are used throughout the kernel for asynchronous processing, interrupt deferral, and work serialization.</p> <p>DragonFly BSD provides two task queue implementations:</p> <ol> <li>Traditional taskqueue - Priority-based task scheduling with coalescing</li> <li>Group taskqueue (gtaskqueue) - CPU-affinity aware queues for device drivers</li> </ol> <p>Source files:</p> <ul> <li><code>sys/kern/subr_taskqueue.c</code> - Traditional taskqueue implementation (675 lines)</li> <li><code>sys/kern/subr_gtaskqueue.c</code> - Group taskqueue implementation (818 lines)</li> <li><code>sys/sys/taskqueue.h</code> - Traditional taskqueue API (172 lines)</li> <li><code>sys/sys/gtaskqueue.h</code> - Group taskqueue API (140 lines)</li> </ul>"},{"location":"sys/kern/taskqueue/#overview","title":"Overview","text":""},{"location":"sys/kern/taskqueue/#purpose","title":"Purpose","text":"<p>Task queues solve several common kernel programming challenges:</p> Problem Solution Interrupt latency Defer work from interrupt to thread context Lock ordering Execute work in a known context Serialization Process work items sequentially Resource constraints Batch operations efficiently"},{"location":"sys/kern/taskqueue/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    subgraph producer[\"Task Producer\"]\n        PROD[\"interrupt, syscall\"]\n    end\n\n    PROD --&gt;|\"taskqueue_enqueue()\"| TASK\n\n    subgraph task[\"struct task\"]\n        TASK[\"ta_functa_contextta_priorityta_pending\"]\n    end\n\n    TASK --&gt; TQ\n\n    subgraph tq[\"struct taskqueue\"]\n        QUEUE[\"Priority-ordered STAILQof pending tasks\"]\n        TQMETA[\"tq_enqueue() callback | tq_threads[]\"]\n    end\n\n    TQ --&gt;|\"taskqueue_run()\"| CONSUMER\n\n    subgraph consumer[\"Task Consumer\"]\n        CONSUMER[\"dedicated threador SWI handler\"]\n    end\n</code></pre>"},{"location":"sys/kern/taskqueue/#traditional-task-queue","title":"Traditional Task Queue","text":""},{"location":"sys/kern/taskqueue/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/taskqueue/#struct-task","title":"struct task","text":"<p>Represents a unit of work to be executed:</p> <pre><code>/* sys/sys/taskqueue.h:58 */\nstruct task {\n    STAILQ_ENTRY(task) ta_link;     /* link for queue */\n    struct taskqueue *ta_queue;      /* taskqueue enqueued on */\n    int     ta_pending;              /* count times queued */\n    int     ta_priority;             /* priority of task in queue */\n    task_fn_t *ta_func;              /* task handler */\n    void    *ta_context;             /* argument for handler */\n};\n</code></pre> <p>Fields:</p> Field Description <code>ta_link</code> Queue linkage <code>ta_queue</code> Current taskqueue (set on enqueue) <code>ta_pending</code> Enqueue counter (coalescing) <code>ta_priority</code> Lower value = higher priority <code>ta_func</code> Function to execute <code>ta_context</code> Argument passed to function"},{"location":"sys/kern/taskqueue/#struct-timeout_task","title":"struct timeout_task","text":"<p>Task with delayed execution support:</p> <pre><code>/* sys/sys/taskqueue.h:67 */\nstruct timeout_task {\n    struct task t;          /* embedded task */\n    struct callout c;       /* timer callout */\n    int    f;               /* DT_CALLOUT_ARMED flag */\n};\n</code></pre>"},{"location":"sys/kern/taskqueue/#struct-taskqueue","title":"struct taskqueue","text":"<p>The queue management structure (internal):</p> <pre><code>/* sys/kern/subr_taskqueue.c:49 */\nstruct taskqueue {\n    STAILQ_ENTRY(taskqueue) tq_link;    /* global queue list */\n    STAILQ_HEAD(, task)     tq_queue;   /* pending tasks */\n    const char              *tq_name;   /* queue name */\n    taskqueue_enqueue_fn    tq_enqueue; /* wakeup callback */\n    void                    *tq_context;/* callback context */\n    struct task             *tq_running;/* currently running task */\n    struct spinlock         tq_lock;    /* queue lock */\n    struct thread           **tq_threads;/* worker threads */\n    int                     tq_tcount;  /* thread count */\n    int                     tq_flags;   /* TQ_FLAGS_* */\n    int                     tq_callouts;/* armed timeout count */\n};\n</code></pre> <p>Queue flags:</p> <pre><code>#define TQ_FLAGS_ACTIVE     (1 &lt;&lt; 0)  /* queue is active */\n#define TQ_FLAGS_BLOCKED    (1 &lt;&lt; 1)  /* enqueue blocked */\n#define TQ_FLAGS_PENDING    (1 &lt;&lt; 2)  /* tasks pending while blocked */\n</code></pre>"},{"location":"sys/kern/taskqueue/#task-function-signature","title":"Task Function Signature","text":"<pre><code>typedef void task_fn_t(void *context, int pending);\n</code></pre> <p>Parameters:</p> <ul> <li><code>context</code> - Value from <code>ta_context</code></li> <li><code>pending</code> - Number of times task was enqueued before execution</li> </ul> <p>The <code>pending</code> count enables coalescing: if a task is enqueued multiple times before execution, only one invocation occurs with <code>pending</code> indicating the count.</p>"},{"location":"sys/kern/taskqueue/#api-reference","title":"API Reference","text":""},{"location":"sys/kern/taskqueue/#task-initialization","title":"Task Initialization","text":"<pre><code>/* Static initializer */\n#define TASK_INITIALIZER(priority, func, context)\n\n/* Runtime initialization */\n#define TASK_INIT(task, priority, func, context)\n\n/* Timeout task initialization */\nvoid _timeout_task_init(struct taskqueue *queue,\n    struct timeout_task *timeout_task, int priority,\n    task_fn_t func, void *context);\n#define TIMEOUT_TASK_INIT(queue, timeout_task, priority, func, context)\n</code></pre> <p>Example:</p> <pre><code>struct task my_task;\nTASK_INIT(&amp;my_task, 0, my_handler, my_arg);\n</code></pre>"},{"location":"sys/kern/taskqueue/#queue-creation","title":"Queue Creation","text":"<pre><code>/* sys/kern/subr_taskqueue.c:114 */\nstruct taskqueue *\ntaskqueue_create(const char *name, int mflags,\n                 taskqueue_enqueue_fn enqueue, void *context);\n</code></pre> <p>Creates a new taskqueue. The <code>enqueue</code> callback is called (with the queue locked) when a task is added to wake up the consumer.</p> <p>Parameters:</p> Parameter Description <code>name</code> Queue name (for debugging) <code>mflags</code> <code>kmalloc()</code> flags (M_WAITOK, M_NOWAIT) <code>enqueue</code> Callback to wake consumer <code>context</code> Argument for enqueue callback <p>Example:</p> <pre><code>/* Create queue with thread-based consumer */\nstruct taskqueue *tq;\ntq = taskqueue_create(\"myqueue\", M_WAITOK,\n                      taskqueue_thread_enqueue, &amp;tq);\n</code></pre>"},{"location":"sys/kern/taskqueue/#starting-worker-threads","title":"Starting Worker Threads","text":"<pre><code>/* sys/kern/subr_taskqueue.c:547 */\nint\ntaskqueue_start_threads(struct taskqueue **tqp, int count, int pri,\n                        int ncpu, const char *fmt, ...);\n</code></pre> <p>Creates worker threads to service the queue.</p> <p>Parameters:</p> Parameter Description <code>tqp</code> Pointer to taskqueue pointer <code>count</code> Number of threads to create <code>pri</code> Thread priority (TDPRI_*) <code>ncpu</code> CPU affinity (-1 = distribute across CPUs) <code>fmt</code> Thread name format string <p>Example:</p> <pre><code>taskqueue_start_threads(&amp;tq, 1, TDPRI_KERN_DAEMON, -1, \"myqueue\");\n</code></pre>"},{"location":"sys/kern/taskqueue/#task-enqueue","title":"Task Enqueue","text":"<pre><code>/* sys/kern/subr_taskqueue.c:246 */\nint taskqueue_enqueue(struct taskqueue *queue, struct task *task);\n</code></pre> <p>Enqueues a task for execution. If already enqueued, increments <code>ta_pending</code>.</p> <p>Returns: 0 on success, EPIPE if queue is being destroyed.</p> <pre><code>/* sys/kern/subr_taskqueue.c:263 */\nint taskqueue_enqueue_optq(struct taskqueue *queue,\n                           struct taskqueue **qpp, struct task *task);\n</code></pre> <p>Enqueue with optional queue migration. Sets <code>*qpp</code> to the actual queue used.</p> <pre><code>/* sys/kern/subr_taskqueue.c:340 */\nint taskqueue_enqueue_timeout(struct taskqueue *queue,\n                              struct timeout_task *timeout_task, int ticks);\n</code></pre> <p>Enqueue task after delay. If <code>ticks</code> is 0, enqueue immediately.</p>"},{"location":"sys/kern/taskqueue/#task-cancellation","title":"Task Cancellation","text":"<pre><code>/* sys/kern/subr_taskqueue.c:432 */\nint taskqueue_cancel(struct taskqueue *queue, struct task *task, u_int *pendp);\n</code></pre> <p>Removes task from queue. Returns EBUSY if task is currently executing. Sets <code>*pendp</code> to the pending count that was cancelled.</p> <pre><code>/* sys/kern/subr_taskqueue.c:444 */\nint taskqueue_cancel_simple(struct task *task);\n</code></pre> <p>Cancel task from any queue (uses task's recorded queue).</p> <pre><code>/* sys/kern/subr_taskqueue.c:468 */\nint taskqueue_cancel_timeout(struct taskqueue *queue,\n                             struct timeout_task *timeout_task, u_int *pendp);\n</code></pre> <p>Cancel timeout task and stop its callout.</p>"},{"location":"sys/kern/taskqueue/#drain-operations","title":"Drain Operations","text":"<pre><code>/* sys/kern/subr_taskqueue.c:489 */\nvoid taskqueue_drain(struct taskqueue *queue, struct task *task);\n</code></pre> <p>Blocks until task completes (if running) and is no longer pending.</p> <pre><code>/* sys/kern/subr_taskqueue.c:501 */\nvoid taskqueue_drain_simple(struct task *task);\n</code></pre> <p>Drain task from any queue.</p> <pre><code>/* sys/kern/subr_taskqueue.c:521 */\nvoid taskqueue_drain_timeout(struct taskqueue *queue,\n                             struct timeout_task *timeout_task);\n</code></pre> <p>Cancel callout and drain timeout task.</p>"},{"location":"sys/kern/taskqueue/#queue-control","title":"Queue Control","text":"<pre><code>/* sys/kern/subr_taskqueue.c:369 */\nvoid taskqueue_block(struct taskqueue *queue);\n</code></pre> <p>Prevents new enqueues from triggering the wakeup callback. Tasks are still queued but the consumer isn't notified.</p> <pre><code>/* sys/kern/subr_taskqueue.c:377 */\nvoid taskqueue_unblock(struct taskqueue *queue);\n</code></pre> <p>Re-enables enqueue notifications. If tasks were queued while blocked, triggers the callback.</p> <pre><code>/* sys/kern/subr_taskqueue.c:150 */\nvoid taskqueue_free(struct taskqueue *queue);\n</code></pre> <p>Destroys a taskqueue. Runs remaining tasks and terminates worker threads.</p> <pre><code>/* sys/kern/subr_taskqueue.c:168 */\nstruct taskqueue *taskqueue_find(const char *name);\n</code></pre> <p>Finds a taskqueue by name from the global list.</p>"},{"location":"sys/kern/taskqueue/#built-in-task-queues","title":"Built-in Task Queues","text":"<p>DragonFly provides several pre-configured taskqueues:</p>"},{"location":"sys/kern/taskqueue/#taskqueue_swi","title":"taskqueue_swi","text":"<p>Software interrupt serviced queue. Runs in SWI context (not preemptible by normal threads).</p> <pre><code>TASKQUEUE_DECLARE(swi);\n\n/* Usage */\ntaskqueue_enqueue(taskqueue_swi, &amp;my_task);\n</code></pre> <p>Defined at <code>subr_taskqueue.c:644</code>: <pre><code>TASKQUEUE_DEFINE(swi, taskqueue_swi_enqueue, 0,\n    register_swi(SWI_TQ, taskqueue_swi_run, NULL, \"swi_taskq\", NULL, -1));\n</code></pre></p>"},{"location":"sys/kern/taskqueue/#taskqueue_swi_mp","title":"taskqueue_swi_mp","text":"<p>MP-safe version of <code>taskqueue_swi</code>. Tasks may run concurrently on different CPUs.</p> <pre><code>TASKQUEUE_DECLARE(swi_mp);\n</code></pre>"},{"location":"sys/kern/taskqueue/#taskqueue_thread","title":"taskqueue_thread[]","text":"<p>Per-CPU thread-based taskqueues:</p> <pre><code>extern struct taskqueue *taskqueue_thread[];\n\n/* Usage - enqueue to current CPU's queue */\ntaskqueue_enqueue(taskqueue_thread[mycpuid], &amp;my_task);\n</code></pre> <p>Initialized at <code>subr_taskqueue.c:657</code>: <pre><code>for (cpu = 0; cpu &lt; ncpus; cpu++) {\n    taskqueue_thread[cpu] = taskqueue_create(\"thread\", M_INTWAIT,\n        taskqueue_thread_enqueue, &amp;taskqueue_thread[cpu]);\n    taskqueue_start_threads(&amp;taskqueue_thread[cpu], 1,\n        TDPRI_KERN_DAEMON, cpu, \"taskq_cpu %d\", cpu);\n}\n</code></pre></p>"},{"location":"sys/kern/taskqueue/#defining-custom-queues","title":"Defining Custom Queues","text":"<p>The <code>TASKQUEUE_DEFINE</code> macro creates a taskqueue with automatic initialization:</p> <pre><code>/* sys/sys/taskqueue.h:137 */\n#define TASKQUEUE_DEFINE(name, enqueue, context, init)\n\n/* Thread-based queue shorthand */\n#define TASKQUEUE_DEFINE_THREAD(name)\n</code></pre> <p>Example:</p> <pre><code>/* Define a custom thread-based taskqueue */\nTASKQUEUE_DEFINE_THREAD(mydriver);\n\n/* Use it */\ntaskqueue_enqueue(taskqueue_mydriver, &amp;task);\n</code></pre>"},{"location":"sys/kern/taskqueue/#implementation-details","title":"Implementation Details","text":""},{"location":"sys/kern/taskqueue/#priority-ordering","title":"Priority Ordering","text":"<p>Tasks are inserted in priority order (lower value = higher priority):</p> <pre><code>/* sys/kern/subr_taskqueue.c:214 */\nprev = STAILQ_LAST(&amp;queue-&gt;tq_queue, task, ta_link);\nif (!prev || prev-&gt;ta_priority &gt;= task-&gt;ta_priority) {\n    STAILQ_INSERT_TAIL(&amp;queue-&gt;tq_queue, task, ta_link);\n} else {\n    /* Find insertion point */\n    for (ins = STAILQ_FIRST(&amp;queue-&gt;tq_queue); ins;\n         prev = ins, ins = STAILQ_NEXT(ins, ta_link))\n        if (ins-&gt;ta_priority &lt; task-&gt;ta_priority)\n            break;\n    /* Insert before 'ins' */\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#task-coalescing","title":"Task Coalescing","text":"<p>Multiple enqueues before execution are coalesced:</p> <pre><code>/* sys/kern/subr_taskqueue.c:205 */\nif (task-&gt;ta_pending) {\n    KKASSERT(queue == task-&gt;ta_queue);\n    task-&gt;ta_pending++;\n    return 0;  /* Already queued, just increment count */\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#thread-loop","title":"Thread Loop","text":"<p>Worker threads run this loop:</p> <pre><code>/* sys/kern/subr_taskqueue.c:609 */\nvoid\ntaskqueue_thread_loop(void *arg)\n{\n    struct taskqueue **tqp, *tq;\n\n    tqp = arg;\n    tq = *tqp;\n    TQ_LOCK(tq);\n    while ((tq-&gt;tq_flags &amp; TQ_FLAGS_ACTIVE) != 0) {\n        taskqueue_run(tq, 1);\n        TQ_SLEEP(tq, tq, \"tqthr\");  /* Wait for work */\n    }\n    tq-&gt;tq_tcount--;\n    TQ_UNLOCK(tq);\n    wakeup_one(tq-&gt;tq_threads);\n    lwkt_exit();\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#locking","title":"Locking","text":"<p>Taskqueues use spinlocks for synchronization:</p> <pre><code>static __inline void TQ_LOCK(struct taskqueue *tq)\n{\n    spin_lock(&amp;tq-&gt;tq_lock);\n}\n\nstatic __inline void TQ_UNLOCK(struct taskqueue *tq)\n{\n    spin_unlock(&amp;tq-&gt;tq_lock);\n}\n\nstatic __inline void TQ_SLEEP(struct taskqueue *tq, void *ident, const char *wmesg)\n{\n    ssleep(ident, &amp;tq-&gt;tq_lock, 0, wmesg, 0);\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#group-task-queues","title":"Group Task Queues","text":"<p>Group taskqueues (gtaskqueue) provide CPU-affinity aware task scheduling, primarily for device drivers and network processing.</p>"},{"location":"sys/kern/taskqueue/#key-differences-from-traditional-taskqueue","title":"Key Differences from Traditional Taskqueue","text":"Feature taskqueue gtaskqueue Priority Numeric (lower = higher) u_short priority Coalescing Via ta_pending count TASK_ENQUEUED flag Function signature <code>(void*, int)</code> <code>(void*)</code> CPU binding Per-queue threads Per-CPU queue groups IRQ affinity Manual Automatic"},{"location":"sys/kern/taskqueue/#data-structures_1","title":"Data Structures","text":""},{"location":"sys/kern/taskqueue/#struct-gtask","title":"struct gtask","text":"<pre><code>/* sys/sys/gtaskqueue.h:54 */\nstruct gtask {\n    STAILQ_ENTRY(gtask) ta_link;    /* link for queue */\n    uint16_t ta_flags;               /* TASK_* flags */\n    u_short ta_priority;             /* priority */\n    gtask_fn_t *ta_func;             /* handler */\n    void    *ta_context;             /* handler argument */\n};\n</code></pre> <p>Task flags:</p> <pre><code>#define TASK_ENQUEUED   0x1  /* currently on queue */\n#define TASK_NOENQUEUE  0x2  /* blocked from enqueue */\n#define TASK_NETWORK    0x4  /* network task (epoch handling) */\n</code></pre>"},{"location":"sys/kern/taskqueue/#struct-grouptask","title":"struct grouptask","text":"<p>Extended task with queue group metadata:</p> <pre><code>/* sys/sys/gtaskqueue.h:68 */\nstruct grouptask {\n    struct gtask            gt_task;        /* embedded gtask */\n    void                    *gt_taskqueue;  /* assigned queue */\n    LIST_ENTRY(grouptask)   gt_list;        /* group list linkage */\n    void                    *gt_uniq;       /* unique identifier */\n    char                    gt_name[GROUPTASK_NAMELEN];\n    device_t                gt_dev;         /* associated device */\n    struct resource         *gt_irq;        /* interrupt resource */\n    int                     gt_cpu;         /* bound CPU */\n};\n</code></pre>"},{"location":"sys/kern/taskqueue/#struct-taskqgroup","title":"struct taskqgroup","text":"<p>Manages a set of per-CPU queues:</p> <pre><code>/* sys/kern/subr_gtaskqueue.c:573 */\nstruct taskqgroup {\n    struct taskqgroup_cpu tqg_queue[MAXCPU];  /* per-CPU queues */\n    struct lock     tqg_lock;                  /* group lock */\n    const char *    tqg_name;                  /* group name */\n    int             tqg_cnt;                   /* active queue count */\n};\n\nstruct taskqgroup_cpu {\n    LIST_HEAD(, grouptask) tgc_tasks;  /* tasks on this CPU */\n    struct gtaskqueue *tgc_taskq;       /* the taskqueue */\n    int         tgc_cnt;                /* task count */\n    int         tgc_cpu;                /* CPU number */\n};\n</code></pre>"},{"location":"sys/kern/taskqueue/#api-reference_1","title":"API Reference","text":""},{"location":"sys/kern/taskqueue/#task-initialization_1","title":"Task Initialization","text":"<pre><code>#define GTASK_INIT(gtask, flags, priority, func, context)\n#define GROUPTASK_INIT(gtask, priority, func, context)\n</code></pre> <p>Example:</p> <pre><code>struct grouptask my_gtask;\nGROUPTASK_INIT(&amp;my_gtask, 0, my_handler, my_arg);\n</code></pre>"},{"location":"sys/kern/taskqueue/#queue-group-creation","title":"Queue Group Creation","text":"<pre><code>/* sys/kern/subr_gtaskqueue.c:782 */\nstruct taskqgroup *\ntaskqgroup_create(const char *name, int cnt, int stride);\n</code></pre> <p>Creates a taskqueue group with <code>cnt</code> queues distributed across CPUs with <code>stride</code> spacing.</p>"},{"location":"sys/kern/taskqueue/#attaching-tasks","title":"Attaching Tasks","text":"<pre><code>/* sys/kern/subr_gtaskqueue.c:643 */\nvoid\ntaskqgroup_attach(struct taskqgroup *qgroup, struct grouptask *gtask,\n    void *uniq, device_t dev, struct resource *irq, const char *name);\n</code></pre> <p>Attaches a grouptask to a queue group. Automatically selects the least loaded queue that doesn't already serve <code>uniq</code>.</p> <pre><code>/* sys/kern/subr_gtaskqueue.c:683 */\nint\ntaskqgroup_attach_cpu(struct taskqgroup *qgroup, struct grouptask *gtask,\n    void *uniq, int cpu, device_t dev, struct resource *irq, const char *name);\n</code></pre> <p>Attaches to a specific CPU's queue.</p>"},{"location":"sys/kern/taskqueue/#enqueue","title":"Enqueue","text":"<pre><code>/* sys/kern/subr_gtaskqueue.c:200 */\nint grouptaskqueue_enqueue(struct gtaskqueue *queue, struct gtask *gtask);\n\n/* Convenience macro */\n#define GROUPTASK_ENQUEUE(gtask) \\\n    grouptaskqueue_enqueue((gtask)-&gt;gt_taskqueue, &amp;(gtask)-&gt;gt_task)\n</code></pre> <p>Returns 0 on success, EAGAIN if blocked (TASK_NOENQUEUE).</p>"},{"location":"sys/kern/taskqueue/#task-control","title":"Task Control","text":"<pre><code>/* sys/kern/subr_gtaskqueue.c:165 */\nvoid grouptask_block(struct grouptask *grouptask);\n</code></pre> <p>Drains and blocks task from being enqueued.</p> <pre><code>/* sys/kern/subr_gtaskqueue.c:183 */\nvoid grouptask_unblock(struct grouptask *grouptask);\n</code></pre> <p>Allows task to be enqueued again.</p>"},{"location":"sys/kern/taskqueue/#detaching","title":"Detaching","text":"<pre><code>/* sys/kern/subr_gtaskqueue.c:731 */\nvoid taskqgroup_detach(struct taskqgroup *qgroup, struct grouptask *gtask);\n</code></pre> <p>Removes grouptask from queue group.</p>"},{"location":"sys/kern/taskqueue/#drain-operations_1","title":"Drain Operations","text":"<pre><code>/* sys/kern/subr_gtaskqueue.c:406 */\nvoid gtaskqueue_drain(struct gtaskqueue *queue, struct gtask *gtask);\n\n/* sys/kern/subr_gtaskqueue.c:414 */\nvoid gtaskqueue_drain_all(struct gtaskqueue *queue);\n\n/* sys/kern/subr_gtaskqueue.c:806 */\nvoid taskqgroup_drain_all(struct taskqgroup *tqg);\n</code></pre>"},{"location":"sys/kern/taskqueue/#built-in-queue-group","title":"Built-in Queue Group","text":""},{"location":"sys/kern/taskqueue/#qgroup_softirq","title":"qgroup_softirq","text":"<p>Pre-defined queue group for soft IRQ processing:</p> <pre><code>/* sys/sys/gtaskqueue.h:137 */\nTASKQGROUP_DECLARE(softirq);\n\n/* Definition at subr_gtaskqueue.c:51 */\nTASKQGROUP_DEFINE(softirq, ncpus, 1);\n</code></pre> <p>Creates one queue per CPU with stride 1.</p>"},{"location":"sys/kern/taskqueue/#defining-custom-queue-groups","title":"Defining Custom Queue Groups","text":"<pre><code>/* sys/sys/gtaskqueue.h:117 */\n#define TASKQGROUP_DEFINE(name, cnt, stride)\n</code></pre> <p>Example:</p> <pre><code>/* Define queue group with one queue per 2 CPUs */\nTASKQGROUP_DEFINE(mydriver, ncpus/2, 2);\n\n/* Attach a task */\ntaskqgroup_attach(qgroup_mydriver, &amp;my_gtask, device,\n                  dev, irq, \"mydriver_task\");\n</code></pre>"},{"location":"sys/kern/taskqueue/#load-balancing","title":"Load Balancing","text":"<p>The <code>taskqgroup_find()</code> function implements load-aware queue selection:</p> <pre><code>/* sys/kern/subr_gtaskqueue.c:604 */\nstatic int\ntaskqgroup_find(struct taskqgroup *qgroup, void *uniq)\n{\n    /* Two passes:\n     * 1. Find queue with least tasks not serving 'uniq'\n     * 2. If all serve 'uniq', find queue with least tasks\n     */\n    for (idx = -1, mincnt = INT_MAX, strict = 1; mincnt == INT_MAX;\n        strict = 0) {\n        for (i = 0; i &lt; qgroup-&gt;tqg_cnt; i++) {\n            if (qgroup-&gt;tqg_queue[i].tgc_cnt &gt; mincnt)\n                continue;\n            if (strict) {\n                LIST_FOREACH(n, &amp;qgroup-&gt;tqg_queue[i].tgc_tasks, gt_list)\n                    if (n-&gt;gt_uniq == uniq)\n                        break;\n                if (n != NULL)\n                    continue;\n            }\n            mincnt = qgroup-&gt;tqg_queue[i].tgc_cnt;\n            idx = i;\n        }\n    }\n    return (idx);\n}\n</code></pre> <p>This ensures: 1. Tasks from same device spread across queues (strict pass) 2. Queue with least total tasks selected (load balancing)</p>"},{"location":"sys/kern/taskqueue/#cpu-binding","title":"CPU Binding","text":"<p>After queue assignment, threads can be bound to specific CPUs:</p> <pre><code>/* sys/kern/subr_gtaskqueue.c:760 */\nvoid\ntaskqgroup_bind(struct taskqgroup *qgroup)\n{\n    struct taskq_bind_task *gtask;\n\n    if (qgroup-&gt;tqg_cnt == 1)\n        return;\n\n    for (i = 0; i &lt; qgroup-&gt;tqg_cnt; i++) {\n        gtask = kmalloc(sizeof(*gtask), M_DEVBUF, M_WAITOK);\n        GTASK_INIT(&amp;gtask-&gt;bt_task, 0, 0, taskqgroup_binder, gtask);\n        gtask-&gt;bt_cpuid = qgroup-&gt;tqg_queue[i].tgc_cpu;\n        grouptaskqueue_enqueue(qgroup-&gt;tqg_queue[i].tgc_taskq,\n                               &amp;gtask-&gt;bt_task);\n    }\n}\n</code></pre> <p>The binder task migrates the worker thread to its assigned CPU:</p> <pre><code>static void\ntaskqgroup_binder(void *ctx)\n{\n    struct taskq_bind_task *gtask = ctx;\n    lwkt_migratecpu(gtask-&gt;bt_cpuid);\n    kfree(gtask, M_DEVBUF);\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#usage-examples","title":"Usage Examples","text":""},{"location":"sys/kern/taskqueue/#simple-deferred-work","title":"Simple Deferred Work","text":"<pre><code>#include &lt;sys/taskqueue.h&gt;\n\nstruct my_softc {\n    struct task work_task;\n    /* ... */\n};\n\nstatic void\nmy_work_handler(void *arg, int pending)\n{\n    struct my_softc *sc = arg;\n\n    /* Process 'pending' accumulated work items */\n    while (pending--) {\n        /* Do work */\n    }\n}\n\nstatic int\nmy_attach(device_t dev)\n{\n    struct my_softc *sc = device_get_softc(dev);\n\n    TASK_INIT(&amp;sc-&gt;work_task, 0, my_work_handler, sc);\n    return (0);\n}\n\nstatic void\nmy_interrupt(void *arg)\n{\n    struct my_softc *sc = arg;\n\n    /* Defer processing to taskqueue */\n    taskqueue_enqueue(taskqueue_swi, &amp;sc-&gt;work_task);\n}\n\nstatic int\nmy_detach(device_t dev)\n{\n    struct my_softc *sc = device_get_softc(dev);\n\n    /* Ensure task completes before detach */\n    taskqueue_drain(taskqueue_swi, &amp;sc-&gt;work_task);\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#timeout-task","title":"Timeout Task","text":"<pre><code>struct my_softc {\n    struct timeout_task delayed_task;\n};\n\nstatic void\nmy_delayed_handler(void *arg, int pending)\n{\n    /* Called after delay expires */\n}\n\nstatic int\nmy_attach(device_t dev)\n{\n    struct my_softc *sc = device_get_softc(dev);\n\n    TIMEOUT_TASK_INIT(taskqueue_thread[0], &amp;sc-&gt;delayed_task,\n                      0, my_delayed_handler, sc);\n    return (0);\n}\n\nstatic void\nmy_start_delayed(struct my_softc *sc)\n{\n    /* Execute in 1 second */\n    taskqueue_enqueue_timeout(taskqueue_thread[0],\n                              &amp;sc-&gt;delayed_task, hz);\n}\n\nstatic int\nmy_detach(device_t dev)\n{\n    struct my_softc *sc = device_get_softc(dev);\n\n    taskqueue_drain_timeout(taskqueue_thread[0], &amp;sc-&gt;delayed_task);\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#device-driver-with-group-taskqueue","title":"Device Driver with Group Taskqueue","text":"<pre><code>#include &lt;sys/gtaskqueue.h&gt;\n\nstruct my_softc {\n    struct grouptask irq_task;\n    device_t dev;\n    struct resource *irq;\n};\n\nstatic void\nmy_irq_handler(void *arg)\n{\n    struct my_softc *sc = arg;\n\n    /* Process interrupt work */\n}\n\nstatic int\nmy_attach(device_t dev)\n{\n    struct my_softc *sc = device_get_softc(dev);\n\n    sc-&gt;dev = dev;\n    sc-&gt;irq = bus_alloc_resource_any(dev, SYS_RES_IRQ, &amp;rid, RF_ACTIVE);\n\n    GROUPTASK_INIT(&amp;sc-&gt;irq_task, 0, my_irq_handler, sc);\n    taskqgroup_attach(qgroup_softirq, &amp;sc-&gt;irq_task, sc,\n                      dev, sc-&gt;irq, \"my_irq\");\n\n    return (0);\n}\n\nstatic void\nmy_interrupt(void *arg)\n{\n    struct my_softc *sc = arg;\n\n    /* Defer to CPU-local taskqueue */\n    GROUPTASK_ENQUEUE(&amp;sc-&gt;irq_task);\n}\n\nstatic int\nmy_detach(device_t dev)\n{\n    struct my_softc *sc = device_get_softc(dev);\n\n    taskqgroup_detach(qgroup_softirq, &amp;sc-&gt;irq_task);\n    bus_release_resource(dev, SYS_RES_IRQ, rid, sc-&gt;irq);\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#custom-taskqueue","title":"Custom Taskqueue","text":"<pre><code>/* Define a driver-specific taskqueue */\nstatic struct taskqueue *my_taskqueue;\n\nstatic void\nmy_init(void)\n{\n    my_taskqueue = taskqueue_create(\"mydriver\", M_WAITOK,\n                                    taskqueue_thread_enqueue,\n                                    &amp;my_taskqueue);\n    taskqueue_start_threads(&amp;my_taskqueue, 2, TDPRI_KERN_DAEMON,\n                            -1, \"mydriver_tq\");\n}\n\nstatic void\nmy_uninit(void)\n{\n    taskqueue_free(my_taskqueue);\n}\n</code></pre>"},{"location":"sys/kern/taskqueue/#performance-considerations","title":"Performance Considerations","text":""},{"location":"sys/kern/taskqueue/#choosing-queue-type","title":"Choosing Queue Type","text":"Use Case Recommended Queue Simple deferred work <code>taskqueue_swi</code> Thread context needed <code>taskqueue_thread[mycpuid]</code> High-frequency driver work <code>qgroup_softirq</code> Custom serialization Create dedicated queue"},{"location":"sys/kern/taskqueue/#avoiding-common-pitfalls","title":"Avoiding Common Pitfalls","text":"<ol> <li>Task structure lifetime - Task must remain valid until drained</li> <li>Per-CPU tasks - Don't share task structures across CPUs</li> <li>Blocking in handlers - Consider thread-based queues for blocking work</li> <li>Priority inversion - Lower priority values execute first</li> </ol>"},{"location":"sys/kern/taskqueue/#lock-ordering","title":"Lock Ordering","text":"<p>Taskqueue locks should generally be acquired after device locks:</p> <pre><code>device lock \u2192 taskqueue lock (via enqueue/drain)\n</code></pre> <p>Avoid holding taskqueue locks while calling into the task handler.</p>"},{"location":"sys/kern/taskqueue/#see-also","title":"See Also","text":"<ul> <li>Synchronization - Kernel locking primitives</li> <li>LWKT - Lightweight kernel threads</li> <li>Scheduling - Thread scheduling</li> <li>Devices - Device driver framework</li> <li>kevent - Event notification mechanism</li> </ul>"},{"location":"sys/kern/time/","title":"Time and Timer Subsystems","text":""},{"location":"sys/kern/time/#overview","title":"Overview","text":"<p>The DragonFly BSD kernel implements a sophisticated multi-layered timekeeping architecture that provides both system-wide and per-CPU timing facilities. The design separates hardware timer abstraction from software timer services, enabling flexible and efficient time management across different hardware platforms.</p>"},{"location":"sys/kern/time/#architecture-layers","title":"Architecture Layers","text":"<pre><code>flowchart TB\n    USER[\"User Applications(syscalls: gettimeofday, nanosleep, etc)\"]\n\n    REPR[\"Time Representation(timeval, timespec, sbintime_t)\"]\n\n    CALLOUT[\"Callout/Timeout Mechanism (kern_timeout.c)\u2022 Wheel-based per-CPU callout queues\u2022 Frontend (struct callout) / Backend (struct _callout)\"]\n\n    SYSTIMER[\"System Timer Infrastructure (kern_systimer.c)\u2022 Software timers built on hardware abstraction\u2022 Periodic and one-shot timer support\"]\n\n    CLOCK[\"Clock Interrupts (kern_clock.c)\u2022 hardclock() - Main system clock at hz (100Hz)\u2022 statclock() - Statistics clock at stathz (128Hz)\u2022 schedclock() - Scheduler clock at 50Hz\"]\n\n    CPUTIMER[\"Hardware Timer Abstraction (kern_cputimer.c)\u2022 struct cputimer with count() method\u2022 Registration and selection of hardware timers\u2022 Examples: TSC, i8254, ACPI-fast, HPET\"]\n\n    HW[\"Hardware Timers(TSC, HPET, i8254, ACPI-fast, etc)\"]\n\n    USER --&gt; REPR\n    REPR --&gt; CALLOUT\n    CALLOUT --&gt; SYSTIMER\n    SYSTIMER --&gt; CLOCK\n    CLOCK --&gt; CPUTIMER\n    CPUTIMER --&gt; HW\n</code></pre>"},{"location":"sys/kern/time/#key-source-files","title":"Key Source Files","text":"File Lines Purpose <code>kern_clock.c</code> 1866 System clock interrupts, hardclock, statclock, schedclock, NTP adjustment <code>kern_cputimer.c</code> 673 Hardware timer abstraction layer (cputimer) <code>kern_systimer.c</code> 418 Software timer infrastructure built on cputimer <code>kern_timeout.c</code> 1153 Callout/timeout mechanism with wheel-based implementation <code>kern_time.c</code> 1247 Time-related system calls (gettimeofday, nanosleep, etc.) <code>kern_ntptime.c</code> 872 NTP time adjustment with PLL/FLL"},{"location":"sys/kern/time/#design-principles","title":"Design Principles","text":"<ol> <li> <p>Hardware Independence: The cputimer abstraction allows the kernel to work with different hardware timer sources without modifying upper layers.</p> </li> <li> <p>Per-CPU Scalability: Most timing facilities are per-CPU to minimize lock contention and cache line bouncing.</p> </li> <li> <p>Multiple Time Bases: The kernel maintains several time representations:</p> </li> <li>Realtime (<code>gettimeofday</code>): Wall clock time, affected by NTP adjustments</li> <li>Monotonic (<code>clock_gettime(CLOCK_MONOTONIC)</code>): Always increasing, not affected by time adjustments</li> <li> <p>Uptime: Time since boot, not affected by suspend/resume</p> </li> <li> <p>Lazy Updates: Many time globals are updated passively to minimize overhead (e.g., <code>time_second</code>, <code>time_uptime</code>).</p> </li> <li> <p>NTP Integration: Built-in support for Network Time Protocol adjustments using phase-locked loop (PLL) and frequency-locked loop (FLL) algorithms.</p> </li> </ol>"},{"location":"sys/kern/time/#key-concepts","title":"Key Concepts","text":""},{"location":"sys/kern/time/#clock-types-and-frequencies","title":"Clock Types and Frequencies","text":"<p>DragonFly BSD uses three main periodic clock interrupts, each with a different frequency and purpose:</p>"},{"location":"sys/kern/time/#hardclock-main-system-clock","title":"hardclock (Main System Clock)","text":"<ul> <li>Frequency: <code>hz</code> (typically 100 Hz, configurable)</li> <li>Function: <code>hardclock()</code> in kern_clock.c:552</li> <li>Purpose: </li> <li>Updates master tick counter (<code>ticks</code>, <code>sbticks</code>)</li> <li>Advances global time values (<code>time_second</code>, <code>time_uptime</code>)</li> <li>Processes callouts (timeouts)</li> <li>Handles NTP time adjustments</li> <li>Per-CPU scheduling accounting</li> <li>Per-CPU State: <code>gd-&gt;gd_hardclock</code> (struct systimer)</li> </ul> <p>The hardclock is the primary system heartbeat. It runs on each CPU and coordinates time advancement across the system.</p> <pre><code>/*\n * Hardclock runs on each CPU at hz frequency (typically 100Hz).\n * Updates time, processes timeouts, and performs per-CPU bookkeeping.\n */\nstatic void\nhardclock(systimer_t info, int in_ipi, struct intrframe *frame)\n{\n    /* Update tick counters */\n    ticks++;\n    sbticks = mftb();\n\n    /* Update passive time globals */\n    gd-&gt;gd_time_seconds = basetime[0].tv_sec;\n\n    /* Process callouts for this CPU */\n    softclock(info, in_ipi, frame);\n\n    /* Scheduling and accounting */\n    ...\n}\n</code></pre>"},{"location":"sys/kern/time/#statclock-statistics-clock","title":"statclock (Statistics Clock)","text":"<ul> <li>Frequency: <code>stathz</code> (typically 128 Hz, default; or profhz for profiling)</li> <li>Function: <code>statclock()</code> in kern_clock.c:892</li> <li>Purpose:</li> <li>CPU usage statistics (user/system time)</li> <li>Profiling support</li> <li>Randomized to prevent synchronization with hardclock</li> <li>Per-process and per-thread accounting</li> <li>Per-CPU State: <code>gd-&gt;gd_statclock</code> (struct systimer)</li> </ul> <p>The statclock runs at a slightly different frequency than hardclock to provide statistical sampling that doesn't synchronize with the main clock. This prevents phase-locking artifacts in profiling data.</p> <pre><code>/*\n * Statclock collects CPU usage statistics.\n * Frequency is randomized to prevent phase locking with hardclock.\n */\nstatic void\nstatclock(systimer_t info, int in_ipi, struct intrframe *frame)\n{\n    struct thread *td = curthread;\n    struct proc *p = td-&gt;td_proc;\n\n    /* Account CPU time to current thread/process */\n    if (CLKF_USERMODE(frame)) {\n        p-&gt;p_uticks++;\n        if (p-&gt;p_nice &gt; NZERO)\n            cp_time[CP_NICE]++;\n        else\n            cp_time[CP_USER]++;\n    } else {\n        p-&gt;p_sticks++;\n        cp_time[CP_SYS]++;\n    }\n\n    /* Profiling support */\n    if (p-&gt;p_profthreads)\n        addupc_intr(p, CLKF_PC(frame), 1);\n}\n</code></pre>"},{"location":"sys/kern/time/#schedclock-scheduler-clock","title":"schedclock (Scheduler Clock)","text":"<ul> <li>Frequency: 50 Hz (fixed)</li> <li>Function: <code>schedclock()</code> in kern_clock.c:1028</li> <li>Purpose:</li> <li>Scheduler hint processing</li> <li>Thread priority recalculation</li> <li>Load balancing decisions</li> <li>Per-CPU State: <code>gd-&gt;gd_schedclock</code> (struct systimer)</li> </ul> <p>The schedclock runs at a lower frequency than the other clocks, handling less time-critical scheduler operations.</p>"},{"location":"sys/kern/time/#timer-types","title":"Timer Types","text":""},{"location":"sys/kern/time/#cputimer-hardware-timer-abstraction","title":"cputimer (Hardware Timer Abstraction)","text":"<p>The <code>cputimer</code> is a hardware abstraction that provides a consistent interface to various hardware timer sources.</p> <p>Key Operations: - <code>count()</code>: Read current timer value (monotonically increasing) - <code>fromhz(hz)</code>: Convert frequency to timer units - <code>fromus(us)</code>: Convert microseconds to timer units - <code>fromnano(ns)</code>: Convert nanoseconds to timer units</p> <p>Common Hardware Timers: - TSC (Time Stamp Counter): Per-CPU cycle counter on x86 - HPET (High Precision Event Timer): Modern high-resolution timer - i8254: Legacy PC timer (8254 PIT) - ACPI-fast: ACPI power management timer</p> <p>Selection: The kernel selects the best available cputimer based on quality rating (see <code>cputimer_register()</code> in kern_cputimer.c:291).</p>"},{"location":"sys/kern/time/#systimer-software-timer","title":"systimer (Software Timer)","text":"<p>The <code>systimer</code> builds software timer facilities on top of the hardware cputimer abstraction.</p> <p>Types: - Periodic: Fires at regular intervals (used by hardclock, statclock, schedclock) - One-shot: Fires once at a specified time (used internally)</p> <p>Key Functions: - <code>systimer_init_periodic()</code> (kern_systimer.c:292): Create periodic timer - <code>systimer_init_periodic_nq()</code> (kern_systimer.c:317): Create periodic timer without initial queue - <code>systimer_add()</code>: Add timer to active queue - <code>systimer_del()</code>: Remove timer from queue</p> <p>Each CPU maintains its own systimer queue to avoid lock contention.</p>"},{"location":"sys/kern/time/#callout-timeout-mechanism","title":"callout (Timeout Mechanism)","text":"<p>Callouts provide a high-level timeout mechanism for scheduling function calls in the future.</p> <p>Key Features: - Wheel-based algorithm: O(1) insertion and deletion using a timing wheel - Per-CPU wheels: Separate callout wheels per CPU for scalability - Two-tier structure:   - Frontend: <code>struct callout</code> - Embedded in client structures   - Backend: <code>struct _callout</code> - Actual queued element with wheel linkage - Flags support: <code>CALLOUT_MPSAFE</code>, <code>CALLOUT_ACTIVE</code>, etc.</p> <p>Common Use Cases: - Network packet timeouts - Device driver timeouts - Timed events in kernel subsystems</p>"},{"location":"sys/kern/time/#time-bases-and-representation","title":"Time Bases and Representation","text":"<p>DragonFly maintains several time bases for different purposes:</p>"},{"location":"sys/kern/time/#realtime-wall-clock-time","title":"Realtime (Wall Clock Time)","text":"<ul> <li>Source: <code>basetime[]</code> FIFO in kern_clock.c</li> <li>Representation: <code>struct timespec</code> (seconds + nanoseconds)</li> <li>Access: <code>gettimeofday()</code>, <code>clock_gettime(CLOCK_REALTIME)</code></li> <li>Characteristics: Can jump forward/backward with NTP adjustments</li> </ul> <p>The <code>basetime</code> array is a FIFO that allows the NTP code to adjust time without holding locks during the entire hardclock interrupt.</p> <pre><code>/*\n * basetime[] is a FIFO of 16 entries that stores real time.\n * NTP adjustments update the write pointer, hardclock reads\n * from the read pointer.\n */\nstatic struct timespec basetime[BASETIME_ARYSIZE];  /* FIFO */\nstatic int basetime_index;  /* Read pointer */\n</code></pre>"},{"location":"sys/kern/time/#monotonic-time","title":"Monotonic Time","text":"<ul> <li>Source: <code>gd-&gt;gd_cpuclock_base</code> + cputimer count</li> <li>Access: <code>clock_gettime(CLOCK_MONOTONIC)</code>, <code>nanouptime()</code></li> <li>Characteristics: Always increases, never affected by time adjustments</li> </ul> <p>Monotonic time is ideal for measuring intervals because it's immune to clock adjustments.</p>"},{"location":"sys/kern/time/#uptime","title":"Uptime","text":"<ul> <li>Source: <code>time_uptime</code> global, updated in hardclock</li> <li>Access: Direct global variable access (for kernel code)</li> <li>Characteristics: Seconds since boot, low-resolution</li> </ul>"},{"location":"sys/kern/time/#per-cpu-time-state","title":"Per-CPU Time State","text":"<p>Each CPU maintains timing state in its <code>globaldata</code> structure (see <code>sys/sys/globaldata.h</code>):</p> <pre><code>struct globaldata {\n    /* ... */\n\n    /* Passive time values (updated in hardclock) */\n    time_t          gd_time_seconds;    /* Cached time_second */\n    time_t          gd_time_uptime;     /* Cached time_uptime */\n\n    /* High-precision time base */\n    sysclock_t      gd_cpuclock_base;   /* Monotonic base for this CPU */\n\n    /* System clocks (periodic systimers) */\n    struct systimer gd_hardclock;       /* Main clock at hz */\n    struct systimer gd_statclock;       /* Stats clock at stathz */\n    struct systimer gd_schedclock;      /* Scheduler clock at 50Hz */\n\n    /* Callout wheel for this CPU */\n    /* (see struct _callout_tailq in kern_timeout.c) */\n\n    /* ... */\n};\n</code></pre>"},{"location":"sys/kern/time/#ntp-time-adjustment","title":"NTP Time Adjustment","text":"<p>The kernel includes a sophisticated NTP implementation (<code>kern_ntptime.c</code>) that adjusts system time gradually to synchronize with network time sources.</p> <p>Key Components: - PLL (Phase-Locked Loop): Adjusts time based on phase error - FLL (Frequency-Locked Loop): Adjusts frequency based on long-term drift - Adaptive switching: Automatically switches between PLL and FLL based on conditions</p> <p>System Calls: - <code>ntp_adjtime()</code>: User-space interface to NTP adjustment (sys/timex.h) - <code>adjtime()</code>: Legacy BSD interface for time adjustment</p> <p>Time Adjustment Variables: <pre><code>long time_tick;              /* Nominal tick in us */\nlong time_adjtime;           /* Adjustment remaining (us) */\nint64_t time_offset;         /* Current offset for NTP (ns scaled) */\nlong time_freq;              /* Frequency adjustment (scaled ppm) */\nlong time_maxerror;          /* Maximum error (us) */\nlong time_esterror;          /* Estimated error (us) */\nint time_status;             /* Clock status flags */\n</code></pre></p> <p>The NTP code runs as part of hardclock and makes small adjustments to <code>basetime[]</code> to gradually bring the system clock in sync with the reference time source.</p>"},{"location":"sys/kern/time/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/time/#struct-cputimer","title":"struct cputimer","text":"<p>Location: <code>sys/sys/systimer.h</code></p> <p>The <code>cputimer</code> structure provides a hardware-independent interface to various timer sources.</p> <pre><code>struct cputimer {\n    SLIST_ENTRY(cputimer) next;\n    const char *name;           /* Timer name (e.g., \"TSC\", \"HPET\") */\n    int pri;                    /* Priority for selection */\n    int type;                   /* Timer type flags */\n    uint64_t freq;              /* Timer frequency in Hz */\n    sysclock_t (*count)(void);  /* Read current count */\n    sysclock_t (*fromhz)(int);  /* Convert hz to timer units */\n    sysclock_t (*fromus)(int);  /* Convert microseconds to timer units */\n    sysclock_t (*fromnano)(int64_t); /* Convert nanoseconds to timer units */\n    void (*construct)(struct cputimer *, sysclock_t); /* Constructor */\n    void (*destruct)(struct cputimer *);              /* Destructor */\n    sysclock_t freq64_count;    /* For high-precision scaling */\n    int64_t freq64_nsec;        /* For nanosecond conversion */\n};\n</code></pre> <p>Key Fields:</p> <ul> <li><code>name</code>: Human-readable timer name (e.g., \"TSC\", \"HPET\", \"i8254\", \"ACPI-fast\")</li> <li><code>pri</code>: Priority for automatic selection (higher = preferred)</li> <li><code>CPUTIMER_PRI_TSC</code> (1000) - TSC when synchronized</li> <li><code>CPUTIMER_PRI_HPET</code> (900) - HPET</li> <li><code>CPUTIMER_PRI_ACPI</code> (800) - ACPI PM timer</li> <li><code>CPUTIMER_PRI_8254</code> (100) - Legacy 8254 PIT</li> <li><code>CPUTIMER_PRI_DUMMY</code> (-1) - Placeholder</li> <li><code>freq</code>: Timer frequency in Hz (used for time conversions)</li> <li><code>count()</code>: Returns current monotonically-increasing timer count</li> <li><code>fromhz()</code>, <code>fromus()</code>, <code>fromnano()</code>: Convert time units to timer ticks</li> </ul> <p>Global Variables: <pre><code>struct cputimer *sys_cputimer;  /* Currently selected timer */\n</code></pre></p> <p>Referenced in: kern_cputimer.c:54</p>"},{"location":"sys/kern/time/#struct-systimer","title":"struct systimer","text":"<p>Location: <code>sys/sys/systimer.h</code></p> <p>The <code>systimer</code> structure represents a software timer built on top of the cputimer hardware abstraction.</p> <pre><code>struct systimer {\n    TAILQ_ENTRY(systimer) node;     /* Queue linkage */\n    sysclock_t time;                /* Next fire time (absolute) */\n    sysclock_t periodic;            /* Period (0 = one-shot) */\n    sysclock_t which;               /* For load distribution */\n    systimer_func_t func;           /* Callback function */\n    void *data;                     /* Callback argument */\n    int flags;                      /* Flags (periodic, etc.) */\n    struct globaldata *gd;          /* CPU this timer runs on */\n    int freq;                       /* For periodic timers */\n};\n\ntypedef void (*systimer_func_t)(struct systimer *, int, struct intrframe *);\n</code></pre> <p>Key Fields:</p> <ul> <li><code>time</code>: Absolute time when timer should fire (in sysclock_t units)</li> <li><code>periodic</code>: If non-zero, timer fires repeatedly at this interval</li> <li><code>func</code>: Callback function invoked when timer fires</li> <li><code>data</code>: Opaque data passed to callback</li> <li><code>gd</code>: Pointer to globaldata - specifies which CPU owns this timer</li> <li><code>freq</code>: For periodic timers, the desired frequency</li> </ul> <p>Timer Types: - Periodic: Used for hardclock, statclock, schedclock - One-shot: Used internally for callout wheel advancement</p> <p>Per-CPU Queue: Each CPU's <code>globaldata</code> contains a queue of active systimers.</p> <p>Referenced in: kern_systimer.c:292, kern_clock.c:552</p>"},{"location":"sys/kern/time/#struct-callout-and-struct-_callout","title":"struct callout and struct _callout","text":"<p>Location: <code>sys/sys/callout.h</code></p> <p>DragonFly uses a two-tier callout structure:</p>"},{"location":"sys/kern/time/#frontend-struct-callout","title":"Frontend: struct callout","text":"<pre><code>struct callout {\n    struct _callout *lh;        /* Link head (backend pointer) */\n};\n</code></pre> <p>The frontend structure is embedded in client data structures (e.g., network mbuf, driver state). It's lightweight and only contains a pointer to the backend.</p>"},{"location":"sys/kern/time/#backend-struct-_callout","title":"Backend: struct _callout","text":"<p>Location: kern_timeout.c:138 (internal structure)</p> <pre><code>struct _callout {\n    union {\n        TAILQ_ENTRY(_callout) tq;       /* Wheel queue linkage */\n        struct _callout *rq_next;       /* Free list linkage */\n    } c_links;\n    struct callout *c_lh;               /* Back-pointer to frontend */\n    struct softclock_pcpu *c_base;      /* CPU this callout runs on */\n    void (*c_func)(void *);             /* Callback function */\n    void *c_arg;                        /* Callback argument */\n    int c_time;                         /* Wheel position (ticks) */\n    int c_flags;                        /* Flags (see below) */\n};\n</code></pre> <p>Key Fields:</p> <ul> <li><code>c_links.tq</code>: Queue linkage for timing wheel</li> <li><code>c_lh</code>: Back-pointer to the frontend callout structure</li> <li><code>c_base</code>: Per-CPU softclock structure (identifies which CPU)</li> <li><code>c_func</code>: Function to call when timer expires</li> <li><code>c_arg</code>: Argument passed to callback function</li> <li><code>c_time</code>: Absolute tick count when callout should fire</li> <li><code>c_flags</code>: Status and control flags</li> </ul> <p>Callout Flags (defined in sys/sys/callout.h): <pre><code>#define CALLOUT_PENDING         0x0001  /* Callout is on wheel */\n#define CALLOUT_ACTIVE          0x0002  /* Callout is active */\n#define CALLOUT_MPSAFE          0x0008  /* Can run without Giant */\n#define CALLOUT_DID_INIT        0x0010  /* callout_init() called */\n#define CALLOUT_AUTOLOCK        0x0020  /* Automatic locking */\n#define CALLOUT_WAITING         0x0040  /* Thread waiting for callout */\n#define CALLOUT_EXECUTED        0x0080  /* Callout executed */\n</code></pre></p> <p>Design Rationale: The two-tier design allows: 1. Lightweight embedding: Client structures only need one pointer 2. O(1) operations: Checking if callout is pending is just a NULL check 3. Efficient migration: Backend can be moved between CPUs without updating client</p> <p>Referenced in: kern_timeout.c:788 (callout_reset), kern_timeout.c:865 (callout_stop)</p>"},{"location":"sys/kern/time/#callout-wheel-structure","title":"Callout Wheel Structure","text":"<p>Location: kern_timeout.c:132</p> <pre><code>struct softclock_pcpu {\n    struct spinlock spin;               /* Protects callout wheel */\n    struct colist callwheel[BUCKETS];   /* Timing wheel */\n    struct colist calltodo;             /* Overflow list */\n    struct colist *callcursor;          /* Current wheel position */\n    struct _callout *running;           /* Currently executing callout */\n    int softticks;                      /* Local tick counter */\n    int curticks;                       /* Current tick being processed */\n    int isrunning;                      /* Processing in progress */\n    struct thread *thread;              /* Softclock thread */\n};\n</code></pre> <p>Key Fields:</p> <ul> <li><code>callwheel[BUCKETS]</code>: Circular array of callout queues (timing wheel)</li> <li>Default <code>BUCKETS = 512</code> (can be tuned)</li> <li>Each bucket represents one tick</li> <li>Wheel covers 512 ticks (5.12 seconds at 100 Hz)</li> <li><code>calltodo</code>: Overflow list for callouts beyond the wheel range</li> <li><code>callcursor</code>: Points to current wheel bucket being processed</li> <li><code>running</code>: Currently executing callout (for detecting recursion)</li> <li><code>softticks</code>: Local tick counter for this CPU</li> <li><code>thread</code>: Dedicated kernel thread for running callouts</li> </ul> <p>Wheel Algorithm: <pre><code>Bucket index = (current_tick + delta_ticks) % BUCKETS\n\nExample at hz=100 (10ms per tick):\n- Bucket 0: Callouts firing at tick 0, 512, 1024, ...\n- Bucket 1: Callouts firing at tick 1, 513, 1025, ...\n- Bucket N: Callouts firing at tick N, N+512, N+1024, ...\n</code></pre></p> <p>For callouts beyond 512 ticks, they're placed on the overflow list and moved to the wheel as time advances.</p> <p>Per-CPU Design: Each CPU has its own <code>softclock_pcpu</code> structure to avoid lock contention.</p> <p>Referenced in: kern_timeout.c:788, kern_clock.c:712 (softclock processing)</p>"},{"location":"sys/kern/time/#time-representation-types","title":"Time Representation Types","text":"<p>DragonFly uses several time representation types:</p>"},{"location":"sys/kern/time/#struct-timeval-traditional-bsd","title":"struct timeval (Traditional BSD)","text":"<pre><code>struct timeval {\n    time_t      tv_sec;     /* Seconds since epoch */\n    suseconds_t tv_usec;    /* Microseconds [0, 999999] */\n};\n</code></pre> <p>Usage: <code>gettimeofday()</code>, legacy interfaces Resolution: Microsecond (1e-6 seconds) Range: ~68 years (32-bit time_t) or ~292 billion years (64-bit time_t)</p>"},{"location":"sys/kern/time/#struct-timespec-posix","title":"struct timespec (POSIX)","text":"<pre><code>struct timespec {\n    time_t  tv_sec;         /* Seconds since epoch */\n    long    tv_nsec;        /* Nanoseconds [0, 999999999] */\n};\n</code></pre> <p>Usage: <code>clock_gettime()</code>, <code>nanosleep()</code>, modern interfaces Resolution: Nanosecond (1e-9 seconds) Range: Same as timeval but with nanosecond precision</p>"},{"location":"sys/kern/time/#sysclock_t-internal-kernel","title":"sysclock_t (Internal Kernel)","text":"<pre><code>typedef int64_t sysclock_t;    /* System clock counter */\n</code></pre> <p>Usage: Internal timing, cputimer counts Resolution: Depends on hardware timer frequency Range: 2^63 ticks (effectively unlimited for reasonable frequencies) Characteristics: Monotonic, never wraps in practice</p>"},{"location":"sys/kern/time/#sbintime_t-signed-binary-time","title":"sbintime_t (Signed Binary Time)","text":"<pre><code>typedef int64_t sbintime_t;    /* 2^-32 second units */\n</code></pre> <p>Usage: High-precision internal timing Resolution: 2^-32 seconds (~233 picoseconds) Range: ~68 years with picosecond precision Characteristics: Fast binary arithmetic, no division needed</p> <p>Conversion: <pre><code>#define SBT_1S  ((sbintime_t)1 &lt;&lt; 32)  /* One second */\n#define SBT_1MS (SBT_1S / 1000)         /* One millisecond */\n#define SBT_1US (SBT_1S / 1000000)      /* One microsecond */\n#define SBT_1NS (SBT_1S / 1000000000)   /* One nanosecond */\n</code></pre></p>"},{"location":"sys/kern/time/#global-time-variables","title":"Global Time Variables","text":"<p>Location: kern_clock.c</p> <pre><code>/* Master tick counters */\nvolatile int ticks;                     /* Tick counter at hz */\nvolatile int64_t sbticks;               /* Tick counter in sbintime_t */\n\n/* Passive time globals (updated in hardclock) */\ntime_t time_second;                     /* Current realtime (seconds) */\ntime_t time_uptime;                     /* Uptime (seconds since boot) */\n\n/* Real time FIFO for NTP */\n#define BASETIME_ARYSIZE 16\nstatic struct timespec basetime[BASETIME_ARYSIZE];\nstatic volatile int basetime_index;     /* Read index (FIFO) */\n\n/* Boot time reference */\nstruct timespec boottime;               /* Time at boot */\n\n/* NTP adjustment state */\nextern long time_tick;                  /* Nominal tick (us) */\nextern long time_adjtime;               /* Adjustment remaining (us) */\nextern int64_t time_offset;             /* NTP offset (scaled ns) */\nextern long time_freq;                  /* Frequency adjustment (ppm) */\n</code></pre> <p>Key Variables:</p> <ul> <li><code>ticks</code>: Global tick counter, incremented at <code>hz</code> frequency (typically 100 Hz)</li> <li>Used for timeout calculations</li> <li>Wraps after ~497 days at hz=100</li> <li><code>sbticks</code>: High-precision tick counter in sbintime_t format</li> <li><code>time_second</code>: Cached realtime in seconds (low overhead to read)</li> <li><code>time_uptime</code>: Seconds since boot (monotonic)</li> <li><code>basetime[]</code>: FIFO of realtime values (NTP adjusts write end, hardclock reads read end)</li> <li><code>boottime</code>: Reference time when system booted</li> </ul> <p>Thread Safety: Most time globals are updated atomically or use lock-free FIFO mechanisms (basetime).</p>"},{"location":"sys/kern/time/#key-functions","title":"Key Functions","text":""},{"location":"sys/kern/time/#hardware-timer-registration-and-selection","title":"Hardware Timer Registration and Selection","text":""},{"location":"sys/kern/time/#cputimer_register","title":"cputimer_register()","text":"<p>Location: kern_cputimer.c:291</p> <pre><code>void cputimer_register(struct cputimer *timer);\n</code></pre> <p>Registers a hardware timer with the kernel. If the new timer has higher priority than the current timer, it becomes the active timer.</p> <p>Parameters: - <code>timer</code>: Pointer to initialized cputimer structure</p> <p>Selection Logic: <pre><code>if (timer-&gt;pri &gt; sys_cputimer-&gt;pri) {\n    sys_cputimer = timer;  /* Switch to better timer */\n}\n</code></pre></p> <p>Common Priorities: - TSC (synchronized): 1000 - HPET: 900 - ACPI PM Timer: 800 - i8254 PIT: 100</p>"},{"location":"sys/kern/time/#cputimer_select","title":"cputimer_select()","text":"<p>Location: kern_cputimer.c:325</p> <pre><code>void cputimer_select(struct cputimer *timer, int prio);\n</code></pre> <p>Explicitly selects a timer, optionally changing its priority.</p>"},{"location":"sys/kern/time/#system-timer-operations","title":"System Timer Operations","text":""},{"location":"sys/kern/time/#systimer_init_periodic","title":"systimer_init_periodic()","text":"<p>Location: kern_systimer.c:292</p> <pre><code>void systimer_init_periodic(systimer_t info, systimer_func_t func,\n                            void *data, int freq);\n</code></pre> <p>Initializes a periodic software timer.</p> <p>Parameters: - <code>info</code>: Pointer to systimer structure to initialize - <code>func</code>: Callback function to invoke on each timer expiration - <code>data</code>: Opaque pointer passed to callback - <code>freq</code>: Desired frequency in Hz</p> <p>Usage Example (hardclock initialization in kern_clock.c:373): <pre><code>systimer_init_periodic(&amp;gd-&gt;gd_hardclock, hardclock, NULL, hz);\n</code></pre></p>"},{"location":"sys/kern/time/#systimer_init_periodic_nq","title":"systimer_init_periodic_nq()","text":"<p>Location: kern_systimer.c:317</p> <pre><code>void systimer_init_periodic_nq(systimer_t info, systimer_func_t func,\n                               void *data, int freq);\n</code></pre> <p>Same as <code>systimer_init_periodic()</code> but doesn't initially add the timer to the active queue. Must call <code>systimer_add()</code> to activate.</p>"},{"location":"sys/kern/time/#systimer_add","title":"systimer_add()","text":"<p>Location: kern_systimer.c:342</p> <pre><code>void systimer_add(systimer_t info);\n</code></pre> <p>Adds a systimer to the active queue for its CPU.</p>"},{"location":"sys/kern/time/#systimer_del","title":"systimer_del()","text":"<p>Location: kern_systimer.c:368</p> <pre><code>void systimer_del(systimer_t info);\n</code></pre> <p>Removes a systimer from the active queue.</p>"},{"location":"sys/kern/time/#callout-operations","title":"Callout Operations","text":""},{"location":"sys/kern/time/#callout_init","title":"callout_init()","text":"<p>Location: kern_timeout.c:636</p> <pre><code>void callout_init(struct callout *c);\nvoid callout_init_mp(struct callout *c);  /* MP-safe variant */\nvoid callout_init_lk(struct callout *c, struct lock *lk);  /* With lock */\n</code></pre> <p>Initializes a callout structure. Must be called before using the callout.</p> <p>Variants: - <code>callout_init()</code>: Basic initialization - <code>callout_init_mp()</code>: Marks callout as MP-safe (no Giant lock needed) - <code>callout_init_lk()</code>: Associates a lock with the callout (automatic locking)</p> <p>Example: <pre><code>struct callout my_callout;\ncallout_init_mp(&amp;my_callout);\n</code></pre></p>"},{"location":"sys/kern/time/#callout_reset","title":"callout_reset()","text":"<p>Location: kern_timeout.c:788</p> <pre><code>void callout_reset(struct callout *c, int ticks, void (*func)(void *), void *arg);\nvoid callout_reset_bycpu(struct callout *c, int ticks, void (*func)(void *),\n                         void *arg, int cpuid);\n</code></pre> <p>Schedules or reschedules a callout to fire after the specified number of ticks.</p> <p>Parameters: - <code>c</code>: Callout structure (must be initialized) - <code>ticks</code>: Number of ticks until expiration (relative to current time) - <code>func</code>: Callback function - <code>arg</code>: Argument passed to callback</p> <p>Behavior: - If callout is already pending, it's cancelled and rescheduled - Returns immediately; callback executes asynchronously - Thread-safe; uses per-CPU locks</p> <p>Example: <pre><code>/* Fire callback in 5 seconds (assuming hz=100) */\ncallout_reset(&amp;my_callout, 5 * hz, my_timeout_func, my_data);\n</code></pre></p> <p>CPU Assignment: - <code>callout_reset()</code>: Uses current CPU - <code>callout_reset_bycpu()</code>: Explicitly specifies CPU</p>"},{"location":"sys/kern/time/#callout_stop","title":"callout_stop()","text":"<p>Location: kern_timeout.c:865</p> <pre><code>int callout_stop(struct callout *c);\nint callout_stop_sync(struct callout *c);\n</code></pre> <p>Cancels a pending callout.</p> <p>Returns: - Non-zero if callout was pending (successfully cancelled) - Zero if callout was not pending</p> <p>Variants: - <code>callout_stop()</code>: Basic stop, returns immediately - <code>callout_stop_sync()</code>: Waits for callback to complete if currently executing</p> <p>Example: <pre><code>if (callout_stop(&amp;my_callout)) {\n    /* Callout was pending and has been cancelled */\n} else {\n    /* Callout wasn't pending or already fired */\n}\n</code></pre></p>"},{"location":"sys/kern/time/#callout_terminate","title":"callout_terminate()","text":"<p>Location: kern_timeout.c:928</p> <pre><code>void callout_terminate(struct callout *c);\n</code></pre> <p>Terminates a callout, stopping it and waiting for any in-progress execution to complete. Called during cleanup/shutdown.</p>"},{"location":"sys/kern/time/#time-retrieval-functions","title":"Time Retrieval Functions","text":""},{"location":"sys/kern/time/#getmicrotime-getnanotime","title":"getmicrotime() / getnanotime()","text":"<p>Location: kern_time.c:462, kern_time.c:478</p> <pre><code>void getmicrotime(struct timeval *tvp);   /* Realtime, low precision */\nvoid getnanotime(struct timespec *tsp);   /* Realtime, low precision */\n</code></pre> <p>Returns cached realtime with low overhead (no hardware timer read).</p> <p>Precision: Updated only at hardclock frequency (typically 100 Hz, 10ms resolution) Use Case: When exact time isn't critical (logging, coarse timestamps)</p>"},{"location":"sys/kern/time/#microtime-nanotime","title":"microtime() / nanotime()","text":"<p>Location: kern_time.c:494, kern_time.c:510</p> <pre><code>void microtime(struct timeval *tvp);      /* Realtime, high precision */\nvoid nanotime(struct timespec *tsp);      /* Realtime, high precision */\n</code></pre> <p>Returns precise realtime by reading the hardware timer.</p> <p>Precision: Hardware timer resolution (nanosecond range for modern timers) Overhead: Higher than getmicrotime/getnanotime (hardware read required) Use Case: When accurate timestamps are needed</p>"},{"location":"sys/kern/time/#getmicrouptime-getnanouptime","title":"getmicrouptime() / getnanouptime()","text":"<p>Location: kern_time.c:526, kern_time.c:542</p> <pre><code>void getmicrouptime(struct timeval *tvp);  /* Monotonic, low precision */\nvoid getnanouptime(struct timespec *tsp);  /* Monotonic, low precision */\n</code></pre> <p>Returns cached monotonic uptime.</p> <p>Characteristics:  - Monotonically increasing - Not affected by time adjustments - Low overhead (no hardware read)</p>"},{"location":"sys/kern/time/#microuptime-nanouptime","title":"microuptime() / nanouptime()","text":"<p>Location: kern_time.c:558, kern_time.c:574</p> <pre><code>void microuptime(struct timeval *tvp);     /* Monotonic, high precision */\nvoid nanouptime(struct timespec *tsp);     /* Monotonic, high precision */\n</code></pre> <p>Returns precise monotonic uptime by reading hardware timer.</p> <p>Use Case: Measuring time intervals accurately</p>"},{"location":"sys/kern/time/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/time/#sys_clock_gettime","title":"sys_clock_gettime()","text":"<p>Location: kern_time.c:672</p> <pre><code>int sys_clock_gettime(struct sysmsg *sysmsg,\n                      const struct clock_gettime_args *uap);\n</code></pre> <p>POSIX clock_gettime() system call.</p> <p>Supported Clocks: - <code>CLOCK_REALTIME</code>: Realtime (can jump with NTP) - <code>CLOCK_MONOTONIC</code>: Monotonic time (always increasing) - <code>CLOCK_UPTIME</code>: Uptime since boot - <code>CLOCK_VIRTUAL</code>: Per-process virtual time - <code>CLOCK_PROF</code>: Per-process profiling time - <code>CLOCK_SECOND</code>: Low-resolution second counter</p>"},{"location":"sys/kern/time/#sys_nanosleep","title":"sys_nanosleep()","text":"<p>Location: kern_time.c:823</p> <pre><code>int sys_nanosleep(struct sysmsg *sysmsg,\n                  const struct nanosleep_args *uap);\n</code></pre> <p>POSIX nanosleep() - sleep for specified time.</p> <p>Implementation: 1. Converts timespec to ticks 2. Calls <code>tsleep()</code> with timeout 3. Handles early wakeup (signals) 4. Returns remaining time if interrupted</p>"},{"location":"sys/kern/time/#sys_gettimeofday","title":"sys_gettimeofday()","text":"<p>Location: kern_time.c:273</p> <pre><code>int sys_gettimeofday(struct sysmsg *sysmsg,\n                     const struct gettimeofday_args *uap);\n</code></pre> <p>Traditional BSD gettimeofday() system call.</p> <p>Implementation: Calls <code>microtime()</code> to get high-precision realtime.</p>"},{"location":"sys/kern/time/#sys_adjtime-sys_ntp_adjtime","title":"sys_adjtime() / sys_ntp_adjtime()","text":"<p>Location: kern_time.c:370, kern_ntptime.c:260</p> <pre><code>int sys_adjtime(struct sysmsg *sysmsg, const struct adjtime_args *uap);\nint sys_ntp_adjtime(struct sysmsg *sysmsg, const struct ntp_adjtime_args *uap);\n</code></pre> <p>Adjust system time gradually (for time synchronization).</p> <ul> <li><code>adjtime()</code>: Simple time adjustment (legacy BSD)</li> <li><code>ntp_adjtime()</code>: Full NTP adjustment interface</li> </ul>"},{"location":"sys/kern/time/#code-flow-examples","title":"Code Flow Examples","text":""},{"location":"sys/kern/time/#example-1-callout-lifecycle","title":"Example 1: Callout Lifecycle","text":"<p>Complete example of using callouts for a timeout:</p> <pre><code>/* Device driver timeout example */\n\nstruct my_device {\n    struct callout watchdog;\n    int timeout_pending;\n    /* ... other fields ... */\n};\n\n/* Initialize callout (in device attach) */\nvoid\nmy_device_attach(struct my_device *dev)\n{\n    callout_init_mp(&amp;dev-&gt;watchdog);\n    dev-&gt;timeout_pending = 0;\n}\n\n/* Start operation with timeout */\nvoid\nmy_device_start_operation(struct my_device *dev)\n{\n    /* Start hardware operation */\n    my_device_hw_start(dev);\n\n    /* Set 5-second watchdog timeout (hz = 100) */\n    callout_reset(&amp;dev-&gt;watchdog, 5 * hz, my_device_timeout, dev);\n    dev-&gt;timeout_pending = 1;\n}\n\n/* Timeout callback (runs in softclock context) */\nvoid\nmy_device_timeout(void *arg)\n{\n    struct my_device *dev = arg;\n\n    kprintf(\"Device operation timed out!\\n\");\n    dev-&gt;timeout_pending = 0;\n\n    /* Reset hardware */\n    my_device_hw_reset(dev);\n}\n\n/* Operation completed successfully */\nvoid\nmy_device_operation_complete(struct my_device *dev)\n{\n    /* Cancel timeout */\n    if (callout_stop(&amp;dev-&gt;watchdog)) {\n        /* Timeout was pending and successfully cancelled */\n        dev-&gt;timeout_pending = 0;\n    }\n\n    /* Process completion */\n    my_device_handle_completion(dev);\n}\n\n/* Cleanup (in device detach) */\nvoid\nmy_device_detach(struct my_device *dev)\n{\n    /* Stop and wait for any in-progress timeout */\n    callout_terminate(&amp;dev-&gt;watchdog);\n}\n</code></pre> <p>Call Flow: <pre><code>1. Device attach\n   \u251c\u2500&gt; callout_init_mp(&amp;dev-&gt;watchdog)\n   \u2514\u2500&gt; Callout ready for use\n\n2. Start operation\n   \u251c\u2500&gt; my_device_hw_start()\n   \u251c\u2500&gt; callout_reset(&amp;dev-&gt;watchdog, 5*hz, callback, dev)\n   \u2502   \u251c\u2500&gt; Allocate backend _callout\n   \u2502   \u251c\u2500&gt; Calculate expiration: ticks + 5*hz\n   \u2502   \u251c\u2500&gt; Insert into callout wheel bucket\n   \u2502   \u2514\u2500&gt; Mark as CALLOUT_PENDING\n   \u2514\u2500&gt; Return to caller\n\n3a. Normal completion path:\n    \u251c\u2500&gt; Operation completes\n    \u251c\u2500&gt; callout_stop(&amp;dev-&gt;watchdog)\n    \u2502   \u251c\u2500&gt; Remove from wheel\n    \u2502   \u251c\u2500&gt; Clear CALLOUT_PENDING\n    \u2502   \u2514\u2500&gt; Return 1 (was pending)\n    \u2514\u2500&gt; Handle completion\n\n3b. Timeout path:\n    \u251c\u2500&gt; hardclock() runs at hz frequency\n    \u251c\u2500&gt; softclock() processes callout wheel\n    \u251c\u2500&gt; Finds expired callout\n    \u251c\u2500&gt; Calls my_device_timeout(dev)\n    \u2502   \u251c\u2500&gt; Log timeout\n    \u2502   \u2514\u2500&gt; Reset hardware\n    \u2514\u2500&gt; Clear CALLOUT_PENDING\n\n4. Device detach\n   \u251c\u2500&gt; callout_terminate(&amp;dev-&gt;watchdog)\n   \u251c\u2500&gt; Stop callout\n   \u2514\u2500&gt; Wait for completion if running\n</code></pre></p>"},{"location":"sys/kern/time/#example-2-hardclock-processing-flow","title":"Example 2: Hardclock Processing Flow","text":"<p>Location: kern_clock.c:552</p> <pre><code>/*\n * Hardclock runs on each CPU at hz frequency.\n * This is the primary system heartbeat.\n */\nstatic void\nhardclock(systimer_t info, int in_ipi, struct intrframe *frame)\n{\n    globaldata_t gd = mycpu();\n    thread_t td = curthread;\n\n    /* Step 1: Update global tick counters */\n    ++ticks;                        /* Increment master tick counter */\n    sbticks = mftb();               /* Read timebase for sbintime */\n\n    /* Step 2: Update passive time globals from basetime FIFO */\n    gd-&gt;gd_time_seconds = basetime[basetime_index].tv_sec;\n    time_second = gd-&gt;gd_time_seconds;\n    time_uptime = (int)(systimer_count() - systimer_offset) / sys_cputimer-&gt;freq;\n\n    /* Step 3: Handle NTP adjustments (on CPU 0 only) */\n    if (gd-&gt;gd_cpuid == 0) {\n        ntp_update_second(&amp;basetime[basetime_index], ticks);\n    }\n\n    /* Step 4: Process callouts (timeouts) */\n    softclock(info, in_ipi, frame);\n\n    /* Step 5: Per-thread accounting */\n    if (td != &amp;gd-&gt;gd_idlethread) {\n        td-&gt;td_wakefromcpu = -1;\n        if (CLKF_USERMODE(frame)) {\n            ++td-&gt;td_uticks;        /* User mode */\n        } else {\n            ++td-&gt;td_sticks;        /* System mode */\n        }\n    }\n\n    /* Step 6: Scheduler operations (every 10 ticks on DragonFly) */\n    if ((ticks % 10) == 0) {\n        scheduler_tick(td, frame);\n    }\n}\n</code></pre> <p>Flow Diagram: <pre><code>Hardware Timer Interrupt\n        \u2193\nsystimer_intr() [kern_systimer.c:69]\n        \u2193\nhardclock() [kern_clock.c:552]\n        \u251c\u2500&gt; Update ticks, sbticks\n        \u251c\u2500&gt; Update time_second, time_uptime (from basetime FIFO)\n        \u251c\u2500&gt; CPU 0: ntp_update_second()\n        \u2502           \u251c\u2500&gt; Apply PLL/FLL adjustments\n        \u2502           \u2514\u2500&gt; Update basetime[] write index\n        \u251c\u2500&gt; softclock() [kern_clock.c:712]\n        \u2502   \u2514\u2500&gt; Process callout wheel\n        \u2502       \u251c\u2500&gt; Advance wheel cursor\n        \u2502       \u251c\u2500&gt; For each expired callout:\n        \u2502       \u2502   \u251c\u2500&gt; Remove from wheel\n        \u2502       \u2502   \u251c\u2500&gt; Mark as executing\n        \u2502       \u2502   \u251c\u2500&gt; Call callback function\n        \u2502       \u2502   \u2514\u2500&gt; Mark as executed\n        \u2502       \u2514\u2500&gt; Check overflow list (calltodo)\n        \u251c\u2500&gt; Thread accounting (td_uticks/td_sticks)\n        \u2514\u2500&gt; Every 10 ticks: scheduler_tick()\n</code></pre></p>"},{"location":"sys/kern/time/#example-3-callout-wheel-operation","title":"Example 3: Callout Wheel Operation","text":"<p>Wheel Structure (BUCKETS = 512):</p> <p>Current ticks = 1000, Wheel bucket index = ticks % BUCKETS = 1000 % 512 = 488</p> <pre><code>flowchart LR\n    subgraph WHEEL[\"Callout Wheel\"]\n        B488[\"488\"]\n        B489[\"489\"]\n        B490[\"490\"]\n        DOTS[\"...\"]\n        B487[\"487\"]\n    end\n\n    CURSOR[\"Currentcursor\"] -.-&gt; B488\n</code></pre> <p>When <code>callout_reset(&amp;c, 50, func, arg)</code> is called:</p> <ul> <li>Expiration tick = 1000 + 50 = 1050</li> <li>Bucket = 1050 % 512 = 26</li> <li>Insert callout into bucket 26</li> </ul> <p>When ticks reaches 1050:</p> <ul> <li>Cursor advances to bucket 26</li> <li>Callout found and executed</li> </ul> <p>Insertion Algorithm: <pre><code>void\ncallout_reset(struct callout *c, int ticks, void (*func)(void *), void *arg)\n{\n    struct softclock_pcpu *sc = &amp;softclock_pcpu_ary[mycpuid];\n    struct _callout *cc;\n    int expire_tick;\n    int bucket;\n\n    spin_lock(&amp;sc-&gt;spin);\n\n    /* Calculate absolute expiration */\n    expire_tick = sc-&gt;softticks + ticks;\n\n    /* Allocate backend if needed */\n    if (c-&gt;lh == NULL) {\n        cc = callout_alloc(sc);\n        c-&gt;lh = cc;\n        cc-&gt;c_lh = c;\n    } else {\n        cc = c-&gt;lh;\n        /* Remove from current position if pending */\n        if (cc-&gt;c_flags &amp; CALLOUT_PENDING) {\n            TAILQ_REMOVE(&amp;sc-&gt;callwheel[cc-&gt;c_time % BUCKETS], cc, c_links.tq);\n        }\n    }\n\n    /* Setup callout */\n    cc-&gt;c_func = func;\n    cc-&gt;c_arg = arg;\n    cc-&gt;c_time = expire_tick;\n    cc-&gt;c_flags = CALLOUT_PENDING | CALLOUT_ACTIVE;\n\n    /* Insert into appropriate bucket */\n    if (ticks &lt; BUCKETS) {\n        /* Within wheel range */\n        bucket = expire_tick % BUCKETS;\n        TAILQ_INSERT_TAIL(&amp;sc-&gt;callwheel[bucket], cc, c_links.tq);\n    } else {\n        /* Beyond wheel range - use overflow list */\n        TAILQ_INSERT_TAIL(&amp;sc-&gt;calltodo, cc, c_links.tq);\n    }\n\n    spin_unlock(&amp;sc-&gt;spin);\n}\n</code></pre></p> <p>Processing Algorithm (in softclock): <pre><code>void\nsoftclock(systimer_t info, int in_ipi, struct intrframe *frame)\n{\n    struct softclock_pcpu *sc = &amp;softclock_pcpu_ary[mycpuid];\n    struct _callout *cc;\n    int curticks;\n\n    spin_lock(&amp;sc-&gt;spin);\n\n    /* Process all ticks since last run */\n    while (sc-&gt;softticks != ticks) {\n        sc-&gt;curticks = ++sc-&gt;softticks;\n        curticks = sc-&gt;curticks;\n\n        /* Advance cursor to current bucket */\n        sc-&gt;callcursor = &amp;sc-&gt;callwheel[curticks % BUCKETS];\n\n        /* Process all callouts in current bucket */\n        while ((cc = TAILQ_FIRST(sc-&gt;callcursor)) != NULL) {\n            if (cc-&gt;c_time != curticks) {\n                /* Not expired yet (wheel wrapped) */\n                break;\n            }\n\n            /* Remove from wheel */\n            TAILQ_REMOVE(sc-&gt;callcursor, cc, c_links.tq);\n            cc-&gt;c_flags &amp;= ~CALLOUT_PENDING;\n\n            /* Mark as running */\n            sc-&gt;running = cc;\n\n            spin_unlock(&amp;sc-&gt;spin);\n\n            /* Execute callback */\n            cc-&gt;c_func(cc-&gt;c_arg);\n\n            spin_lock(&amp;sc-&gt;spin);\n\n            /* Mark completed */\n            sc-&gt;running = NULL;\n            cc-&gt;c_flags |= CALLOUT_EXECUTED;\n        }\n\n        /* Move callouts from overflow list to wheel if in range */\n        migrate_callouts_from_overflow(sc);\n    }\n\n    spin_unlock(&amp;sc-&gt;spin);\n}\n</code></pre></p>"},{"location":"sys/kern/time/#example-4-ntp-time-adjustment-flow","title":"Example 4: NTP Time Adjustment Flow","text":"<p>Location: kern_ntptime.c, kern_clock.c</p> <pre><code>/*\n * NTP adjustment happens in hardclock on CPU 0\n */\nvoid\nntp_update_second(struct timespec *btp, int ticks)\n{\n    int64_t time_adj;\n\n    /* Phase-Locked Loop (PLL) adjustment */\n    if (time_status &amp; STA_PLL) {\n        time_adj = time_offset / 1000;  /* Scale down */\n\n        /* Apply adjustment limit */\n        if (time_adj &gt; MAXPHASE)\n            time_adj = MAXPHASE;\n        else if (time_adj &lt; -MAXPHASE)\n            time_adj = -MAXPHASE;\n\n        /* Apply adjustment to current second */\n        btp-&gt;tv_nsec += time_adj;\n\n        /* Reduce offset */\n        time_offset -= time_adj * 1000;\n    }\n\n    /* Frequency-Locked Loop (FLL) adjustment */\n    if (time_status &amp; STA_FLL) {\n        /* Adjust frequency based on long-term drift */\n        time_freq += (time_offset / (1LL &lt;&lt; 32));\n    }\n\n    /* Normalize timespec */\n    if (btp-&gt;tv_nsec &gt;= 1000000000) {\n        btp-&gt;tv_nsec -= 1000000000;\n        btp-&gt;tv_sec++;\n    } else if (btp-&gt;tv_nsec &lt; 0) {\n        btp-&gt;tv_nsec += 1000000000;\n        btp-&gt;tv_sec--;\n    }\n\n    /* Update basetime FIFO write pointer */\n    basetime_index = (basetime_index + 1) % BASETIME_ARYSIZE;\n    basetime[basetime_index] = *btp;\n}\n</code></pre> <p>Flow: <pre><code>NTP Daemon (ntpd in userspace)\n        \u2193\nsys_ntp_adjtime() [kern_ntptime.c:260]\n        \u251c\u2500&gt; Lock ntp_lock\n        \u251c\u2500&gt; Update time_offset, time_freq\n        \u251c\u2500&gt; Set time_status (PLL/FLL mode)\n        \u2514\u2500&gt; Unlock ntp_lock\n\nEvery hardclock on CPU 0:\n        \u2193\nhardclock() [kern_clock.c:552]\n        \u251c\u2500&gt; Read basetime[read_index]\n        \u251c\u2500&gt; ntp_update_second(&amp;basetime[write_index])\n        \u2502   \u251c\u2500&gt; Calculate adjustment based on time_offset\n        \u2502   \u251c\u2500&gt; Apply PLL correction (phase)\n        \u2502   \u251c\u2500&gt; Apply FLL correction (frequency)\n        \u2502   \u251c\u2500&gt; Adjust tv_nsec by small amount\n        \u2502   \u251c\u2500&gt; Normalize timespec\n        \u2502   \u2514\u2500&gt; Increment write_index\n        \u2514\u2500&gt; Update time_second from basetime[read_index]\n\nResult: Time gradually converges to NTP reference\n</code></pre></p> <p>Basetime FIFO: <pre><code>basetime[16] = FIFO of timespec values\n\nWrite end (NTP adjusts):     Read end (hardclock uses):\n        \u2193                             \u2193\n[0][1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]\n\n- NTP code writes adjusted time to write_index\n- hardclock reads from read_index\n- FIFO provides lock-free communication\n- 16 entries give ~160ms buffer at hz=100\n</code></pre></p>"},{"location":"sys/kern/time/#best-practices","title":"Best Practices","text":""},{"location":"sys/kern/time/#choosing-timer-facilities","title":"Choosing Timer Facilities","text":"<p>Use callouts when: - Need to schedule a callback in the future - Timeout needed for asynchronous operations - Per-device or per-connection timers</p> <p>Use systimers when: - Need periodic system-wide events - Implementing new clock types - Low-level kernel timing infrastructure</p> <p>Use sleep/tsleep when: - Need to block a thread for a duration - Waiting for an event with timeout - User-space style blocking</p>"},{"location":"sys/kern/time/#callout-usage-guidelines","title":"Callout Usage Guidelines","text":"<ol> <li>Always initialize: Call <code>callout_init()</code> or variant before use</li> <li>MP-safe by default: Use <code>callout_init_mp()</code> for new code</li> <li>Cancel on cleanup: Use <code>callout_terminate()</code> during shutdown</li> <li>Check return values: <code>callout_stop()</code> returns whether callout was pending</li> <li>Avoid long callbacks: Callouts run in softclock context; keep them brief</li> <li>Use appropriate CPU: Consider <code>callout_reset_bycpu()</code> for load distribution</li> </ol>"},{"location":"sys/kern/time/#time-retrieval-guidelines","title":"Time Retrieval Guidelines","text":"<p>For logging/coarse timestamps: <pre><code>getmicrotime(&amp;tv);    /* Low overhead, ~10ms resolution */\n</code></pre></p> <p>For accurate timestamps: <pre><code>microtime(&amp;tv);       /* Higher overhead, hardware precision */\n</code></pre></p> <p>For measuring intervals: <pre><code>microuptime(&amp;tv1);\n/* ... operation ... */\nmicrouptime(&amp;tv2);\ntimevalsub(&amp;tv2, &amp;tv1);  /* tv2 now contains elapsed time */\n</code></pre></p> <p>Why use *uptime variants for intervals?: - Not affected by NTP adjustments - Monotonically increasing - Accurate for performance measurements</p>"},{"location":"sys/kern/time/#ntp-considerations","title":"NTP Considerations","text":"<ul> <li>NTP adjustments are gradual (typically &lt;500ppm frequency adjustment)</li> <li>Realtime can jump backward with <code>settimeofday()</code> but not with NTP</li> <li>For intervals, always use monotonic time (*uptime functions)</li> <li>Maximum NTP adjustment: \u00b1500ms with PLL, more with FLL</li> </ul>"},{"location":"sys/kern/time/#see-also","title":"See Also","text":"<ul> <li>LWKT (Lightweight Kernel Threads) - Thread scheduling and execution</li> <li>Synchronization Primitives - Locks and synchronization used by timers</li> <li>Memory Management - Memory allocation for timer structures</li> </ul>"},{"location":"sys/kern/time/#references","title":"References","text":"<ul> <li><code>sys/kern/kern_clock.c</code> - System clock implementation</li> <li><code>sys/kern/kern_timeout.c</code> - Callout/timeout mechanism</li> <li><code>sys/kern/kern_systimer.c</code> - Software timer infrastructure</li> <li><code>sys/kern/kern_cputimer.c</code> - Hardware timer abstraction</li> <li><code>sys/kern/kern_time.c</code> - Time-related system calls</li> <li><code>sys/kern/kern_ntptime.c</code> - NTP time adjustment</li> <li><code>sys/sys/systimer.h</code> - Timer data structure definitions</li> <li><code>sys/sys/callout.h</code> - Callout API definitions</li> <li><code>sys/sys/time.h</code> - Time representation types</li> </ul>"},{"location":"sys/kern/tracing/","title":"Kernel Tracing and Debugging","text":"<p>DragonFly BSD provides two distinct tracing facilities for kernel debugging and process monitoring:</p> <ol> <li>KTR (Kernel Trace) - High-performance in-kernel event logging</li> <li>ktrace - Process-level system call tracing to files</li> </ol>"},{"location":"sys/kern/tracing/#source-files","title":"Source Files","text":"File Lines Description <code>kern_ktr.c</code> 682 KTR ring buffer implementation <code>kern_ktrace.c</code> 687 ktrace system call tracing <code>kern_debug.c</code> 100 Debugging utilities <code>sys/ktr.h</code> 235 KTR data structures and macros <code>sys/ktrace.h</code> 228 ktrace data structures"},{"location":"sys/kern/tracing/#ktr-kernel-trace-buffer","title":"KTR: Kernel Trace Buffer","text":"<p>KTR provides a lightweight, per-CPU ring buffer for recording kernel events. It is designed for minimal overhead and is used extensively throughout the kernel for debugging and performance analysis.</p>"},{"location":"sys/kern/tracing/#architecture","title":"Architecture","text":"<pre><code>flowchart TD\n    subgraph arch[\"KTR Architecture\"]\n        subgraph cpu0[\"CPU 0\"]\n            KC0[\"ktr_cpuktr_bufktr_idx\"]\n        end\n        subgraph cpu1[\"CPU 1\"]\n            KC1[\"ktr_cpuktr_bufktr_idx\"]\n        end\n        subgraph cpu2[\"CPU 2\"]\n            KC2[\"ktr_cpuktr_bufktr_idx\"]\n        end\n        subgraph cpuN[\"CPU N\"]\n            KCN[\"ktr_cpuktr_bufktr_idx\"]\n        end\n\n        KC0 --&gt; E0[\"entries[0..N]\"]\n        KC1 --&gt; E1[\"entries[0..N]\"]\n        KC2 --&gt; E2[\"entries[0..N]\"]\n        KCN --&gt; EN[\"entries[0..N]\"]\n    end\n</code></pre> <p>Each CPU maintains its own trace buffer to avoid lock contention. The default buffer size is 2048 entries per CPU (<code>KTR_ENTRIES</code>), configurable at compile time.</p>"},{"location":"sys/kern/tracing/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/tracing/#ktr_entry-sysktrh78","title":"ktr_entry (<code>sys/ktr.h:78</code>)","text":"<pre><code>struct ktr_entry {\n    u_int64_t ktr_timestamp;        /* TSC or time_t */\n    struct ktr_info *ktr_info;      /* event metadata */\n    const char *ktr_file;           /* source file */\n    void *ktr_caller1;              /* return address 1 */\n    void *ktr_caller2;              /* return address 2 */\n    int32_t ktr_line;               /* source line number */\n    int32_t ktr_unused;\n    int32_t ktr_data[KTR_BUFSIZE/4]; /* event-specific data (192 bytes) */\n};\n</code></pre>"},{"location":"sys/kern/tracing/#ktr_info-sysktrh71","title":"ktr_info (<code>sys/ktr.h:71</code>)","text":"<pre><code>struct ktr_info {\n    const char *kf_name;            /* subsystem name (e.g., \"tokens_acq\") */\n    int32_t *kf_master_enable;      /* pointer to enable bitmask */\n    const char *kf_format;          /* printf-style format string */\n    int kf_data_size;               /* size of event arguments */\n};\n</code></pre>"},{"location":"sys/kern/tracing/#ktr_cpu-sysktrh94","title":"ktr_cpu (<code>sys/ktr.h:94</code>)","text":"<pre><code>struct ktr_cpu {\n    struct ktr_cpu_core core;\n} __cachealign;                     /* cache-aligned to avoid false sharing */\n\nstruct ktr_cpu_core {\n    struct ktr_entry *ktr_buf;      /* ring buffer */\n    int ktr_idx;                    /* next write index */\n};\n</code></pre>"},{"location":"sys/kern/tracing/#defining-trace-points","title":"Defining Trace Points","text":"<p>KTR uses a macro-based system to define trace points with compile-time type checking:</p> <pre><code>/* Declare a master enable variable */\nKTR_INFO_MASTER(tokens);\n\n/* Define individual trace points */\nKTR_INFO(KTR_TOKENS, tokens, acquire, 0,\n    \"acquire %p by %p\",\n    struct lwkt_token *tok, struct thread *td);\n\nKTR_INFO(KTR_TOKENS, tokens, release, 1,\n    \"release %p by %p\",\n    struct lwkt_token *tok, struct thread *td);\n</code></pre> <p>The <code>KTR_INFO</code> macro: 1. Creates a bitmask for enabling/disabling the trace point 2. Generates a packed struct for the arguments 3. Creates a type-checking wrapper function 4. Registers sysctl nodes under <code>debug.ktr.&lt;master&gt;</code></p>"},{"location":"sys/kern/tracing/#logging-events","title":"Logging Events","text":"<pre><code>/* Log an event */\nKTR_LOG(tokens_acquire, tok, curthread);\n\n/* Conditional logging */\nKTR_COND_LOG(tokens_acquire, some_condition, tok, curthread);\n</code></pre>"},{"location":"sys/kern/tracing/#implementation-details","title":"Implementation Details","text":""},{"location":"sys/kern/tracing/#entry-recording-kern_ktrc496","title":"Entry Recording (<code>kern_ktr.c:496</code>)","text":"<pre><code>struct ktr_entry *\nktr_begin_write_entry(struct ktr_info *info, const char *file, int line)\n{\n    struct ktr_cpu_core *kcpu;\n    struct ktr_entry *entry;\n    int cpu;\n\n    cpu = mycpu-&gt;gd_cpuid;\n    kcpu = &amp;ktr_cpu[cpu].core;\n\n    if (panicstr)           /* stop logging during panic */\n        return NULL;\n    if (kcpu-&gt;ktr_buf == NULL)  /* too early in boot */\n        return NULL;\n\n    crit_enter();\n    entry = kcpu-&gt;ktr_buf + (kcpu-&gt;ktr_idx &amp; ktr_entries_mask);\n    ++kcpu-&gt;ktr_idx;\n\n    /* Timestamp using TSC if available */\n    if (cpu_feature &amp; CPUID_TSC) {\n        entry-&gt;ktr_timestamp = rdtsc() - tsc_offsets[cpu];\n    } else {\n        entry-&gt;ktr_timestamp = get_approximate_time_t();\n    }\n\n    entry-&gt;ktr_info = info;\n    entry-&gt;ktr_file = file;\n    entry-&gt;ktr_line = line;\n    crit_exit();\n\n    return entry;\n}\n</code></pre> <p>Key design points: - Uses critical section (not locks) for minimal overhead - Per-CPU buffers eliminate cross-CPU contention - Ring buffer overwrites oldest entries when full - TSC timestamps provide nanosecond-resolution timing</p>"},{"location":"sys/kern/tracing/#tsc-synchronization-kern_ktrc249","title":"TSC Synchronization (<code>kern_ktr.c:249</code>)","text":"<p>On SMP systems, TSC values may drift between CPUs. KTR provides optional resynchronization:</p> <pre><code>/* Enabled via sysctl debug.ktr.resynchronize=1 */\nstatic void\nktr_resync_callback(void *dummy)\n{\n    lwkt_cpusync_init(&amp;cs, smp_active_mask, ktr_resync_remote, ...);\n    lwkt_cpusync_interlock(&amp;cs);\n    ktr_sync_tsc = rdtsc();\n    lwkt_cpusync_deinterlock(&amp;cs);\n}\n</code></pre> <p>This runs every 100ms when enabled, using LWKT CPU synchronization to measure TSC offsets across all CPUs.</p>"},{"location":"sys/kern/tracing/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Description <code>debug.ktr.entries</code> Number of entries per CPU (read-only) <code>debug.ktr.version</code> KTR format version <code>debug.ktr.stacktrace</code> Include caller addresses <code>debug.ktr.resynchronize</code> Enable TSC resync (10Hz) <code>debug.ktr.&lt;master&gt;_enable</code> Bitmask for subsystem <code>debug.ktr.&lt;master&gt;.&lt;event&gt;_mask</code> Bit value for event"},{"location":"sys/kern/tracing/#ddb-integration","title":"DDB Integration","text":"<p>The <code>show ktr</code> command in DDB displays trace entries:</p> <pre><code>db&gt; show ktr\ncpu0 1234: tokens_acquire  from(0xffffffff80123456,0xffffffff80234567)\ncpu1 1235: tokens_release  from(0xffffffff80345678,0xffffffff80456789)\n</code></pre> <p>Modifiers: - <code>v</code> - Verbose output with timestamps and file:line - <code>a</code> - Auto-scroll (continuous display) - <code>c&lt;N&gt;</code> - Filter by CPU number</p>"},{"location":"sys/kern/tracing/#early-boot-support","title":"Early Boot Support","text":"<p>KTR provides static buffers for CPU 0 during early boot before memory allocation is available:</p> <pre><code>/* 256-entry static buffer for boot (kern_ktr.c:205) */\nstatic struct ktr_entry ktr_buf0[KTR_ENTRIES_BOOT0];\n\nstruct ktr_cpu ktr_cpu[MAXCPU] = {\n    { .core.ktr_buf = &amp;ktr_buf0[0] }\n};\n</code></pre> <p>Full-sized buffers are allocated at <code>SI_BOOT2_KLD</code> priority.</p>"},{"location":"sys/kern/tracing/#ktrace-process-system-call-tracing","title":"ktrace: Process System Call Tracing","text":"<p>ktrace provides per-process tracing of system calls, signals, I/O, and context switches. Traces are written to files for offline analysis with <code>kdump(1)</code>.</p>"},{"location":"sys/kern/tracing/#architecture_1","title":"Architecture","text":"<pre><code>flowchart TD\n    subgraph proc[\"Process\"]\n        FLAG[\"p_traceflag\"]\n        NODE[\"p_tracenode\"]\n    end\n\n    FLAG --&gt;|flags| KN\n    NODE --&gt;|ref| KN\n\n    subgraph kn[\"ktrace_node\"]\n        KN[\"kn_vpkn_refs\"]\n    end\n\n    KN --&gt; VNODE[\"vnode(trace file)\"]\n\n    subgraph events[\"Trace Events\"]\n        SYSCALL[\"ktrsyscall\"]\n        SYSRET[\"ktrsysret\"]\n        NAMEI[\"ktrnamei\"]\n        GENIO[\"ktrgenio\"]\n        PSIG[\"ktrpsig\"]\n        CSW[\"ktrcsw\"]\n    end\n\n    proc --&gt;|\"syscall/signal/etc\"| events\n    events --&gt;|write| RECORD[\"ktr_header + event data\"]\n    RECORD --&gt; VNODE\n</code></pre>"},{"location":"sys/kern/tracing/#data-structures_1","title":"Data Structures","text":""},{"location":"sys/kern/tracing/#ktrace_node-sysktraceh48","title":"ktrace_node (<code>sys/ktrace.h:48</code>)","text":"<pre><code>struct ktrace_node {\n    struct vnode *kn_vp;    /* trace file vnode */\n    int kn_refs;            /* reference count */\n};\n</code></pre> <p>Multiple processes can share a trace file through reference counting.</p>"},{"location":"sys/kern/tracing/#ktr_header-sysktraceh71","title":"ktr_header (<code>sys/ktrace.h:71</code>)","text":"<pre><code>struct ktr_header {\n    int     ktr_len;        /* length of following data */\n    short   ktr_type;       /* record type */\n    short   ktr_flags;      /* KTRH_THREADED, CPU ID */\n    pid_t   ktr_pid;        /* process ID */\n    lwpid_t ktr_tid;        /* LWP/thread ID */\n    char    ktr_comm[MAXCOMLEN+1];  /* command name */\n    struct  timeval ktr_time;       /* timestamp */\n    caddr_t ktr_buf;        /* pointer to data (kernel only) */\n};\n</code></pre>"},{"location":"sys/kern/tracing/#record-types","title":"Record Types","text":"Type Constant Structure Description 1 <code>KTR_SYSCALL</code> <code>ktr_syscall</code> System call entry 2 <code>KTR_SYSRET</code> <code>ktr_sysret</code> System call return 3 <code>KTR_NAMEI</code> (string) Path name lookup 4 <code>KTR_GENIO</code> <code>ktr_genio</code> Generic I/O data 5 <code>KTR_PSIG</code> <code>ktr_psig</code> Signal delivery 6 <code>KTR_CSW</code> <code>ktr_csw</code> Context switch 7 <code>KTR_USER</code> (user data) User-defined trace 9 <code>KTR_SYSCTL</code> (string) Sysctl MIB name"},{"location":"sys/kern/tracing/#trace-functions","title":"Trace Functions","text":""},{"location":"sys/kern/tracing/#ktrsyscall-kern_ktracec110","title":"ktrsyscall (<code>kern_ktrace.c:110</code>)","text":"<p>Records system call entry with arguments:</p> <pre><code>void\nktrsyscall(struct lwp *lp, int code, int narg, union sysunion *uap)\n{\n    struct ktr_header kth;\n    struct ktr_syscall *ktp;\n\n    lp-&gt;lwp_traceflag |= KTRFAC_ACTIVE;  /* prevent recursion */\n    ktrgetheader(&amp;kth, KTR_SYSCALL);\n\n    ktp = ktrgetsyscall(&amp;kth, &amp;ktp_cache, narg);\n    ktp-&gt;ktr_code = code;\n    ktp-&gt;ktr_narg = narg;\n    /* Copy arguments */\n    for (i = 0; i &lt; narg; i++)\n        ktp-&gt;ktr_args[i] = args[i];\n\n    ktrwrite(lp, &amp;kth, NULL);\n    lp-&gt;lwp_traceflag &amp;= ~KTRFAC_ACTIVE;\n}\n</code></pre>"},{"location":"sys/kern/tracing/#ktrwrite-kern_ktracec592","title":"ktrwrite (<code>kern_ktrace.c:592</code>)","text":"<p>Writes trace records to the file:</p> <pre><code>static void\nktrwrite(struct lwp *lp, struct ktr_header *kth, struct uio *uio)\n{\n    ktrace_node_t tracenode;\n\n    tracenode = ktrinherit(lp-&gt;lwp_proc-&gt;p_tracenode);\n\n    /* Lock vnode to ensure timestamp ordering */\n    vn_lock(tracenode-&gt;kn_vp, LK_EXCLUSIVE | LK_RETRY);\n    microtime(&amp;kth-&gt;ktr_time);  /* timestamp after lock */\n\n    error = VOP_WRITE(tracenode-&gt;kn_vp, &amp;auio, IO_UNIT | IO_APPEND, cred);\n\n    if (error) {\n        /* Disable tracing for all processes using this file */\n        allproc_scan(ktrace_clear_callback, &amp;info, 0);\n    }\n\n    vn_unlock(tracenode-&gt;kn_vp);\n    ktrdestroy(&amp;tracenode);\n}\n</code></pre>"},{"location":"sys/kern/tracing/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/tracing/#sys_ktrace-kern_ktracec288","title":"sys_ktrace (<code>kern_ktrace.c:288</code>)","text":"<pre><code>int sys_ktrace(struct sysmsg *sysmsg, const struct ktrace_args *uap);\n\nstruct ktrace_args {\n    const char *fname;  /* trace file path */\n    int ops;            /* operation (set/clear/clearfile) */\n    int facs;           /* facilities bitmask */\n    pid_t pid;          /* target process or process group */\n};\n</code></pre> <p>Operations: - <code>KTROP_SET</code> - Enable tracing for specified facilities - <code>KTROP_CLEAR</code> - Disable tracing for specified facilities - <code>KTROP_CLEARFILE</code> - Stop all tracing to the file</p> <p>Flags: - <code>KTRFLAG_DESCEND</code> - Apply to all descendants</p>"},{"location":"sys/kern/tracing/#sys_utrace-kern_ktracec439","title":"sys_utrace (<code>kern_ktrace.c:439</code>)","text":"<p>Allows user programs to inject custom trace records:</p> <pre><code>int sys_utrace(struct sysmsg *sysmsg, const struct utrace_args *uap);\n\nstruct utrace_args {\n    const void *addr;   /* user data */\n    size_t len;         /* data length (max 2048) */\n};\n</code></pre>"},{"location":"sys/kern/tracing/#permission-checking","title":"Permission Checking","text":"<p>ktrace enforces strict permission checks (<code>kern_ktrace.c:667</code>):</p> <pre><code>static int\nktrcanset(struct thread *calltd, struct proc *targetp)\n{\n    /* Jail check */\n    if (!PRISON_CHECK(caller, target))\n        return (0);\n\n    /* UID/GID matching or root */\n    if ((caller-&gt;cr_uid == target-&gt;cr_ruid &amp;&amp;\n         target-&gt;cr_ruid == target-&gt;cr_svuid &amp;&amp;\n         caller-&gt;cr_rgid == target-&gt;cr_rgid &amp;&amp;\n         target-&gt;cr_rgid == target-&gt;cr_svgid &amp;&amp;\n         (targetp-&gt;p_traceflag &amp; KTRFAC_ROOT) == 0 &amp;&amp;\n         (targetp-&gt;p_flags &amp; P_SUGID) == 0) ||\n         caller-&gt;cr_uid == 0)\n        return (1);\n\n    return (0);\n}\n</code></pre> <p>Key restrictions: - Cannot trace across jail boundaries - Cannot trace setuid/setgid processes (unless root) - <code>KTRFAC_ROOT</code> flag prevents non-root from modifying traces set by root</p>"},{"location":"sys/kern/tracing/#trace-inheritance","title":"Trace Inheritance","text":"<p>When a process forks, it can inherit tracing:</p> <pre><code>ktrace_node_t\nktrinherit(ktrace_node_t tracenode)\n{\n    if (tracenode) {\n        atomic_add_int(&amp;tracenode-&gt;kn_refs, 1);\n    }\n    return tracenode;\n}\n</code></pre> <p>The <code>KTRFAC_INHERIT</code> flag in <code>p_traceflag</code> controls whether children inherit the trace settings.</p>"},{"location":"sys/kern/tracing/#debugging-utilities","title":"Debugging Utilities","text":"<p><code>kern_debug.c</code> provides minimal debugging support:</p>"},{"location":"sys/kern/tracing/#stack-trace","title":"Stack Trace","text":"<pre><code>void print_backtrace(int count)\n</code></pre> <p>Prints a kernel stack backtrace. Requires <code>DDB</code> kernel option; otherwise prints an error message.</p>"},{"location":"sys/kern/tracing/#debug-sysctls","title":"Debug Sysctls","text":"<p>Test sysctls for bit manipulation:</p> <pre><code>SYSCTL_BIT32(_debug, OID_AUTO, b32_0, ...);   /* bit 0 of 32-bit */\nSYSCTL_BIT32(_debug, OID_AUTO, b32_31, ...);  /* bit 31 of 32-bit */\nSYSCTL_BIT64(_debug, OID_AUTO, b64_0, ...);   /* bit 0 of 64-bit */\nSYSCTL_BIT64(_debug, OID_AUTO, b64_63, ...);  /* bit 63 of 64-bit */\n</code></pre>"},{"location":"sys/kern/tracing/#comparison-ktr-vs-ktrace","title":"Comparison: KTR vs ktrace","text":"Feature KTR ktrace Purpose Kernel debugging Process tracing Overhead Very low Moderate (file I/O) Output In-memory ring buffer File Scope Kernel-wide Per-process Analysis DDB, ktrdump kdump(1) Persistence Lost on reboot Persistent User Access sysctl only ktrace(1), kdump(1) Compile-time Optional (KTR option) Optional (KTRACE option)"},{"location":"sys/kern/tracing/#usage-examples","title":"Usage Examples","text":""},{"location":"sys/kern/tracing/#ktr-enable-token-tracing","title":"KTR: Enable Token Tracing","text":"<pre><code># Enable all token-related traces\nsysctl debug.ktr.tokens_enable=-1\n\n# View in DDB\ndb&gt; show ktr/v\n</code></pre>"},{"location":"sys/kern/tracing/#ktrace-trace-a-process","title":"ktrace: Trace a Process","text":"<pre><code># Start tracing\nktrace -f /tmp/trace.out -p 1234\n\n# Or trace a command\nktrace -f /tmp/trace.out ls -la\n\n# Analyze\nkdump -f /tmp/trace.out\n</code></pre>"},{"location":"sys/kern/tracing/#user-trace-from-application","title":"User Trace from Application","text":"<pre><code>#include &lt;sys/ktrace.h&gt;\n\nvoid log_event(const char *msg) {\n    utrace(msg, strlen(msg) + 1);\n}\n</code></pre>"},{"location":"sys/kern/tracing/#see-also","title":"See Also","text":"<ul> <li>LWKT Threading - Token and thread tracing</li> <li>System Calls - Syscall dispatch and tracing hooks</li> <li>Processes - Process structure and tracing flags</li> </ul>"},{"location":"sys/kern/tty-pty/","title":"Pseudo-Terminals (PTY)","text":"<p>Pseudo-terminals provide a bidirectional communication channel that emulates a hardware terminal. They consist of a master/slave pair where the master side (ptc) is used by programs like terminal emulators and SSH daemons, while the slave side (pts) appears as a regular terminal device to shell processes.</p> <p>For background on pseudo-terminals and their role in terminal emulation, see Wikipedia: Pseudoterminal.</p>"},{"location":"sys/kern/tty-pty/#source-files","title":"Source Files","text":"File Lines Description <code>sys/kern/tty_pty.c</code> 1,355 Pseudo-terminal driver implementation"},{"location":"sys/kern/tty-pty/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TD\n    EMU[\"Terminal Emulator(xterm, ssh, etc.)\"]\n    EMU --&gt;|\"open('/dev/ptmx')\"| MASTER\n\n    subgraph master[\"PTY Master (ptc)\"]\n        MASTER[\"/dev/ptm/N or /dev/ptyXXptcread() - Reads slave's outputptcwrite() - Writes to slave's input\"]\n    end\n\n    MASTER &lt;--&gt;|\"Internal queues(t_outq, t_rawq, t_canq)\"| SLAVE\n\n    subgraph slave[\"PTY Slave (pts)\"]\n        SLAVE[\"/dev/pts/N or /dev/ttyXXptsread() - Reads processed inputptswrite() - Writes to output queue\"]\n    end\n\n    SLAVE --&gt;|\"Line disciplineprocessing\"| APP[\"Shell / Application Process\"]\n</code></pre>"},{"location":"sys/kern/tty-pty/#data-flow","title":"Data Flow","text":""},{"location":"sys/kern/tty-pty/#master-to-slave-input","title":"Master to Slave (Input)","text":"<pre><code>flowchart TD\n    PW[\"ptcwrite()\"] --&gt;|\"Master writes data\"| LRINT[\"l_rint() for each char\"]\n    LRINT --&gt;|\"Canonical mode\"| RAWQ1[\"t_rawq\"]\n    RAWQ1 --&gt;|\"on line completion\"| CANQ[\"t_canq\"]\n    LRINT --&gt;|\"Raw mode\"| RAWQ2[\"t_rawq\"]\n    CANQ --&gt; PSR[\"ptsread()\"]\n    RAWQ2 --&gt; PSR\n    PSR --&gt;|\"Slave reads processed data\"| OUT1[\"Output\"]\n</code></pre>"},{"location":"sys/kern/tty-pty/#slave-to-master-output","title":"Slave to Master (Output)","text":"<pre><code>flowchart TD\n    PSW[\"ptswrite()\"] --&gt;|\"Slave writes data\"| LWRITE[\"l_write()\"]\n    LWRITE --&gt;|\"Line discipline output processing\"| TTYOUT[\"ttyoutput()\"]\n    TTYOUT --&gt;|\"Output post-processing (OPOST)\"| OUTQ[\"t_outq\"]\n    OUTQ --&gt;|\"Output queue\"| PTSSTART[\"ptsstart()\"]\n    PTSSTART --&gt;|\"Start output (wakes master)\"| PTCREAD[\"ptcread()\"]\n    PTCREAD --&gt;|\"Master reads slave's output\"| OUT2[\"Output\"]\n</code></pre>"},{"location":"sys/kern/tty-pty/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/tty-pty/#per-pty-state","title":"Per-PTY State","text":"<p>Each pseudo-terminal pair maintains state in <code>struct pt_ioctl</code> (<code>tty_pty.c:142</code>):</p> <pre><code>struct pt_ioctl {\n    int         pt_flags;       /* State flags (PF_*) */\n    int         pt_refs;        /* Reference count */\n    int         pt_uminor;      /* Unit minor number */\n    struct kqinfo pt_kqr;       /* kqueue read info */\n    struct kqinfo pt_kqw;       /* kqueue write info */\n    u_char      pt_send;        /* Packet mode events to send */\n    u_char      pt_ucntl;       /* User control mode byte */\n    struct tty  pt_tty;         /* Embedded tty structure */\n    cdev_t      devs;           /* Slave device */\n    cdev_t      devc;           /* Master device */\n    struct prison *pt_prison;   /* Jail association */\n};\n</code></pre>"},{"location":"sys/kern/tty-pty/#pty-state-flags","title":"PTY State Flags","text":"<p>Control flags in <code>pt_flags</code> (<code>tty_pty.c:157</code>):</p> <pre><code>/* Master-side operational modes */\n#define PF_PKT          0x0008  /* Packet mode enabled */\n#define PF_STOPPED      0x0010  /* Output stopped by user */\n#define PF_REMOTE       0x0020  /* Remote mode (flow-controlled input) */\n#define PF_NOSTOP       0x0040  /* Don't send STOP/START packets */\n#define PF_UCNTL        0x0080  /* User control mode */\n\n#define PF_PTCSTATEMASK 0x00FF  /* Mask for ptc operational state */\n\n/* Open state tracking */\n#define PF_UNIX98       0x0100  /* Unix98 PTY (dynamically allocated) */\n#define PF_SOPEN        0x0200  /* Slave side is open */\n#define PF_MOPEN        0x0400  /* Master side is open */\n#define PF_SCLOSED      0x0800  /* Slave was opened then closed */\n#define PF_TERMINATED   0x8000  /* PTY is being destroyed */\n</code></pre>"},{"location":"sys/kern/tty-pty/#device-naming","title":"Device Naming","text":"<p>DragonFly supports two PTY naming schemes:</p>"},{"location":"sys/kern/tty-pty/#bsd-style-ptys-legacy","title":"BSD-Style PTYs (Legacy)","text":"<p>Pre-allocated at boot, limited to 256 pairs:</p> Device Pattern Example Master <code>/dev/pty[pqrsPQRS][0-9a-v]</code> <code>/dev/ptyp0</code> Slave <code>/dev/tty[pqrsPQRS][0-9a-v]</code> <code>/dev/ttyp0</code>"},{"location":"sys/kern/tty-pty/#unix98-style-ptys","title":"Unix98-Style PTYs","text":"<p>Dynamically allocated via <code>/dev/ptmx</code>, up to 1000 pairs:</p> Device Pattern Example Clone device <code>/dev/ptmx</code> (open to allocate) Master <code>/dev/ptm/N</code> <code>/dev/ptm/0</code> Slave <code>/dev/pts/N</code> <code>/dev/pts/0</code>"},{"location":"sys/kern/tty-pty/#pty-allocation","title":"PTY Allocation","text":""},{"location":"sys/kern/tty-pty/#unix98-allocation","title":"Unix98 Allocation","text":"<p>Opening <code>/dev/ptmx</code> triggers the clone handler (<code>tty_pty.c:215</code>):</p> <pre><code>static int\nptyclone(struct dev_clone_args *ap)\n{\n    int unit;\n    struct pt_ioctl *pti;\n\n    /* Allocate unit from clone bitmap (max MAXPTYS=1000) */\n    unit = devfs_clone_bitmap_get(&amp;DEVFS_CLONE_BITMAP(pty), MAXPTYS);\n    if (unit &lt; 0) {\n        ap-&gt;a_dev = NULL;\n        return 1;                   /* No PTYs available */\n    }\n\n    /* Allocate or reuse pti structure */\n    if ((pti = ptis[unit]) == NULL) {\n        lwkt_gettoken(&amp;tty_token);\n        pti = kmalloc(sizeof(*pti), M_PTY, M_WAITOK | M_ZERO);\n        if (ptis[unit] == NULL) {\n            ptis[unit] = pti;\n            ttyinit(&amp;pti-&gt;pt_tty);\n        } else {\n            kfree(pti, M_PTY);      /* Race: another thread allocated */\n        }\n        lwkt_reltoken(&amp;tty_token);\n    }\n\n    /* Create device nodes */\n    pti-&gt;devc = make_only_dev(&amp;ptc98_ops, unit,\n                              ap-&gt;a_cred-&gt;cr_ruid,\n                              0, 0600, \"ptm/%d\", unit);\n    pti-&gt;devs = make_dev(&amp;pts98_ops, unit,\n                         ap-&gt;a_cred-&gt;cr_ruid,\n                         GID_TTY, 0620, \"pts/%d\", unit);\n\n    /* Initialize PTY state */\n    pti-&gt;pt_tty.t_dev = pti-&gt;devs;\n    pti-&gt;pt_flags = PF_UNIX98;\n    pti-&gt;pt_uminor = unit;\n    pti-&gt;devs-&gt;si_drv1 = pti-&gt;devc-&gt;si_drv1 = pti;\n    pti-&gt;devs-&gt;si_tty = pti-&gt;devc-&gt;si_tty = &amp;pti-&gt;pt_tty;\n    ttyregister(&amp;pti-&gt;pt_tty);\n\n    ap-&gt;a_dev = pti-&gt;devc;          /* Return master device */\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#bsd-style-initialization","title":"BSD-Style Initialization","text":"<p>BSD PTYs are pre-created at boot (<code>tty_pty.c:186</code>):</p> <pre><code>static void\nptyinit(int n)\n{\n    cdev_t devs, devc;\n    char *names = \"pqrsPQRS\";       /* 8 groups */\n    struct pt_ioctl *pti;\n\n    if (n &amp; ~0xff)\n        return;                     /* Only 256 supported */\n\n    pti = kmalloc(sizeof(*pti), M_PTY, M_WAITOK | M_ZERO);\n\n    /* Create /dev/ttyXX (slave) */\n    pti-&gt;devs = devs = make_dev(&amp;pts_ops, n, 0, 0, 0666,\n                                \"tty%c%c\",\n                                names[n / 32], hex2ascii(n % 32));\n\n    /* Create /dev/ptyXX (master) */\n    pti-&gt;devc = devc = make_dev(&amp;ptc_ops, n, 0, 0, 0666,\n                                \"pty%c%c\",\n                                names[n / 32], hex2ascii(n % 32));\n\n    pti-&gt;pt_tty.t_dev = devs;\n    pti-&gt;pt_uminor = n;\n    devs-&gt;si_drv1 = devc-&gt;si_drv1 = pti;\n    devs-&gt;si_tty = devc-&gt;si_tty = &amp;pti-&gt;pt_tty;\n    ttyinit(&amp;pti-&gt;pt_tty);\n    ttyregister(&amp;pti-&gt;pt_tty);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#master-side-operations","title":"Master Side Operations","text":""},{"location":"sys/kern/tty-pty/#opening-the-master","title":"Opening the Master","text":"<p><code>ptcopen()</code> (<code>tty_pty.c:610</code>) initializes the master side:</p> <pre><code>static int\nptcopen(struct dev_open_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct tty *tp;\n    struct pt_ioctl *pti;\n\n    pti = dev-&gt;si_drv1;\n    if (pti == NULL)\n        return(ENXIO);\n\n    lwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);\n    if (pti_hold(pti)) {            /* Check for termination race */\n        lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n        return(ENXIO);\n    }\n\n    /* Check jail association */\n    if (pti-&gt;pt_prison &amp;&amp; pti-&gt;pt_prison != ap-&gt;a_cred-&gt;cr_prison) {\n        pti_done(pti);\n        lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n        return(EBUSY);\n    }\n\n    tp = dev-&gt;si_tty;\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\n    /* Only one master open at a time */\n    if (tp-&gt;t_oproc) {\n        pti_done(pti);\n        lwkt_reltoken(&amp;tp-&gt;t_token);\n        lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n        return (EIO);\n    }\n\n    /* Clear zombie state if slave not open */\n    if ((pti-&gt;pt_flags &amp; PF_SOPEN) == 0)\n        tp-&gt;t_state &amp;= ~TS_ZOMBIE;\n\n    /* Install callbacks */\n    tp-&gt;t_oproc = ptsstart;         /* Output start routine */\n    tp-&gt;t_stop = ptsstop;           /* Output stop routine */\n    tp-&gt;t_unhold = ptsunhold;       /* Reference release callback */\n\n    /* Carrier on - wakes slave if waiting */\n    (void)(*linesw[tp-&gt;t_line].l_modem)(tp, 1);\n\n    /* Initialize master state */\n    tp-&gt;t_lflag &amp;= ~EXTPROC;\n    pti-&gt;pt_prison = ap-&gt;a_cred-&gt;cr_prison;\n    pti-&gt;pt_flags &amp;= ~PF_PTCSTATEMASK;\n    pti-&gt;pt_send = 0;\n    pti-&gt;pt_ucntl = 0;\n\n    /* Set ownership on slave device */\n    pti-&gt;devs-&gt;si_uid = ap-&gt;a_cred-&gt;cr_uid;\n    pti-&gt;devs-&gt;si_gid = ap-&gt;a_cred-&gt;cr_uid ? GID_TTY : 0;\n    pti-&gt;devs-&gt;si_perms = 0600;\n\n    pti-&gt;pt_flags |= PF_MOPEN;\n    pti_done(pti);\n\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#reading-from-master","title":"Reading from Master","text":"<p><code>ptcread()</code> (<code>tty_pty.c:742</code>) retrieves slave output:</p> <pre><code>static int\nptcread(struct dev_read_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct tty *tp = dev-&gt;si_tty;\n    struct pt_ioctl *pti = dev-&gt;si_drv1;\n    char buf[BUFSIZ];\n    int error = 0, cc;\n\n    lwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\n    for (;;) {\n        if (tp-&gt;t_state &amp; TS_ISOPEN) {\n            /* Handle packet mode events */\n            if ((pti-&gt;pt_flags &amp; PF_PKT) &amp;&amp; pti-&gt;pt_send) {\n                error = ureadc((int)pti-&gt;pt_send, ap-&gt;a_uio);\n                if (error)\n                    goto out;\n\n                /* Include termios on TIOCPKT_IOCTL */\n                if (pti-&gt;pt_send &amp; TIOCPKT_IOCTL) {\n                    cc = szmin(ap-&gt;a_uio-&gt;uio_resid,\n                               sizeof(tp-&gt;t_termios));\n                    uiomove((caddr_t)&amp;tp-&gt;t_termios, cc, ap-&gt;a_uio);\n                }\n                pti-&gt;pt_send = 0;\n                goto out;\n            }\n\n            /* Handle user control mode */\n            if ((pti-&gt;pt_flags &amp; PF_UCNTL) &amp;&amp; pti-&gt;pt_ucntl) {\n                error = ureadc((int)pti-&gt;pt_ucntl, ap-&gt;a_uio);\n                pti-&gt;pt_ucntl = 0;\n                goto out;\n            }\n\n            /* Check for data in output queue */\n            if (tp-&gt;t_outq.c_cc &amp;&amp; (tp-&gt;t_state &amp; TS_TTSTOP) == 0)\n                break;\n        }\n\n        /* Check for disconnect */\n        if ((tp-&gt;t_state &amp; TS_CONNECTED) == 0) {\n            error = 0;              /* EOF */\n            goto out;\n        }\n\n        /* Non-blocking check */\n        if (ap-&gt;a_ioflag &amp; IO_NDELAY) {\n            error = EWOULDBLOCK;\n            goto out;\n        }\n\n        /* Sleep waiting for data */\n        error = tsleep(TSA_PTC_READ(tp), PCATCH, \"ptcin\", 0);\n        if (error)\n            goto out;\n    }\n\n    /* Prepend status byte in packet/ucntl mode */\n    if (pti-&gt;pt_flags &amp; (PF_PKT|PF_UCNTL))\n        error = ureadc(0, ap-&gt;a_uio);\n\n    /* Copy data from output queue */\n    while (ap-&gt;a_uio-&gt;uio_resid &gt; 0 &amp;&amp; error == 0) {\n        cc = clist_qtob(&amp;tp-&gt;t_outq, buf,\n                        szmin(ap-&gt;a_uio-&gt;uio_resid, BUFSIZ));\n        if (cc &lt;= 0)\n            break;\n        error = uiomove(buf, cc, ap-&gt;a_uio);\n    }\n\n    /* Wake writers waiting for queue space */\n    ttwwakeup(tp);\n\nout:\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#writing-to-master","title":"Writing to Master","text":"<p><code>ptcwrite()</code> (<code>tty_pty.c:996</code>) sends input to the slave:</p> <pre><code>static int\nptcwrite(struct dev_write_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct tty *tp = dev-&gt;si_tty;\n    u_char *cp = NULL;\n    int cc = 0;\n    u_char locbuf[BUFSIZ];\n    int cnt = 0;\n    struct pt_ioctl *pti = dev-&gt;si_drv1;\n    int error = 0;\n\n    lwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\nagain:\n    if ((tp-&gt;t_state &amp; TS_ISOPEN) == 0)\n        goto block;\n\n    /* Remote mode: write directly to canonical queue */\n    if (pti-&gt;pt_flags &amp; PF_REMOTE) {\n        if (tp-&gt;t_canq.c_cc)\n            goto block;             /* Wait for queue to empty */\n\n        while ((ap-&gt;a_uio-&gt;uio_resid &gt; 0 || cc &gt; 0) &amp;&amp;\n               tp-&gt;t_canq.c_cc &lt; TTYHOG - 1) {\n            if (cc == 0) {\n                cc = szmin(ap-&gt;a_uio-&gt;uio_resid, BUFSIZ);\n                cc = imin(cc, TTYHOG - 1 - tp-&gt;t_canq.c_cc);\n                cp = locbuf;\n                error = uiomove(cp, cc, ap-&gt;a_uio);\n                if (error)\n                    goto out;\n            }\n            if (cc &gt; 0) {\n                cc = clist_btoq((char *)cp, cc, &amp;tp-&gt;t_canq);\n                if (cc &gt; 0)\n                    break;\n            }\n        }\n        /* Adjust for unwritten data */\n        ap-&gt;a_uio-&gt;uio_resid += cc;\n        clist_putc(0, &amp;tp-&gt;t_canq); /* Null terminator */\n        ttwakeup(tp);\n        wakeup(TSA_PTS_READ(tp));\n        goto out;\n    }\n\n    /* Normal mode: feed through line discipline */\n    while (ap-&gt;a_uio-&gt;uio_resid &gt; 0 || cc &gt; 0) {\n        if (cc == 0) {\n            cc = szmin(ap-&gt;a_uio-&gt;uio_resid, BUFSIZ);\n            cp = locbuf;\n            error = uiomove(cp, cc, ap-&gt;a_uio);\n            if (error)\n                goto out;\n        }\n\n        while (cc &gt; 0) {\n            /* Check for input queue overflow */\n            if ((tp-&gt;t_rawq.c_cc + tp-&gt;t_canq.c_cc) &gt;= TTYHOG - 2 &amp;&amp;\n               (tp-&gt;t_canq.c_cc &gt; 0 || !(tp-&gt;t_lflag &amp; ICANON))) {\n                wakeup(TSA_HUP_OR_INPUT(tp));\n                goto block;\n            }\n            /* Process character through line discipline */\n            (*linesw[tp-&gt;t_line].l_rint)(*cp++, tp);\n            cnt++;\n            cc--;\n        }\n        cc = 0;\n    }\n    goto out;\n\nblock:\n    /* Wait for slave to open or queue space */\n    if ((tp-&gt;t_state &amp; TS_CONNECTED) == 0) {\n        ap-&gt;a_uio-&gt;uio_resid += cc;\n        error = EIO;\n        goto out;\n    }\n    if (ap-&gt;a_ioflag &amp; IO_NDELAY) {\n        ap-&gt;a_uio-&gt;uio_resid += cc;\n        error = (cnt == 0) ? EWOULDBLOCK : 0;\n        goto out;\n    }\n    error = tsleep(TSA_PTC_WRITE(tp), PCATCH, \"ptcout\", 0);\n    if (error) {\n        ap-&gt;a_uio-&gt;uio_resid += cc;\n        goto out;\n    }\n    goto again;\n\nout:\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#closing-the-master","title":"Closing the Master","text":"<p><code>ptcclose()</code> (<code>tty_pty.c:687</code>) tears down the connection:</p> <pre><code>static int\nptcclose(struct dev_close_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct tty *tp;\n    struct pt_ioctl *pti = dev-&gt;si_drv1;\n\n    lwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);\n    if (pti_hold(pti))\n        panic(\"ptcclose on terminated pti\");\n\n    tp = dev-&gt;si_tty;\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\n    /* Signal carrier loss to slave */\n    (void)(*linesw[tp-&gt;t_line].l_modem)(tp, 0);\n\n    /* Mark master closed, zombie if slave still open */\n    pti-&gt;pt_flags &amp;= ~PF_MOPEN;\n    if (pti-&gt;pt_flags &amp; PF_SOPEN)\n        tp-&gt;t_state |= TS_ZOMBIE;\n\n    /* Disconnect and flush */\n    if (tp-&gt;t_state &amp; TS_ISOPEN) {\n        tp-&gt;t_state &amp;= ~(TS_CARR_ON | TS_CONNECTED);\n        ttyflush(tp, FREAD | FWRITE);\n    }\n    tp-&gt;t_oproc = NULL;             /* Mark as closed */\n\n    /* Reset ownership */\n    pti-&gt;pt_prison = NULL;\n    pti-&gt;devs-&gt;si_uid = 0;\n    pti-&gt;devs-&gt;si_gid = 0;\n    pti-&gt;devs-&gt;si_perms = 0666;\n\n    pti_done(pti);\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#slave-side-operations","title":"Slave Side Operations","text":""},{"location":"sys/kern/tty-pty/#opening-the-slave","title":"Opening the Slave","text":"<p><code>ptsopen()</code> (<code>tty_pty.c:351</code>) connects to an existing master:</p> <pre><code>static int\nptsopen(struct dev_open_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct tty *tp;\n    int error;\n    struct pt_ioctl *pti;\n\n    if (dev-&gt;si_drv1 == NULL)\n        return(ENXIO);\n    pti = dev-&gt;si_drv1;\n\n    lwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);\n    if (pti_hold(pti)) {\n        lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n        return(ENXIO);\n    }\n\n    tp = dev-&gt;si_tty;\n\n    /* Initialize tty on first open */\n    if ((tp-&gt;t_state &amp; TS_ISOPEN) == 0) {\n        ttychars(tp);               /* Set default characters */\n        tp-&gt;t_iflag = TTYDEF_IFLAG;\n        tp-&gt;t_oflag = TTYDEF_OFLAG;\n        tp-&gt;t_lflag = TTYDEF_LFLAG;\n        tp-&gt;t_cflag = TTYDEF_CFLAG;\n        tp-&gt;t_ispeed = tp-&gt;t_ospeed = TTYDEF_SPEED;\n    } else if ((tp-&gt;t_state &amp; TS_XCLUDE) &amp;&amp;\n               caps_priv_check(ap-&gt;a_cred, SYSCAP_RESTRICTEDROOT)) {\n        pti_done(pti);\n        lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n        return (EBUSY);             /* Exclusive access */\n    } else if (pti-&gt;pt_prison != ap-&gt;a_cred-&gt;cr_prison) {\n        pti_done(pti);\n        lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n        return (EBUSY);             /* Wrong jail */\n    }\n\n    /* Connect if master present, else clear zombie */\n    if (tp-&gt;t_oproc)\n        (void)(*linesw[tp-&gt;t_line].l_modem)(tp, 1);\n    else if ((pti-&gt;pt_flags &amp; PF_SOPEN) == 0)\n        tp-&gt;t_state &amp;= ~TS_ZOMBIE;\n\n    /* Wait for carrier (master) */\n    while ((tp-&gt;t_state &amp; TS_CARR_ON) == 0) {\n        if (ap-&gt;a_oflags &amp; FNONBLOCK)\n            break;\n        error = ttysleep(tp, TSA_CARR_ON(tp), PCATCH, \"ptsopn\", 0);\n        if (error) {\n            pti_done(pti);\n            lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n            return (error);\n        }\n    }\n\n    /* Complete open via line discipline */\n    error = (*linesw[tp-&gt;t_line].l_open)(dev, tp);\n\n    if (error == 0) {\n        pti-&gt;pt_flags |= PF_SOPEN;\n        pti-&gt;pt_flags &amp;= ~PF_SCLOSED;\n        ptcwakeup(tp, FREAD|FWRITE);\n    }\n\n    pti_done(pti);\n    lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#reading-from-slave","title":"Reading from Slave","text":"<p><code>ptsread()</code> (<code>tty_pty.c:479</code>) reads processed input:</p> <pre><code>static int\nptsread(struct dev_read_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct proc *p = curproc;\n    struct tty *tp = dev-&gt;si_tty;\n    struct pt_ioctl *pti = dev-&gt;si_drv1;\n    struct lwp *lp;\n    int error = 0;\n\n    lp = curthread-&gt;td_lwp;\n    lwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);\n\nagain:\n    /* Remote mode: read from canonical queue directly */\n    if (pti-&gt;pt_flags &amp; PF_REMOTE) {\n        /* Check for background process */\n        while (isbackground(p, tp)) {\n            if (SIGISMEMBER(p-&gt;p_sigignore, SIGTTIN) ||\n                SIGISMEMBER(lp-&gt;lwp_sigmask, SIGTTIN) ||\n                p-&gt;p_pgrp-&gt;pg_jobc == 0 ||\n                (p-&gt;p_flags &amp; P_PPWAIT)) {\n                lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n                return (EIO);\n            }\n            pgsignal(p-&gt;p_pgrp, SIGTTIN, 1);\n            error = ttysleep(tp, &amp;lbolt, PCATCH, \"ptsbg\", 0);\n            if (error) {\n                lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n                return (error);\n            }\n        }\n\n        /* Wait for data */\n        if (tp-&gt;t_canq.c_cc == 0) {\n            if (ap-&gt;a_ioflag &amp; IO_NDELAY) {\n                lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n                return (EWOULDBLOCK);\n            }\n            error = ttysleep(tp, TSA_PTS_READ(tp), PCATCH, \"ptsin\", 0);\n            if (error) {\n                lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n                return (error);\n            }\n            goto again;\n        }\n\n        /* Read until null terminator */\n        while (tp-&gt;t_canq.c_cc &gt; 1 &amp;&amp; ap-&gt;a_uio-&gt;uio_resid &gt; 0)\n            if (ureadc(clist_getc(&amp;tp-&gt;t_canq), ap-&gt;a_uio) &lt; 0) {\n                error = EFAULT;\n                break;\n            }\n        if (tp-&gt;t_canq.c_cc == 1)\n            clist_getc(&amp;tp-&gt;t_canq);     /* Remove null terminator */\n    } else {\n        /* Normal mode: use line discipline */\n        if (tp-&gt;t_oproc)\n            error = (*linesw[tp-&gt;t_line].l_read)(tp, ap-&gt;a_uio,\n                                                  ap-&gt;a_ioflag);\n    }\n\n    ptcwakeup(tp, FWRITE);\n    lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#writing-from-slave","title":"Writing from Slave","text":"<p><code>ptswrite()</code> (<code>tty_pty.c:548</code>) outputs through line discipline:</p> <pre><code>static int\nptswrite(struct dev_write_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct tty *tp;\n    int ret;\n\n    tp = dev-&gt;si_tty;\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\n    if (tp-&gt;t_oproc == NULL) {\n        lwkt_reltoken(&amp;tp-&gt;t_token);\n        return (EIO);               /* No master */\n    }\n\n    ret = (*linesw[tp-&gt;t_line].l_write)(tp, ap-&gt;a_uio, ap-&gt;a_ioflag);\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    return ret;\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#packet-mode","title":"Packet Mode","text":"<p>Packet mode (<code>TIOCPKT</code>) allows the master to receive out-of-band notifications about slave state changes. This is used by applications like <code>rlogin</code> to handle window size changes and flow control.</p>"},{"location":"sys/kern/tty-pty/#packet-mode-events","title":"Packet Mode Events","text":"<pre><code>/* Event flags sent to master (first byte of read) */\n#define TIOCPKT_DATA        0x00    /* Normal data follows */\n#define TIOCPKT_FLUSHREAD   0x01    /* Flush read queue */\n#define TIOCPKT_FLUSHWRITE  0x02    /* Flush write queue */\n#define TIOCPKT_STOP        0x04    /* Output stopped (^S) */\n#define TIOCPKT_START       0x08    /* Output started (^Q) */\n#define TIOCPKT_NOSTOP      0x10    /* No more ^S/^Q needed */\n#define TIOCPKT_DOSTOP      0x20    /* Resume ^S/^Q processing */\n#define TIOCPKT_IOCTL       0x40    /* Termios changed (followed by termios) */\n</code></pre>"},{"location":"sys/kern/tty-pty/#enabling-packet-mode","title":"Enabling Packet Mode","text":"<pre><code>int flag = 1;\nioctl(master_fd, TIOCPKT, &amp;flag);\n</code></pre>"},{"location":"sys/kern/tty-pty/#reading-packet-mode-data","title":"Reading Packet Mode Data","text":"<pre><code>char buf[1024];\nint n = read(master_fd, buf, sizeof(buf));\nif (n &gt; 0) {\n    if (buf[0] == TIOCPKT_DATA) {\n        /* Normal data in buf[1..n-1] */\n    } else {\n        /* Status event in buf[0] */\n        if (buf[0] &amp; TIOCPKT_STOP)\n            printf(\"Output stopped\\n\");\n        if (buf[0] &amp; TIOCPKT_START)\n            printf(\"Output started\\n\");\n        if (buf[0] &amp; TIOCPKT_IOCTL) {\n            /* struct termios follows in buf[1..] */\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#remote-mode","title":"Remote Mode","text":"<p>Remote mode (<code>TIOCREMOTE</code>) puts the PTY into a special state where all flow control is handled by the master application. The master writes directly to the canonical queue with explicit record boundaries.</p> <pre><code>int flag = 1;\nioctl(master_fd, TIOCREMOTE, &amp;flag);\n</code></pre> <p>In remote mode: - Input bypasses line discipline processing - Each write is null-terminated in <code>t_canq</code> - Slave reads complete records at a time - No automatic echo or editing</p>"},{"location":"sys/kern/tty-pty/#user-control-mode","title":"User Control Mode","text":"<p>User control mode (<code>TIOCUCNTL</code>) allows the master to receive single-byte control messages from the slave via special ioctls (<code>UIOCCMD(n)</code>):</p> <pre><code>/* Enable on master */\nint flag = 1;\nioctl(master_fd, TIOCUCNTL, &amp;flag);\n\n/* Slave sends control byte */\nioctl(slave_fd, UIOCCMD(42), NULL);\n\n/* Master receives as first byte of read */\nchar buf[1];\nread(master_fd, buf, 1);  /* buf[0] == 42 */\n</code></pre>"},{"location":"sys/kern/tty-pty/#extended-processing","title":"Extended Processing","text":"<p>External processing (<code>EXTPROC</code>) mode indicates that an external program (typically on a remote system) is handling line discipline processing:</p> <pre><code>int flag = 1;\nioctl(fd, TIOCEXT, &amp;flag);  /* Enable EXTPROC */\n</code></pre> <p>When enabled, local line editing is disabled and the master receives <code>TIOCPKT_IOCTL</code> notifications when termios settings change.</p>"},{"location":"sys/kern/tty-pty/#pty-specific-ioctls","title":"PTY-Specific ioctls","text":""},{"location":"sys/kern/tty-pty/#master-side-ioctls","title":"Master-Side ioctls","text":"ioctl Description <code>TIOCPKT</code> Enable/disable packet mode <code>TIOCUCNTL</code> Enable/disable user control mode <code>TIOCREMOTE</code> Enable/disable remote mode <code>TIOCISPTMASTER</code> Check if device is Unix98 master <code>TIOCSIG</code> Send signal to slave process group"},{"location":"sys/kern/tty-pty/#common-ioctls","title":"Common ioctls","text":"ioctl Description <code>TIOCEXT</code> Enable/disable external processing <code>TIOCGPGRP</code> Get foreground process group <code>TIOCSPGRP</code> Set foreground process group <code>TIOCGWINSZ</code> Get window size <code>TIOCSWINSZ</code> Set window size"},{"location":"sys/kern/tty-pty/#reference-counting-and-cleanup","title":"Reference Counting and Cleanup","text":""},{"location":"sys/kern/tty-pty/#reference-management","title":"Reference Management","text":"<p><code>pti_hold()</code> and <code>pti_done()</code> manage references to prevent premature destruction (<code>tty_pty.c:281</code>):</p> <pre><code>static int\npti_hold(struct pt_ioctl *pti)\n{\n    if (pti-&gt;pt_flags &amp; PF_TERMINATED)\n        return(ENXIO);\n    ++pti-&gt;pt_refs;\n    return(0);\n}\n\nstatic void\npti_done(struct pt_ioctl *pti)\n{\n    lwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);\n    if (--pti-&gt;pt_refs == 0) {\n        /* Check for cleanup conditions */\n        if ((pti-&gt;pt_flags &amp; PF_UNIX98) == 0) {\n            lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n            return;                 /* BSD PTYs never freed */\n        }\n\n        if ((pti-&gt;pt_flags &amp; (PF_SOPEN|PF_MOPEN)) == 0 &amp;&amp;\n            pti-&gt;pt_tty.t_refs == 0) {\n            /* Both sides closed, no session reference */\n            pti-&gt;pt_flags |= PF_TERMINATED;\n\n            /* Destroy devices */\n            if (pti-&gt;devs) {\n                destroy_dev(pti-&gt;devs);\n                pti-&gt;devs = NULL;\n            }\n            if (pti-&gt;devc) {\n                destroy_dev(pti-&gt;devc);\n                pti-&gt;devc = NULL;\n            }\n\n            ttyunregister(&amp;pti-&gt;pt_tty);\n            pti-&gt;pt_tty.t_dev = NULL;\n\n            /* Release bitmap slot */\n            devfs_clone_bitmap_put(&amp;DEVFS_CLONE_BITMAP(pty),\n                                   pti-&gt;pt_uminor);\n            /* Note: pti structure remains allocated */\n        }\n    }\n    lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#session-unhold-callback","title":"Session Unhold Callback","text":"<p>The <code>ptsunhold()</code> callback handles the case where a session holds a reference to the TTY after file descriptors are closed (<code>tty_pty.c:864</code>):</p> <pre><code>static void\nptsunhold(struct tty *tp)\n{\n    struct pt_ioctl *pti = tp-&gt;t_dev-&gt;si_drv1;\n\n    lwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n    pti_hold(pti);\n    --tp-&gt;t_refs;\n    pti_done(pti);                  /* May trigger cleanup */\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    lwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#kqueue-support","title":"kqueue Support","text":"<p>The PTY master supports kqueue for event notification (<code>tty_pty.c:886</code>):</p> <pre><code>static int\nptckqfilter(struct dev_kqfilter_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct knote *kn = ap-&gt;a_kn;\n    struct tty *tp = dev-&gt;si_tty;\n    struct klist *klist;\n\n    switch (kn-&gt;kn_filter) {\n    case EVFILT_READ:\n        klist = &amp;tp-&gt;t_rkq.ki_note;\n        kn-&gt;kn_fop = &amp;ptcread_filtops;\n        break;\n    case EVFILT_WRITE:\n        klist = &amp;tp-&gt;t_wkq.ki_note;\n        kn-&gt;kn_fop = &amp;ptcwrite_filtops;\n        break;\n    default:\n        ap-&gt;a_result = EOPNOTSUPP;\n        return (0);\n    }\n\n    kn-&gt;kn_hook = (caddr_t)dev;\n    knote_insert(klist, kn);\n    return (0);\n}\n</code></pre> <p>Read filter returns true when output queue has data or packet events pending. Write filter returns true when input queues have space.</p>"},{"location":"sys/kern/tty-pty/#synchronization","title":"Synchronization","text":"<p>The PTY subsystem uses multiple tokens for synchronization:</p> <pre><code>/* Typical locking pattern */\nlwkt_gettoken(&amp;pti-&gt;pt_tty.t_token);  /* PTY-specific state */\nlwkt_gettoken(&amp;tp-&gt;t_token);          /* TTY state */\n\n/* ... access protected state ... */\n\nlwkt_reltoken(&amp;tp-&gt;t_token);\nlwkt_reltoken(&amp;pti-&gt;pt_tty.t_token);\n</code></pre> <p>Global token for allocation: <pre><code>lwkt_gettoken(&amp;tty_token);            /* PTY array access */\n</code></pre></p>"},{"location":"sys/kern/tty-pty/#initialization","title":"Initialization","text":"<p>PTY subsystem initialization (<code>tty_pty.c:1334</code>):</p> <pre><code>static void\nptc_drvinit(void *unused)\n{\n    int i;\n\n    /* Create /dev/ptmx clone device for Unix98 */\n    make_autoclone_dev(&amp;ptc_ops, &amp;DEVFS_CLONE_BITMAP(pty), ptyclone,\n                       0, 0, 0666, \"ptmx\");\n\n    /* Allocate pti pointer array */\n    ptis = kmalloc(sizeof(struct pt_ioctl *) * MAXPTYS, M_PTY,\n                   M_WAITOK | M_ZERO);\n\n    /* Pre-create 256 BSD-style PTYs */\n    for (i = 0; i &lt; 256; i++) {\n        ptyinit(i);\n    }\n}\n\nSYSINIT(ptcdev, SI_SUB_DRIVERS, SI_ORDER_MIDDLE + CDEV_MAJOR_C,\n        ptc_drvinit, NULL);\n</code></pre>"},{"location":"sys/kern/tty-pty/#debugging","title":"Debugging","text":"<p>Debug level controllable via sysctl:</p> <pre><code>static int pty_debug_level = 0;\nSYSCTL_INT(_kern, OID_AUTO, pty_debug, CTLFLAG_RW, &amp;pty_debug_level,\n           0, \"Change pty debug level\");\n</code></pre>"},{"location":"sys/kern/tty-pty/#usage-example","title":"Usage Example","text":""},{"location":"sys/kern/tty-pty/#opening-a-pty-pair-unix98","title":"Opening a PTY Pair (Unix98)","text":"<pre><code>#include &lt;fcntl.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n\nint master_fd = open(\"/dev/ptmx\", O_RDWR | O_NOCTTY);\nif (master_fd &lt; 0)\n    err(1, \"open ptmx\");\n\n/* Get slave device path */\nchar *slave_path = ptsname(master_fd);\n\n/* Grant and unlock slave */\ngrantpt(master_fd);\nunlockpt(master_fd);\n\n/* Open slave in child process */\nint slave_fd = open(slave_path, O_RDWR);\n</code></pre>"},{"location":"sys/kern/tty-pty/#typical-terminal-emulator-pattern","title":"Typical Terminal Emulator Pattern","text":"<pre><code>pid_t pid = fork();\nif (pid == 0) {\n    /* Child: become session leader, set controlling terminal */\n    setsid();\n    int slave_fd = open(slave_path, O_RDWR);\n    ioctl(slave_fd, TIOCSCTTY, 0);\n\n    /* Redirect stdio */\n    dup2(slave_fd, STDIN_FILENO);\n    dup2(slave_fd, STDOUT_FILENO);\n    dup2(slave_fd, STDERR_FILENO);\n    close(slave_fd);\n\n    execl(\"/bin/sh\", \"sh\", NULL);\n} else {\n    /* Parent: communicate via master_fd */\n    close(slave_fd);\n    /* read/write master_fd ... */\n}\n</code></pre>"},{"location":"sys/kern/tty-pty/#see-also","title":"See Also","text":"<ul> <li>TTY Subsystem - Core terminal infrastructure</li> <li>Processes - Process model and sessions</li> <li>Signals - Signal delivery to process groups</li> <li>pty(4) - Pseudo-terminal driver manual page</li> <li>posix_openpt(3) - POSIX pseudo-terminal interface</li> </ul>"},{"location":"sys/kern/tty/","title":"TTY Subsystem","text":"<p>The TTY (teletype) subsystem provides the kernel's terminal abstraction layer, handling character I/O between user processes and terminal devices. This subsystem implements line discipline processing, input/output queuing, job control integration, and the classic UNIX terminal interface.</p> <p>For historical context on terminal devices and their evolution from physical teletypes to modern pseudo-terminals, see Wikipedia: Computer terminal.</p>"},{"location":"sys/kern/tty/#source-files","title":"Source Files","text":"File Lines Description <code>sys/kern/tty.c</code> 2,962 Core TTY operations and processing <code>sys/kern/tty_conf.c</code> 185 Line discipline configuration <code>sys/kern/tty_subr.c</code> 302 Character list (clist) routines <code>sys/kern/tty_cons.c</code> 594 Console device abstraction <code>sys/kern/tty_tty.c</code> 341 Controlling terminal (<code>/dev/tty</code>) <code>sys/sys/tty.h</code> 293 TTY structures and definitions"},{"location":"sys/kern/tty/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TD\n    USER[\"User Process\"]\n    USER --&gt;|\"read()/write()\"| DEV[\"/dev/ttyXX(or /dev/tty, /dev/console)\"]\n    DEV --&gt; LDISC[\"Line Discipline(linesw)Input/output processing\"]\n    LDISC --&gt; TTY[\"struct tty(t_rawq, t_canq, t_outq)\"]\n    TTY --&gt;|\"t_oproc()Hardware-specific output\"| HW[\"Hardware/PTYPhysical device or pseudo-terminal\"]\n</code></pre>"},{"location":"sys/kern/tty/#core-data-structures","title":"Core Data Structures","text":""},{"location":"sys/kern/tty/#the-tty-structure","title":"The TTY Structure","text":"<p>The <code>struct tty</code> (<code>sys/sys/tty.h:58</code>) is the central data structure representing a terminal device:</p> <pre><code>struct tty {\n    struct lwkt_token t_token;      /* Per-tty token for MPSAFE */\n    struct clist t_rawq;            /* Raw input queue (unprocessed) */\n    struct clist t_canq;            /* Canonical input queue (line-edited) */\n    struct clist t_outq;            /* Output queue */\n\n    int     t_line;                 /* Line discipline index */\n    int     t_state;                /* TS_* device and driver state */\n    int     t_flags;                /* Pending state changes */\n\n    struct pgrp *t_pgrp;            /* Foreground process group */\n    struct session *t_session;      /* Enclosing session */\n\n    struct termios t_termios;       /* Terminal attributes (POSIX) */\n    struct winsize t_winsize;       /* Window size for TIOCGWINSZ */\n\n    void    (*t_oproc)(struct tty *);   /* Start output routine */\n    void    (*t_stop)(struct tty *, int); /* Stop output routine */\n    int     (*t_param)(struct tty *, struct termios *); /* Set params */\n\n    cdev_t  t_dev;                  /* Associated device */\n    int     t_timeout;              /* Timeout for DCD check */\n    int     t_gen;                  /* Generation count */\n\n    /* Reference counting */\n    struct ttyref *t_refs;          /* Reference list */\n    int     t_refcnt;               /* Reference count */\n};\n</code></pre>"},{"location":"sys/kern/tty/#terminal-state-flags","title":"Terminal State Flags","text":"<p>The <code>t_state</code> field tracks the terminal's current state (<code>tty.h:145</code>):</p> <pre><code>/* Device and driver state flags */\n#define TS_SO_OLOWAT    0x00001   /* Wake when output drops below low water */\n#define TS_ASYNC        0x00002   /* Asynchronous I/O mode (SIGIO) */\n#define TS_BUSY         0x00004   /* Output in progress */\n#define TS_CARR_ON      0x00008   /* Carrier is present (DCD) */\n#define TS_FLUSH        0x00010   /* Flushing output */\n#define TS_ISOPEN       0x00020   /* Device is open */\n#define TS_TBLOCK       0x00040   /* Input blocked (tandem flow control) */\n#define TS_TIMEOUT      0x00080   /* Wait for output drain timeout */\n#define TS_TTSTOP       0x00100   /* Output stopped (^S or hardware) */\n#define TS_ZOMBIE       0x00200   /* Connection lost */\n\n/* State for input processing */\n#define TS_ERASE        0x00400   /* Within multi-char erase sequence */\n#define TS_LNCH         0x00800   /* Next char is literal (^V) */\n#define TS_TYPEN        0x01000   /* Retyping suspended input */\n#define TS_LOCAL        0x02000   /* Ignore carrier (CLOCAL) */\n#define TS_CAN_BYPASS_L_RINT 0x10000  /* Can bypass l_rint for speed */\n\n/* State for connection status */\n#define TS_CONNECTED    0x20000   /* Connection open */\n#define TS_REGISTERED   0x40000   /* TTY registered */\n</code></pre>"},{"location":"sys/kern/tty/#the-termios-structure","title":"The termios Structure","text":"<p>Terminal attributes are stored in POSIX <code>struct termios</code> (<code>sys/sys/_termios.h</code>):</p> <pre><code>struct termios {\n    tcflag_t c_iflag;       /* Input modes */\n    tcflag_t c_oflag;       /* Output modes */\n    tcflag_t c_cflag;       /* Control modes */\n    tcflag_t c_lflag;       /* Local modes */\n    cc_t     c_cc[NCCS];    /* Control characters */\n    speed_t  c_ispeed;      /* Input baud rate */\n    speed_t  c_ospeed;      /* Output baud rate */\n};\n</code></pre> <p>Key flag groups:</p> Field Key Flags Description <code>c_iflag</code> <code>ICRNL</code>, <code>INLCR</code>, <code>IGNCR</code>, <code>IXON</code>, <code>IXOFF</code> Input character mapping, flow control <code>c_oflag</code> <code>OPOST</code>, <code>ONLCR</code>, <code>OXTABS</code> Output post-processing <code>c_cflag</code> <code>CLOCAL</code>, <code>CREAD</code>, <code>HUPCL</code>, <code>CSIZE</code>, <code>PARENB</code> Hardware control, character size, parity <code>c_lflag</code> <code>ICANON</code>, <code>ECHO</code>, <code>ISIG</code>, <code>IEXTEN</code> Canonical mode, echo, signals <code>c_cc[]</code> <code>VINTR</code>, <code>VQUIT</code>, <code>VERASE</code>, <code>VKILL</code>, <code>VEOF</code>, <code>VMIN</code>, <code>VTIME</code> Special control characters"},{"location":"sys/kern/tty/#line-disciplines","title":"Line Disciplines","text":"<p>Line disciplines provide pluggable input/output processing. The default terminal discipline handles canonical editing, while other disciplines support protocols like SLIP and PPP.</p>"},{"location":"sys/kern/tty/#line-discipline-interface","title":"Line Discipline Interface","text":"<p>Each line discipline implements the <code>struct linesw</code> interface (<code>tty.h:213</code>):</p> <pre><code>struct linesw {\n    l_open_t    *l_open;    /* Open routine */\n    l_close_t   *l_close;   /* Close routine */\n    l_read_t    *l_read;    /* Read from input queue */\n    l_write_t   *l_write;   /* Write to output queue */\n    l_ioctl_t   *l_ioctl;   /* Discipline-specific ioctls */\n    l_rint_t    *l_rint;    /* Receive input character */\n    l_start_t   *l_start;   /* Start output */\n    l_modem_t   *l_modem;   /* Modem status change */\n};\n</code></pre>"},{"location":"sys/kern/tty/#available-disciplines","title":"Available Disciplines","text":"<p>DragonFly supports several line disciplines (<code>tty_conf.c:63</code>):</p> Index Name Description 0 <code>TTYDISC</code> Standard terminal discipline 1 <code>TABLDISC</code> Tablet discipline (stub) 2 <code>SLIPDISC</code> SLIP protocol (if compiled) 3 <code>PPPDISC</code> PPP protocol (if compiled)"},{"location":"sys/kern/tty/#registration-api","title":"Registration API","text":"<p>Line disciplines register dynamically (<code>tty_conf.c:112</code>):</p> <pre><code>int ldisc_register(int disc, struct linesw *lsw);\nvoid ldisc_deregister(int disc);\n</code></pre> <p>The standard terminal discipline is implemented by:</p> <pre><code>struct linesw termios_disc = {\n    .l_open   = tty_open,      /* tty.c:389 */\n    .l_close  = tty_close,     /* tty.c:435 */\n    .l_read   = ttread,        /* tty.c:849 */\n    .l_write  = ttwrite,       /* tty.c:1108 */\n    .l_ioctl  = ttioctl,       /* tty.c:1377 */\n    .l_rint   = ttyinput,      /* tty.c:504 */\n    .l_start  = ttstart,       /* tty.c:2106 */\n    .l_modem  = ttymodem       /* tty.c:2172 */\n};\n</code></pre>"},{"location":"sys/kern/tty/#input-processing","title":"Input Processing","text":"<p>Input processing transforms raw device input into processed data for applications, handling special characters, line editing, and flow control.</p>"},{"location":"sys/kern/tty/#input-character-flow","title":"Input Character Flow","text":"<pre><code>flowchart TD\n    HW[\"Hardware Interrupt\"] --&gt; INPUT[\"ttyinput()Receive single character\"]\n    INPUT --&gt; CHECK{ICANON?}\n\n    CHECK --&gt;|\"ICANON=0\"| RAWQ1[\"t_rawq\"]\n    CHECK --&gt;|\"ICANON=1\"| EDIT[\"Line editing(erase, kill, etc.)\"]\n    EDIT --&gt; CANQ[\"t_canq(when line complete)\"]\n\n    RAWQ1 --&gt; READ[\"ttread()Process reads from queue\"]\n    CANQ --&gt; READ\n</code></pre>"},{"location":"sys/kern/tty/#the-ttyinput-function","title":"The ttyinput() Function","text":"<p><code>ttyinput()</code> (<code>tty.c:504</code>) processes each input character:</p> <pre><code>int\nttyinput(int c, struct tty *tp)\n{\n    tcflag_t iflag = tp-&gt;t_iflag;\n    tcflag_t lflag = tp-&gt;t_lflag;\n    cc_t *cc = tp-&gt;t_cc;\n    int i, err;\n\n    /* Check for literal next (^V) */\n    if (ISSET(tp-&gt;t_state, TS_LNCH)) {\n        CLR(tp-&gt;t_state, TS_LNCH);\n        /* Store character literally, even if special */\n    }\n\n    /* Input character mapping (ICRNL, INLCR, etc.) */\n    if (c == '\\r') {\n        if (ISSET(iflag, IGNCR))\n            return 0;               /* Ignore CR */\n        else if (ISSET(iflag, ICRNL))\n            c = '\\n';               /* CR -&gt; NL */\n    } else if (c == '\\n' &amp;&amp; ISSET(iflag, INLCR))\n        c = '\\r';                   /* NL -&gt; CR */\n\n    /* Signal characters (ISIG mode) */\n    if (ISSET(lflag, ISIG)) {\n        if (CCEQ(cc[VINTR], c)) {\n            pgsignal(tp-&gt;t_pgrp, SIGINT, 1);\n            goto endcase;\n        }\n        if (CCEQ(cc[VQUIT], c)) {\n            pgsignal(tp-&gt;t_pgrp, SIGQUIT, 1);\n            goto endcase;\n        }\n        if (CCEQ(cc[VSUSP], c)) {\n            pgsignal(tp-&gt;t_pgrp, SIGTSTP, 1);\n            goto endcase;\n        }\n    }\n\n    /* Flow control (IXON mode) */\n    if (ISSET(iflag, IXON)) {\n        if (CCEQ(cc[VSTOP], c)) {   /* ^S - stop output */\n            if (!ISSET(tp-&gt;t_state, TS_TTSTOP)) {\n                SET(tp-&gt;t_state, TS_TTSTOP);\n                (*tp-&gt;t_stop)(tp, 0);\n            }\n            return 0;\n        }\n        if (CCEQ(cc[VSTART], c)) {  /* ^Q - start output */\n            CLR(tp-&gt;t_state, TS_TTSTOP);\n            goto restartoutput;\n        }\n    }\n\n    /* Canonical mode editing */\n    if (ISSET(lflag, ICANON)) {\n        /* Handle ERASE (backspace), KILL (line delete), etc. */\n        if (CCEQ(cc[VERASE], c)) {\n            ttyrubo(tp, 1);         /* Rub out one character */\n            return 0;\n        }\n        if (CCEQ(cc[VKILL], c)) {\n            ttyflush(tp, FREAD);    /* Flush input */\n            ttyecho(c, tp);\n            return 0;\n        }\n    }\n\n    /* Queue the character */\n    if (ISSET(lflag, ICANON)) {\n        if (CCEQ(cc[VEOF], c) || c == '\\n') {\n            /* Line complete - transfer to canonical queue */\n            catq(&amp;tp-&gt;t_rawq, &amp;tp-&gt;t_canq);\n            ttwakeup(tp);           /* Wake readers */\n        } else {\n            clist_putc(c, &amp;tp-&gt;t_rawq);\n        }\n    } else {\n        clist_putc(c, &amp;tp-&gt;t_rawq);\n        ttwakeup(tp);               /* Wake readers immediately */\n    }\n\n    /* Echo handling */\n    if (ISSET(lflag, ECHO))\n        ttyecho(c, tp);\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/tty/#canonical-vs-raw-mode","title":"Canonical vs Raw Mode","text":"<p>Canonical mode (<code>ICANON</code> set): - Input buffered until newline or EOF - Line editing enabled (erase, kill, word-erase) - Special characters processed (EOF, EOL) - Data stored in <code>t_rawq</code>, transferred to <code>t_canq</code> on line completion</p> <p>Raw mode (<code>ICANON</code> clear): - Characters available immediately - No line editing - <code>VMIN</code>/<code>VTIME</code> control read behavior - Data stored directly in <code>t_rawq</code></p>"},{"location":"sys/kern/tty/#input-tandem-flow-control","title":"Input Tandem Flow Control","text":"<p>When input queues fill, tandem flow control prevents overflow (<code>tty.c:680</code>):</p> <pre><code>/* If input high water mark reached, send STOP character */\nif (tp-&gt;t_rawq.c_cc &gt;= I_HIGH_WATER &amp;&amp; ISSET(iflag, IXOFF)) {\n    if (!ISSET(tp-&gt;t_state, TS_TBLOCK)) {\n        SET(tp-&gt;t_state, TS_TBLOCK);\n        ttstart(tp);    /* Send XOFF (^S) to remote */\n    }\n}\n</code></pre>"},{"location":"sys/kern/tty/#output-processing","title":"Output Processing","text":"<p>Output processing transforms application data before transmission, handling newline mapping, tab expansion, and flow control.</p>"},{"location":"sys/kern/tty/#the-ttyoutput-function","title":"The ttyoutput() Function","text":"<p><code>ttyoutput()</code> (<code>tty.c:236</code>) processes a single output character:</p> <pre><code>int\nttyoutput(int c, struct tty *tp)\n{\n    tcflag_t oflag = tp-&gt;t_oflag;\n    int col;\n\n    if (!ISSET(oflag, OPOST)) {\n        /* Raw output - no processing */\n        if (clist_putc(c, &amp;tp-&gt;t_outq))\n            return c;               /* Queue full */\n        return -1;\n    }\n\n    /* ONLCR: Map NL to CR-NL */\n    if (c == '\\n' &amp;&amp; ISSET(oflag, ONLCR)) {\n        if (clist_putc('\\r', &amp;tp-&gt;t_outq))\n            return c;\n    }\n\n    /* OXTABS: Expand tabs to spaces */\n    if (c == '\\t' &amp;&amp; ISSET(oflag, OXTABS)) {\n        col = 8 - (tp-&gt;t_column &amp; 7);\n        while (col-- &gt; 0) {\n            if (clist_putc(' ', &amp;tp-&gt;t_outq))\n                return c;\n        }\n        return -1;\n    }\n\n    /* Normal character */\n    if (clist_putc(c, &amp;tp-&gt;t_outq))\n        return c;\n\n    /* Update column tracking */\n    switch (c) {\n    case '\\b':  if (tp-&gt;t_column &gt; 0) tp-&gt;t_column--; break;\n    case '\\t':  tp-&gt;t_column = (tp-&gt;t_column + 8) &amp; ~7; break;\n    case '\\n':  tp-&gt;t_column = 0; break;\n    case '\\r':  tp-&gt;t_column = 0; break;\n    default:    if (c &gt;= ' ') tp-&gt;t_column++; break;\n    }\n\n    return -1;\n}\n</code></pre>"},{"location":"sys/kern/tty/#output-flow-control","title":"Output Flow Control","text":"<p>Output stops when hardware or software flow control is active:</p> <pre><code>/* Check if output should proceed */\nvoid ttstart(struct tty *tp)\n{\n    if (!ISSET(tp-&gt;t_state, TS_TTSTOP | TS_TIMEOUT | TS_BUSY))\n        (*tp-&gt;t_oproc)(tp);     /* Call hardware-specific output */\n}\n</code></pre>"},{"location":"sys/kern/tty/#character-lists-clist","title":"Character Lists (Clist)","text":"<p>The clist subsystem provides efficient FIFO queues for terminal I/O, implemented as circular buffers (<code>tty_subr.c</code>).</p>"},{"location":"sys/kern/tty/#clist-structure","title":"Clist Structure","text":"<pre><code>struct clist {\n    int     c_cc;       /* Character count */\n    int     c_ccmax;    /* Maximum capacity */\n    int     c_cchead;   /* Head index (read position) */\n    short   *c_data;    /* Data buffer (with quote bits) */\n};\n</code></pre> <p>The <code>short</code> data type stores both the character (low byte) and a quote bit (high bit) that marks literal characters that should bypass special handling.</p>"},{"location":"sys/kern/tty/#key-clist-operations","title":"Key Clist Operations","text":"Function File:Line Description <code>clist_alloc_cblocks()</code> <code>tty_subr.c:68</code> Allocate clist buffer <code>clist_free_cblocks()</code> <code>tty_subr.c:91</code> Free clist buffer <code>clist_getc()</code> <code>tty_subr.c:107</code> Get character from head <code>clist_putc()</code> <code>tty_subr.c:134</code> Put character at tail <code>b_to_q()</code> <code>tty_subr.c:176</code> Copy block to clist <code>q_to_b()</code> <code>tty_subr.c:212</code> Copy clist to block <code>unputc()</code> <code>tty_subr.c:254</code> Remove last character <code>catq()</code> <code>tty_subr.c:282</code> Concatenate two clists"},{"location":"sys/kern/tty/#clist-quoting","title":"Clist Quoting","text":"<p>The quote bit marks characters as literal (<code>tty_subr.c:156</code>):</p> <pre><code>int\nclist_putc(int c, struct clist *cl)\n{\n    int i;\n\n    if (cl-&gt;c_cc &gt;= cl-&gt;c_ccmax)\n        return -1;              /* Queue full */\n\n    i = (cl-&gt;c_cchead + cl-&gt;c_cc) % cl-&gt;c_ccmax;\n    cl-&gt;c_data[i] = (short)c;   /* Quote bit in high byte */\n    cl-&gt;c_cc++;\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/tty/#tty-operations","title":"TTY Operations","text":""},{"location":"sys/kern/tty/#opening-a-terminal","title":"Opening a Terminal","text":"<p><code>tty_open()</code> (<code>tty.c:389</code>) initializes a terminal for use:</p> <pre><code>int\ntty_open(cdev_t device, struct tty *tp)\n{\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\n    /* Initialize termios to sane defaults */\n    if (!ISSET(tp-&gt;t_state, TS_ISOPEN)) {\n        tp-&gt;t_termios = deftermios;     /* Default settings */\n        SET(tp-&gt;t_state, TS_ISOPEN);\n        bzero(&amp;tp-&gt;t_winsize, sizeof(tp-&gt;t_winsize));\n    }\n\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/tty/#reading-from-a-terminal","title":"Reading from a Terminal","text":"<p><code>ttread()</code> (<code>tty.c:849</code>) handles read operations:</p> <pre><code>int\nttread(struct tty *tp, struct uio *uio, int flag)\n{\n    struct clist *qp;\n    int c, first, error = 0;\n    tcflag_t lflag;\n    cc_t *cc = tp-&gt;t_cc;\n\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\nloop:\n    lflag = tp-&gt;t_lflag;\n\n    /* Select appropriate queue */\n    qp = ISSET(lflag, ICANON) ? &amp;tp-&gt;t_canq : &amp;tp-&gt;t_rawq;\n\n    /* Check for carrier loss */\n    if (!ISSET(tp-&gt;t_state, TS_CONNECTED)) {\n        if (!ISSET(tp-&gt;t_state, TS_ZOMBIE)) {\n            /* First detect - become zombie */\n            SET(tp-&gt;t_state, TS_ZOMBIE);\n        }\n        lwkt_reltoken(&amp;tp-&gt;t_token);\n        return 0;                   /* EOF on carrier loss */\n    }\n\n    /* Wait for data if queue empty */\n    if (qp-&gt;c_cc &lt;= 0) {\n        if (flag &amp; IO_NDELAY) {\n            lwkt_reltoken(&amp;tp-&gt;t_token);\n            return EWOULDBLOCK;\n        }\n        error = ttysleep(tp, TSA_HUP_OR_INPUT(tp),\n                        PCATCH, \"ttyin\", 0);\n        if (error)\n            goto out;\n        goto loop;\n    }\n\n    /* Read characters from queue */\n    first = 1;\n    while (uio-&gt;uio_resid &gt; 0) {\n        c = clist_getc(qp);\n        if (c &lt; 0)\n            break;\n\n        /* Check for EOF in canonical mode */\n        if (ISSET(lflag, ICANON) &amp;&amp; CCEQ(cc[VEOF], c))\n            break;\n\n        error = ureadc(c, uio);\n        if (error)\n            break;\n\n        /* Line-delimited in canonical mode */\n        if (ISSET(lflag, ICANON) &amp;&amp; (c == '\\n' || CCEQ(cc[VEOL], c)))\n            break;\n\n        first = 0;\n    }\n\nout:\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/tty/#writing-to-a-terminal","title":"Writing to a Terminal","text":"<p><code>ttwrite()</code> (<code>tty.c:1108</code>) handles write operations:</p> <pre><code>int\nttwrite(struct tty *tp, struct uio *uio, int flag)\n{\n    cc_t *cp;\n    int cc, ce, c;\n    int i, hiwat, cnt, error;\n    char obuf[OBUFSIZ];\n\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\n    hiwat = tp-&gt;t_ohiwat;\n    cnt = uio-&gt;uio_resid;\n\nloop:\n    /* Wait if output queue full */\n    while (tp-&gt;t_outq.c_cc &gt; hiwat) {\n        if (flag &amp; IO_NDELAY) {\n            if (cnt == uio-&gt;uio_resid) {\n                lwkt_reltoken(&amp;tp-&gt;t_token);\n                return EWOULDBLOCK;\n            }\n            goto out;\n        }\n\n        SET(tp-&gt;t_state, TS_SO_OLOWAT);\n        error = ttysleep(tp, TSA_OLOWAT(tp), PCATCH, \"ttyout\", hz);\n        if (error)\n            goto out;\n    }\n\n    /* Copy user data and process output */\n    while (uio-&gt;uio_resid &gt; 0) {\n        cc = min(uio-&gt;uio_resid, OBUFSIZ);\n        error = uiomove(obuf, cc, uio);\n        if (error)\n            break;\n\n        /* Process each character through ttyoutput() */\n        for (cp = obuf, ce = cc; ce &gt; 0; cp++, ce--) {\n            c = *cp;\n            if (ttyoutput(c, tp) &gt;= 0) {\n                /* Queue full - wait for drain */\n                ttstart(tp);\n                goto loop;\n            }\n        }\n    }\n\nout:\n    /* Start output */\n    ttstart(tp);\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/tty/#terminal-ioctl-handling","title":"Terminal ioctl Handling","text":"<p><code>ttioctl()</code> (<code>tty.c:1377</code>) processes terminal control requests:</p> <pre><code>int\nttioctl(struct tty *tp, u_long cmd, caddr_t data, int flag)\n{\n    struct proc *p = curproc;\n    int error;\n\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\n    switch (cmd) {\n    /* Get terminal attributes */\n    case TIOCGETA:\n        bcopy(&amp;tp-&gt;t_termios, data, sizeof(struct termios));\n        break;\n\n    /* Set terminal attributes */\n    case TIOCSETA:\n    case TIOCSETAW:     /* Wait for output drain first */\n    case TIOCSETAF:     /* Flush input first */\n        if (cmd == TIOCSETAW || cmd == TIOCSETAF) {\n            error = ttywait(tp);\n            if (error)\n                goto out;\n            if (cmd == TIOCSETAF)\n                ttyflush(tp, FREAD);\n        }\n        bcopy(data, &amp;tp-&gt;t_termios, sizeof(struct termios));\n        /* Notify driver of parameter change */\n        if (tp-&gt;t_param)\n            (*tp-&gt;t_param)(tp, &amp;tp-&gt;t_termios);\n        break;\n\n    /* Get/set window size */\n    case TIOCGWINSZ:\n        bcopy(&amp;tp-&gt;t_winsize, data, sizeof(struct winsize));\n        break;\n    case TIOCSWINSZ:\n        if (bcmp(&amp;tp-&gt;t_winsize, data, sizeof(struct winsize))) {\n            bcopy(data, &amp;tp-&gt;t_winsize, sizeof(struct winsize));\n            pgsignal(tp-&gt;t_pgrp, SIGWINCH, 1);\n        }\n        break;\n\n    /* Get/set foreground process group */\n    case TIOCGPGRP:\n        *(int *)data = tp-&gt;t_pgrp ? tp-&gt;t_pgrp-&gt;pg_id : NO_PID;\n        break;\n    case TIOCSPGRP:\n        error = tty_set_pgrp(tp, *(int *)data);\n        break;\n\n    /* Set controlling terminal */\n    case TIOCSCTTY:\n        error = tty_set_ctty(tp, p);\n        break;\n\n    /* Flush queues */\n    case TIOCFLUSH:\n        ttyflush(tp, *(int *)data);\n        break;\n\n    /* Send break */\n    case TIOCSBRK:\n    case TIOCCBRK:\n        /* Passed to hardware driver */\n        break;\n    }\n\nout:\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/tty/#key-ioctl-commands","title":"Key ioctl Commands","text":"Command Description <code>TIOCGETA</code> Get terminal attributes (termios) <code>TIOCSETA</code> Set terminal attributes immediately <code>TIOCSETAW</code> Set attributes after output drains <code>TIOCSETAF</code> Set attributes after flush <code>TIOCGWINSZ</code> Get window size <code>TIOCSWINSZ</code> Set window size (sends SIGWINCH) <code>TIOCGPGRP</code> Get foreground process group <code>TIOCSPGRP</code> Set foreground process group <code>TIOCSCTTY</code> Set controlling terminal <code>TIOCNOTTY</code> Give up controlling terminal <code>TIOCFLUSH</code> Flush input/output queues <code>TIOCCONS</code> Redirect console output <code>TIOCDRAIN</code> Wait for output to drain"},{"location":"sys/kern/tty/#console-subsystem","title":"Console Subsystem","text":"<p>The console subsystem (<code>tty_cons.c</code>) provides a unified interface to the system console, abstracting the underlying hardware (VGA, serial, etc.).</p>"},{"location":"sys/kern/tty/#console-architecture","title":"Console Architecture","text":"<pre><code>flowchart TD\n    KPRINTF[\"kprintf() / printf()\"] --&gt; CNPUTC[\"cnputc()Kernel console output\"]\n    CNPUTC --&gt; CNTAB[\"cn_tabActive console driver\"]\n    CNTAB --&gt;|\"cn_putc()Hardware-specific output\"| HW[\"VGA /Serial\"]\n</code></pre>"},{"location":"sys/kern/tty/#console-device-structure","title":"Console Device Structure","text":"<pre><code>struct consdev {\n    cn_probe_t  *cn_probe;      /* Probe for hardware */\n    cn_init_t   *cn_init;       /* Initialize hardware */\n    cn_init_t   *cn_init_fini;  /* Finalize initialization */\n    cn_term_t   *cn_term;       /* Terminate console */\n    cn_getc_t   *cn_getc;       /* Get character (polled) */\n    cn_checkc_t *cn_checkc;     /* Check for character */\n    cn_putc_t   *cn_putc;       /* Output character (polled) */\n    cn_poll_t   *cn_poll;       /* Poll mode control */\n    cn_dbctl_t  *cn_dbctl;      /* Debugger control */\n\n    short       cn_pri;         /* Priority (CN_DEAD to CN_REMOTE) */\n    short       cn_probegood;   /* Probe succeeded */\n    void        *cn_private;    /* Driver-private data */\n    cdev_t      cn_dev;         /* Associated device */\n};\n</code></pre>"},{"location":"sys/kern/tty/#console-priority-levels","title":"Console Priority Levels","text":"<pre><code>#define CN_DEAD     0   /* Device doesn't exist */\n#define CN_NORMAL   1   /* Normal priority (VGA) */\n#define CN_INTERNAL 2   /* Fallback internal device */\n#define CN_REMOTE   3   /* High priority (serial console) */\n</code></pre>"},{"location":"sys/kern/tty/#console-initialization","title":"Console Initialization","text":"<p><code>cninit()</code> (<code>tty_cons.c:140</code>) probes and initializes the console:</p> <pre><code>void cninit(void)\n{\n    struct consdev *best_cp, *cp, **list;\n\n    /* Find console with highest priority */\n    best_cp = NULL;\n    SET_FOREACH(list, cons_set) {\n        cp = *list;\n        if (cp-&gt;cn_probe == NULL)\n            continue;\n        (*cp-&gt;cn_probe)(cp);\n        if (cp-&gt;cn_pri &gt; CN_DEAD &amp;&amp; cp-&gt;cn_probegood &amp;&amp;\n            (best_cp == NULL || cp-&gt;cn_pri &gt; best_cp-&gt;cn_pri))\n            best_cp = cp;\n    }\n\n    /* Initialize selected console */\n    if (best_cp != NULL) {\n        (*best_cp-&gt;cn_init)(best_cp);\n        if (cn_tab != NULL &amp;&amp; cn_tab != best_cp)\n            if (cn_tab-&gt;cn_term != NULL)\n                (*cn_tab-&gt;cn_term)(cn_tab);\n    }\n    cn_tab = best_cp;\n}\n</code></pre>"},{"location":"sys/kern/tty/#console-io-functions","title":"Console I/O Functions","text":"Function Description <code>cnputc()</code> Output character to console <code>cngetc()</code> Get character from console (blocking) <code>cncheckc()</code> Check for pending character (non-blocking) <code>cnpoll()</code> Enable/disable polled mode"},{"location":"sys/kern/tty/#console-muting","title":"Console Muting","text":"<p>The console can be muted for security (<code>tty_cons.c:258</code>):</p> <pre><code>/* Controlled via sysctl kern.consmute */\nstatic int cn_mute;\n\nSYSCTL_PROC(_kern, OID_AUTO, consmute, CTLTYPE_INT|CTLFLAG_RW,\n    0, sizeof cn_mute, sysctl_kern_consmute, \"I\", \"\");\n</code></pre>"},{"location":"sys/kern/tty/#controlling-terminal-devtty","title":"Controlling Terminal (/dev/tty)","text":"<p>The <code>/dev/tty</code> device (<code>tty_tty.c</code>) provides each process access to its controlling terminal, if any.</p>"},{"location":"sys/kern/tty/#the-cttyvp-macro","title":"The cttyvp() Macro","text":"<pre><code>#define cttyvp(p) (((p)-&gt;p_flags &amp; P_CONTROLT) ? \\\n                    (p)-&gt;p_session-&gt;s_ttyvp : NULL)\n</code></pre> <p>A process has a controlling terminal when: 1. <code>P_CONTROLT</code> flag is set in <code>p_flags</code> 2. Session has a valid <code>s_ttyvp</code> (controlling terminal vnode)</p>"},{"location":"sys/kern/tty/#device-operations","title":"Device Operations","text":"<p>The ctty device redirects operations to the actual controlling terminal:</p> <pre><code>static int cttyread(struct dev_read_args *ap)\n{\n    struct proc *p = curproc;\n    struct vnode *ttyvp;\n    int error;\n\n    ttyvp = cttyvp(p);\n    if (ttyvp == NULL)\n        return (EIO);       /* No controlling terminal */\n\n    error = vget(ttyvp, LK_EXCLUSIVE | LK_RETRY);\n    if (error == 0) {\n        error = VOP_READ(ttyvp, ap-&gt;a_uio, ap-&gt;a_ioflag, NOCRED);\n        vput(ttyvp);\n    }\n    return (error);\n}\n</code></pre>"},{"location":"sys/kern/tty/#controlling-terminal-ioctls","title":"Controlling Terminal ioctls","text":"<p>Special handling for controlling terminal ioctls (<code>tty_tty.c:231</code>):</p> <pre><code>static int cttyioctl(struct dev_ioctl_args *ap)\n{\n    struct vnode *ttyvp;\n    struct proc *p = curproc;\n\n    ttyvp = cttyvp(p);\n    if (ttyvp == NULL)\n        return (EIO);\n\n    /* Prevent infinite recursion */\n    if (ap-&gt;a_cmd == TIOCSCTTY)\n        return EINVAL;\n\n    /* Handle TIOCNOTTY for non-session-leaders */\n    if (ap-&gt;a_cmd == TIOCNOTTY) {\n        if (!SESS_LEADER(p)) {\n            p-&gt;p_flags &amp;= ~P_CONTROLT;\n            return (0);\n        }\n        return (EINVAL);\n    }\n\n    return (VOP_IOCTL(ttyvp, ap-&gt;a_cmd, ap-&gt;a_data,\n                      ap-&gt;a_fflag, ap-&gt;a_cred, ap-&gt;a_sysmsg));\n}\n</code></pre>"},{"location":"sys/kern/tty/#job-control-integration","title":"Job Control Integration","text":"<p>The TTY subsystem integrates tightly with job control, managing foreground process groups and delivering signals.</p>"},{"location":"sys/kern/tty/#session-and-process-group-association","title":"Session and Process Group Association","text":"<pre><code>/* Assign controlling terminal to session */\nint tty_set_ctty(struct tty *tp, struct proc *p)\n{\n    struct session *sess = p-&gt;p_session;\n\n    /* Must be session leader without existing ctty */\n    if (!SESS_LEADER(p) || sess-&gt;s_ttyvp != NULL)\n        return EPERM;\n\n    sess-&gt;s_ttyp = tp;\n    tp-&gt;t_session = sess;\n    tp-&gt;t_pgrp = p-&gt;p_pgrp;\n    p-&gt;p_flags |= P_CONTROLT;\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/tty/#signal-delivery","title":"Signal Delivery","text":"<p>Input signals are sent to the foreground process group:</p> <pre><code>/* Signal handling in ttyinput() */\nif (CCEQ(cc[VINTR], c)) {\n    pgsignal(tp-&gt;t_pgrp, SIGINT, 1);    /* ^C */\n}\nif (CCEQ(cc[VQUIT], c)) {\n    pgsignal(tp-&gt;t_pgrp, SIGQUIT, 1);   /* ^\\ */\n}\nif (CCEQ(cc[VSUSP], c)) {\n    pgsignal(tp-&gt;t_pgrp, SIGTSTP, 1);   /* ^Z */\n}\n</code></pre>"},{"location":"sys/kern/tty/#hangup-handling","title":"Hangup Handling","text":"<p><code>ttymodem()</code> (<code>tty.c:2172</code>) handles carrier detect changes:</p> <pre><code>int ttymodem(struct tty *tp, int flag)\n{\n    if (flag) {\n        /* Carrier detected */\n        SET(tp-&gt;t_state, TS_CARR_ON | TS_CONNECTED);\n        wakeup(TSA_CARR_ON(tp));\n    } else {\n        /* Carrier lost */\n        if (!ISSET(tp-&gt;t_state, TS_ZOMBIE)) {\n            SET(tp-&gt;t_state, TS_ZOMBIE);\n            ttyflush(tp, FREAD | FWRITE);\n            if (tp-&gt;t_session &amp;&amp; tp-&gt;t_session-&gt;s_leader) {\n                ksignal(tp-&gt;t_session-&gt;s_leader, SIGHUP);\n            }\n            pgsignal(tp-&gt;t_pgrp, SIGHUP, 1);\n        }\n    }\n    return !ISSET(tp-&gt;t_state, TS_ZOMBIE);\n}\n</code></pre>"},{"location":"sys/kern/tty/#synchronization","title":"Synchronization","text":"<p>The TTY subsystem uses per-terminal tokens for MPSAFE operation.</p>"},{"location":"sys/kern/tty/#tty-token","title":"TTY Token","text":"<p>Each terminal has a dedicated token (<code>tty.h:58</code>):</p> <pre><code>struct tty {\n    struct lwkt_token t_token;  /* Per-tty token */\n    ...\n};\n</code></pre>"},{"location":"sys/kern/tty/#locking-pattern","title":"Locking Pattern","text":"<pre><code>void some_tty_operation(struct tty *tp)\n{\n    lwkt_gettoken(&amp;tp-&gt;t_token);\n\n    /* Protected access to tty state */\n    ...\n\n    lwkt_reltoken(&amp;tp-&gt;t_token);\n}\n</code></pre>"},{"location":"sys/kern/tty/#global-tokens","title":"Global Tokens","text":"<p>Two global tokens protect console and VGA state (<code>tty_cons.c:152</code>):</p> <pre><code>lwkt_gettoken(&amp;tty_token);    /* General TTY operations */\nlwkt_gettoken(&amp;vga_token);    /* VGA console access */\n</code></pre>"},{"location":"sys/kern/tty/#sleeping-and-wakeup","title":"Sleeping and Wakeup","text":""},{"location":"sys/kern/tty/#sleep-addresses","title":"Sleep Addresses","text":"<p>The TTY subsystem uses macros for typed sleep addresses:</p> <pre><code>#define TSA_CARR_ON(tp)     ((void *)&amp;(tp)-&gt;t_rawq)\n#define TSA_HUP_OR_INPUT(tp) ((void *)&amp;(tp)-&gt;t_rawq.c_cc)\n#define TSA_OLOWAT(tp)      ((void *)&amp;(tp)-&gt;t_outq)\n#define TSA_OCOMPLETE(tp)   ((void *)&amp;(tp)-&gt;t_outq.c_cc)\n</code></pre>"},{"location":"sys/kern/tty/#ttysleep","title":"ttysleep()","text":"<p><code>ttysleep()</code> (<code>tty.c:2054</code>) wraps sleeping with generation count:</p> <pre><code>int ttysleep(struct tty *tp, void *chan, int slpflags,\n             char *wmesg, int timo)\n{\n    int gen = tp-&gt;t_gen;\n    int error;\n\n    error = tsleep(chan, slpflags, wmesg, timo);\n\n    /* Check if tty was revoked while sleeping */\n    if (error == 0 &amp;&amp; gen != tp-&gt;t_gen)\n        error = ERESTART;\n\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/tty/#wakeup-functions","title":"Wakeup Functions","text":"Function Description <code>ttwakeup()</code> Wake readers waiting on input <code>ttwwakeup()</code> Wake writers waiting for output queue space"},{"location":"sys/kern/tty/#debugging-interface","title":"Debugging Interface","text":"<p>The console provides direct kernel access for debugging:</p> <pre><code>/* Break to debugger configuration */\nint break_to_debugger;      /* CTL-ALT-ESC on keyboard */\nint alt_break_to_debugger;  /* CR ~ ^B on serial */\n\nSYSCTL_INT(_kern, OID_AUTO, break_to_debugger, CTLFLAG_RW,\n    &amp;break_to_debugger, 0, \"\");\nSYSCTL_INT(_kern, OID_AUTO, alt_break_to_debugger, CTLFLAG_RW,\n    &amp;alt_break_to_debugger, 0, \"\");\n</code></pre> <p>The <code>cndbctl()</code> function (<code>tty_cons.c:571</code>) enables/disables debugger mode.</p>"},{"location":"sys/kern/tty/#see-also","title":"See Also","text":"<ul> <li>Pseudo-Terminals (PTY) - Master/slave pseudo-terminal implementation</li> <li>Processes - Process model and job control</li> <li>Signals - Signal delivery mechanisms</li> <li>Devices - Device driver framework</li> <li>IPC - Inter-process communication overview</li> <li>termios(4) - Terminal interface manual page</li> </ul>"},{"location":"sys/kern/utilities/","title":"Kernel Utilities and Miscellaneous Subsystems","text":"<p>This document covers miscellaneous kernel utility subsystems in DragonFly BSD that don't fit into other major categories. These utilities provide essential support functions used throughout the kernel.</p>"},{"location":"sys/kern/utilities/#overview","title":"Overview","text":"Utility Source Purpose CPU Topology <code>subr_cpu_topology.c</code> CPU hierarchy detection and management Event Handlers <code>subr_eventhandler.c</code> Generic kernel event notification Unit Number Allocation <code>subr_unit.c</code> Efficient unit number management Variant Symlinks (varsym) <code>kern_varsym.c</code> Variable substitution in symlinks Extended I/O (XIO) <code>kern_xio.c</code> Page-based buffer management"},{"location":"sys/kern/utilities/#cpu-topology","title":"CPU Topology","text":"<p>The CPU topology subsystem (<code>subr_cpu_topology.c</code>) detects and manages the hierarchical structure of CPUs in multi-core and multi-socket systems.</p>"},{"location":"sys/kern/utilities/#topology-levels","title":"Topology Levels","text":"<pre><code>/* Topology hierarchy from top to bottom */\n#define PACKAGE_LEVEL   0   /* Physical package/socket */\n#define CHIP_LEVEL      1   /* Physical chip (socket) */\n#define CORE_LEVEL      2   /* Physical core */\n#define THREAD_LEVEL    3   /* Logical thread (HT/SMT) */\n</code></pre>"},{"location":"sys/kern/utilities/#cpu_node_t-structure","title":"cpu_node_t Structure","text":"<p>Each node in the topology tree (<code>subr_cpu_topology.c:63</code>):</p> <pre><code>struct cpu_node {\n    cpu_node_t      *parent_node;           /* Parent in tree */\n    cpu_node_t      *child_node[MAXCPU];    /* Children */\n    int             child_no;               /* Number of children */\n    cpumask_t       members;                /* CPUs in this node */\n    uint8_t         type;                   /* Level type */\n    uint8_t         compute_unit_id;        /* AMD compute unit */\n    long            phys_mem;               /* NUMA: memory attached */\n};\n</code></pre>"},{"location":"sys/kern/utilities/#topology-detection","title":"Topology Detection","text":"<p>The system builds a tree representing CPU relationships:</p> <pre><code>flowchart TD\n    ROOT[\"PACKAGE (root)\"]\n    ROOT --&gt; CHIP0[\"CHIP 0\"]\n    ROOT --&gt; CHIP1[\"CHIP 1\"]\n    ROOT --&gt; CHIP2[\"CHIP 2\"]\n\n    CHIP0 --&gt; CORE00[\"CORE 0\"]\n    CHIP0 --&gt; CORE01[\"CORE 1\"]\n    CHIP1 --&gt; CORE10[\"CORE 0\"]\n    CHIP1 --&gt; CORE11[\"CORE 1\"]\n    CHIP2 --&gt; CORE20[\"CORE 0\"]\n    CHIP2 --&gt; CORE21[\"CORE 1\"]\n\n    CORE00 --&gt; THR00[\"THR 0,1\"]\n    CORE01 --&gt; THR01[\"THR 0,1\"]\n    CORE10 --&gt; THR10[\"...\"]\n    CORE11 --&gt; THR11[\"...\"]\n    CORE20 --&gt; THR20[\"...\"]\n    CORE21 --&gt; THR21[\"...\"]\n</code></pre>"},{"location":"sys/kern/utilities/#api-functions","title":"API Functions","text":"<pre><code>/* Get CPU node by CPU ID */\ncpu_node_t *get_cpu_node_by_cpuid(int cpuid);\n\n/* Get CPU node by chip ID */\nconst cpu_node_t *get_cpu_node_by_chipid(int chip_id);\n\n/* Get sibling mask at specified level */\ncpumask_t get_cpumask_from_level(int cpuid, uint8_t level_type);\n\n/* Get CPU IDs */\nint get_cpu_ht_id(int cpuid);       /* Thread ID within core */\nint get_cpu_core_id(int cpuid);     /* Core ID within chip */\nint get_cpu_phys_id(int cpuid);     /* Physical package ID */\n\n/* NUMA support */\nlong get_highest_node_memory(void);  /* Highest memory on any node */\n</code></pre>"},{"location":"sys/kern/utilities/#sysctl-interface","title":"Sysctl Interface","text":"<p>The topology is exposed via sysctl:</p> <pre><code>hw.cpu_topology.tree              - ASCII tree diagram\nhw.cpu_topology.level_description - Level meaning\nhw.cpu_topology.members           - All CPUs in system\nhw.cpu_topology.cpu0.physical_id  - Package ID for CPU 0\nhw.cpu_topology.cpu0.core_id      - Core ID for CPU 0\nhw.cpu_topology.cpu0.physical_siblings - CPUs in same package\nhw.cpu_topology.cpu0.core_siblings - CPUs in same core\n</code></pre>"},{"location":"sys/kern/utilities/#global-variables","title":"Global Variables","text":"<pre><code>/* Available after SI_BOOT2_CPU_TOPOLOGY */\nint cpu_topology_levels_number;    /* 2, 3, or 4 levels */\nint cpu_topology_ht_ids;           /* Threads per core */\nint cpu_topology_core_ids;         /* Cores per chip */\nint cpu_topology_phys_ids;         /* Physical packages */\ncpu_node_t *root_cpu_node;         /* Root of topology tree */\n</code></pre>"},{"location":"sys/kern/utilities/#event-handler-framework","title":"Event Handler Framework","text":"<p>The event handler framework (<code>subr_eventhandler.c</code>) provides a generic mechanism for registering callbacks that are invoked when specific events occur.</p>"},{"location":"sys/kern/utilities/#event-lists","title":"Event Lists","text":"<p>Events are organized in named lists:</p> <pre><code>struct eventhandler_list {\n    TAILQ_HEAD(, eventhandler_entry) el_entries;  /* Handlers */\n    char            *el_name;                      /* Event name */\n    int             el_flags;                      /* State flags */\n    TAILQ_ENTRY(eventhandler_list) el_link;       /* Global list */\n};\n</code></pre>"},{"location":"sys/kern/utilities/#registration","title":"Registration","text":"<pre><code>#include &lt;sys/eventhandler.h&gt;\n\n/* Register a handler */\neventhandler_tag\neventhandler_register(struct eventhandler_list *list,\n                      const char *name,\n                      void *func,\n                      void *arg,\n                      int priority);\n\n/* Deregister a handler */\nvoid\neventhandler_deregister(struct eventhandler_list *list,\n                        eventhandler_tag tag);\n\n/* Find a list by name */\nstruct eventhandler_list *\neventhandler_find_list(const char *name);\n</code></pre>"},{"location":"sys/kern/utilities/#priority-ordering","title":"Priority Ordering","text":"<p>Handlers are invoked in priority order (lower first):</p> <pre><code>/* Standard priorities */\n#define EVENTHANDLER_PRI_FIRST      0\n#define EVENTHANDLER_PRI_ANY        10000\n#define EVENTHANDLER_PRI_LAST       20000\n</code></pre>"},{"location":"sys/kern/utilities/#common-events","title":"Common Events","text":"Event Arguments When Invoked <code>shutdown_pre_sync</code> <code>(void *arg, int howto)</code> Before filesystem sync <code>shutdown_post_sync</code> <code>(void *arg, int howto)</code> After sync, before halt <code>shutdown_final</code> <code>(void *arg, int howto)</code> Final shutdown <code>process_exit</code> <code>(void *arg, struct proc *p)</code> Process termination <code>process_fork</code> <code>(void *arg, struct proc *p1, struct proc *p2, int flags)</code> Fork"},{"location":"sys/kern/utilities/#usage-example","title":"Usage Example","text":"<pre><code>/* Handler function */\nstatic void\nmydev_shutdown(void *arg, int howto)\n{\n    struct mydev_softc *sc = arg;\n    mydev_flush(sc);\n}\n\n/* Register during attach */\nsc-&gt;shutdown_tag = EVENTHANDLER_REGISTER(shutdown_pre_sync,\n    mydev_shutdown, sc, SHUTDOWN_PRI_DEFAULT);\n\n/* Deregister during detach */\nEVENTHANDLER_DEREGISTER(shutdown_pre_sync, sc-&gt;shutdown_tag);\n</code></pre>"},{"location":"sys/kern/utilities/#unit-number-allocation","title":"Unit Number Allocation","text":"<p>The unit allocator (<code>subr_unit.c</code>) provides efficient allocation of unit numbers for devices and other resources using a mixed run-length/bitmap approach.</p>"},{"location":"sys/kern/utilities/#design-goals","title":"Design Goals","text":"<ul> <li>Lowest free number first policy</li> <li>Memory-efficient for sparse allocations</li> <li>O(1) allocation in common cases</li> <li>Thread-safe with optional custom locking</li> </ul>"},{"location":"sys/kern/utilities/#data-structures","title":"Data Structures","text":"<pre><code>/* Unit range header */\nstruct unrhdr {\n    TAILQ_HEAD(, unr)   head;       /* List of ranges */\n    u_int               low;        /* Lowest valid unit */\n    u_int               high;       /* Highest valid unit */\n    u_int               busy;       /* Allocated count */\n    u_int               alloc;      /* Memory block count */\n    u_int               first;      /* Units allocated from start */\n    u_int               last;       /* Units free at end */\n    struct lock         *lock;      /* Locking */\n};\n\n/* Range element (run-length or bitmap) */\nstruct unr {\n    TAILQ_ENTRY(unr)    list;\n    u_int               len;        /* Length or bitmap count */\n    void                *ptr;       /* NULL=free, unrhdr=alloc, else bitmap */\n};\n</code></pre>"},{"location":"sys/kern/utilities/#memory-efficiency","title":"Memory Efficiency","text":"<p>The allocator automatically optimizes storage:</p> <ul> <li>Ideal split: Just tracks first allocated and last free counts</li> <li>Run-length: Consecutive free/allocated ranges stored as count</li> <li>Bitmap: Mixed regions use compact bitmaps</li> </ul> <p>Memory usage examples: - Single contiguous run: 44 bytes (x86) - 1000 units, random pattern: ~252 bytes worst case - Worst case (alternating): 44 + N/4 bytes</p>"},{"location":"sys/kern/utilities/#api","title":"API","text":"<pre><code>/* Create a new unit number space */\nstruct unrhdr *new_unrhdr(int low, int high, struct lock *lock);\n\n/* Delete a unit number space */\nvoid delete_unrhdr(struct unrhdr *uh);\n\n/* Allocate a unit number (with locking) */\nint alloc_unr(struct unrhdr *uh);\n\n/* Allocate with lock already held */\nint alloc_unrl(struct unrhdr *uh);\n\n/* Free a unit number */\nvoid free_unr(struct unrhdr *uh, u_int item);\n</code></pre>"},{"location":"sys/kern/utilities/#usage-example_1","title":"Usage Example","text":"<pre><code>static struct unrhdr *mydev_units;\n\n/* Initialize unit allocator */\nstatic void\nmydev_init(void)\n{\n    mydev_units = new_unrhdr(0, MAXUNITS - 1, NULL);\n}\n\n/* Allocate a unit */\nstatic int\nmydev_attach(device_t dev)\n{\n    int unit = alloc_unr(mydev_units);\n    if (unit &lt; 0)\n        return ENOMEM;\n    /* Use unit number... */\n    return 0;\n}\n\n/* Free a unit */\nstatic int\nmydev_detach(device_t dev)\n{\n    free_unr(mydev_units, sc-&gt;unit);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/utilities/#variant-symlinks-varsym","title":"Variant Symlinks (varsym)","text":"<p>The variant symlink subsystem (<code>kern_varsym.c</code>) provides variable storage and substitution for variant symlinks and general-purpose variables.</p>"},{"location":"sys/kern/utilities/#variable-scopes","title":"Variable Scopes","text":"<p>Variables exist at different scope levels:</p> Level Scope Description <code>VARSYM_PROC</code> Process Per-process variables <code>VARSYM_USER</code> User Per-user (UID) variables <code>VARSYM_PRISON</code> Jail Per-jail variables <code>VARSYM_SYS</code> System System-wide variables"},{"location":"sys/kern/utilities/#variable-substitution","title":"Variable Substitution","text":"<p>When resolving symlinks, <code>${variable}</code> patterns are substituted:</p> <pre><code>/* Called from namei during symlink resolution */\nint varsymreplace(char *cp, int linklen, int maxlen);\n</code></pre> <p>Example symlink: <code>/home/${USER}/data</code> resolves to <code>/home/john/data</code></p>"},{"location":"sys/kern/utilities/#data-structures_1","title":"Data Structures","text":"<pre><code>struct varsym {\n    int             vs_refs;        /* Reference count */\n    int             vs_namelen;     /* Name length */\n    char            *vs_name;       /* Variable name */\n    char            *vs_data;       /* Variable value */\n};\n\nstruct varsymset {\n    TAILQ_HEAD(, varsyment) vx_queue;   /* Variables in set */\n    struct lock     vx_lock;            /* Lock */\n    int             vx_setsize;         /* Total size */\n};\n</code></pre>"},{"location":"sys/kern/utilities/#system-calls","title":"System Calls","text":"<pre><code>/* Set a variable */\nint sys_varsym_set(int level, const char *name, const char *data);\n\n/* Get a variable */\nint sys_varsym_get(int mask, const char *wild, char *buf, int bufsize);\n\n/* List variables */\nint sys_varsym_list(int level, char *buf, int maxsize, int *marker);\n</code></pre>"},{"location":"sys/kern/utilities/#kernel-api","title":"Kernel API","text":"<pre><code>/* Find a variable (returns held reference) */\nvarsym_t varsymfind(int mask, const char *name, int namelen);\n\n/* Drop reference */\nvoid varsymdrop(varsym_t sym);\n\n/* Create/delete variable */\nint varsymmake(int level, const char *name, const char *data);\n\n/* Initialize variable set */\nvoid varsymset_init(struct varsymset *vss, struct varsymset *copy);\n\n/* Clean variable set */\nvoid varsymset_clean(struct varsymset *vss);\n</code></pre>"},{"location":"sys/kern/utilities/#search-order","title":"Search Order","text":"<p>Variable lookup searches scopes in order:</p> <ol> <li>Process scope (<code>VARSYM_PROC_MASK</code>)</li> <li>User scope (<code>VARSYM_USER_MASK</code>)</li> <li>Prison scope (if jailed) or System scope (<code>VARSYM_SYS_MASK</code>)</li> </ol>"},{"location":"sys/kern/utilities/#limits","title":"Limits","text":"<pre><code>#define MAXVARSYM_NAME  64      /* Maximum variable name length */\n#define MAXVARSYM_DATA  256     /* Maximum variable data length */\n#define MAXVARSYM_SET   16384   /* Maximum total size per set */\n</code></pre>"},{"location":"sys/kern/utilities/#extended-io-xio","title":"Extended I/O (XIO)","text":"<p>The XIO subsystem (<code>kern_xio.c</code>) provides a page-based buffer abstraction that can represent memory from any address space without requiring KVM mappings.</p>"},{"location":"sys/kern/utilities/#design","title":"Design","text":"<p>XIO buffers are: - Vmspace agnostic - Can represent user or kernel memory - Not KVM mapped - Low overhead for passing between threads - Page-based - Collection of held vm_page_t references</p>"},{"location":"sys/kern/utilities/#xio_t-structure","title":"xio_t Structure","text":"<pre><code>struct xio {\n    int             xio_flags;      /* State flags */\n    int             xio_bytes;      /* Valid bytes */\n    int             xio_error;      /* Error code */\n    int             xio_offset;     /* Offset in first page */\n    int             xio_npages;     /* Number of pages */\n    vm_page_t       *xio_pages;     /* Page array */\n    vm_page_t       xio_internal_pages[XIO_INTERNAL_PAGES];\n};\n\n/* Flags */\n#define XIOF_WRITE      0x0001      /* Pages may be modified */\n</code></pre>"},{"location":"sys/kern/utilities/#initialization","title":"Initialization","text":"<pre><code>/* Initialize empty XIO */\nvoid xio_init(xio_t xio);\n\n/* Initialize from kernel buffer */\nint xio_init_kbuf(xio_t xio, void *kbase, size_t kbytes);\n\n/* Initialize from page array */\nint xio_init_pages(xio_t xio, struct vm_page **mbase,\n                   int npages, int xflags);\n</code></pre>"},{"location":"sys/kern/utilities/#data-transfer","title":"Data Transfer","text":"<pre><code>/* Copy between XIO and UIO */\nint xio_uio_copy(xio_t xio, int uoffset, struct uio *uio, size_t *sizep);\n\n/* XIO to userspace */\nint xio_copy_xtou(xio_t xio, int uoffset, void *uptr, int bytes);\n\n/* Userspace to XIO */\nint xio_copy_utox(xio_t xio, int uoffset, const void *uptr, int bytes);\n\n/* XIO to kernel */\nint xio_copy_xtok(xio_t xio, int uoffset, void *kptr, int bytes);\n\n/* Kernel to XIO */\nint xio_copy_ktox(xio_t xio, int uoffset, const void *kptr, int bytes);\n</code></pre>"},{"location":"sys/kern/utilities/#cleanup","title":"Cleanup","text":"<pre><code>/* Release XIO resources (unholds pages) */\nvoid xio_release(xio_t xio);\n</code></pre>"},{"location":"sys/kern/utilities/#usage-example_2","title":"Usage Example","text":"<pre><code>/* Create XIO from kernel buffer */\nxio_t xio;\nchar kbuf[4096];\n\nxio_init(&amp;xio);\nif (xio_init_kbuf(&amp;xio, kbuf, sizeof(kbuf)) == 0) {\n    /* Pass xio to another thread/function */\n    do_io_with_xio(&amp;xio);\n\n    /* Copy data back to userspace */\n    xio_copy_xtou(&amp;xio, 0, user_ptr, xio.xio_bytes);\n\n    xio_release(&amp;xio);\n}\n</code></pre>"},{"location":"sys/kern/utilities/#additional-utility-files","title":"Additional Utility Files","text":""},{"location":"sys/kern/utilities/#kernel-printf-subr_prfc","title":"Kernel Printf (subr_prf.c)","text":"<p>Implements kernel print functions:</p> <pre><code>int kprintf(const char *fmt, ...);      /* Kernel printf */\nint ksnprintf(char *buf, size_t size, const char *fmt, ...);\nint kvprintf(const char *fmt, __va_list ap);\nvoid log(int level, const char *fmt, ...);  /* Syslog logging */\n</code></pre>"},{"location":"sys/kern/utilities/#kernel-log-subr_logc","title":"Kernel Log (subr_log.c)","text":"<p>Provides the <code>/dev/klog</code> interface for syslogd:</p> <ul> <li>Kernel message buffer management</li> <li>Log message priorities</li> <li>Poll/select support for log device</li> </ul>"},{"location":"sys/kern/utilities/#profiling-subr_profc","title":"Profiling (subr_prof.c)","text":"<p>Kernel profiling support:</p> <pre><code>void addupc_intr(struct proc *p, u_long pc, u_int ticks);\nvoid addupc_task(struct proc *p, u_long pc, u_int ticks);\n</code></pre>"},{"location":"sys/kern/utilities/#power-management-subr_powerc","title":"Power Management (subr_power.c)","text":"<p>Basic power management hooks:</p> <pre><code>int power_pm_get_type(void);\nvoid power_pm_suspend(int type);\nvoid power_pm_resume(void);\n</code></pre>"},{"location":"sys/kern/utilities/#scanf-subr_scanfc","title":"Scanf (subr_scanf.c)","text":"<p>Kernel implementations of scanf-like parsing:</p> <pre><code>int ksscanf(const char *buf, const char *fmt, ...);\nint kvsscanf(const char *buf, const char *fmt, __va_list ap);\n</code></pre>"},{"location":"sys/kern/utilities/#uuid-kern_uuidc","title":"UUID (kern_uuid.c)","text":"<p>UUID generation and manipulation:</p> <pre><code>void kern_uuidgen(struct uuid *store, int count);\nint snprintf_uuid(char *buf, size_t sz, struct uuid *uuid);\nint parse_uuid(const char *str, struct uuid *uuid);\nint uuidcmp(struct uuid *a, struct uuid *b);\n</code></pre>"},{"location":"sys/kern/utilities/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/kern/utilities/#cpu-topology_1","title":"CPU Topology","text":"<ul> <li>AMD Compute Units: Special handling for AMD's module architecture</li> <li>NUMA Support: Memory-per-node tracking for scheduler optimization</li> <li>Dynamic Detection: Topology built at boot from APIC information</li> </ul>"},{"location":"sys/kern/utilities/#variant-symlinks","title":"Variant Symlinks","text":"<ul> <li>Hierarchical Scopes: Process \u2192 User \u2192 Jail \u2192 System lookup order</li> <li>Jail Integration: Per-jail variable sets</li> <li>Dynamic Substitution: Variables resolved at symlink traversal time</li> </ul>"},{"location":"sys/kern/utilities/#xio-buffers","title":"XIO Buffers","text":"<ul> <li>Zero-Copy Paths: Designed for efficient I/O without unnecessary copies</li> <li>LWBUF Integration: Uses lightweight buffer mapping for page access</li> <li>Page Hold Semantics: Pages held (not wired) for efficiency</li> </ul>"},{"location":"sys/kern/utilities/#see-also","title":"See Also","text":"<ul> <li>LWKT Threading - Thread and CPU management</li> <li>Synchronization - Locking primitives</li> <li>Shutdown &amp; Panic - Event handler usage</li> <li>Sysctl Framework - Sysctl interface implementation</li> </ul>"},{"location":"sys/kern/ipc/mbufs/","title":"Mbuf Memory Buffers","text":"<p>The mbuf (memory buffer) subsystem provides efficient, flexible memory management for network data. Mbufs are the fundamental unit of memory used throughout the DragonFly BSD networking stack for storing packet data, protocol headers, and socket buffers.</p> <p>Source files:</p> <ul> <li><code>sys/kern/uipc_mbuf.c</code> - Core mbuf allocation and manipulation</li> <li><code>sys/kern/uipc_mbuf2.c</code> - Extended mbuf operations and packet tags</li> <li><code>sys/sys/mbuf.h</code> - Structure definitions and macros</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#design-overview","title":"Design Overview","text":"<p>The mbuf system is designed around several key principles:</p> <ol> <li>Fixed-size allocation - Mbufs are a single size (<code>MSIZE</code>), reducing fragmentation</li> <li>Cluster attachment - Large data uses external clusters rather than mbuf chains</li> <li>Per-CPU caching - Object caches provide lock-free allocation on each CPU</li> <li>Reference counting - Clusters can be shared across multiple mbufs</li> <li>Zero-copy optimization - Data can be shared without copying when safe</li> </ol>"},{"location":"sys/kern/ipc/mbufs/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":"<ul> <li>Per-CPU statistics: <code>mbstat[SMP_MAXCPU]</code> and <code>mbtypes[SMP_MAXCPU]</code> arrays with <code>__cachealign</code> for cache-line isolation</li> <li>Object cache integration: Uses DragonFly's <code>objcache(9)</code> for efficient per-CPU allocation</li> <li>Message-passing support: Embedded <code>netmsg</code> structures in mbuf headers for LWKT message passing</li> <li>Jumbo cluster support: Native support for jumbo frames via <code>MJUMPAGESIZE</code> clusters</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/mbufs/#struct-mbuf","title":"struct mbuf","text":"<p>The core mbuf structure (<code>sys/mbuf.h:206</code>):</p> <pre><code>struct mbuf {\n    struct m_hdr m_hdr;\n    union {\n        struct {\n            struct pkthdr MH_pkthdr;    /* M_PKTHDR set */\n            union {\n                struct m_ext MH_ext;    /* M_EXT set */\n                char MH_databuf[MHLEN];\n            } MH_dat;\n        } MH;\n        char M_databuf[MLEN];           /* !M_PKTHDR, !M_EXT */\n    } M_dat;\n};\n</code></pre> <p>Convenience macros provide field access:</p> Macro Field Description <code>m_next</code> <code>m_hdr.mh_next</code> Next buffer in chain <code>m_nextpkt</code> <code>m_hdr.mh_nextpkt</code> Next chain in queue <code>m_data</code> <code>m_hdr.mh_data</code> Pointer to data <code>m_len</code> <code>m_hdr.mh_len</code> Data length in this mbuf <code>m_flags</code> <code>m_hdr.mh_flags</code> Flags (M_EXT, M_PKTHDR, etc.) <code>m_type</code> <code>m_hdr.mh_type</code> Type of data <code>m_pkthdr</code> <code>M_dat.MH.MH_pkthdr</code> Packet header (if M_PKTHDR) <code>m_ext</code> <code>M_dat.MH.MH_dat.MH_ext</code> External storage (if M_EXT)"},{"location":"sys/kern/ipc/mbufs/#struct-m_hdr","title":"struct m_hdr","text":"<p>The mbuf header (<code>sys/mbuf.h:79</code>):</p> <pre><code>struct m_hdr {\n    struct mbuf *mh_next;       /* next buffer in chain */\n    union {\n        struct mbuf *mh_nextpkt;\n        STAILQ_ENTRY(mbuf) mh_stailqpkt;\n    };\n    caddr_t mh_data;            /* location of data */\n    int mh_len;                 /* amount of data */\n    int mh_flags;               /* flags */\n    short mh_type;              /* type of data */\n    short mh_pad;\n    union {\n        struct netmsg_packet mhm_pkt;   /* hardware-&gt;proto msg */\n        struct netmsg_pru_send mhm_snd; /* userspace-&gt;proto msg */\n        struct netmsg_inarp mhm_arp;    /* arpinput msg */\n        struct netmsg_ctlinput mhm_ctl; /* ctlinput msg */\n        struct netmsg_genpkt mhm_gen;   /* generic pkt msg */\n        struct netmsg_forward mhm_fwd;  /* forwarding msg */\n    } mh_msgu;\n};\n</code></pre> <p>The embedded <code>netmsg</code> union enables efficient LWKT message passing without separate allocation.</p>"},{"location":"sys/kern/ipc/mbufs/#struct-pkthdr","title":"struct pkthdr","text":"<p>Packet header for first mbuf in chain (<code>sys/mbuf.h:153</code>):</p> <pre><code>struct pkthdr {\n    struct ifnet *rcvif;        /* receive interface */\n    struct packet_tags tags;    /* list of packet tags */\n    void *header;               /* pointer to packet header */\n    int len;                    /* total packet length */\n    int csum_flags;             /* checksum flags */\n    int csum_data;              /* checksum data */\n    uint16_t csum_iphlen;       /* IP header length */\n    uint8_t csum_thlen;         /* TCP/UDP header length */\n    uint8_t csum_lhlen;         /* link header length */\n    uint16_t tso_segsz;         /* TSO segment size */\n    uint16_t ether_vlantag;     /* VLAN tag */\n    uint16_t hash;              /* packet hash */\n    /* ... additional fields ... */\n    struct pkthdr_pf pf;        /* PF state */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#struct-m_ext","title":"struct m_ext","text":"<p>External storage descriptor (<code>sys/mbuf.h:194</code>):</p> <pre><code>struct m_ext {\n    caddr_t ext_buf;            /* start of buffer */\n    void (*ext_free)(void *);   /* free function */\n    u_int ext_size;             /* size of buffer */\n    void (*ext_ref)(void *);    /* reference function */\n    void *ext_arg;              /* argument for callbacks */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#struct-mbcluster","title":"struct mbcluster","text":"<p>Cluster metadata for reference counting (<code>uipc_mbuf.c:101</code>):</p> <pre><code>struct mbcluster {\n    int32_t mcl_refs;           /* reference count */\n    void *mcl_data;             /* pointer to cluster data */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#struct-mbstat","title":"struct mbstat","text":"<p>Per-CPU statistics (<code>sys/mbuf.h:342</code>):</p> <pre><code>struct mbstat {\n    u_long m_mbufs;         /* mbufs obtained */\n    u_long m_clusters;      /* clusters obtained */\n    u_long m_jclusters;     /* jumbo clusters obtained */\n    u_long m_clfree;        /* free clusters */\n    u_long m_drops;         /* allocation failures */\n    u_long m_wait;          /* times waited for space */\n    u_long m_drain;         /* times drained protocols */\n    u_long m_mcfail;        /* m_copym failures */\n    u_long m_mpfail;        /* m_pullup failures */\n    /* ... size constants ... */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#memory-layout","title":"Memory Layout","text":""},{"location":"sys/kern/ipc/mbufs/#size-constants","title":"Size Constants","text":"Constant Description <code>MSIZE</code> Total mbuf size (256 bytes typical) <code>MLEN</code> Data area in plain mbuf (<code>MSIZE - sizeof(m_hdr)</code>) <code>MHLEN</code> Data area with packet header (<code>MLEN - sizeof(pkthdr)</code>) <code>MCLBYTES</code> Standard cluster size (2048 bytes) <code>MJUMPAGESIZE</code> Jumbo cluster size (PAGE_SIZE) <code>MINCLSIZE</code> Minimum size to use cluster (<code>MHLEN + 1</code>)"},{"location":"sys/kern/ipc/mbufs/#mbuf-variants","title":"Mbuf Variants","text":"<pre><code>block-beta\n    columns 3\n    block:plain[\"Plain mbuf (no M_PKTHDR, no M_EXT)\"]:1\n        columns 1\n        mh1[\"m_hdr\"]\n        md1[\"m_dat[MLEN]\u2190 m_data points here\"]\n    end\n    block:pkthdr[\"Packet header mbuf (M_PKTHDR, no M_EXT)\"]:1\n        columns 1\n        mh2[\"m_hdr\"]\n        ph2[\"pkthdr\"]\n        mpd2[\"m_pktdat[MHLEN]\u2190 m_data points here\"]\n    end\n    block:ext[\"Mbuf with cluster (M_EXT)\"]:1\n        columns 1\n        mh3[\"m_hdr\"]\n        ph3[\"pkthdr (optional)\"]\n        me3[\"m_ext\"]\n    end\n    space:3\n    block:cluster[\"External Cluster\"]:1\n        columns 1\n        cd[\"cluster data(MCLBYTES)\"]\n    end\n    me3 --&gt; cd\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#mbuf-flags","title":"Mbuf Flags","text":""},{"location":"sys/kern/ipc/mbufs/#core-flags","title":"Core Flags","text":"Flag Value Description <code>M_EXT</code> 0x0001 Has external storage <code>M_PKTHDR</code> 0x0002 Start of record/packet <code>M_EOR</code> 0x0004 End of record <code>M_CLCACHE</code> 0x2000 Allocated from cluster cache <code>M_EXT_CLUSTER</code> 0x4000 Standard cluster (not custom) <code>M_PHCACHE</code> 0x8000 Allocated from packet header cache"},{"location":"sys/kern/ipc/mbufs/#packet-flags","title":"Packet Flags","text":"Flag Value Description <code>M_BCAST</code> 0x0100 Broadcast packet <code>M_MCAST</code> 0x0200 Multicast packet <code>M_FRAG</code> 0x0400 Fragment of larger packet <code>M_FIRSTFRAG</code> 0x0800 First fragment <code>M_LASTFRAG</code> 0x1000 Last fragment <code>M_VLANTAG</code> 0x20000 VLAN tag valid <code>M_HASH</code> 0x100000 Hash field valid"},{"location":"sys/kern/ipc/mbufs/#mbuf-types","title":"Mbuf Types","text":"Type Value Description <code>MT_FREE</code> 0 On free list <code>MT_DATA</code> 1 Dynamic data <code>MT_HEADER</code> 2 Packet header <code>MT_SONAME</code> 3 Socket name <code>MT_CONTROL</code> 5 Control message <code>MT_OOBDATA</code> 6 Out-of-band data"},{"location":"sys/kern/ipc/mbufs/#object-caches","title":"Object Caches","text":"<p>The mbuf subsystem uses eight specialized object caches for efficient allocation (<code>uipc_mbuf.c:91-98</code>):</p> Cache Contents <code>mbuf_cache</code> Plain mbufs <code>mbufphdr_cache</code> Mbufs with packet header <code>mclmeta_cache</code> Standard cluster metadata <code>mjclmeta_cache</code> Jumbo cluster metadata <code>mbufcluster_cache</code> Mbuf + standard cluster <code>mbufphdrcluster_cache</code> Mbuf + pkthdr + standard cluster <code>mbufjcluster_cache</code> Mbuf + jumbo cluster <code>mbufphdrjcluster_cache</code> Mbuf + pkthdr + jumbo cluster <p>Each cache is configured with constructor/destructor functions and uses per-CPU magazines for lock-free fast-path allocation.</p>"},{"location":"sys/kern/ipc/mbufs/#cache-initialization","title":"Cache Initialization","text":"<p>Caches are initialized in <code>mbinit()</code> (<code>uipc_mbuf.c:253</code>):</p> <pre><code>SYSINIT(mbuf, SI_BOOT2_MACHDEP, SI_ORDER_FIRST, mbinit, NULL);\n</code></pre> <p>The <code>mbinit_cluster()</code> function creates cluster caches using <code>SYSINIT</code> at <code>SI_ORDER_ANY</code> to ensure VM is ready.</p>"},{"location":"sys/kern/ipc/mbufs/#allocation-functions","title":"Allocation Functions","text":""},{"location":"sys/kern/ipc/mbufs/#basic-allocation","title":"Basic Allocation","text":"Function Description <code>m_get(how, type)</code> Allocate plain mbuf <code>m_gethdr(how, type)</code> Allocate mbuf with packet header <code>m_getcl(how, type, flags)</code> Allocate mbuf with standard cluster <code>m_getjcl(how, type, flags, size)</code> Allocate mbuf with jumbo cluster <code>m_getl(len, how, type, flags, psize)</code> Allocate appropriate mbuf for length <code>m_getc(len, how, type)</code> Allocate mbuf chain for length <p>The <code>how</code> parameter is either <code>M_WAITOK</code> (can block) or <code>M_NOWAIT</code> (fails immediately if unavailable).</p>"},{"location":"sys/kern/ipc/mbufs/#m_get-m_gethdr","title":"m_get / m_gethdr","text":"<p>Basic mbuf allocation (<code>uipc_mbuf.c:423-457</code>):</p> <pre><code>struct mbuf *\nm_get(int how, int type)\n{\n    struct mbuf *m;\n    int ntries = 0;\n\nretryonce:\n    m = objcache_get(mbuf_cache, MB_OCFLAG(how));\n    if (m == NULL) {\n        if (how == M_WAITOK &amp;&amp; ntries++ == 0) {\n            m_reclaim();\n            goto retryonce;\n        }\n        ++mbstat[mycpu-&gt;gd_cpuid].m_drops;\n        return (NULL);\n    }\n    /* ... initialization ... */\n    return (m);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_getcl","title":"m_getcl","text":"<p>Combined mbuf+cluster allocation (<code>uipc_mbuf.c:517-545</code>):</p> <pre><code>struct mbuf *\nm_getcl(int how, short type, int flags)\n{\n    struct mbuf *m;\n    int ntries = 0;\n\nretryonce:\n    if (flags &amp; M_PKTHDR)\n        m = objcache_get(mbufphdrcluster_cache, MB_OCFLAG(how));\n    else\n        m = objcache_get(mbufcluster_cache, MB_OCFLAG(how));\n    /* ... */\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_getl-inline","title":"m_getl (Inline)","text":"<p>Smart allocation based on length (<code>sys/mbuf.h:583-601</code>):</p> <pre><code>static __inline struct mbuf *\nm_getl(int len, int how, int type, int flags, int *psize)\n{\n    struct mbuf *m;\n    int size;\n\n    if (len &gt;= MINCLSIZE) {\n        m = m_getcl(how, type, flags);\n        size = MCLBYTES;\n    } else if (flags &amp; M_PKTHDR) {\n        m = m_gethdr(how, type);\n        size = MHLEN;\n    } else {\n        m = m_get(how, type);\n        size = MLEN;\n    }\n    if (psize != NULL)\n        *psize = size;\n    return m;\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_getc","title":"m_getc","text":"<p>Allocate chain for specified length (<code>uipc_mbuf.c:1154-1201</code>):</p> <pre><code>struct mbuf *\nm_getc(int len, int how, int type)\n{\n    struct mbuf *nfirst = NULL, *n;\n    int nsize;\n\n    while (len &gt; 0) {\n        n = m_getl(len, how, type, nfirst == NULL ? M_PKTHDR : 0, &amp;nsize);\n        if (n == NULL)\n            goto failed;\n        if (nfirst == NULL)\n            nfirst = n;\n        /* ... chain building ... */\n        len -= nsize;\n    }\n    return (nfirst);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#deallocation-functions","title":"Deallocation Functions","text":""},{"location":"sys/kern/ipc/mbufs/#m_free","title":"m_free","text":"<p>Free single mbuf (<code>uipc_mbuf.c:1307-1456</code>):</p> <pre><code>struct mbuf *\nm_free(struct mbuf *m)\n{\n    struct mbuf *n;\n    struct globaldata *gd = mycpu;\n\n    KASSERT(m-&gt;m_type != MT_FREE, (\"freeing free mbuf %p\", m));\n    --mbtypes[gd-&gt;gd_cpuid].stats[m-&gt;m_type];\n    n = m-&gt;m_next;\n\n    /* Clean up and return to appropriate cache */\n    switch (m-&gt;m_flags &amp; (M_CLCACHE | M_EXT | M_EXT_CLUSTER)) {\n    case M_CLCACHE | M_EXT | M_EXT_CLUSTER:\n        /* Return to combined mbuf+cluster cache if not shared */\n        if (m_sharecount(m) == 1) {\n            m-&gt;m_data = m-&gt;m_ext.ext_buf;\n            objcache_put(mbufcluster_cache, m);\n        } else {\n            /* Cluster shared, must disconnect */\n            m-&gt;m_ext.ext_free(m-&gt;m_ext.ext_arg);\n            objcache_dtor(mbufcluster_cache, m);\n        }\n        break;\n    /* ... other cases ... */\n    }\n    return (n);\n}\n</code></pre> <p>Key handling for shared clusters: when <code>m_sharecount(m) &gt; 1</code>, the mbuf cannot be returned to the combined cache and must be destroyed.</p>"},{"location":"sys/kern/ipc/mbufs/#m_freem","title":"m_freem","text":"<p>Free entire mbuf chain (<code>uipc_mbuf.c:1469-1474</code>):</p> <pre><code>void\nm_freem(struct mbuf *m)\n{\n    while (m)\n        m = m_free(m);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#cluster-reference-counting","title":"Cluster Reference Counting","text":"<p>Clusters use atomic reference counting for safe sharing (<code>uipc_mbuf.c:1263-1296</code>):</p>"},{"location":"sys/kern/ipc/mbufs/#m_mclref","title":"m_mclref","text":"<pre><code>static void\nm_mclref(void *arg)\n{\n    struct mbcluster *mcl = arg;\n    atomic_add_int(&amp;mcl-&gt;mcl_refs, 1);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_mclfree","title":"m_mclfree","text":"<pre><code>static void\nm_mclfree(void *arg)\n{\n    struct mbcluster *mcl = arg;\n    if (atomic_fetchadd_int(&amp;mcl-&gt;mcl_refs, -1) == 1) {\n        --mbstat[mycpu-&gt;gd_cpuid].m_clusters;\n        objcache_put(mclmeta_cache, mcl);\n    }\n}\n</code></pre> <p>The <code>atomic_fetchadd_int()</code> returns the previous value, so a return of 1 means the reference count is now 0.</p>"},{"location":"sys/kern/ipc/mbufs/#m_sharecount","title":"m_sharecount","text":"<p>Check if cluster is shared (<code>uipc_mbuf.c:1040-1049</code>):</p> <pre><code>int\nm_sharecount(struct mbuf *m)\n{\n    if (m-&gt;m_flags &amp; (M_EXT | M_EXT_CLUSTER)) {\n        struct mbcluster *mcl = m-&gt;m_ext.ext_arg;\n        return mcl-&gt;mcl_refs;\n    }\n    return 1;  /* Not external, single reference */\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#chain-manipulation-functions","title":"Chain Manipulation Functions","text":""},{"location":"sys/kern/ipc/mbufs/#m_copym","title":"m_copym","text":"<p>Create read-only copy of mbuf chain (<code>uipc_mbuf.c:1530-1603</code>):</p> <pre><code>struct mbuf *\nm_copym(const struct mbuf *m, int off0, int len, int wait)\n</code></pre> <ul> <li>Copies from offset <code>off0</code> for <code>len</code> bytes (or <code>M_COPYALL</code>)</li> <li>Clusters are shared (reference count incremented), not copied</li> <li>Result is read-only due to shared clusters</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_copypacket","title":"m_copypacket","text":"<p>Optimized full packet copy (<code>uipc_mbuf.c:1614-1665</code>):</p> <pre><code>struct mbuf *\nm_copypacket(struct mbuf *m, int how)\n</code></pre> <ul> <li>Equivalent to <code>m_copym(m, 0, M_COPYALL, how)</code></li> <li>Preserves alignment of first mbuf</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_dup","title":"m_dup","text":"<p>Create writable copy (<code>uipc_mbuf.c:1704-1759</code>):</p> <pre><code>struct mbuf *\nm_dup(struct mbuf *m, int how)\n</code></pre> <ul> <li>Copies all data (clusters are copied, not shared)</li> <li>Result is fully writable</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_cat","title":"m_cat","text":"<p>Concatenate chains (<code>uipc_mbuf.c:1841-1857</code>):</p> <pre><code>void\nm_cat(struct mbuf *m, struct mbuf *n)\n{\n    m = m_last(m);\n    while (n) {\n        if (m-&gt;m_flags &amp; M_EXT ||\n            m-&gt;m_data + m-&gt;m_len + n-&gt;m_len &gt;= &amp;m-&gt;m_dat[MLEN]) {\n            /* Just link chains */\n            m-&gt;m_next = n;\n            return;\n        }\n        /* Copy data into trailing space */\n        bcopy(mtod(n, caddr_t), mtod(m, caddr_t) + m-&gt;m_len, n-&gt;m_len);\n        m-&gt;m_len += n-&gt;m_len;\n        n = m_free(n);\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_adj","title":"m_adj","text":"<p>Trim data from head or tail (<code>uipc_mbuf.c:1859-1928</code>):</p> <pre><code>void\nm_adj(struct mbuf *mp, int req_len)\n</code></pre> <ul> <li>Positive <code>req_len</code>: trim from head</li> <li>Negative <code>req_len</code>: trim from tail</li> <li>Updates <code>m_pkthdr.len</code> if present</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_pullup","title":"m_pullup","text":"<p>Make initial bytes contiguous (<code>uipc_mbuf.c:2103-2159</code>):</p> <pre><code>struct mbuf *\nm_pullup(struct mbuf *n, int len)\n</code></pre> <ul> <li>Ensures first <code>len</code> bytes are in first mbuf's data area</li> <li>Required for protocol header access via casting</li> <li>Frees original and returns NULL on failure</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_pulldown","title":"m_pulldown","text":"<p>Make arbitrary region contiguous (<code>uipc_mbuf2.c:89-234</code>):</p> <pre><code>struct mbuf *\nm_pulldown(struct mbuf *m, int off, int len, int *offp)\n</code></pre> <ul> <li>Makes bytes <code>[off, off+len)</code> contiguous</li> <li>More flexible than <code>m_pullup()</code></li> <li>Returns mbuf containing the region; <code>*offp</code> is offset within that mbuf</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_split","title":"m_split","text":"<p>Partition chain (<code>uipc_mbuf.c:2171-2229</code>):</p> <pre><code>struct mbuf *\nm_split(struct mbuf *m0, int len0, int wait)\n</code></pre> <ul> <li>Returns tail of chain starting at offset <code>len0</code></li> <li>Original chain is truncated to <code>len0</code> bytes</li> <li>May share clusters (result may be read-only)</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_copydata","title":"m_copydata","text":"<p>Copy to linear buffer (<code>uipc_mbuf.c:1671-1697</code>):</p> <pre><code>void\nm_copydata(const struct mbuf *m, int off, int len, void *cp)\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_copyback","title":"m_copyback","text":"<p>Copy from linear buffer (<code>uipc_mbuf.c:2408-2416</code>):</p> <pre><code>void\nm_copyback(struct mbuf *m0, int off, int len, const void *cp)\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_defrag","title":"m_defrag","text":"<p>Defragment chain (<code>uipc_mbuf.c:2591-2659</code>):</p> <pre><code>struct mbuf *\nm_defrag(struct mbuf *m0, int how)\n</code></pre> <ul> <li>Creates shortest possible chain</li> <li>Useful before DMA operations requiring few scatter-gather entries</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_unshare","title":"m_unshare","text":"<p>Create writable chain (<code>uipc_mbuf.c:1958-2093</code>):</p> <pre><code>struct mbuf *\nm_unshare(struct mbuf *m0, int how)\n</code></pre> <ul> <li>Replaces shared clusters with private copies</li> <li>Compacts chain where possible</li> <li>Used before encryption/compression that modifies data in place</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#device-interface-functions","title":"Device Interface Functions","text":""},{"location":"sys/kern/ipc/mbufs/#m_devget","title":"m_devget","text":"<p>Copy from device memory to mbuf chain (<code>uipc_mbuf.c:2235-2270</code>):</p> <pre><code>struct mbuf *\nm_devget(void *buf, int len, int offset, struct ifnet *ifp)\n</code></pre> <ul> <li>Creates chain from linear device buffer</li> <li>Sets <code>m_pkthdr.rcvif</code> to receiving interface</li> <li>Leaves room for <code>max_linkhdr</code> in first mbuf</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_devpad","title":"m_devpad","text":"<p>Pad packet to minimum length (<code>uipc_mbuf.c:2275-2318</code>):</p> <pre><code>int\nm_devpad(struct mbuf *m, int padto)\n</code></pre> <ul> <li>Pads packet to <code>padto</code> bytes</li> <li>Required for Ethernet minimum frame size</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#packet-tags","title":"Packet Tags","text":"<p>Packet tags attach auxiliary information to mbufs without modifying the mbuf structure.</p>"},{"location":"sys/kern/ipc/mbufs/#struct-m_tag","title":"struct m_tag","text":"<p>Tag structure (<code>sys/mbuf.h:138</code>):</p> <pre><code>struct m_tag {\n    SLIST_ENTRY(m_tag) m_tag_link;  /* List linkage */\n    uint16_t m_tag_id;              /* Tag ID */\n    uint16_t m_tag_len;             /* Data length */\n    uint32_t m_tag_cookie;          /* ABI/Module ID */\n};\n</code></pre> <p>Data follows immediately after the structure.</p>"},{"location":"sys/kern/ipc/mbufs/#tag-functions","title":"Tag Functions","text":"Function Description <code>m_tag_alloc(cookie, type, len, how)</code> Allocate tag with data <code>m_tag_free(t)</code> Free tag <code>m_tag_prepend(m, t)</code> Add tag to mbuf <code>m_tag_unlink(m, t)</code> Remove tag from mbuf <code>m_tag_delete(m, t)</code> Remove and free tag <code>m_tag_delete_chain(m)</code> Free all tags <code>m_tag_locate(m, cookie, type, t)</code> Find tag by cookie/type <code>m_tag_copy(t, how)</code> Copy single tag <code>m_tag_copy_chain(to, from, how)</code> Copy all tags"},{"location":"sys/kern/ipc/mbufs/#tag-usage-example","title":"Tag Usage Example","text":"<pre><code>struct m_tag *tag;\n\n/* Allocate and attach */\ntag = m_tag_alloc(MTAG_ABI_COMPAT, type, sizeof(data), M_NOWAIT);\nif (tag != NULL) {\n    bcopy(&amp;data, tag + 1, sizeof(data));\n    m_tag_prepend(m, tag);\n}\n\n/* Find and use */\ntag = m_tag_locate(m, MTAG_ABI_COMPAT, type, NULL);\nif (tag != NULL) {\n    struct data *dp = (struct data *)(tag + 1);\n    /* use dp */\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#utility-macros","title":"Utility Macros","text":""},{"location":"sys/kern/ipc/mbufs/#data-access","title":"Data Access","text":"<pre><code>mtod(m, t)          /* Cast m-&gt;m_data to type t */\nmtodoff(m, t, off)  /* Cast m-&gt;m_data + off to type t */\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#space-calculation","title":"Space Calculation","text":"<pre><code>M_LEADINGSPACE(m)   /* Bytes available before m_data */\nM_TRAILINGSPACE(m)  /* Bytes available after data */\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#writability-check","title":"Writability Check","text":"<pre><code>M_WRITABLE(m)       /* True if mbuf data is writable */\nM_EXT_WRITABLE(m)   /* True if cluster is writable (sharecount == 1) */\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#alignment","title":"Alignment","text":"<pre><code>M_ALIGN(m, len)     /* Align for 'len' bytes at end of plain mbuf */\nMH_ALIGN(m, len)    /* Align for 'len' bytes at end of pkthdr mbuf */\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#prepend","title":"Prepend","text":"<pre><code>M_PREPEND(m, plen, how)  /* Prepend 'plen' bytes to mbuf */\n</code></pre> <p>Adjusts <code>m_data</code> and <code>m_len</code>; allocates new mbuf if insufficient leading space.</p>"},{"location":"sys/kern/ipc/mbufs/#statistics-and-debugging","title":"Statistics and Debugging","text":""},{"location":"sys/kern/ipc/mbufs/#per-cpu-statistics","title":"Per-CPU Statistics","text":"<pre><code>extern struct mbstat mbstat[SMP_MAXCPU] __cachealign;\nextern struct mbtypes_stat mbtypes[SMP_MAXCPU] __cachealign;\n</code></pre> <p>Statistics are updated without locking using CPU-local arrays.</p>"},{"location":"sys/kern/ipc/mbufs/#debug-support","title":"Debug Support","text":"<p>When <code>MBUF_DEBUG</code> is defined:</p> <ul> <li><code>m-&gt;m_hdr.mh_lastfunc</code> tracks last function that touched mbuf</li> <li><code>mbuftrackid()</code> records operations</li> <li><code>_m_free()</code> / <code>_m_freem()</code> include caller name</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#sysctl-interface","title":"Sysctl Interface","text":"<ul> <li><code>kern.ipc.nmbclusters</code> - Maximum clusters</li> <li><code>kern.ipc.nmbufs</code> - Maximum mbufs</li> <li><code>kern.ipc.mbuf_wait</code> - Wait count for allocation</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#memory-reclamation","title":"Memory Reclamation","text":"<p>When allocation fails with <code>M_WAITOK</code>, the system attempts reclamation (<code>uipc_mbuf.c:410-421</code>):</p> <pre><code>static void\nm_reclaim(void)\n{\n    struct domain *dp;\n    struct protosw *pr;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        for (pr = dp-&gt;dom_protosw; pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n            if (pr-&gt;pr_drain)\n                (*pr-&gt;pr_drain)();\n        }\n    }\n}\n</code></pre> <p>Protocols implement <code>pr_drain</code> to release cached mbufs.</p>"},{"location":"sys/kern/ipc/mbufs/#see-also","title":"See Also","text":"<ul> <li>Socket Core - Socket buffer management using mbufs</li> <li>VFS Buffer Cache - Similar caching concepts</li> <li>Memory Management - Kernel memory subsystem</li> </ul>"},{"location":"sys/kern/ipc/mqueue/","title":"POSIX Message Queues","text":"<p>POSIX message queues provide named, priority-based message passing between processes. DragonFly's implementation (<code>sys/kern/sys_mqueue.c</code>) follows IEEE Std 1003.1-2001 and was derived from NetBSD.</p>"},{"location":"sys/kern/ipc/mqueue/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/mqueue/#message-queue","title":"Message Queue","text":"<p>The <code>struct mqueue</code> (<code>sys/sys/mqueue.h:77</code>) represents a message queue:</p> <pre><code>struct mqueue {\n    char            mq_name[MQ_NAMELEN];\n    struct lock     mq_mtx;\n    int             mq_send_cv;     /* sleep channel for senders */\n    int             mq_recv_cv;     /* sleep channel for receivers */\n    struct mq_attr  mq_attrib;\n    /* Notification */\n    struct kqinfo   mq_rkq;         /* kqueue for read */\n    struct kqinfo   mq_wkq;         /* kqueue for write */\n    struct sigevent mq_sig_notify;\n    struct proc *   mq_notify_proc;\n    /* Permissions */\n    mode_t          mq_mode;\n    uid_t           mq_euid;\n    gid_t           mq_egid;\n    /* Message storage */\n    u_int           mq_refcnt;\n    TAILQ_HEAD(, mq_msg) mq_head[1 + MQ_PQSIZE];\n    uint32_t        mq_bitmap;\n    LIST_ENTRY(mqueue) mq_list;\n    struct timespec mq_atime;\n    struct timespec mq_mtime;\n    struct timespec mq_btime;\n};\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#message-attributes","title":"Message Attributes","text":"<p>The <code>struct mq_attr</code> (<code>sys/sys/mqueue.h:40</code>) holds queue configuration:</p> <pre><code>struct mq_attr {\n    long    mq_flags;       /* O_NONBLOCK, MQ_UNLINK, MQ_RECEIVE */\n    long    mq_maxmsg;      /* maximum messages in queue */\n    long    mq_msgsize;     /* maximum message size */\n    long    mq_curmsgs;     /* current message count */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#message-structure","title":"Message Structure","text":"<p>Individual messages use <code>struct mq_msg</code> (<code>sys/sys/mqueue.h:111</code>):</p> <pre><code>struct mq_msg {\n    TAILQ_ENTRY(mq_msg) msg_queue;\n    size_t              msg_len;\n    u_int               msg_prio;\n    int8_t              msg_ptr[1];     /* variable-length data */\n};\n</code></pre> <p>The <code>msg_ptr</code> field uses the struct hack pattern - actual allocation includes space for the message body.</p>"},{"location":"sys/kern/ipc/mqueue/#priority-queue-implementation","title":"Priority Queue Implementation","text":""},{"location":"sys/kern/ipc/mqueue/#constant-time-insertion","title":"Constant-Time Insertion","text":"<p>Messages are stored in priority queues for O(1) insertion. The queue uses 32 priority levels (<code>MQ_PQSIZE = 32</code>) plus one reserved queue:</p> <pre><code>#define MQ_PQSIZE   32      /* number of priority queues */\n#define MQ_PQRESQ   0       /* reserved queue index */\n</code></pre> <p>The <code>mq_head</code> array contains <code>MQ_PQSIZE + 1</code> TAILQ heads. Index 0 is reserved for overflow when <code>mq_prio_max</code> exceeds 32.</p>"},{"location":"sys/kern/ipc/mqueue/#bitmap-tracking","title":"Bitmap Tracking","text":"<p>A 32-bit bitmap (<code>mq_bitmap</code>) tracks which priority queues contain messages:</p> <pre><code>/* Inserting message at priority msg_prio */\nu_int idx = MQ_PQSIZE - msg_prio;\nTAILQ_INSERT_TAIL(&amp;mq-&gt;mq_head[idx], msg, msg_queue);\nmq-&gt;mq_bitmap |= (1 &lt;&lt; --idx);\n</code></pre> <p>The priority-to-index mapping (<code>MQ_PQSIZE - msg_prio</code>) ensures higher priorities map to lower indices, so <code>ffs()</code> (find first set) returns the highest priority queue.</p>"},{"location":"sys/kern/ipc/mqueue/#receiving-highest-priority","title":"Receiving Highest Priority","text":"<p><code>mq_receive1()</code> uses <code>ffs()</code> on the bitmap to find the highest-priority non-empty queue (<code>sys_mqueue.c:685-699</code>):</p> <pre><code>msg = TAILQ_FIRST(&amp;mq-&gt;mq_head[MQ_PQRESQ]);\nif (__predict_true(msg == NULL)) {\n    idx = ffs(mq-&gt;mq_bitmap);\n    msg = TAILQ_FIRST(&amp;mq-&gt;mq_head[idx]);\n}\nTAILQ_REMOVE(&amp;mq-&gt;mq_head[idx], msg, msg_queue);\n\n/* Clear bit if queue now empty */\nif (__predict_true(idx) &amp;&amp; TAILQ_EMPTY(&amp;mq-&gt;mq_head[idx])) {\n    mq-&gt;mq_bitmap &amp;= ~(1 &lt;&lt; --idx);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#reserved-queue","title":"Reserved Queue","text":"<p>If <code>mq_prio_max</code> is increased beyond 32 via sysctl, <code>mqueue_linear_insert()</code> (<code>sys_mqueue.c:204-218</code>) performs linear insertion into <code>MQ_PQRESQ</code>:</p> <pre><code>static inline void\nmqueue_linear_insert(struct mqueue *mq, struct mq_msg *msg)\n{\n    struct mq_msg *mit;\n\n    TAILQ_FOREACH(mit, &amp;mq-&gt;mq_head[MQ_PQRESQ], msg_queue) {\n        if (msg-&gt;msg_prio &gt; mit-&gt;msg_prio)\n            break;\n    }\n    if (mit == NULL)\n        TAILQ_INSERT_TAIL(&amp;mq-&gt;mq_head[MQ_PQRESQ], msg, msg_queue);\n    else\n        TAILQ_INSERT_BEFORE(mit, msg, msg_queue);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/mqueue/#lock-ordering","title":"Lock Ordering","text":"<p>The implementation uses a two-level locking hierarchy (<code>sys_mqueue.c:33-42</code>):</p> <pre><code>mqlist_mtx          (global list lock)\n  -&gt; mqueue::mq_mtx (per-queue lock)\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#global-list-lock","title":"Global List Lock","text":"<p><code>mqlist_mtx</code> protects: - The global <code>mqueue_head</code> list - Per-process <code>p-&gt;p_mqueue_cnt</code> counter</p>"},{"location":"sys/kern/ipc/mqueue/#per-queue-lock","title":"Per-Queue Lock","text":"<p>Each queue's <code>mq_mtx</code> (a <code>struct lock</code> with <code>LK_CANRECURSE</code>) protects: - Queue attributes (<code>mq_attrib</code>) - Message queues (<code>mq_head[]</code>, <code>mq_bitmap</code>) - Notification state</p>"},{"location":"sys/kern/ipc/mqueue/#blocking-operations","title":"Blocking Operations","text":"<p>Senders and receivers block using <code>lksleep()</code> with the queue lock held:</p> <pre><code>/* Receiver waiting for messages */\nerror = lksleep(&amp;mq-&gt;mq_send_cv, &amp;mq-&gt;mq_mtx, PCATCH, \"mqsend\", t);\n\n/* Sender waiting for space */\nerror = lksleep(&amp;mq-&gt;mq_recv_cv, &amp;mq-&gt;mq_mtx, PCATCH, \"mqrecv\", t);\n</code></pre> <p>Wakeups use <code>wakeup_one()</code> to wake a single waiter.</p>"},{"location":"sys/kern/ipc/mqueue/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/mqueue/#mq_open","title":"mq_open()","text":"<p><code>sys_mq_open()</code> (<code>sys_mqueue.c:414-612</code>) creates or opens a message queue:</p> <ol> <li>Validates access mode flags</li> <li>Copies name from userspace (max <code>MQ_NAMELEN</code> = <code>NAME_MAX + 1</code>)</li> <li>If <code>O_CREAT</code>:</li> <li>Checks per-process limit (<code>mq_open_max</code>)</li> <li>Validates or uses default attributes</li> <li>Allocates new <code>struct mqueue</code></li> <li>Allocates file descriptor (<code>DTYPE_MQUEUE</code>)</li> <li>Looks up existing queue under <code>mqlist_mtx</code></li> <li>If found: checks permissions with <code>vaccess()</code></li> <li>If not found and <code>O_CREAT</code>: inserts new queue into global list</li> <li>Returns descriptor</li> </ol> <p>Default attributes when none specified: <pre><code>attr.mq_maxmsg = mq_def_maxmsg;     /* 32 */\nattr.mq_msgsize = MQ_DEF_MSGSIZE - sizeof(struct mq_msg);  /* ~1000 */\n</code></pre></p>"},{"location":"sys/kern/ipc/mqueue/#mq_send-mq_timedsend","title":"mq_send() / mq_timedsend()","text":"<p><code>mq_send1()</code> (<code>sys_mqueue.c:782-913</code>) sends a message:</p> <ol> <li>Validates priority (&lt; <code>mq_prio_max</code>)</li> <li>Allocates message structure with data</li> <li>Copies data from userspace</li> <li>Acquires queue via <code>mqueue_get()</code></li> <li>Validates message size against <code>mq_msgsize</code></li> <li>If queue full and blocking: sleeps on <code>mq_recv_cv</code></li> <li>Inserts message into appropriate priority queue</li> <li>If notification registered and queue was empty: signals process</li> <li>Increments <code>mq_curmsgs</code>, wakes one receiver</li> </ol>"},{"location":"sys/kern/ipc/mqueue/#mq_receive-mq_timedreceive","title":"mq_receive() / mq_timedreceive()","text":"<p><code>mq_receive1()</code> (<code>sys_mqueue.c:623-725</code>) receives a message:</p> <ol> <li>Acquires queue via <code>mqueue_get()</code></li> <li>Validates buffer size (&gt;= <code>mq_msgsize</code>)</li> <li>If queue empty and blocking: sleeps on <code>mq_send_cv</code></li> <li>Finds highest-priority message via bitmap</li> <li>Removes message from queue</li> <li>Decrements <code>mq_curmsgs</code>, wakes one sender</li> <li>Copies message data and priority to userspace</li> <li>Frees message structure</li> </ol>"},{"location":"sys/kern/ipc/mqueue/#mq_notify","title":"mq_notify()","text":"<p><code>sys_mq_notify()</code> (<code>sys_mqueue.c:956-1002</code>) registers for notification:</p> <pre><code>if (uap-&gt;notification) {\n    if (mq-&gt;mq_notify_proc == NULL) {\n        memcpy(&amp;mq-&gt;mq_sig_notify, &amp;sig, sizeof(struct sigevent));\n        mq-&gt;mq_notify_proc = curproc;\n    } else {\n        error = EBUSY;  /* already registered */\n    }\n} else {\n    mq-&gt;mq_notify_proc = NULL;  /* unregister */\n}\n</code></pre> <p>Only <code>SIGEV_SIGNAL</code> notification is fully implemented. The signal is sent via <code>ksignal()</code> when a message arrives to an empty queue.</p>"},{"location":"sys/kern/ipc/mqueue/#mq_getattr-mq_setattr","title":"mq_getattr() / mq_setattr()","text":"<p><code>sys_mq_getattr()</code> returns current attributes. <code>sys_mq_setattr()</code> only modifies <code>O_NONBLOCK</code>:</p> <pre><code>if (nonblock)\n    mq-&gt;mq_attrib.mq_flags |= O_NONBLOCK;\nelse\n    mq-&gt;mq_attrib.mq_flags &amp;= ~O_NONBLOCK;\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#mq_unlink","title":"mq_unlink()","text":"<p><code>sys_mq_unlink()</code> (<code>sys_mqueue.c:1077-1142</code>) marks a queue for deletion:</p> <ol> <li>Looks up queue by name</li> <li>Checks permissions (owner or root)</li> <li>Sets <code>MQ_UNLINK</code> flag</li> <li>Wakes all waiters</li> <li>If no references: removes from list and destroys</li> <li>Otherwise: last <code>mq_close()</code> destroys it</li> </ol>"},{"location":"sys/kern/ipc/mqueue/#mq_close","title":"mq_close()","text":"<p><code>sys_mq_close()</code> delegates to <code>sys_close()</code>. The actual cleanup happens in <code>mq_close_fop()</code> (<code>sys_mqueue.c:373-408</code>):</p> <pre><code>p-&gt;p_mqueue_cnt--;\nmq-&gt;mq_refcnt--;\n\nif (mq-&gt;mq_notify_proc == p)\n    mq-&gt;mq_notify_proc = NULL;\n\nif (mq-&gt;mq_refcnt == 0 &amp;&amp; (mq-&gt;mq_attrib.mq_flags &amp; MQ_UNLINK)) {\n    LIST_REMOVE(mq, mq_list);\n    destroy = true;\n}\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#timeout-handling","title":"Timeout Handling","text":""},{"location":"sys/kern/ipc/mqueue/#abstimeout2timo","title":"abstimeout2timo()","text":"<p>Converts absolute <code>timespec</code> to relative ticks (<code>sys_mqueue.c:240-259</code>):</p> <pre><code>int\nabstimeout2timo(struct timespec *ts, int *timo)\n{\n    error = itimespecfix(ts);\n    if (error)\n        return error;\n\n    getnanotime(&amp;tsd);\n    timespecsub(ts, &amp;tsd, ts);      /* ts = ts - now */\n\n    if (ts-&gt;tv_sec &lt; 0 || (ts-&gt;tv_sec == 0 &amp;&amp; ts-&gt;tv_nsec &lt;= 0))\n        return ETIMEDOUT;           /* already expired */\n\n    *timo = tstohz(ts);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#file-operations","title":"File Operations","text":"<p>The <code>mqops</code> structure (<code>sys_mqueue.c:97-106</code>):</p> <pre><code>static struct fileops mqops = {\n    .fo_read = badfo_readwrite,\n    .fo_write = badfo_readwrite,\n    .fo_ioctl = badfo_ioctl,\n    .fo_stat = mq_stat_fop,\n    .fo_close = mq_close_fop,\n    .fo_kqfilter = mq_kqfilter_fop,\n    .fo_shutdown = badfo_shutdown,\n    .fo_seek = badfo_seek\n};\n</code></pre> <p>Note: Direct read/write on the descriptor is not supported - use <code>mq_send()</code>/<code>mq_receive()</code>.</p>"},{"location":"sys/kern/ipc/mqueue/#kqueue-support","title":"kqueue Support","text":"<p><code>mq_kqfilter_fop()</code> supports <code>EVFILT_READ</code> and <code>EVFILT_WRITE</code>:</p> <ul> <li><code>EVFILT_READ</code>: ready when <code>mq_curmsgs &gt; 0</code></li> <li><code>EVFILT_WRITE</code>: ready when <code>mq_curmsgs &lt; mq_maxmsg</code></li> </ul>"},{"location":"sys/kern/ipc/mqueue/#flags","title":"Flags","text":""},{"location":"sys/kern/ipc/mqueue/#user-visible-flags","title":"User-Visible Flags","text":"Flag Usage <code>O_RDONLY</code> Open for receive only <code>O_WRONLY</code> Open for send only <code>O_RDWR</code> Open for send and receive <code>O_CREAT</code> Create queue if not exists <code>O_EXCL</code> Fail if queue exists (with <code>O_CREAT</code>) <code>O_NONBLOCK</code> Non-blocking operations"},{"location":"sys/kern/ipc/mqueue/#internal-flags","title":"Internal Flags","text":"Flag Value Description <code>MQ_UNLINK</code> 0x10000000 Queue marked for deletion <code>MQ_RECEIVE</code> 0x20000000 Receiver is waiting (suppresses notification)"},{"location":"sys/kern/ipc/mqueue/#sysctl-tunables","title":"Sysctl Tunables","text":"Sysctl Default Description <code>kern.mqueue.mq_open_max</code> 512 Max descriptors per process <code>kern.mqueue.mq_prio_max</code> 32 Max message priority <code>kern.mqueue.mq_max_msgsize</code> 16384 Max message size <code>kern.mqueue.mq_def_maxmsg</code> 32 Default max messages per queue <code>kern.mqueue.mq_max_maxmsg</code> 512 Max allowed messages per queue"},{"location":"sys/kern/ipc/mqueue/#resource-limits","title":"Resource Limits","text":"<p>Each process tracks open mqueue descriptors in <code>p-&gt;p_mqueue_cnt</code>. Opening a queue fails with <code>EMFILE</code> if the count reaches <code>mq_open_max</code>.</p>"},{"location":"sys/kern/ipc/mqueue/#source-reference","title":"Source Reference","text":"File Description <code>sys/kern/sys_mqueue.c</code> POSIX message queue implementation <code>sys/sys/mqueue.h</code> Message queue structures and constants"},{"location":"sys/kern/ipc/pipes/","title":"Pipes","text":"<p>Pipes provide unidirectional byte streams for inter-process communication. DragonFly's implementation (<code>sys/kern/sys_pipe.c</code>) replaces the traditional socket-based approach with a high-performance VM-backed design featuring per-CPU caching and busy-wait optimization.</p>"},{"location":"sys/kern/ipc/pipes/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/pipes/#pipe-buffer","title":"Pipe Buffer","text":"<p>Each direction of a pipe uses a <code>struct pipebuf</code> (<code>sys/sys/pipe.h:53</code>):</p> <pre><code>struct pipebuf {\n    struct {\n        struct lwkt_token rlock;\n        size_t      rindex;     /* current read index (FIFO) */\n        int32_t     rip;        /* read-in-progress flag */\n        struct timespec atime;  /* time of last access */\n    } __cachealign;\n    struct {\n        struct lwkt_token wlock;\n        size_t      windex;     /* current write index (FIFO) */\n        int32_t     wip;        /* write-in-progress flag */\n        struct timespec mtime;  /* time of last modify */\n    } __cachealign;\n    size_t      size;           /* size of buffer */\n    caddr_t     buffer;         /* kva of buffer */\n    struct vm_object *object;   /* VM object containing buffer */\n    struct kqinfo   kq;         /* for select/poll/kq */\n    struct sigio    *sigio;     /* async I/O info */\n    uint32_t    state;          /* pipe status flags */\n    int         lticks;         /* timestamp optimization */\n} __cachealign;\n</code></pre> <p>The structure uses <code>__cachealign</code> to separate read and write fields onto different cache lines, reducing false sharing between reader and writer.</p>"},{"location":"sys/kern/ipc/pipes/#pipe-structure","title":"Pipe Structure","text":"<p>The main <code>struct pipe</code> (<code>sys/sys/pipe.h:91</code>) contains two buffers for full-duplex communication:</p> <pre><code>struct pipe {\n    struct pipebuf  bufferA;    /* data storage */\n    struct pipebuf  bufferB;    /* data storage */\n    struct timespec ctime;      /* creation time */\n    struct pipe     *next;      /* per-CPU cache linkage */\n    uint32_t        open_count; /* reference count */\n    uint64_t        inum;       /* inode number */\n} __cachealign;\n</code></pre> <p>Each file descriptor identifies which buffer it reads from using the low bit of <code>fp-&gt;f_data</code>: bit 0 clear reads from <code>bufferA</code>, bit 1 set reads from <code>bufferB</code>.</p>"},{"location":"sys/kern/ipc/pipes/#state-flags","title":"State Flags","text":"<p>Buffer state tracked in <code>pipebuf.state</code> (<code>sys/sys/pipe.h:80-85</code>):</p> Flag Value Description <code>PIPE_ASYNC</code> 0x0004 Async I/O enabled (SIGIO) <code>PIPE_WANTR</code> 0x0008 Reader is sleeping <code>PIPE_WANTW</code> 0x0010 Writer is sleeping <code>PIPE_REOF</code> 0x0040 Read EOF (peer closed write) <code>PIPE_WEOF</code> 0x0080 Write EOF (shutdown) <code>PIPE_CLOSED</code> 0x1000 This side fully closed"},{"location":"sys/kern/ipc/pipes/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/pipes/#pipe-and-pipe2","title":"pipe() and pipe2()","text":"<p><code>sys_pipe()</code> and <code>sys_pipe2()</code> (<code>sys_pipe.c:262-274</code>) create a pipe pair:</p> <pre><code>int sys_pipe(struct sysmsg *sysmsg, const struct pipe_args *uap)\n{\n    return kern_pipe(sysmsg-&gt;sysmsg_fds, 0);\n}\n\nint sys_pipe2(struct sysmsg *sysmsg, const struct pipe2_args *uap)\n{\n    if ((uap-&gt;flags &amp; ~(O_CLOEXEC | O_CLOFORK | O_NONBLOCK)) != 0)\n        return (EINVAL);\n    return kern_pipe(sysmsg-&gt;sysmsg_fds, uap-&gt;flags);\n}\n</code></pre> <p><code>pipe2()</code> accepts flags: <code>O_CLOEXEC</code> (close on exec), <code>O_CLOFORK</code> (close on fork), and <code>O_NONBLOCK</code>.</p>"},{"location":"sys/kern/ipc/pipes/#kern_pipe","title":"kern_pipe()","text":"<p>The core creation logic (<code>sys_pipe.c:276-349</code>):</p> <ol> <li>Allocates a <code>struct pipe</code> via <code>pipe_create()</code></li> <li>Allocates two file descriptors with <code>falloc()</code></li> <li>Configures read-side fd (bit 0 = 0) and write-side fd (bit 0 = 1)</li> <li>Sets <code>f_ops</code> to <code>pipeops</code> for both</li> <li>Activates descriptors with <code>fsetfd()</code></li> </ol> <p>Both file descriptors have <code>FREAD | FWRITE</code> flags set, though traditionally one is the read end and one is the write end.</p>"},{"location":"sys/kern/ipc/pipes/#vm-backed-buffers","title":"VM-Backed Buffers","text":""},{"location":"sys/kern/ipc/pipes/#buffer-allocation","title":"Buffer Allocation","text":"<p><code>pipespace()</code> (<code>sys_pipe.c:359-405</code>) allocates kernel virtual address space backed by a VM object:</p> <pre><code>static int\npipespace(struct pipe *pipe, struct pipebuf *pb, size_t size)\n{\n    size = (size + PAGE_MASK) &amp; ~(size_t)PAGE_MASK;\n    if (size &lt; 16384)\n        size = 16384;\n    if (size &gt; 1024*1024)\n        size = 1024*1024;\n\n    npages = round_page(size) / PAGE_SIZE;\n\n    if (object == NULL || object-&gt;size != npages) {\n        object = vm_object_allocate(OBJT_DEFAULT, npages);\n        buffer = (caddr_t)vm_map_min(kernel_map);\n\n        error = vm_map_find(kernel_map, object, NULL,\n                0, (vm_offset_t *)&amp;buffer, size,\n                PAGE_SIZE, TRUE,\n                VM_MAPTYPE_NORMAL, VM_SUBSYS_PIPE,\n                VM_PROT_ALL, VM_PROT_ALL, 0);\n        /* ... */\n    }\n    pb-&gt;rindex = 0;\n    pb-&gt;windex = 0;\n    return (0);\n}\n</code></pre> <p>Key points: - Buffer size clamped between 16KB and 1MB - Uses <code>OBJT_DEFAULT</code> VM objects (pageable, swap-backed) - Each buffer has an independent <code>vm_object</code> for performance - Default size controlled by sysctl <code>kern.pipe.size</code> (32KB)</p>"},{"location":"sys/kern/ipc/pipes/#per-cpu-pipe-cache","title":"Per-CPU Pipe Cache","text":""},{"location":"sys/kern/ipc/pipes/#cache-design","title":"Cache Design","text":"<p>To reduce allocation overhead, pipes are cached per-CPU (<code>sys_pipe.c:111-118</code>):</p> <pre><code>#define PIPEQ_MAX_CACHE 16      /* per-cpu pipe structure cache */\n\nstatic int pipe_maxcache = PIPEQ_MAX_CACHE;\nstatic struct pipegdlock *pipe_gdlocks;\n</code></pre> <p>The cache lives in <code>globaldata_t</code>: - <code>gd-&gt;gd_pipeq</code> - linked list of cached pipes - <code>gd-&gt;gd_pipeqcount</code> - number of cached pipes</p>"},{"location":"sys/kern/ipc/pipes/#cache-initialization","title":"Cache Initialization","text":"<p><code>pipeinit()</code> (<code>sys_pipe.c:148-177</code>) scales the cache based on system memory:</p> <pre><code>static void\npipeinit(void *dummy)\n{\n    size_t mbytes = kmem_lim_size();\n\n    if (pipe_maxcache == PIPEQ_MAX_CACHE) {\n        if (mbytes &gt;= 7 * 1024)\n            pipe_maxcache *= 2;\n        if (mbytes &gt;= 15 * 1024)\n            pipe_maxcache *= 2;\n    }\n\n    /* Reduce cache on systems with many CPUs */\n    if (ncpus &gt; 64) {\n        pipe_maxcache = pipe_maxcache * 64 / ncpus;\n        if (pipe_maxcache &lt; PIPEQ_MAX_CACHE)\n            pipe_maxcache = PIPEQ_MAX_CACHE;\n    }\n    /* ... */\n}\n</code></pre>"},{"location":"sys/kern/ipc/pipes/#allocation-from-cache","title":"Allocation from Cache","text":"<p><code>pipe_create()</code> (<code>sys_pipe.c:414-448</code>) checks the per-CPU cache first:</p> <pre><code>static int\npipe_create(struct pipe **pipep)\n{\n    globaldata_t gd = mycpu;\n    struct pipe *pipe;\n\n    if ((pipe = gd-&gt;gd_pipeq) != NULL) {\n        gd-&gt;gd_pipeq = pipe-&gt;next;\n        --gd-&gt;gd_pipeqcount;\n        pipe-&gt;next = NULL;\n    } else {\n        pipe = kmalloc(sizeof(*pipe), M_PIPE, M_WAITOK | M_ZERO);\n        pipe-&gt;inum = gd-&gt;gd_anoninum++ * ncpus + gd-&gt;gd_cpuid + 2;\n        lwkt_token_init(&amp;pipe-&gt;bufferA.rlock, \"piper\");\n        lwkt_token_init(&amp;pipe-&gt;bufferA.wlock, \"pipew\");\n        lwkt_token_init(&amp;pipe-&gt;bufferB.rlock, \"piper\");\n        lwkt_token_init(&amp;pipe-&gt;bufferB.wlock, \"pipew\");\n    }\n    /* ... allocate buffer space ... */\n}\n</code></pre> <p>The inode number generation (<code>gd-&gt;gd_anoninum++ * ncpus + gd-&gt;gd_cpuid + 2</code>) ensures unique inums across CPUs without synchronization.</p>"},{"location":"sys/kern/ipc/pipes/#return-to-cache","title":"Return to Cache","text":"<p>When both ends close, <code>pipeclose()</code> (<code>sys_pipe.c:1272-1287</code>) returns the pipe to the cache:</p> <pre><code>if (atomic_fetchadd_int(&amp;pipe-&gt;open_count, -1) == 1) {\n    gd = mycpu;\n    if (gd-&gt;gd_pipeqcount &gt;= pipe_maxcache) {\n        mtx_lock(&amp;pipe_gdlocks[gd-&gt;gd_cpuid].mtx);\n        pipe_free_kmem(rpb);\n        pipe_free_kmem(wpb);\n        mtx_unlock(&amp;pipe_gdlocks[gd-&gt;gd_cpuid].mtx);\n        kfree(pipe, M_PIPE);\n    } else {\n        rpb-&gt;state = 0;\n        wpb-&gt;state = 0;\n        pipe-&gt;next = gd-&gt;gd_pipeq;\n        gd-&gt;gd_pipeq = pipe;\n        ++gd-&gt;gd_pipeqcount;\n    }\n}\n</code></pre> <p>The per-CPU mutex (<code>pipe_gdlocks</code>) serializes access to <code>kernel_map</code> during bulk teardown scenarios (e.g., mass process termination).</p>"},{"location":"sys/kern/ipc/pipes/#read-operation","title":"Read Operation","text":""},{"location":"sys/kern/ipc/pipes/#pipe_read","title":"pipe_read()","text":"<p><code>pipe_read()</code> (<code>sys_pipe.c:453-698</code>) implements reading:</p> <ol> <li>Buffer selection: Determines read buffer based on <code>fp-&gt;f_data</code> bit 0</li> <li>Quick NBIO check: Returns <code>EAGAIN</code> early if non-blocking and buffer empty</li> <li>Serialization: Acquires <code>rlock</code> token and calls <code>pipe_start_uio()</code> to serialize against other readers</li> <li>Copy loop: Reads available data via <code>uiomove()</code></li> </ol> <p>Key features of the read loop:</p> <pre><code>while (uio-&gt;uio_resid) {\n    size = rpb-&gt;windex - rpb-&gt;rindex;\n    cpu_lfence();  /* memory barrier before reading buffer */\n\n    if (size) {\n        rindex = rpb-&gt;rindex &amp; (rpb-&gt;size - 1);\n        nsize = szmin(size, uio-&gt;uio_resid);\n\n        /* Limit to half buffer to avoid ping-pong */\n        if (nsize &gt; (rpb-&gt;size &gt;&gt; 1))\n            nsize = rpb-&gt;size &gt;&gt; 1;\n\n        error = uiomove(&amp;rpb-&gt;buffer[rindex], nsize, uio);\n        rpb-&gt;rindex += nsize;\n\n        /* Wake writer if buffer less than half full */\n        if (size - nsize &lt;= (rpb-&gt;size &gt;&gt; 1))\n            pipesignal(rpb, PIPE_WANTW);\n        continue;\n    }\n    /* ... blocking logic ... */\n}\n</code></pre> <p>The buffer uses power-of-2 sizing, so <code>rindex &amp; (size - 1)</code> computes the circular buffer offset efficiently.</p>"},{"location":"sys/kern/ipc/pipes/#busy-wait-optimization","title":"Busy-Wait Optimization","text":"<p>Before sleeping, the reader busy-waits for a configurable period (<code>sys_pipe.c:599-615</code>):</p> <pre><code>#ifdef _RDTSC_SUPPORTED_\nif (pipe_delay) {\n    int64_t tsc_target;\n    int good = 0;\n\n    tsc_target = tsc_get_target(pipe_delay);\n    while (tsc_test_target(tsc_target) == 0) {\n        cpu_lfence();\n        if (rpb-&gt;windex != rpb-&gt;rindex) {\n            good = 1;\n            break;\n        }\n        cpu_pause();\n    }\n    if (good)\n        continue;\n}\n#endif\n</code></pre> <p>The <code>pipe_delay</code> sysctl (default 4000ns = 4us) trades CPU cycles for reduced IPI/wakeup latency. This is effective for synchronous producer-consumer patterns.</p>"},{"location":"sys/kern/ipc/pipes/#write-operation","title":"Write Operation","text":""},{"location":"sys/kern/ipc/pipes/#pipe_write","title":"pipe_write()","text":"<p><code>pipe_write()</code> (<code>sys_pipe.c:700-987</code>) mirrors the read path:</p> <ol> <li>Buffer selection: Writes to the peer's read buffer</li> <li>EOF check: Returns <code>EPIPE</code> if <code>PIPE_WEOF</code> is set</li> <li>Atomicity: Writes &lt;= <code>PIPE_BUF</code> (512 bytes) are atomic</li> </ol> <pre><code>while (uio-&gt;uio_resid) {\n    space = wpb-&gt;size - (wpb-&gt;windex - wpb-&gt;rindex);\n\n    /* Writes &lt;= PIPE_BUF must be atomic */\n    if ((space &lt; uio-&gt;uio_resid) &amp;&amp; (orig_resid &lt;= PIPE_BUF))\n        space = 0;\n\n    if (space &gt; 0) {\n        /* Limit to half buffer for pipelining */\n        if (space &gt; (wpb-&gt;size &gt;&gt; 1))\n            space = (wpb-&gt;size &gt;&gt; 1);\n\n        /* Handle wraparound */\n        windex = wpb-&gt;windex &amp; (wpb-&gt;size - 1);\n        segsize = wpb-&gt;size - windex;\n        if (segsize &gt; space)\n            segsize = space;\n\n        error = uiomove(&amp;wpb-&gt;buffer[windex], segsize, uio);\n        if (error == 0 &amp;&amp; segsize &lt; space) {\n            segsize = space - segsize;\n            error = uiomove(&amp;wpb-&gt;buffer[0], segsize, uio);\n        }\n\n        cpu_sfence();  /* ensure data visible before windex update */\n        wpb-&gt;windex += space;\n        pipesignal(wpb, PIPE_WANTR);\n        continue;\n    }\n    /* ... blocking logic with busy-wait ... */\n}\n</code></pre> <p>The store fence (<code>cpu_sfence()</code>) ensures buffer contents are visible to readers before <code>windex</code> is updated.</p>"},{"location":"sys/kern/ipc/pipes/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/pipes/#token-based-locking","title":"Token-Based Locking","text":"<p>Each buffer has separate read and write tokens: - <code>rlock</code> - held during reads, protects <code>rindex</code> - <code>wlock</code> - held during writes, protects <code>windex</code></p> <p>This allows concurrent read and write operations on the same buffer.</p>"},{"location":"sys/kern/ipc/pipes/#uio-serialization","title":"UIO Serialization","text":"<p>The <code>rip</code> and <code>wip</code> fields serialize multiple concurrent reads or writes (<code>sys_pipe.c:228-253</code>):</p> <pre><code>static __inline int\npipe_start_uio(int *ipp)\n{\n    int error;\n    while (*ipp) {\n        *ipp = -1;  /* mark as contended */\n        error = tsleep(ipp, PCATCH, \"pipexx\", 0);\n        if (error)\n            return (error);\n    }\n    *ipp = 1;  /* mark as in-progress */\n    return (0);\n}\n\nstatic __inline void\npipe_end_uio(int *ipp)\n{\n    if (*ipp &lt; 0) {\n        *ipp = 0;\n        wakeup(ipp);  /* wake contending thread */\n    } else {\n        *ipp = 0;\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/pipes/#signal-and-wakeup","title":"Signal and Wakeup","text":"<p><code>pipesignal()</code> (<code>sys_pipe.c:188-203</code>) atomically clears wait flags and wakes sleepers:</p> <pre><code>static __inline void\npipesignal(struct pipebuf *pb, uint32_t flags)\n{\n    uint32_t oflags, nflags;\n\n    for (;;) {\n        oflags = pb-&gt;state;\n        cpu_ccfence();\n        nflags = oflags &amp; ~flags;\n        if (atomic_cmpset_int(&amp;pb-&gt;state, oflags, nflags))\n            break;\n    }\n    if (oflags &amp; flags)\n        wakeup(pb);\n}\n</code></pre>"},{"location":"sys/kern/ipc/pipes/#shutdown-and-close","title":"Shutdown and Close","text":""},{"location":"sys/kern/ipc/pipes/#pipe_shutdown","title":"pipe_shutdown()","text":"<p><code>pipe_shutdown()</code> (<code>sys_pipe.c:1113-1183</code>) implements partial close semantics:</p> <pre><code>switch(how) {\ncase SHUT_RDWR:\ncase SHUT_RD:\n    atomic_set_int(&amp;rpb-&gt;state, PIPE_REOF | PIPE_WEOF);\n    /* wake waiters */\n    if (how == SHUT_RD)\n        break;\n    /* fall through */\ncase SHUT_WR:\n    atomic_set_int(&amp;wpb-&gt;state, PIPE_REOF | PIPE_WEOF);\n    /* wake waiters */\n    break;\n}\n</code></pre> <p>This requires all four tokens (both rlock and wlock for both buffers) since it modifies state on both sides.</p>"},{"location":"sys/kern/ipc/pipes/#pipeclose","title":"pipeclose()","text":"<p><code>pipeclose()</code> (<code>sys_pipe.c:1203-1288</code>) handles final cleanup:</p> <ol> <li>Sets <code>PIPE_CLOSED | PIPE_REOF | PIPE_WEOF</code> on own buffer</li> <li>Sets <code>PIPE_REOF | PIPE_WEOF</code> on peer buffer</li> <li>Wakes all waiters on both sides</li> <li>When <code>open_count</code> reaches 0, returns to cache or frees</li> </ol>"},{"location":"sys/kern/ipc/pipes/#file-operations","title":"File Operations","text":"<p>The <code>pipeops</code> structure (<code>sys_pipe.c:89-98</code>):</p> <pre><code>static struct fileops pipeops = {\n    .fo_read = pipe_read, \n    .fo_write = pipe_write,\n    .fo_ioctl = pipe_ioctl,\n    .fo_kqfilter = pipe_kqfilter,\n    .fo_stat = pipe_stat,\n    .fo_close = pipe_close,\n    .fo_shutdown = pipe_shutdown,\n    .fo_seek = badfo_seek\n};\n</code></pre>"},{"location":"sys/kern/ipc/pipes/#supported-ioctls","title":"Supported ioctls","text":"<p><code>pipe_ioctl()</code> (<code>sys_pipe.c:992-1048</code>) supports:</p> ioctl Description <code>FIOASYNC</code> Enable/disable async I/O (SIGIO) <code>FIONREAD</code> Return bytes available for read <code>FIOSETOWN</code> Set owner for SIGIO <code>FIOGETOWN</code> Get owner for SIGIO <code>TIOCSPGRP</code> Set process group (deprecated) <code>TIOCGPGRP</code> Get process group (deprecated)"},{"location":"sys/kern/ipc/pipes/#kqueue-support","title":"kqueue Support","text":"<p><code>pipe_kqfilter()</code> (<code>sys_pipe.c:1290-1325</code>) supports <code>EVFILT_READ</code> and <code>EVFILT_WRITE</code>:</p> <pre><code>switch (kn-&gt;kn_filter) {\ncase EVFILT_READ:\n    kn-&gt;kn_fop = &amp;pipe_rfiltops;\n    break;\ncase EVFILT_WRITE:\n    kn-&gt;kn_fop = &amp;pipe_wfiltops;\n    break;\n}\nknote_insert(&amp;rpb-&gt;kq.ki_note, kn);\n</code></pre> <p>The filter operations are marked <code>FILTEROP_MPSAFE</code> and rely on the knote's <code>KN_PROCESSING</code> flag for synchronization rather than pipe tokens.</p>"},{"location":"sys/kern/ipc/pipes/#sysctl-tunables","title":"Sysctl Tunables","text":"Sysctl Default Description <code>kern.pipe.size</code> 32768 Default buffer size for new pipes <code>kern.pipe.maxcache</code> 16-64 Per-CPU cache size (scaled by memory) <code>kern.pipe.delay</code> 4000 Busy-wait time in nanoseconds"},{"location":"sys/kern/ipc/pipes/#source-reference","title":"Source Reference","text":"File Description <code>sys/kern/sys_pipe.c</code> Pipe implementation <code>sys/sys/pipe.h</code> Pipe structures and flags"},{"location":"sys/kern/ipc/protocol-dispatch/","title":"Protocol Dispatch","text":"<p>The protocol dispatch layer provides the framework for routing socket operations to protocol-specific handlers in DragonFly BSD. It manages protocol registration, domain initialization, and message-based operation dispatch using the LWKT subsystem for multi-processor scalability.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#overview","title":"Overview","text":"<p>DragonFly's protocol dispatch architecture consists of three main components:</p> <ul> <li>Domains - Protocol families (e.g., AF_LOCAL, AF_INET) that group related protocols</li> <li>Protocol Switch - Tables mapping socket types to protocol handlers</li> <li>Network Messages - LWKT messages that dispatch operations to protocol threads</li> </ul> <p>This design allows protocol operations to execute on dedicated threads, avoiding lock contention and enabling parallel processing across CPUs.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#source-files","title":"Source Files","text":"File Description <code>sys/kern/uipc_domain.c</code> Domain management and initialization <code>sys/kern/uipc_proto.c</code> Local domain protocol registration <code>sys/kern/uipc_msg.c</code> Network message dispatch wrappers <code>sys/sys/domain.h</code> Domain structure definition <code>sys/sys/protosw.h</code> Protocol switch structure and flags <code>sys/net/netmsg.h</code> Network message structures"},{"location":"sys/kern/ipc/protocol-dispatch/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#struct-domain","title":"struct domain","text":"<p>The domain structure (<code>sys/sys/domain.h:44</code>) groups protocols by address family:</p> <pre><code>struct domain {\n    int     dom_family;             /* AF_xxx */\n    char    *dom_name;              /* domain name (e.g., \"local\") */\n    void    (*dom_init)(void);      /* initialization routine */\n    int     (*dom_externalize)(struct mbuf *, int, struct thread *);\n                                    /* externalize access rights */\n    void    (*dom_dispose)(struct mbuf *);\n                                    /* dispose of internalized rights */\n    struct  protosw *dom_protosw;   /* protocol switch table start */\n    struct  protosw *dom_protoswNPROTOSW;\n                                    /* protocol switch table end */\n    SLIST_ENTRY(domain) dom_next;   /* next domain in list */\n    int     (*dom_rtattach)(void **, int);\n                                    /* initialize routing table */\n    int     dom_rtoffset;           /* arg to rtattach (sockaddr offset) */\n    int     dom_maxrtkey;           /* for routing layer */\n    void    *(*dom_ifattach)(struct ifnet *);\n                                    /* per-interface attach */\n    void    (*dom_ifdetach)(struct ifnet *, void *);\n                                    /* per-interface detach */\n};\n</code></pre> <p>Key fields:</p> <ul> <li><code>dom_family</code> - Address family identifier (AF_LOCAL, AF_INET, AF_INET6, etc.)</li> <li><code>dom_externalize</code> - Called to externalize access rights in control messages (used by Unix domain sockets for file descriptor passing)</li> <li><code>dom_dispose</code> - Called to dispose of internalized access rights</li> <li><code>dom_protosw</code> / <code>dom_protoswNPROTOSW</code> - Bounds of the protocol switch array for this domain</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#struct-protosw","title":"struct protosw","text":"<p>The protocol switch structure (<code>sys/sys/protosw.h:86</code>) defines protocol behavior:</p> <pre><code>struct protosw {\n    short   pr_type;                /* socket type (SOCK_STREAM, etc.) */\n    const struct domain *pr_domain; /* back pointer to domain */\n    short   pr_protocol;            /* protocol number, if any */\n    short   pr_flags;               /* protocol flags (PR_*) */\n\n    /* Protocol-layer operations (rarely used directly) */\n    void    (*pr_input)(struct mbuf *, ...);\n                                    /* input from below */\n    int     (*pr_output)(struct mbuf *, struct socket *, ...);\n                                    /* output to network */\n    void    (*pr_ctlinput)(int, struct sockaddr *, void *, void *);\n                                    /* control input */\n    int     (*pr_ctloutput)(struct socket *, struct sockopt *);\n                                    /* control output */\n\n    /* Initialization */\n    void    (*pr_init)(void);       /* protocol init */\n\n    /* Timer (deprecated) */\n    void    (*pr_fasttimo)(void);   /* fast timeout */\n    void    (*pr_slowtimo)(void);   /* slow timeout */\n\n    /* Drain excess resources */\n    void    (*pr_drain)(void);\n\n    /* User-request operations */\n    struct  pr_usrreqs *pr_usrreqs;\n};\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#protocol-flags-pr_","title":"Protocol Flags (PR_*)","text":"<p>Protocol flags control dispatch behavior (<code>sys/sys/protosw.h:129</code>):</p> Flag Value Description <code>PR_ATOMIC</code> 0x01 Exchange atomic messages only <code>PR_ADDR</code> 0x02 Addresses given with messages <code>PR_CONNREQUIRED</code> 0x04 Connection required for data transfer <code>PR_WANTRCVD</code> 0x08 Protocol wants <code>pru_rcvd</code> calls <code>PR_RIGHTS</code> 0x10 Protocol supports access rights passing <code>PR_SYNC_PORT</code> 0x20 Use synchronous port (netisr_sync_port) <code>PR_ASYNC_SEND</code> 0x40 Allow asynchronous <code>pru_send</code> <code>PR_ASYNC_RCVD</code> 0x80 Allow asynchronous <code>pru_rcvd</code> <code>PR_MPSAFE</code> 0x0100 Protocol handler is MP-safe <p>Key flag semantics:</p> <ul> <li><code>PR_ATOMIC</code> - Each send/receive operates on complete messages (datagrams)</li> <li><code>PR_CONNREQUIRED</code> - Stream protocols requiring connection before data transfer</li> <li><code>PR_SYNC_PORT</code> - Forces all operations through a single serializing port</li> <li><code>PR_ASYNC_SEND</code> / <code>PR_ASYNC_RCVD</code> - Enable fire-and-forget message dispatch for performance</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#struct-pr_usrreqs","title":"struct pr_usrreqs","text":"<p>User request handlers for socket operations (<code>sys/sys/protosw.h:158</code>):</p> <pre><code>struct pr_usrreqs {\n    void    (*pru_abort)(netmsg_t);         /* abort connection */\n    void    (*pru_accept)(netmsg_t);        /* accept incoming conn */\n    void    (*pru_attach)(netmsg_t);        /* attach protocol */\n    void    (*pru_bind)(netmsg_t);          /* bind to address */\n    void    (*pru_connect)(netmsg_t);       /* connect to peer */\n    void    (*pru_connect2)(netmsg_t);      /* connect two sockets */\n    void    (*pru_control)(netmsg_t);       /* ioctl operations */\n    void    (*pru_detach)(netmsg_t);        /* detach protocol */\n    void    (*pru_disconnect)(netmsg_t);    /* disconnect */\n    void    (*pru_listen)(netmsg_t);        /* listen for connections */\n    void    (*pru_peeraddr)(netmsg_t);      /* get peer address */\n    void    (*pru_rcvd)(netmsg_t);          /* received data consumed */\n    void    (*pru_rcvoob)(netmsg_t);        /* receive OOB data */\n    void    (*pru_send)(netmsg_t);          /* send data */\n    void    (*pru_sense)(netmsg_t);         /* stat-like operation */\n    void    (*pru_shutdown)(netmsg_t);      /* shutdown connection */\n    void    (*pru_sockaddr)(netmsg_t);      /* get local address */\n    void    (*pru_sosend)(struct socket *, struct sockaddr *,\n                          struct uio *, struct mbuf *,\n                          struct mbuf *, int, struct thread *);\n                                            /* optimized send path */\n    void    (*pru_soreceive)(struct socket *, struct sockaddr **,\n                             struct uio *, struct sockbuf *,\n                             struct mbuf **, int *);\n                                            /* optimized receive path */\n    void    (*pru_savefaddr)(struct socket *, const struct sockaddr *);\n                                            /* save foreign address */\n};\n</code></pre> <p>All standard handlers take a <code>netmsg_t</code> parameter, which encapsulates the operation request and allows asynchronous execution.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#network-message-structures","title":"Network Message Structures","text":"<p>Network messages (<code>sys/net/netmsg.h</code>) carry operation requests between threads:</p> <pre><code>struct netmsg_base {\n    struct lwkt_msg     nm_lmsg;        /* LWKT message header */\n    netisr_fn_t         nm_dispatch;    /* dispatch handler */\n    struct socket       *nm_so;         /* associated socket */\n};\n\ntypedef union netmsg *netmsg_t;\n</code></pre> <p>Operation-specific message types extend <code>netmsg_base</code>:</p> <pre><code>struct netmsg_pru_attach {\n    struct netmsg_base  base;\n    int                 nm_proto;       /* protocol number */\n    struct pru_attach_info *nm_ai;      /* attach info */\n};\n\nstruct netmsg_pru_connect {\n    struct netmsg_base  base;\n    struct sockaddr     *nm_nam;        /* target address */\n    struct thread       *nm_td;         /* calling thread */\n    struct mbuf         *nm_m;          /* data mbuf (for sendto) */\n    int                 nm_flags;       /* flags */\n    int                 nm_reconnect;   /* reconnect indicator */\n};\n\nstruct netmsg_pru_send {\n    struct netmsg_base  base;\n    int                 nm_flags;       /* MSG_* flags */\n    int                 nm_priv;        /* privilege level */\n    struct mbuf         *nm_m;          /* data mbuf chain */\n    struct sockaddr     *nm_addr;       /* target address */\n    struct mbuf         *nm_control;    /* control mbuf */\n    struct thread       *nm_td;         /* calling thread */\n};\n\nstruct netmsg_pru_rcvd {\n    struct netmsg_base  base;\n    int                 nm_flags;       /* MSG_* flags */\n    int                 nm_pru_flags;   /* PRUR_* flags */\n};\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#domain-registration","title":"Domain Registration","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#domain_set-macro","title":"DOMAIN_SET Macro","text":"<p>Domains register themselves using the <code>DOMAIN_SET()</code> macro (<code>sys/sys/domain.h:74</code>):</p> <pre><code>#define DOMAIN_SET(name)                                        \\\n    SYSINIT(domain_add_ ## name, SI_SUB_PROTO_DOMAIN,           \\\n            SI_ORDER_FIRST, net_add_domain, &amp;name ## domain)\n\n/* Example from uipc_proto.c */\nDOMAIN_SET(local);\n</code></pre> <p>This creates a SYSINIT entry that calls <code>net_add_domain()</code> during the <code>SI_SUB_PROTO_DOMAIN</code> initialization phase.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#domain-registration-flow","title":"Domain Registration Flow","text":"<pre><code>flowchart TD\n    boot[\"boot\"] --&gt; PROTO[\"SI_SUB_PROTO_DOMAIN phase\"]\n    PROTO --&gt; add1[\"net_add_domain(&amp;localdomain)\"]\n    PROTO --&gt; add2[\"net_add_domain(&amp;inetdomain)\"]\n    PROTO --&gt; add3[\"net_add_domain(&amp;inet6domain)\"]\n    PROTO --&gt; addN[\"... (other domains)\"]\n    boot --&gt; END[\"SI_SUB_PROTO_END phase\"]\n    END --&gt; init[\"net_init_domains()\"]\n    init --&gt; initdom[\"For each registered domain:net_init_domain(dom)\"]\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#net_add_domain","title":"net_add_domain()","text":"<p>Adds a domain to the global list (<code>sys/kern/uipc_domain.c:99</code>):</p> <pre><code>void net_add_domain(void *data)\n{\n    struct domain *dp = data;\n\n    crit_enter();\n    SLIST_INSERT_HEAD(&amp;domains, dp, dom_next);\n    crit_exit();\n}\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#net_init_domain","title":"net_init_domain()","text":"<p>Initializes a domain and its protocols (<code>sys/kern/uipc_domain.c:110</code>):</p> <pre><code>static void net_init_domain(struct domain *dp)\n{\n    struct protosw *pr;\n    u_char pr_flags[256];       /* Track protocol flags by protocol number */\n    int warn_deprecation;\n\n    /* Skip if no protocols */\n    if (dp-&gt;dom_protosw == NULL)\n        return;\n\n    /* Check for deprecated timer callbacks */\n    warn_deprecation = 0;\n\n    /* Initialize each protocol in the domain */\n    for (pr = dp-&gt;dom_protosw; pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n        /* Fill in default user request handlers if not specified */\n        pr_usrreqs_init(pr);\n\n        /* Track and validate protocol flags */\n        if (pr-&gt;pr_protocol &amp;&amp; pr-&gt;pr_protocol &lt; 256) {\n            if (pr_flags[pr-&gt;pr_protocol])\n                kprintf(\"domain %s: duplicate proto %d\\n\",\n                        dp-&gt;dom_name, pr-&gt;pr_protocol);\n            pr_flags[pr-&gt;pr_protocol] = pr-&gt;pr_flags;\n        }\n\n        /* Call protocol's init function */\n        if (pr-&gt;pr_init)\n            (*pr-&gt;pr_init)();\n\n        /* Deprecated: timer callbacks */\n        if (pr-&gt;pr_fasttimo || pr-&gt;pr_slowtimo)\n            warn_deprecation = 1;\n    }\n\n    /* Call domain's init function */\n    if (dp-&gt;dom_init)\n        (*dp-&gt;dom_init)();\n\n    if (warn_deprecation)\n        kprintf(\"domain %s: pr_fasttimo or pr_slowtimo \"\n                \"not longer supported\\n\", dp-&gt;dom_name);\n}\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#pr_usrreqs_init","title":"pr_usrreqs_init()","text":"<p>Fills in default handlers for unspecified operations (<code>sys/kern/uipc_domain.c:70</code>):</p> <pre><code>static void pr_usrreqs_init(struct protosw *pr)\n{\n    struct pr_usrreqs *pu = pr-&gt;pr_usrreqs;\n\n    if (pu == NULL) {\n        pr-&gt;pr_usrreqs = &amp;pru_default_notsupp;\n        return;\n    }\n\n    /* Fill in default \"not supported\" handlers */\n    if (pu-&gt;pru_abort == NULL)\n        pu-&gt;pru_abort = pr_generic_notsupp;\n    if (pu-&gt;pru_accept == NULL)\n        pu-&gt;pru_accept = pr_generic_notsupp;\n    if (pu-&gt;pru_attach == NULL)\n        pu-&gt;pru_attach = pr_generic_notsupp;\n    /* ... (similar for all other handlers) ... */\n}\n</code></pre> <p>The default handler <code>pr_generic_notsupp()</code> returns <code>EOPNOTSUPP</code> for unsupported operations.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#protocol-lookup","title":"Protocol Lookup","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#pffindtype","title":"pffindtype()","text":"<p>Finds a protocol by family and socket type (<code>sys/kern/uipc_domain.c:171</code>):</p> <pre><code>struct protosw *pffindtype(int family, int type)\n{\n    struct domain *dp;\n    struct protosw *pr;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        if (dp-&gt;dom_family == family) {\n            /* Scan protocol switch table */\n            for (pr = dp-&gt;dom_protosw;\n                 pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n                if (pr-&gt;pr_type &amp;&amp; pr-&gt;pr_type == type)\n                    return pr;\n            }\n        }\n    }\n    return NULL;\n}\n</code></pre> <p>Used when creating a socket with <code>protocol=0</code> (default protocol for the type).</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#pffindproto","title":"pffindproto()","text":"<p>Finds a protocol by family, protocol number, and type (<code>sys/kern/uipc_domain.c:194</code>):</p> <pre><code>struct protosw *pffindproto(int family, int protocol, int type)\n{\n    struct domain *dp;\n    struct protosw *pr;\n    struct protosw *maybe = NULL;\n\n    if (family == 0)\n        return NULL;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        if (dp-&gt;dom_family == family) {\n            for (pr = dp-&gt;dom_protosw;\n                 pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n                if (pr-&gt;pr_protocol == protocol) {\n                    if (pr-&gt;pr_type == type)\n                        return pr;\n                    if (type == SOCK_RAW &amp;&amp; pr-&gt;pr_type == 0 &amp;&amp;\n                        maybe == NULL)\n                        maybe = pr;  /* Wildcard match for raw */\n                }\n            }\n        }\n    }\n    return maybe;\n}\n</code></pre> <p>Allows wildcard matching for <code>SOCK_RAW</code> sockets when exact type match fails.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#network-message-dispatch","title":"Network Message Dispatch","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#message-dispatch-model","title":"Message Dispatch Model","text":"<p>Socket operations use LWKT messages for thread-safe protocol dispatch:</p> <pre><code>flowchart TD\n    US[\"User Space\"] --&gt; SL[\"Socket Layer(sosend, soreceive, etc.)\"]\n    SL --&gt; PRU[\"so_pru_*() Wrappers(uipc_msg.c)\"]\n    PRU --&gt; BUILD[\"Build netmsg_pru_* structure\"]\n    PRU --&gt; SETDISP[\"Set dispatch handler (nm_dispatch)\"]\n    PRU --&gt; SEND[\"Send to protocol's message port\"]\n    SEND --&gt; SYNC[\"lwkt_domsg()[synchronous]\"]\n    SEND --&gt; ASYNC[\"lwkt_sendmsg()[asynchronous]\"]\n    SYNC --&gt; WAIT[\"Wait for reply\"]\n    ASYNC --&gt; FIRE[\"Fire and forget\"]\n    WAIT --&gt; PT[\"Protocol Thread\"]\n    FIRE --&gt; PT\n    PT --&gt; DISP[\"nm_dispatch(netmsg)\"]\n    DISP --&gt; HANDLER[\"pru_handler(netmsg)\"]\n    HANDLER --&gt; REPLY[\"lwkt_replymsg()[when done]\"]\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#synchronous-dispatch","title":"Synchronous Dispatch","text":"<p>Most operations use synchronous dispatch (<code>sys/kern/uipc_msg.c:163</code>):</p> <pre><code>int so_pru_attach(struct socket *so, int proto, struct pru_attach_info *ai)\n{\n    struct netmsg_pru_attach msg;\n    int error;\n\n    /* Initialize message */\n    netmsg_init(&amp;msg.base, so, &amp;netisr_adone_rport,\n                0, so-&gt;so_proto-&gt;pr_usrreqs-&gt;pru_attach);\n    msg.nm_proto = proto;\n    msg.nm_ai = ai;\n\n    /* Send and wait for reply */\n    error = lwkt_domsg(so-&gt;so_port, &amp;msg.base.lmsg, 0);\n    return error;\n}\n</code></pre> <p>The <code>lwkt_domsg()</code> call: 1. Sends the message to <code>so-&gt;so_port</code> (protocol thread's port) 2. Blocks until the protocol handler calls <code>lwkt_replymsg()</code> 3. Returns the error code from the reply</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#asynchronous-dispatch","title":"Asynchronous Dispatch","text":"<p>Performance-critical operations support async dispatch (<code>sys/kern/uipc_msg.c:285</code>):</p> <pre><code>int so_pru_send(struct socket *so, int flags, struct mbuf *m,\n                struct sockaddr *addr, struct mbuf *control,\n                struct thread *td)\n{\n    struct netmsg_pru_send msg;\n    int error;\n\n    /* Initialize message */\n    netmsg_init(&amp;msg.base, so, &amp;netisr_adone_rport,\n                0, so-&gt;so_proto-&gt;pr_usrreqs-&gt;pru_send);\n    msg.nm_flags = flags;\n    msg.nm_m = m;\n    msg.nm_addr = addr;\n    msg.nm_control = control;\n    msg.nm_td = td;\n\n    /* Check if async send is allowed */\n    if (so-&gt;so_proto-&gt;pr_flags &amp; PR_ASYNC_SEND) {\n        /* Async path: don't wait for completion */\n        lwkt_sendmsg(so-&gt;so_port, &amp;msg.base.lmsg);\n        return 0;\n    }\n\n    /* Sync path */\n    error = lwkt_domsg(so-&gt;so_port, &amp;msg.base.lmsg, 0);\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#direct-execution","title":"Direct Execution","text":"<p>For same-CPU optimization, direct variants bypass message passing (<code>sys/kern/uipc_msg.c:421</code>):</p> <pre><code>int so_pru_attach_direct(struct socket *so, int proto,\n                         struct pru_attach_info *ai)\n{\n    struct netmsg_pru_attach msg;\n    netisr_fn_t func = so-&gt;so_proto-&gt;pr_usrreqs-&gt;pru_attach;\n\n    /* Initialize message (no reply port needed) */\n    netmsg_init(&amp;msg.base, so, &amp;netisr_adone_rport, 0, func);\n    msg.nm_proto = proto;\n    msg.nm_ai = ai;\n\n    /* Call handler directly */\n    func((netmsg_t)&amp;msg);\n\n    return msg.base.lmsg.ms_error;\n}\n</code></pre> <p>Direct variants are used when: - The caller is already on the protocol thread - The operation is part of connection setup (e.g., <code>sonewconn()</code>) - Performance is critical and serialization overhead must be avoided</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#per-socket-async-rcvd-messages","title":"Per-Socket Async rcvd Messages","text":"<p>For protocols with <code>PR_ASYNC_RCVD</code>, each socket maintains a dedicated rcvd message (<code>sys/kern/uipc_msg.c:372</code>):</p> <pre><code>void so_pru_rcvd_async(struct socket *so)\n{\n    struct netmsg_pru_rcvd *msg;\n\n    /* Use socket's pre-allocated message */\n    msg = &amp;so-&gt;so_rcvd_msg;\n\n    /* Only send if not already in flight */\n    spin_lock(&amp;so-&gt;so_rcvd_spin);\n    if ((msg-&gt;nm_pru_flags &amp; PRUR_ASYNC) == 0) {\n        msg-&gt;nm_pru_flags |= PRUR_ASYNC;\n        spin_unlock(&amp;so-&gt;so_rcvd_spin);\n\n        netmsg_init(&amp;msg-&gt;base, so, &amp;netisr_apanic_rport,\n                    0, so-&gt;so_proto-&gt;pr_usrreqs-&gt;pru_rcvd);\n        msg-&gt;nm_flags = 0;\n\n        lwkt_sendmsg(so-&gt;so_port, &amp;msg-&gt;base.lmsg);\n    } else {\n        spin_unlock(&amp;so-&gt;so_rcvd_spin);\n    }\n}\n</code></pre> <p>This avoids allocating messages for frequent rcvd notifications in streaming protocols.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#message-initialization","title":"Message Initialization","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#netmsg_init","title":"netmsg_init()","text":"<p>Initializes a network message (<code>sys/kern/uipc_msg.c:65</code>):</p> <pre><code>void netmsg_init(netmsg_base_t msg, struct socket *so,\n                 struct lwkt_port *rport, int flags, netisr_fn_t dispatch)\n{\n    lwkt_initmsg(&amp;msg-&gt;lmsg, rport, flags);\n    msg-&gt;nm_dispatch = dispatch;\n    msg-&gt;nm_so = so;\n}\n</code></pre> <p>Parameters: - <code>msg</code> - Message to initialize - <code>so</code> - Associated socket - <code>rport</code> - Reply port (where completion notification is sent) - <code>flags</code> - LWKT message flags - <code>dispatch</code> - Handler function to call on the protocol thread</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#reply-ports","title":"Reply Ports","text":"<p>Common reply ports:</p> <ul> <li><code>netisr_adone_rport</code> - Async-done reply port (sets ms_error, no wakeup)</li> <li><code>netisr_apanic_rport</code> - Panics if a reply is received (for fire-and-forget)</li> <li><code>curthread-&gt;td_msgport</code> - Current thread's port (for sync operations)</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#control-inputoutput","title":"Control Input/Output","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#kpfctlinput","title":"kpfctlinput()","text":"<p>Broadcasts control input to all protocols (<code>sys/kern/uipc_domain.c:226</code>):</p> <pre><code>void kpfctlinput(int cmd, struct sockaddr *sa)\n{\n    struct domain *dp;\n    struct protosw *pr;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        for (pr = dp-&gt;dom_protosw;\n             pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n            if (pr-&gt;pr_ctlinput)\n                (*pr-&gt;pr_ctlinput)(cmd, sa, NULL, NULL);\n        }\n    }\n}\n</code></pre> <p>Used for network-wide events like interface state changes or ICMP notifications.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#kpfctlinput2","title":"kpfctlinput2()","text":"<p>Extended version with additional context (<code>sys/kern/uipc_domain.c:240</code>):</p> <pre><code>void kpfctlinput2(int cmd, struct sockaddr *sa, void *ctlparam)\n{\n    struct domain *dp;\n    struct protosw *pr;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        if (dp-&gt;dom_family != sa-&gt;sa_family)\n            continue;\n        for (pr = dp-&gt;dom_protosw;\n             pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n            if (pr-&gt;pr_ctlinput)\n                (*pr-&gt;pr_ctlinput)(cmd, sa, ctlparam, NULL);\n        }\n    }\n}\n</code></pre> <p>Filters by address family for efficiency.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#so_pr_ctloutput","title":"so_pr_ctloutput()","text":"<p>Wrapper for protocol control output (<code>sys/kern/uipc_msg.c:100</code>):</p> <pre><code>int so_pr_ctloutput(struct socket *so, struct sockopt *sopt)\n{\n    return so-&gt;so_proto-&gt;pr_ctloutput(so, sopt);\n}\n</code></pre> <p>Called from <code>sosetopt()</code>/<code>sogetopt()</code> for protocol-specific options.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#local-domain-example","title":"Local Domain Example","text":"<p>The local (Unix) domain (<code>sys/kern/uipc_proto.c</code>) demonstrates domain registration:</p> <pre><code>/* Protocol switch table */\nstruct protosw localsw[] = {\n    {\n        .pr_type = SOCK_STREAM,\n        .pr_domain = &amp;localdomain,\n        .pr_flags = PR_CONNREQUIRED | PR_WANTRCVD | PR_RIGHTS |\n                    PR_SYNC_PORT,\n        .pr_ctloutput = uipc_ctloutput,\n        .pr_usrreqs = &amp;uipc_usrreqs,\n    },\n    {\n        .pr_type = SOCK_DGRAM,\n        .pr_domain = &amp;localdomain,\n        .pr_flags = PR_ATOMIC | PR_ADDR | PR_RIGHTS | PR_SYNC_PORT,\n        .pr_ctloutput = uipc_ctloutput,\n        .pr_usrreqs = &amp;uipc_usrreqs,\n    },\n    {\n        .pr_type = SOCK_SEQPACKET,\n        .pr_domain = &amp;localdomain,\n        .pr_flags = PR_ATOMIC | PR_CONNREQUIRED | PR_WANTRCVD |\n                    PR_RIGHTS | PR_SYNC_PORT,\n        .pr_ctloutput = uipc_ctloutput,\n        .pr_usrreqs = &amp;uipc_usrreqs,\n    },\n};\n\n/* Domain structure */\nstruct domain localdomain = {\n    .dom_family = AF_LOCAL,\n    .dom_name = \"local\",\n    .dom_init = unp_init,\n    .dom_externalize = unp_externalize,\n    .dom_dispose = unp_dispose,\n    .dom_protosw = localsw,\n    .dom_protoswNPROTOSW = &amp;localsw[NELEM(localsw)],\n};\n\n/* Register domain via SYSINIT */\nDOMAIN_SET(local);\n</code></pre> <p>Key observations:</p> <ul> <li><code>PR_SYNC_PORT</code> - All local domain operations use <code>netisr_sync_port</code> for serialization</li> <li><code>PR_RIGHTS</code> - File descriptor passing is supported</li> <li><code>PR_WANTRCVD</code> - Stream and seqpacket protocols need rcvd notifications for flow control</li> <li><code>dom_externalize</code> / <code>dom_dispose</code> - Hooks for FD passing control message handling</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#message-port-assignment","title":"Message Port Assignment","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#socket-creation","title":"Socket Creation","text":"<p>When a socket is created (<code>sys/kern/uipc_socket.c</code>):</p> <pre><code>int socreate(int dom, struct socket **aso, int type, int proto,\n             struct thread *td)\n{\n    struct protosw *prp;\n    struct socket *so;\n\n    /* Find protocol */\n    prp = pffindproto(dom, proto, type);\n    if (prp == NULL)\n        prp = pffindtype(dom, type);\n    if (prp == NULL)\n        return EPROTONOSUPPORT;\n\n    /* Allocate socket */\n    so = soalloc(1, prp);\n\n    /* Assign message port based on protocol flags */\n    if (prp-&gt;pr_flags &amp; PR_SYNC_PORT) {\n        so-&gt;so_port = netisr_sync_port;\n    } else {\n        so-&gt;so_port = netisr_cpuport(0);  /* CPU 0 by default */\n    }\n\n    /* Attach protocol */\n    error = so_pru_attach(so, proto, &amp;ai);\n    ...\n}\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#connection-accept","title":"Connection Accept","text":"<p>For accepted connections (<code>sys/kern/uipc_socket2.c</code>):</p> <pre><code>struct socket *sonewconn_faddr(struct socket *head, int connstatus,\n                               struct sockaddr *faddr, boolean_t keep_ref)\n{\n    struct socket *so;\n    struct protosw *prp = head-&gt;so_proto;\n\n    /* Allocate socket */\n    so = soalloc(1, prp);\n\n    /* Assign port - typically use current CPU */\n    if (prp-&gt;pr_flags &amp; PR_SYNC_PORT) {\n        so-&gt;so_port = netisr_sync_port;\n    } else {\n        so-&gt;so_port = netisr_cpuport(mycpuid);\n    }\n    ...\n}\n</code></pre> <p>Using <code>mycpuid</code> distributes accepted connections across CPUs for better scaling.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#initialization-timeline","title":"Initialization Timeline","text":"<pre><code>flowchart TD\n    boot[\"boot\"] --&gt; KMEM[\"SI_SUB_KMEM: Memory allocator ready\"]\n    KMEM --&gt; PROTO[\"SI_SUB_PROTO_DOMAIN: Domain registration\"]\n    PROTO --&gt; add1[\"net_add_domain(&amp;localdomain)\"]\n    PROTO --&gt; add2[\"net_add_domain(&amp;inetdomain)\"]\n    PROTO --&gt; addN[\"... (other domains)\"]\n    PROTO --&gt; PREDRV[\"SI_SUB_PRE_DRIVERS: Pre-driver init\"]\n    PREDRV --&gt; PROTOIF[\"SI_SUB_PROTO_IF: Network interface init\"]\n    PROTOIF --&gt; PROTOEND[\"SI_SUB_PROTO_END: Domain initialization\"]\n    PROTOEND --&gt; initdoms[\"net_init_domains()\"]\n    initdoms --&gt; initlocal[\"net_init_domain(&amp;localdomain)\"]\n    initlocal --&gt; usrreqs[\"pr_usrreqs_init() for each protocol\"]\n    initlocal --&gt; prinit[\"pr-&gt;pr_init() for each protocol\"]\n    initlocal --&gt; dominit[\"dom-&gt;dom_init() [unp_init]\"]\n    initdoms --&gt; initother[\"... (other domains)\"]\n    initother --&gt; OP[\"System operational\"]\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#error-handling","title":"Error Handling","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#default-handlers","title":"Default Handlers","text":"<p>Operations without protocol support return <code>EOPNOTSUPP</code>:</p> <pre><code>static void pr_generic_notsupp(netmsg_t msg)\n{\n    lwkt_replymsg(&amp;msg-&gt;lmsg, EOPNOTSUPP);\n}\n\nstatic struct pr_usrreqs pru_default_notsupp = {\n    .pru_abort = pr_generic_notsupp,\n    .pru_accept = pr_generic_notsupp,\n    .pru_attach = pr_generic_notsupp,\n    /* ... all handlers set to pr_generic_notsupp ... */\n};\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#protocol-lookup-failures","title":"Protocol Lookup Failures","text":"<p>Socket creation returns appropriate errors:</p> <ul> <li><code>EPROTONOSUPPORT</code> - Unknown protocol for the domain</li> <li><code>EAFNOSUPPORT</code> - Unknown address family (domain not registered)</li> <li><code>ESOCKTNOSUPPORT</code> - Socket type not supported by protocol</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#see-also","title":"See Also","text":"<ul> <li>Sockets - Socket layer implementation</li> <li>Unix Domain Sockets - Local domain implementation</li> <li>LWKT Threading - Message passing subsystem</li> <li>Mbufs - Memory buffer management</li> </ul>"},{"location":"sys/kern/ipc/sockets/","title":"Socket Layer","text":"<p>The socket layer provides the kernel interface for network communication in DragonFly BSD. It implements the BSD socket API, managing connection state, buffering, and protocol dispatch through a message-passing architecture optimized for multi-processor systems.</p>"},{"location":"sys/kern/ipc/sockets/#overview","title":"Overview","text":"<p>DragonFly's socket implementation extends the traditional BSD socket model with:</p> <ul> <li>Per-socket message ports for protocol thread routing</li> <li>LWKT tokens for fine-grained synchronization</li> <li>Reference counting with atomic operations</li> <li>Signaling socket buffers with integrated event notification</li> <li>Protocol-specific optimized send/receive paths</li> </ul>"},{"location":"sys/kern/ipc/sockets/#source-files","title":"Source Files","text":"File Description <code>sys/kern/uipc_socket.c</code> Core socket operations (create, bind, listen, accept, connect, send, receive, close) <code>sys/kern/uipc_socket2.c</code> Socket state management, wakeup routines, new connection handling <code>sys/kern/uipc_sockbuf.c</code> Socket buffer (sockbuf) mbuf chain manipulation <code>sys/sys/socketvar.h</code> Socket structure definitions and macros <code>sys/sys/sockbuf.h</code> Generic socket buffer definitions"},{"location":"sys/kern/ipc/sockets/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/sockets/#struct-socket","title":"struct socket","text":"<p>The primary socket structure (<code>sys/sys/socketvar.h:116</code>):</p> <pre><code>struct socket {\n    short   so_type;            /* generic type (SOCK_STREAM, etc.) */\n    short   so_options;         /* from socket call (SO_REUSEADDR, etc.) */\n    short   so_linger;          /* time to linger while closing */\n    short   so_state;           /* internal state flags SS_* */\n    void    *so_pcb;            /* protocol control block */\n    struct  protosw *so_proto;  /* protocol handle */\n    struct  socket *so_head;    /* back pointer to accept socket */\n    lwkt_port_t so_port;        /* message port for protocol thread */\n\n    /* Accept queue management */\n    TAILQ_HEAD(, socket) so_incomp;  /* incomplete connections */\n    TAILQ_HEAD(, socket) so_comp;    /* completed connections */\n    TAILQ_ENTRY(socket) so_list;     /* list of unaccepted connections */\n    short   so_qlen;            /* count of so_comp */\n    short   so_incqlen;         /* count of so_incomp */\n    short   so_qlimit;          /* max queued connections */\n\n    /* Error and signal handling */\n    u_short so_error;           /* error affecting connection */\n    u_short so_rerror;          /* error affecting receiving */\n    struct  sigio *so_sigio;    /* async I/O / SIGURG info */\n    u_long  so_oobmark;         /* chars to out-of-band mark */\n\n    /* Socket buffers */\n    struct signalsockbuf so_rcv;    /* receive buffer */\n    struct signalsockbuf so_snd;    /* send buffer */\n\n    /* Upcall support */\n    void    (*so_upcall)(struct socket *, void *, int);\n    void    *so_upcallarg;\n\n    /* Credentials and reference counting */\n    struct  ucred *so_cred;     /* user credentials */\n    int     so_refs;            /* reference count */\n\n    /* Async receive message handling */\n    struct spinlock so_rcvd_spin;\n    struct netmsg_pru_rcvd so_rcvd_msg;\n\n    uint32_t so_user_cookie;    /* user-specified metadata */\n};\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#struct-signalsockbuf","title":"struct signalsockbuf","text":"<p>The signaling socket buffer wraps the basic <code>struct sockbuf</code> with synchronization and event notification (<code>sys/sys/socketvar.h:72</code>):</p> <pre><code>struct signalsockbuf {\n    struct sockbuf sb;              /* embedded basic sockbuf */\n    struct kqinfo ssb_kq;           /* kqueue/select info */\n    struct notifymsglist ssb_mlist; /* pending predicate messages */\n    uint32_t ssb_flags;             /* flags (atomic operations) */\n    u_int   ssb_timeo;              /* timeout for read/write */\n    long    ssb_lowat;              /* low water mark */\n    u_long  ssb_hiwat;              /* high water mark */\n    u_long  ssb_mbmax;              /* max mbuf chars to use */\n    struct lwkt_token ssb_token;    /* frontend/backend serializer */\n};\n</code></pre> <p>The embedded <code>struct sockbuf</code> (<code>sys/sys/sockbuf.h:48</code>) manages the mbuf chain:</p> <pre><code>struct sockbuf {\n    u_long  sb_cc;              /* actual chars in buffer */\n    u_long  sb_mbcnt;           /* chars of mbufs used */\n    u_long  sb_cc_prealloc;     /* preallocated data count */\n    u_long  sb_mbcnt_prealloc;  /* preallocated mbuf count */\n    u_long  sb_climit;          /* data limit for I/O */\n    struct  mbuf *sb_mb;        /* the mbuf chain */\n    struct  mbuf *sb_lastmbuf;  /* last mbuf in chain */\n    struct  mbuf *sb_lastrecord;/* last record in chain */\n};\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#socket-state-flags-ss_","title":"Socket State Flags (SS_*)","text":"<p>Socket states are tracked via <code>so_state</code> (<code>sys/sys/socketvar.h:186</code>):</p> Flag Value Description <code>SS_NOFDREF</code> 0x0001 No file descriptor reference <code>SS_ISCONNECTED</code> 0x0002 Connected to peer <code>SS_ISCONNECTING</code> 0x0004 Connection in progress <code>SS_ISDISCONNECTING</code> 0x0008 Disconnection in progress <code>SS_CANTSENDMORE</code> 0x0010 Cannot send more data <code>SS_CANTRCVMORE</code> 0x0020 Cannot receive more data <code>SS_RCVATMARK</code> 0x0040 At out-of-band mark <code>SS_ISCLOSING</code> 0x0080 Close in progress <code>SS_ASSERTINPROG</code> 0x0100 Race debugging for sonewconn <code>SS_ASYNC</code> 0x0200 Async I/O notify enabled <code>SS_ISCONFIRMING</code> 0x0400 Deciding on connection request <code>SS_INCOMP</code> 0x0800 Incomplete connection <code>SS_COMP</code> 0x1000 Complete but unaccepted connection <code>SS_ISDISCONNECTED</code> 0x2000 Fully disconnected <code>SS_ACCEPTMECH</code> 0x4000 Allow bind override vs accepted"},{"location":"sys/kern/ipc/sockets/#signaling-sockbuf-flags-ssb_","title":"Signaling Sockbuf Flags (SSB_*)","text":"<p>Socket buffer flags (<code>sys/sys/socketvar.h:90</code>):</p> Flag Value Description <code>SSB_LOCK</code> 0x0001 Lock on data queue <code>SSB_WANT</code> 0x0002 Someone waiting for lock <code>SSB_WAIT</code> 0x0004 Someone waiting for data/space <code>SSB_ASYNC</code> 0x0010 Async I/O, need signals <code>SSB_UPCALL</code> 0x0020 Upcall requested <code>SSB_NOINTR</code> 0x0040 Operations not interruptible <code>SSB_KNOTE</code> 0x0100 Kernel note attached <code>SSB_MEVENT</code> 0x0200 Message event notification needed <code>SSB_STOP</code> 0x0400 Backpressure indicator <code>SSB_AUTOSIZE</code> 0x0800 Automatically size buffer <code>SSB_AUTOLOWAT</code> 0x1000 Automatically scale lowat <code>SSB_WAKEUP</code> 0x2000 Wakeup event race handling <code>SSB_PREALLOC</code> 0x4000 Preallocation supported <code>SSB_STOPSUPP</code> 0x8000 SSB_STOP supported"},{"location":"sys/kern/ipc/sockets/#socket-lifecycle","title":"Socket Lifecycle","text":""},{"location":"sys/kern/ipc/sockets/#socket-creation","title":"Socket Creation","text":"<p><code>socreate()</code> (<code>sys/kern/uipc_socket.c</code>) creates a new socket:</p> <pre><code>flowchart TD\n    A[\"socreate(domain, aso, type, proto, td)\"] --&gt; B[\"Find protocol via pffindtype() or pffindproto()\"]\n    B --&gt; C[\"soalloc(1, prp)\"]\n    C --&gt; C1[\"kmalloc(sizeof(struct socket), M_SOCKET, ...)\"]\n    C --&gt; C2[\"Initialize TAILQ heads for accept queues\"]\n    C --&gt; C3[\"Initialize ssb_token for both send/receive buffers\"]\n    C --&gt; D[\"Set so_proto, hold credentials (crhold)\"]\n    D --&gt; E{\"Assign message port\"}\n    E --&gt;|\"PR_SYNC_PORT\"| F1[\"netisr_sync_port\"]\n    E --&gt;|\"Default\"| F2[\"netisr_cpuport(0)\"]\n    F1 --&gt; G[\"so_pru_attach() \u2192 Protocol-specific attach\"]\n    F2 --&gt; G\n</code></pre> <p>Socket allocation uses <code>kmalloc()</code> with <code>M_SOCKET</code> type. Each socket gets LWKT tokens for its send and receive buffers.</p>"},{"location":"sys/kern/ipc/sockets/#binding-and-listening","title":"Binding and Listening","text":"<p><code>sobind()</code> binds a socket to a local address:</p> <pre><code>int sobind(struct socket *so, struct sockaddr *nam, struct thread *td)\n{\n    return so_pru_bind(so, nam, td);\n}\n</code></pre> <p><code>solisten()</code> marks a socket as accepting connections:</p> <pre><code>flowchart TD\n    A[\"solisten(so, backlog, td)\"] --&gt; B[\"Reject if already connected\"]\n    B --&gt; C[\"so_pru_listen() \u2192 Protocol-specific listen setup\"]\n    C --&gt; D[\"Set SS_ACCEPTCONN, configure so_qlimit\"]\n</code></pre> <p>The backlog parameter sets <code>so_qlimit</code>, clamped between 0 and <code>somaxconn</code> (default 128).</p>"},{"location":"sys/kern/ipc/sockets/#connection-establishment","title":"Connection Establishment","text":""},{"location":"sys/kern/ipc/sockets/#active-side-client","title":"Active Side (Client)","text":"<p><code>soconnect()</code> initiates a connection:</p> <pre><code>flowchart TD\n    A[\"soconnect(so, nam, td, sync)\"] --&gt; B[\"Verify not already connecting\"]\n    B --&gt; C[\"soisconnecting(so)\"]\n    C --&gt; C1[\"Set SS_ISCONNECTING, clear SS_ISCONNECTED\"]\n    C1 --&gt; D[\"so_pru_connect() \u2192 Protocol-specific connect\"]\n</code></pre> <p>The <code>sync</code> parameter controls whether to wait for completion.</p>"},{"location":"sys/kern/ipc/sockets/#passive-side-server","title":"Passive Side (Server)","text":"<p><code>sonewconn()</code> creates a socket for an incoming connection (<code>sys/kern/uipc_socket2.c:347</code>):</p> <pre><code>flowchart TD\n    A[\"sonewconn_faddr(head, connstatus, faddr, keep_ref)\"] --&gt; B[\"Check queue limits(so_qlen &gt; 3 * so_qlimit / 2)\"]\n    B --&gt; C[\"soalloc(1, head-&gt;so_proto)\"]\n    C --&gt; D{\"Assign message port\"}\n    D --&gt;|\"PR_SYNC_PORT\"| E1[\"netisr_sync_port\"]\n    D --&gt;|\"Default\"| E2[\"netisr_cpuport(mycpuid)\"]\n    E1 --&gt; F[\"Inherit options from head socket\"]\n    E2 --&gt; F\n    F --&gt; G[\"soreserve() \u2192 Reserve buffer space\"]\n    G --&gt; H[\"so_pru_attach_direct() \u2192 Protocol attach\"]\n    H --&gt; I{\"connstatus?\"}\n    I --&gt;|\"Yes (already connected)\"| J1[\"Insert into so_comp queue\"]\n    J1 --&gt; J2[\"Set SS_COMP\"]\n    J2 --&gt; J3[\"sorwakeup(head) \u2192 Wake acceptors\"]\n    I --&gt;|\"No (in progress)\"| K1[\"If so_incomp full, abort oldest\"]\n    K1 --&gt; K2[\"Insert into so_incomp queue\"]\n    K2 --&gt; K3[\"Set SS_INCOMP\"]\n</code></pre> <p>When a connection completes, <code>soisconnected()</code> moves it from <code>so_incomp</code> to <code>so_comp</code>:</p> <pre><code>flowchart TD\n    A[\"soisconnected(so)\"] --&gt; B[\"Get pool token for head socket\"]\n    B --&gt; C[\"Clear SS_ISCONNECTING, set SS_ISCONNECTED\"]\n    C --&gt; D{\"On incomp queue(SS_INCOMP)?\"}\n    D --&gt;|\"Yes\"| E1[\"Check for accept filter\"]\n    E1 --&gt; E2[\"TAILQ_REMOVE from so_incomp\"]\n    E2 --&gt; E3[\"TAILQ_INSERT_TAIL to so_comp\"]\n    E3 --&gt; E4[\"Set SS_COMP, clear SS_INCOMP\"]\n    E4 --&gt; E5[\"sorwakeup(head) \u2192 Wake acceptors\"]\n    D --&gt;|\"No\"| F[\"wakeup(&amp;so-&gt;so_timeo)\"]\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#accept","title":"Accept","text":"<p><code>soaccept()</code> extracts a completed connection:</p> <pre><code>int soaccept(struct socket *so, struct sockaddr **nam)\n{\n    int error;\n\n    if ((so-&gt;so_state &amp; SS_NOFDREF) == 0)\n        panic(\"soaccept: !NOFDREF\");\n    soclrstate(so, SS_NOFDREF);  /* File descriptor now references socket */\n    error = so_pru_accept_direct(so, nam);\n    return error;\n}\n</code></pre> <p>The accept happens in <code>kern_accept()</code> which removes the socket from <code>so_comp</code> queue.</p>"},{"location":"sys/kern/ipc/sockets/#data-transfer","title":"Data Transfer","text":""},{"location":"sys/kern/ipc/sockets/#sending-data","title":"Sending Data","text":"<p><code>sosend()</code> is the generic send function (<code>sys/kern/uipc_socket.c</code>):</p> <pre><code>flowchart TD\n    A[\"sosend(so, addr, uio, top, control, flags, td)\"] --&gt; B[\"Acquire ssb_lock on send buffer\"]\n    B --&gt; C[\"Loop while data remains\"]\n    C --&gt; D[\"Wait for space if needed (ssb_wait)\"]\n    D --&gt; E[\"Check for errors, signals\"]\n    E --&gt; F[\"Allocate mbufs for data\"]\n    F --&gt; F1[\"m_uiomove() for large transfers\"]\n    F --&gt; F2[\"Copy from uio to mbuf chain\"]\n    F1 --&gt; G[\"so_pru_send() \u2192 Protocol send\"]\n    F2 --&gt; G\n    G --&gt; C\n    G --&gt; H[\"Release ssb_lock\"]\n</code></pre> <p>Protocol-optimized variants:</p> <ul> <li><code>sosendtcp()</code> - Optimized for TCP streams</li> <li><code>sosendudp()</code> - Optimized for UDP datagrams</li> </ul>"},{"location":"sys/kern/ipc/sockets/#receiving-data","title":"Receiving Data","text":"<p><code>soreceive()</code> is the generic receive function:</p> <pre><code>flowchart TD\n    A[\"soreceive(so, paddr, uio, sio, controlp, flagsp)\"] --&gt; B[\"Acquire ssb_lock on receive buffer\"]\n    B --&gt; C[\"Handle out-of-band data if MSG_OOB\"]\n    C --&gt; D[\"Wait for data if needed (ssb_wait)\"]\n    D --&gt; E[\"Extract address (MT_SONAME) if present\"]\n    E --&gt; F[\"Extract control data (MT_CONTROL) if present\"]\n    F --&gt; G[\"Copy data to uio\"]\n    G --&gt; G1[\"Handle MSG_PEEK (don't remove data)\"]\n    G --&gt; G2[\"Track OOB mark position\"]\n    G --&gt; G3[\"sbdrop() to remove consumed data\"]\n    G1 --&gt; H[\"Release ssb_lock\"]\n    G2 --&gt; H\n    G3 --&gt; H\n</code></pre> <p><code>sorecvtcp()</code> provides an optimized path for TCP.</p>"},{"location":"sys/kern/ipc/sockets/#socket-shutdown-and-close","title":"Socket Shutdown and Close","text":"<p><code>soshutdown()</code> performs half-close:</p> <pre><code>int soshutdown(struct socket *so, int how)\n{\n    if (how != SHUT_WR)\n        sorflush(so);           /* Flush receive buffer */\n    if (how != SHUT_RD)\n        return so_pru_shutdown(so);\n    return 0;\n}\n</code></pre> <p><code>soclose()</code> fully closes a socket:</p> <pre><code>flowchart TD\n    A[\"soclose(so, fflag)\"] --&gt; B{\"Listening?\"}\n    B --&gt;|\"Yes\"| C[\"Drop all pending connections\"]\n    C --&gt; C1[\"For each in so_incomp: soabort_async()\"]\n    C --&gt; C2[\"For each in so_comp: soabort_async()\"]\n    C1 --&gt; D{\"Connected and SO_LINGER?\"}\n    C2 --&gt; D\n    B --&gt;|\"No\"| D\n    D --&gt;|\"Yes\"| E1[\"sodisconnect()\"]\n    E1 --&gt; E2[\"Wait up to so_linger time\"]\n    E2 --&gt; F[\"Drop protocol attachment\"]\n    D --&gt;|\"No\"| F\n    F --&gt; G[\"sofree(so)\"]\n</code></pre> <p><code>sofree()</code> handles reference counting and final cleanup:</p> <pre><code>flowchart TD\n    A[\"sofree(so)\"] --&gt; B[\"Decrement so_refs atomically\"]\n    B --&gt; C{\"refs == 0 and SS_NOFDREF?\"}\n    C --&gt;|\"Yes\"| D[\"Remove from head's queue if applicable\"]\n    D --&gt; E[\"ssb_release() for both buffers\"]\n    E --&gt; F[\"crfree(so_cred)\"]\n    F --&gt; G[\"kfree(so, M_SOCKET)\"]\n    C --&gt;|\"No\"| H[\"Return\"]\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#socket-buffers","title":"Socket Buffers","text":""},{"location":"sys/kern/ipc/sockets/#buffer-space-management","title":"Buffer Space Management","text":"<p><code>soreserve()</code> reserves space for both buffers (<code>sys/kern/uipc_socket2.c:664</code>):</p> <pre><code>int soreserve(struct socket *so, u_long sndcc, u_long rcvcc, struct rlimit *rl)\n{\n    if (so-&gt;so_snd.ssb_lowat == 0)\n        atomic_set_int(&amp;so-&gt;so_snd.ssb_flags, SSB_AUTOLOWAT);\n    if (ssb_reserve(&amp;so-&gt;so_snd, sndcc, so, rl) == 0)\n        goto bad;\n    if (ssb_reserve(&amp;so-&gt;so_rcv, rcvcc, so, rl) == 0)\n        goto bad2;\n    /* Set default lowat values */\n    if (so-&gt;so_rcv.ssb_lowat == 0)\n        so-&gt;so_rcv.ssb_lowat = 1;\n    if (so-&gt;so_snd.ssb_lowat == 0)\n        so-&gt;so_snd.ssb_lowat = MCLBYTES;\n    ...\n}\n</code></pre> <p><code>ssb_reserve()</code> allocates buffer space with resource limits:</p> <pre><code>int ssb_reserve(struct signalsockbuf *ssb, u_long cc, struct socket *so,\n                struct rlimit *rl)\n{\n    /* Apply sb_max limit for user sockets */\n    if (rl &amp;&amp; cc &gt; sb_max_adj)\n        cc = sb_max_adj;\n    /* Account against user's resource limits */\n    if (!chgsbsize(so-&gt;so_cred-&gt;cr_uidinfo, &amp;ssb-&gt;ssb_hiwat, cc, ...))\n        return 0;\n    /* Set mbuf limit based on efficiency factor */\n    ssb-&gt;ssb_mbmax = cc * sb_efficiency;  /* default: cc * 8 */\n    ...\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#space-calculation","title":"Space Calculation","text":"<p><code>ssb_space()</code> returns available buffer space (<code>sys/sys/socketvar.h:270</code>):</p> <pre><code>static __inline long ssb_space(struct signalsockbuf *ssb)\n{\n    long bleft, mleft;\n\n    if (ssb-&gt;ssb_flags &amp; SSB_STOP)\n        return 0;  /* Backpressure active */\n    bleft = ssb-&gt;ssb_hiwat - ssb-&gt;ssb_cc;\n    mleft = ssb-&gt;ssb_mbmax - ssb-&gt;ssb_mbcnt;\n    return (bleft &lt; mleft) ? bleft : mleft;\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#appending-data","title":"Appending Data","text":"<p><code>sbappend()</code> appends mbufs to the buffer (<code>sys/kern/uipc_sockbuf.c:83</code>):</p> <pre><code>void sbappend(struct sockbuf *sb, struct mbuf *m)\n{\n    struct mbuf *n;\n\n    if (m) {\n        n = sb-&gt;sb_lastrecord;\n        if (n) {\n            if (n-&gt;m_flags &amp; M_EOR) {\n                sbappendrecord(sb, m);  /* Start new record */\n                return;\n            }\n        }\n        n = sb-&gt;sb_lastmbuf;\n        if (n) {\n            if (n-&gt;m_flags &amp; M_EOR) {\n                sbappendrecord(sb, m);\n                return;\n            }\n        }\n        sbcompress(sb, m, n);  /* Compress into existing */\n    }\n}\n</code></pre> <p><code>sbappendstream()</code> is optimized for stream protocols (TCP):</p> <pre><code>void sbappendstream(struct sockbuf *sb, struct mbuf *m)\n{\n    KKASSERT(m-&gt;m_nextpkt == NULL);\n    sbcompress(sb, m, sb-&gt;sb_lastmbuf);\n}\n</code></pre> <p><code>sbappendrecord()</code> starts a new record:</p> <pre><code>void sbappendrecord(struct sockbuf *sb, struct mbuf *m0)\n{\n    /* Break first mbuf off from chain */\n    firstmbuf = m0;\n    secondmbuf = m0-&gt;m_next;\n    m0-&gt;m_next = NULL;\n\n    /* Insert as new record */\n    if (sb-&gt;sb_mb == NULL)\n        sb-&gt;sb_mb = firstmbuf;\n    else\n        sb-&gt;sb_lastrecord-&gt;m_nextpkt = firstmbuf;\n    sb-&gt;sb_lastrecord = firstmbuf;\n    sb-&gt;sb_lastmbuf = firstmbuf;\n    sballoc(sb, firstmbuf);\n\n    /* Compress rest of chain */\n    sbcompress(sb, secondmbuf, firstmbuf);\n}\n</code></pre> <p><code>sbappendaddr()</code> prepends sender's address (for datagram sockets):</p> <pre><code>int sbappendaddr(struct sockbuf *sb, const struct sockaddr *asa,\n                 struct mbuf *m0, struct mbuf *control)\n{\n    /* Allocate mbuf for address */\n    MGET(m, M_NOWAIT, MT_SONAME);\n    m-&gt;m_len = asa-&gt;sa_len;\n    bcopy(asa, mtod(m, caddr_t), asa-&gt;sa_len);\n\n    /* Chain: address \u2192 control \u2192 data */\n    if (n)  /* control tail */\n        n-&gt;m_next = m0;\n    else\n        control = m0;\n    m-&gt;m_next = control;\n\n    /* Insert as new record */\n    ...\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#compression","title":"Compression","text":"<p><code>sbcompress()</code> coalesces small mbufs (<code>sys/kern/uipc_sockbuf.c:339</code>):</p> <pre><code>void sbcompress(struct sockbuf *sb, struct mbuf *m, struct mbuf *tailm)\n{\n    while (m) {\n        /* Skip empty mbufs unless EOR */\n        if (m-&gt;m_len == 0 &amp;&amp; (eor == 0 || ...)) {\n            /* defer to free_chain */\n            continue;\n        }\n\n        /* Try to coalesce with preceding mbuf */\n        if (tailm &amp;&amp; !(tailm-&gt;m_flags &amp; (M_EOR | M_SOLOCKED)) &amp;&amp;\n            M_WRITABLE(tailm) &amp;&amp;\n            m-&gt;m_len &lt;= MCLBYTES / 4 &amp;&amp;     /* Don't copy too much */\n            m-&gt;m_len &lt;= M_TRAILINGSPACE(tailm) &amp;&amp;\n            tailm-&gt;m_type == m-&gt;m_type) {\n            /* Copy data to tail of existing mbuf */\n            bcopy(mtod(m, caddr_t),\n                  mtod(tailm, caddr_t) + tailm-&gt;m_len,\n                  m-&gt;m_len);\n            tailm-&gt;m_len += m-&gt;m_len;\n            sb-&gt;sb_cc += m-&gt;m_len;\n            /* Move to free chain */\n            continue;\n        }\n\n        /* Insert whole mbuf */\n        if (tailm == NULL) {\n            sb-&gt;sb_mb = m;\n            sb-&gt;sb_lastrecord = m;\n        } else {\n            tailm-&gt;m_next = m;\n        }\n        sb-&gt;sb_lastmbuf = m;\n        sballoc(sb, m);\n        tailm = m;\n        m = m-&gt;m_next;\n        tailm-&gt;m_next = NULL;\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#dropping-data","title":"Dropping Data","text":"<p><code>sbdrop()</code> removes data from the front (<code>sys/kern/uipc_sockbuf.c:472</code>):</p> <pre><code>void sbdrop(struct sockbuf *sb, int len)\n{\n    struct mbuf *m;\n\n    crit_enter();\n    m = sb-&gt;sb_mb;\n    while (m &amp;&amp; len &gt; 0) {\n        if (m-&gt;m_len &gt; len) {\n            m-&gt;m_len -= len;\n            m-&gt;m_data += len;\n            sb-&gt;sb_cc -= len;\n            break;\n        }\n        len -= m-&gt;m_len;\n        m = sbunlinkmbuf(sb, m, &amp;free_chain);\n        if (m == NULL &amp;&amp; len)\n            m = sb-&gt;sb_mb;  /* Move to next record */\n    }\n    /* Remove trailing zero-length mbufs */\n    while (m &amp;&amp; m-&gt;m_len == 0)\n        m = sbunlinkmbuf(sb, m, &amp;free_chain);\n    crit_exit();\n\n    if (free_chain)\n        m_freem(free_chain);\n}\n</code></pre> <p><code>sbdroprecord()</code> removes entire first record:</p> <pre><code>void sbdroprecord(struct sockbuf *sb)\n{\n    struct mbuf *m, *n;\n\n    m = sb-&gt;sb_mb;\n    if (m) {\n        sb-&gt;sb_mb = m-&gt;m_nextpkt;  /* Advance to next record */\n        if (sb-&gt;sb_mb == NULL) {\n            sb-&gt;sb_lastrecord = NULL;\n            sb-&gt;sb_lastmbuf = NULL;\n        }\n        m-&gt;m_nextpkt = NULL;\n        for (n = m; n; n = n-&gt;m_next)\n            sbfree(sb, n);\n        m_freem(m);\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#wakeup-and-notification","title":"Wakeup and Notification","text":""},{"location":"sys/kern/ipc/sockets/#sowakeup","title":"sowakeup()","text":"<p><code>sowakeup()</code> notifies waiters of buffer state changes (<code>sys/kern/uipc_socket2.c:551</code>):</p> <pre><code>void sowakeup(struct socket *so, struct signalsockbuf *ssb)\n{\n    uint32_t flags;\n\n    /* Fast path: if WAKEUP already set and no special features */\n    flags = atomic_fetchadd_int(&amp;ssb-&gt;ssb_flags, 0);\n    if ((flags &amp; SSB_NOTIFY_MASK) == 0) {\n        if (flags &amp; SSB_WAKEUP)\n            return;\n    }\n\n    /* Check conditions and set WAKEUP flag */\n    for (;;) {\n        long space;\n        flags = ssb-&gt;ssb_flags;\n\n        if (ssb-&gt;ssb_flags &amp; SSB_PREALLOC)\n            space = ssb_space_prealloc(ssb);\n        else\n            space = ssb_space(ssb);\n\n        /* Wake if: space available, data ready, or end condition */\n        if ((ssb == &amp;so-&gt;so_snd &amp;&amp; space &gt;= ssb-&gt;ssb_lowat) ||\n            (ssb == &amp;so-&gt;so_rcv &amp;&amp; ssb-&gt;ssb_cc &gt;= ssb-&gt;ssb_lowat) ||\n            (ssb == &amp;so-&gt;so_snd &amp;&amp; (so-&gt;so_state &amp; SS_CANTSENDMORE)) ||\n            (ssb == &amp;so-&gt;so_rcv &amp;&amp; (so-&gt;so_state &amp; SS_CANTRCVMORE))) {\n            if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags,\n                              (flags | SSB_WAKEUP) &amp; ~SSB_WAIT)) {\n                if (flags &amp; SSB_WAIT)\n                    wakeup(&amp;ssb-&gt;ssb_cc);\n                break;\n            }\n        } else {\n            break;\n        }\n    }\n\n    /* Handle async signals and upcalls */\n    if ((so-&gt;so_state &amp; SS_ASYNC) &amp;&amp; so-&gt;so_sigio != NULL)\n        pgsigio(so-&gt;so_sigio, SIGIO, 0);\n    if (ssb-&gt;ssb_flags &amp; SSB_UPCALL)\n        (*so-&gt;so_upcall)(so, so-&gt;so_upcallarg, M_NOWAIT);\n    KNOTE(&amp;ssb-&gt;ssb_kq.ki_note, 0);\n\n    /* Process predicate message notifications */\n    if (ssb-&gt;ssb_flags &amp; SSB_MEVENT) {\n        /* Scan ssb_mlist and reply to satisfied predicates */\n        ...\n    }\n}\n</code></pre> <p>Convenience macros: - <code>sorwakeup(so)</code> \u2192 <code>sowakeup(so, &amp;so-&gt;so_rcv)</code> - <code>sowwakeup(so)</code> \u2192 <code>sowakeup(so, &amp;so-&gt;so_snd)</code></p>"},{"location":"sys/kern/ipc/sockets/#ssb_wait","title":"ssb_wait()","text":"<p><code>ssb_wait()</code> blocks waiting for buffer state change (<code>sys/kern/uipc_socket2.c:103</code>):</p> <pre><code>int ssb_wait(struct signalsockbuf *ssb)\n{\n    uint32_t flags;\n    int pflags, error;\n\n    pflags = (ssb-&gt;ssb_flags &amp; SSB_NOINTR) ? 0 : PCATCH;\n\n    for (;;) {\n        flags = ssb-&gt;ssb_flags;\n        cpu_ccfence();\n\n        /* Check if WAKEUP already set */\n        if (flags &amp; SSB_WAKEUP) {\n            if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags,\n                                  flags &amp; ~SSB_WAKEUP)) {\n                error = 0;\n                break;\n            }\n            continue;\n        }\n\n        /* Set WAIT and sleep */\n        tsleep_interlock(&amp;ssb-&gt;ssb_cc, pflags);\n        if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags, flags | SSB_WAIT)) {\n            error = tsleep(&amp;ssb-&gt;ssb_cc, pflags | PINTERLOCKED,\n                           \"sbwait\", ssb-&gt;ssb_timeo);\n            break;\n        }\n    }\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#connection-state-transitions","title":"Connection State Transitions","text":"<p><code>soisconnecting()</code> - marks connection attempt starting: <pre><code>void soisconnecting(struct socket *so)\n{\n    soclrstate(so, SS_ISCONNECTED | SS_ISDISCONNECTING);\n    sosetstate(so, SS_ISCONNECTING);\n}\n</code></pre></p> <p><code>soisdisconnecting()</code> - marks graceful disconnect starting: <pre><code>void soisdisconnecting(struct socket *so)\n{\n    soclrstate(so, SS_ISCONNECTING);\n    sosetstate(so, SS_ISDISCONNECTING | SS_CANTRCVMORE | SS_CANTSENDMORE);\n    wakeup(&amp;so-&gt;so_timeo);\n    sowwakeup(so);\n    sorwakeup(so);\n}\n</code></pre></p> <p><code>soisdisconnected()</code> - marks socket fully disconnected: <pre><code>void soisdisconnected(struct socket *so)\n{\n    soclrstate(so, SS_ISCONNECTING | SS_ISCONNECTED | SS_ISDISCONNECTING);\n    sosetstate(so, SS_CANTRCVMORE | SS_CANTSENDMORE | SS_ISDISCONNECTED);\n    wakeup(&amp;so-&gt;so_timeo);\n    sbdrop(&amp;so-&gt;so_snd.sb, so-&gt;so_snd.ssb_cc);  /* Discard pending sends */\n    sowwakeup(so);\n    sorwakeup(so);\n}\n</code></pre></p> <p><code>socantsendmore()</code> / <code>socantrcvmore()</code> - half-close notifications: <pre><code>void socantsendmore(struct socket *so)\n{\n    sosetstate(so, SS_CANTSENDMORE);\n    sowwakeup(so);\n}\n\nvoid socantrcvmore(struct socket *so)\n{\n    sosetstate(so, SS_CANTRCVMORE);\n    sorwakeup(so);\n}\n</code></pre></p>"},{"location":"sys/kern/ipc/sockets/#socket-options","title":"Socket Options","text":""},{"location":"sys/kern/ipc/sockets/#sosetopt","title":"sosetopt()","text":"<p><code>sosetopt()</code> sets socket options (<code>sys/kern/uipc_socket.c</code>):</p> <pre><code>int sosetopt(struct socket *so, struct sockopt *sopt)\n{\n    switch (sopt-&gt;sopt_name) {\n    case SO_LINGER:\n        /* Set linger time on close */\n        so-&gt;so_linger = l.l_linger;\n        if (l.l_onoff)\n            so-&gt;so_options |= SO_LINGER;\n        else\n            so-&gt;so_options &amp;= ~SO_LINGER;\n        break;\n\n    case SO_SNDBUF:\n        /* Set send buffer size */\n        ssb_reserve(&amp;so-&gt;so_snd, optval, so, ...);\n        break;\n\n    case SO_RCVBUF:\n        /* Set receive buffer size */\n        ssb_reserve(&amp;so-&gt;so_rcv, optval, so, ...);\n        break;\n\n    case SO_SNDLOWAT:\n    case SO_RCVLOWAT:\n        /* Set low water marks */\n        ssb-&gt;ssb_lowat = (optval &gt; ssb-&gt;ssb_hiwat) ?\n                         ssb-&gt;ssb_hiwat : optval;\n        break;\n\n    case SO_SNDTIMEO:\n    case SO_RCVTIMEO:\n        /* Set timeout values */\n        ssb-&gt;ssb_timeo = val;\n        break;\n\n    /* Boolean options */\n    case SO_DEBUG:\n    case SO_KEEPALIVE:\n    case SO_DONTROUTE:\n    case SO_BROADCAST:\n    case SO_REUSEADDR:\n    case SO_REUSEPORT:\n    case SO_OOBINLINE:\n    case SO_TIMESTAMP:\n    case SO_NOSIGPIPE:\n        if (optval)\n            so-&gt;so_options |= sopt-&gt;sopt_name;\n        else\n            so-&gt;so_options &amp;= ~sopt-&gt;sopt_name;\n        break;\n    }\n\n    /* Forward to protocol if handler exists */\n    if (so-&gt;so_proto-&gt;pr_ctloutput)\n        so_pr_ctloutput(so, sopt);\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#sogetopt","title":"sogetopt()","text":"<p><code>sogetopt()</code> retrieves socket options with similar structure, copying values to user space.</p>"},{"location":"sys/kern/ipc/sockets/#kqueue-integration","title":"kqueue Integration","text":""},{"location":"sys/kern/ipc/sockets/#filter-operations","title":"Filter Operations","text":"<p>Socket kqueue filters (<code>sys/kern/uipc_socket.c:2549</code>):</p> <pre><code>int sokqfilter(struct file *fp, struct knote *kn)\n{\n    struct socket *so = (struct socket *)kn-&gt;kn_fp-&gt;f_data;\n\n    switch (kn-&gt;kn_filter) {\n    case EVFILT_READ:\n        if (so-&gt;so_options &amp; SO_ACCEPTCONN)\n            kn-&gt;kn_fop = &amp;solisten_filtops;  /* Listen socket */\n        else\n            kn-&gt;kn_fop = &amp;soread_filtops;    /* Data socket */\n        ssb = &amp;so-&gt;so_rcv;\n        break;\n    case EVFILT_WRITE:\n        kn-&gt;kn_fop = &amp;sowrite_filtops;\n        ssb = &amp;so-&gt;so_snd;\n        break;\n    case EVFILT_EXCEPT:\n        kn-&gt;kn_fop = &amp;soexcept_filtops;\n        ssb = &amp;so-&gt;so_rcv;\n        break;\n    }\n\n    knote_insert(&amp;ssb-&gt;ssb_kq.ki_note, kn);\n    atomic_set_int(&amp;ssb-&gt;ssb_flags, SSB_KNOTE);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#read-filter","title":"Read Filter","text":"<p><code>filt_soread()</code> checks read readiness:</p> <pre><code>static int filt_soread(struct knote *kn, long hint)\n{\n    struct socket *so = (struct socket *)kn-&gt;kn_fp-&gt;f_data;\n\n    /* Handle out-of-band data request */\n    if (kn-&gt;kn_sfflags &amp; NOTE_OOB) {\n        if (so-&gt;so_oobmark || (so-&gt;so_state &amp; SS_RCVATMARK)) {\n            kn-&gt;kn_fflags |= NOTE_OOB;\n            return 1;\n        }\n        return 0;\n    }\n\n    kn-&gt;kn_data = so-&gt;so_rcv.ssb_cc;  /* Available data */\n\n    /* Handle EOF conditions */\n    if (so-&gt;so_state &amp; SS_CANTRCVMORE) {\n        if (kn-&gt;kn_data == 0)\n            kn-&gt;kn_flags |= EV_NODATA;\n        kn-&gt;kn_flags |= EV_EOF;\n        kn-&gt;kn_fflags = so-&gt;so_error;\n        if (so-&gt;so_state &amp; SS_CANTSENDMORE)\n            kn-&gt;kn_flags |= EV_HUP;\n        return 1;\n    }\n\n    if (so-&gt;so_error || so-&gt;so_rerror)\n        return 1;\n\n    /* Check low water mark */\n    if (kn-&gt;kn_sfflags &amp; NOTE_LOWAT)\n        return (kn-&gt;kn_data &gt;= kn-&gt;kn_sdata);\n    return (kn-&gt;kn_data &gt;= so-&gt;so_rcv.ssb_lowat) ||\n           !TAILQ_EMPTY(&amp;so-&gt;so_comp);\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#listen-filter","title":"Listen Filter","text":"<p><code>filt_solisten()</code> checks for pending connections:</p> <pre><code>static int filt_solisten(struct knote *kn, long hint)\n{\n    struct socket *so = (struct socket *)kn-&gt;kn_fp-&gt;f_data;\n    int qlen = so-&gt;so_qlen;\n\n    if (soavailconn &gt; 0 &amp;&amp; qlen &gt; soavailconn)\n        qlen = soavailconn;\n    kn-&gt;kn_data = qlen;\n\n    return !TAILQ_EMPTY(&amp;so-&gt;so_comp);\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/sockets/#state-manipulation","title":"State Manipulation","text":"<p>Socket state is modified atomically:</p> <pre><code>static __inline void sosetstate(struct socket *so, int state)\n{\n    atomic_set_int(&amp;so-&gt;so_state, state);\n}\n\nstatic __inline void soclrstate(struct socket *so, int state)\n{\n    atomic_clear_int(&amp;so-&gt;so_state, state);\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#buffer-locking","title":"Buffer Locking","text":"<p><code>_ssb_lock()</code> acquires exclusive buffer access (<code>sys/kern/uipc_socket2.c:148</code>):</p> <pre><code>int _ssb_lock(struct signalsockbuf *ssb)\n{\n    uint32_t flags;\n    int pflags, error;\n\n    pflags = (ssb-&gt;ssb_flags &amp; SSB_NOINTR) ? 0 : PCATCH;\n\n    for (;;) {\n        flags = ssb-&gt;ssb_flags;\n        if (flags &amp; SSB_LOCK) {\n            /* Already locked, wait */\n            tsleep_interlock(&amp;ssb-&gt;ssb_flags, pflags);\n            if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags,\n                                  flags | SSB_WANT)) {\n                error = tsleep(&amp;ssb-&gt;ssb_flags,\n                               pflags | PINTERLOCKED, \"sblock\", 0);\n                if (error)\n                    break;\n            }\n        } else {\n            /* Acquire lock */\n            if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags,\n                                  flags | SSB_LOCK)) {\n                lwkt_gettoken(&amp;ssb-&gt;ssb_token);\n                error = 0;\n                break;\n            }\n        }\n    }\n    return error;\n}\n</code></pre> <p>The inline <code>ssb_lock()</code> provides the fast path.</p>"},{"location":"sys/kern/ipc/sockets/#reference-counting","title":"Reference Counting","text":"<p>Socket references prevent premature deallocation:</p> <pre><code>static __inline void soreference(struct socket *so)\n{\n    atomic_add_int(&amp;so-&gt;so_refs, 1);\n}\n\nvoid sofree(struct socket *so)\n{\n    /* Atomically decrement and check */\n    if (atomic_fetchadd_int(&amp;so-&gt;so_refs, -1) != 1)\n        return;\n    /* Last reference, actually free if SS_NOFDREF */\n    ...\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#sysctl-parameters","title":"Sysctl Parameters","text":"<p>Key tunable parameters (<code>sys/kern/uipc_socket2.c</code>):</p> Sysctl Default Description <code>kern.ipc.maxsockbuf</code> 512KB Maximum socket buffer size <code>kern.ipc.maxsockets</code> varies Maximum number of sockets <code>kern.ipc.sockbuf_waste_factor</code> 8 Buffer efficiency factor <code>kern.ipc.soaccept_reuse</code> 1 Allow quick local port reuse <code>kern.ipc.somaxconn</code> 128 Maximum listen backlog"},{"location":"sys/kern/ipc/sockets/#accept-filters","title":"Accept Filters","text":"<p>Accept filters allow deferred connection acceptance:</p> <pre><code>struct accept_filter {\n    char    accf_name[16];\n    void    (*accf_callback)(struct socket *so, void *arg, int waitflag);\n    void *  (*accf_create)(struct socket *so, char *arg);\n    void    (*accf_destroy)(struct socket *so);\n    SLIST_ENTRY(accept_filter) accf_next;\n};\n</code></pre> <p>When <code>SO_ACCEPTFILTER</code> is set, new connections go through the filter callback before being moved to <code>so_comp</code>. This allows filtering based on initial data (e.g., HTTP request line).</p>"},{"location":"sys/kern/ipc/sockets/#error-handling","title":"Error Handling","text":""},{"location":"sys/kern/ipc/sockets/#connection-errors","title":"Connection Errors","text":"<p>Errors are stored in <code>so_error</code> and propagated to waiters:</p> <pre><code>/* In protocol code when error occurs */\nso-&gt;so_error = error;\nsorwakeup(so);\nsowwakeup(so);\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#receive-buffer-errors","title":"Receive Buffer Errors","text":"<p>Receive-specific errors use <code>so_rerror</code>:</p> <pre><code>void soroverflow(struct socket *so)\n{\n    if (so-&gt;so_options &amp; SO_RERROR) {\n        so-&gt;so_rerror = ENOBUFS;\n        sorwakeup(so);\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#see-also","title":"See Also","text":"<ul> <li>Mbufs - Memory buffer management</li> <li>IPC Overview - Inter-process communication</li> <li>LWKT Threading - Thread subsystem</li> <li>Synchronization - Locking primitives</li> </ul>"},{"location":"sys/kern/ipc/sysv-msg/","title":"System V Message Queues","text":"<p>System V message queues provide inter-process communication through kernel-managed message buffers. DragonFly's implementation derives from FreeBSD and follows the SVID (System V Interface Definition) specification.</p> <p>Source files: - <code>sys/kern/sysv_msg.c</code> - Implementation - <code>sys/sys/msg.h</code> - Public interface</p>"},{"location":"sys/kern/ipc/sysv-msg/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/sysv-msg/#message-queue-descriptor","title":"Message Queue Descriptor","text":"<pre><code>struct msqid_ds {\n    struct  ipc_perm msg_perm;  /* permission bits */\n    struct  msg *msg_first;     /* first message in queue */\n    struct  msg *msg_last;      /* last message in queue */\n    msglen_t msg_cbytes;        /* bytes currently in queue */\n    msgqnum_t msg_qnum;         /* number of messages */\n    msglen_t msg_qbytes;        /* max bytes allowed */\n    pid_t   msg_lspid;          /* last msgsnd() pid */\n    pid_t   msg_lrpid;          /* last msgrcv() pid */\n    time_t  msg_stime;          /* last msgsnd() time */\n    time_t  msg_rtime;          /* last msgrcv() time */\n    time_t  msg_ctime;          /* last msgctl() time */\n};\n</code></pre> <p>Defined in <code>sys/sys/msg.h:68-84</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#internal-message-header","title":"Internal Message Header","text":"<pre><code>struct msg {\n    struct  msg *msg_next;  /* next msg in chain */\n    long    msg_type;       /* message type (&gt;0) or 0 if free */\n    u_short msg_ts;         /* message size in bytes */\n    short   msg_spot;       /* index of first segment in msgpool */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_msg.c:45-52</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#segment-map","title":"Segment Map","text":"<pre><code>struct msgmap {\n    short   next;   /* next segment index, or -1 if available */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_msg.c:104-108</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#memory-layout","title":"Memory Layout","text":"<p>Messages are stored in a segmented buffer pool:</p> <pre><code>block-beta\n    columns 5\n\n    block:pool:5\n        seg0[\"seg 0\"]\n        seg1[\"seg 1\"]\n        seg2[\"seg 2\"]\n        seg3[\"seg 3\"]\n        segN[\"...\"]\n    end\n\n    space:5\n\n    msgmaps[\"msgmaps[] - tracks segment linkage via indices\"]:5\n    msghdrs[\"msghdrs[] - preallocated message headers\"]:5\n    msqids[\"msqids[] - preallocated queue descriptors\"]:5\n\n    msgmaps --&gt; seg0\n    msgmaps --&gt; seg1\n</code></pre> <p>The <code>msgpool</code> contains MSGMAX bytes divided into segments.</p> <p>Each segment is <code>MSGSSZ</code> bytes (default 8, must be power of 2 between 8-1024). Segments are linked via <code>msgmaps[].next</code> forming a free list or per-message chain.</p>"},{"location":"sys/kern/ipc/sysv-msg/#system-limits","title":"System Limits","text":"Parameter Default Description <code>MSGSSZ</code> 8 Segment size (bytes, power of 2) <code>MSGSEG</code> 2048 Total segments (&lt;32767) <code>MSGMAX</code> MSGSSZ*MSGSEG Max message size <code>MSGMNB</code> 2048 Max bytes per queue <code>MSGMNI</code> 40 Max queue identifiers <code>MSGTQL</code> 40 Max messages system-wide <p>Defined in <code>sys/kern/sysv_msg.c:55-70</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#synchronization","title":"Synchronization","text":"<p>A single LWKT token protects all message queue operations:</p> <pre><code>static struct lwkt_token msg_token = LWKT_TOKEN_INITIALIZER(msg_token);\n</code></pre> <p>Defined at <code>sys/kern/sysv_msg.c:119</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#queue-locking","title":"Queue Locking","text":"<p>The <code>MSG_LOCKED</code> flag (value <code>01000</code>) in <code>msg_perm.mode</code> prevents queue reallocation while a process is copying message data to/from userspace:</p> <pre><code>#define MSG_LOCKED  01000\n</code></pre> <p>This lock serializes concurrent senders when resources are scarce, ensuring first-come-first-served semantics.</p>"},{"location":"sys/kern/ipc/sysv-msg/#initialization","title":"Initialization","text":"<p><code>msginit()</code> runs at <code>SI_SUB_SYSV_MSG</code>:</p> <ol> <li>Allocates <code>msgpool</code> (MSGMAX bytes)</li> <li>Allocates <code>msgmaps[]</code> (MSGSEG entries)</li> <li>Allocates <code>msghdrs[]</code> (MSGTQL entries)</li> <li>Allocates <code>msqids[]</code> (MSGMNI entries)</li> <li>Validates <code>msgssz</code> is power of 2 in [8,1024]</li> <li>Links free segment list via <code>free_msgmaps</code></li> <li>Links free header list via <code>free_msghdrs</code></li> <li>Marks all queue slots as available (<code>msg_qbytes = 0</code>)</li> </ol> <p>See <code>sys/kern/sysv_msg.c:121-174</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/sysv-msg/#msgget-create-or-access-queue","title":"msgget - Create or Access Queue","text":"<pre><code>int sys_msgget(struct sysmsg *sysmsg, const struct msgget_args *uap)\n</code></pre> <p>Arguments: <code>key</code>, <code>msgflg</code></p> <p>Operation: 1. Check jail capabilities (<code>PRISON_CAP_SYS_SYSVIPC</code>) 2. If <code>key != IPC_PRIVATE</code>, search for existing queue with matching key 3. If found and <code>IPC_CREAT|IPC_EXCL</code> set, return <code>EEXIST</code> 4. If not found and <code>IPC_CREAT</code> set, allocate new slot 5. Initialize permissions, timestamps, byte limits 6. Return unique msqid: <code>(index &amp; 0xffff) | (seq &lt;&lt; 16)</code></p> <p>The sequence number prevents stale ID reuse after queue deletion.</p> <p>See <code>sys/kern/sysv_msg.c:344-450</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#msgsnd-send-message","title":"msgsnd - Send Message","text":"<pre><code>int sys_msgsnd(struct sysmsg *sysmsg, const struct msgsnd_args *uap)\n</code></pre> <p>Arguments: <code>msqid</code>, <code>msgp</code>, <code>msgsz</code>, <code>msgflg</code></p> <p>Operation: 1. Validate msqid and permissions (<code>IPC_W</code>) 2. Calculate segments needed: <code>howmany(msgsz, msgssz)</code> 3. Wait loop for resources:    - Queue not locked    - Space available (<code>msgsz + msg_cbytes &lt;= msg_qbytes</code>)    - Enough free segments    - Free message header available 4. If <code>IPC_NOWAIT</code> and resources unavailable, return <code>EAGAIN</code> 5. Set <code>MSG_LOCKED</code> to prevent queue reallocation during copy 6. Allocate message header from <code>free_msghdrs</code> 7. Allocate segments from <code>free_msgmaps</code>, linking them 8. <code>copyin()</code> message type (must be &gt; 0) and body segment by segment 9. Append to queue (<code>msg_last-&gt;msg_next = msghdr</code>) 10. Update statistics, clear <code>MSG_LOCKED</code>, <code>wakeup()</code> waiters</p> <p>See <code>sys/kern/sysv_msg.c:455-782</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#msgrcv-receive-message","title":"msgrcv - Receive Message","text":"<pre><code>int sys_msgrcv(struct sysmsg *sysmsg, const struct msgrcv_args *uap)\n</code></pre> <p>Arguments: <code>msqid</code>, <code>msgp</code>, <code>msgsz</code>, <code>msgtyp</code>, <code>msgflg</code></p> <p>Message Selection by <code>msgtyp</code>: - <code>msgtyp == 0</code>: First message (FIFO) - <code>msgtyp &gt; 0</code>: First message with <code>msg_type == msgtyp</code> - <code>msgtyp &lt; 0</code>: First message with <code>msg_type &lt;= |msgtyp|</code> (lowest type first)</p> <p>Operation: 1. Validate msqid and permissions (<code>IPC_R</code>) 2. Search queue for matching message 3. If not found and <code>IPC_NOWAIT</code> set, return <code>ENOMSG</code> 4. If not found, <code>tsleep()</code> on queue address 5. Remove message from queue, update <code>msg_first</code>/<code>msg_last</code> 6. <code>copyout()</code> message type and body segment by segment 7. If <code>msgsz &lt; msg_ts</code> and <code>MSG_NOERROR</code> not set, return <code>E2BIG</code> 8. Free message header and segments via <code>msg_freehdr()</code> 9. <code>wakeup()</code> waiters, return actual bytes copied</p> <p>See <code>sys/kern/sysv_msg.c:787-1070</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#msgctl-control-operations","title":"msgctl - Control Operations","text":"<pre><code>int sys_msgctl(struct sysmsg *sysmsg, const struct msgctl_args *uap)\n</code></pre> <p>Commands:</p> Command Description <code>IPC_STAT</code> Copy <code>msqid_ds</code> to user buffer <code>IPC_SET</code> Update uid, gid, mode, qbytes <code>IPC_RMID</code> Remove queue and free all messages <p>IPC_RMID Operation: 1. Free all message headers via <code>msg_freehdr()</code> 2. Set <code>msg_qbytes = 0</code> to mark slot available 3. <code>wakeup()</code> all blocked processes (they get <code>EIDRM</code>)</p> <p>See <code>sys/kern/sysv_msg.c:202-339</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#segment-allocation","title":"Segment Allocation","text":""},{"location":"sys/kern/ipc/sysv-msg/#allocation-in-msgsnd","title":"Allocation (in msgsnd)","text":"<pre><code>while (segs_needed &gt; 0) {\n    next = free_msgmaps;\n    free_msgmaps = msgmaps[next].next;\n    nfree_msgmaps--;\n    msgmaps[next].next = msghdr-&gt;msg_spot;\n    msghdr-&gt;msg_spot = next;\n    segs_needed--;\n}\n</code></pre> <p>Segments are prepended to the message's chain, so the chain is reversed from allocation order. See <code>sys/kern/sysv_msg.c:657-675</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#deallocation-msg_freehdr","title":"Deallocation (msg_freehdr)","text":"<pre><code>static void msg_freehdr(struct msg *msghdr)\n{\n    while (msghdr-&gt;msg_ts &gt; 0) {\n        next = msgmaps[msghdr-&gt;msg_spot].next;\n        msgmaps[msghdr-&gt;msg_spot].next = free_msgmaps;\n        free_msgmaps = msghdr-&gt;msg_spot;\n        nfree_msgmaps++;\n        msghdr-&gt;msg_spot = next;\n        msghdr-&gt;msg_ts -= msginfo.msgssz;\n    }\n    msghdr-&gt;msg_next = free_msghdrs;\n    free_msghdrs = msghdr;\n}\n</code></pre> <p>See <code>sys/kern/sysv_msg.c:176-197</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#id-encoding","title":"ID Encoding","text":"<p>Message queue IDs encode both index and sequence number:</p> <pre><code>#define MSQID(ix,ds)    ((ix) &amp; 0xffff | (((ds).msg_perm.seq &lt;&lt; 16) &amp; 0xffff0000))\n#define MSQID_IX(id)    ((id) &amp; 0xffff)\n#define MSQID_SEQ(id)   (((id) &gt;&gt; 16) &amp; 0xffff)\n</code></pre> <p>The sequence number increments on each allocation, preventing use of stale IDs after a queue is removed and its slot reused.</p> <p>See <code>sys/kern/sysv_msg.c:96-98</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#jail-support","title":"Jail Support","text":"<p>All system calls check jail capabilities before proceeding:</p> <pre><code>if (pr &amp;&amp; !PRISON_CAP_ISSET(pr-&gt;pr_caps, PRISON_CAP_SYS_SYSVIPC))\n    return (ENOSYS);\n</code></pre>"},{"location":"sys/kern/ipc/sysv-msg/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Type Description <code>kern.ipc.msgmax</code> RD Max chars in a message <code>kern.ipc.msgmni</code> RD Max queue identifiers <code>kern.ipc.msgmnb</code> RD Max chars in queue <code>kern.ipc.msgtql</code> RD Max messages in system <code>kern.ipc.msgssz</code> RD Segment size <code>kern.ipc.msgseg</code> RD Number of segments <code>kern.ipc.msqids</code> RD Queue ID array (raw) <p>Tunable at boot via <code>kern.ipc.msgseg</code>, <code>kern.ipc.msgssz</code>, <code>kern.ipc.msgmni</code>.</p> <p>See <code>sys/kern/sysv_msg.c:1079-1096</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#error-handling","title":"Error Handling","text":"Error Condition <code>ENOSYS</code> Jail lacks SYSVIPC capability <code>EINVAL</code> Invalid msqid, deleted queue, or bad parameters <code>EEXIST</code> <code>IPC_CREAT|IPC_EXCL</code> and queue exists <code>ENOENT</code> Queue not found, no <code>IPC_CREAT</code> <code>ENOSPC</code> No free queue slots <code>EAGAIN</code> <code>IPC_NOWAIT</code> and resources unavailable <code>EIDRM</code> Queue deleted while waiting <code>EINTR</code> Signal received while waiting <code>E2BIG</code> Message too large, <code>MSG_NOERROR</code> not set <code>ENOMSG</code> No matching message, <code>IPC_NOWAIT</code> set"},{"location":"sys/kern/ipc/sysv-sem/","title":"System V Semaphores","text":"<p>System V semaphores provide counting semaphores for process synchronization. DragonFly's implementation derives from FreeBSD and follows the SVID specification, supporting atomic operations on semaphore sets.</p> <p>Source files: - <code>sys/kern/sysv_sem.c</code> - Implementation - <code>sys/sys/sem.h</code> - Public interface</p>"},{"location":"sys/kern/ipc/sysv-sem/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/sysv-sem/#semaphore-set-descriptor","title":"Semaphore Set Descriptor","text":"<pre><code>struct semid_ds {\n    struct  ipc_perm sem_perm;  /* permission struct */\n    struct  sem *sem_base;      /* pointer to first semaphore */\n    unsigned short sem_nsems;   /* number of semaphores in set */\n    time_t  sem_otime;          /* last semop() time */\n    time_t  sem_ctime;          /* last change time */\n};\n</code></pre> <p>Defined in <code>sys/sys/sem.h:34-45</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#semaphore-pool-entry","title":"Semaphore Pool Entry","text":"<pre><code>struct semid_pool {\n    struct lock lk;         /* per-set exclusive lock */\n    struct semid_ds ds;     /* the semid_ds descriptor */\n    long gen;               /* generation counter */\n};\n</code></pre> <p>Defined in <code>sys/sys/sem.h:51-55</code>. The <code>gen</code> field detects destroy/recreate races where credentials might match.</p>"},{"location":"sys/kern/ipc/sysv-sem/#individual-semaphore","title":"Individual Semaphore","text":"<pre><code>struct sem {\n    u_short semval;     /* current value */\n    pid_t   sempid;     /* pid of last operation */\n    u_short semncnt;    /* processes waiting for semval &gt; cval */\n    u_short semzcnt;    /* processes waiting for semval == 0 */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_sem.c:39-44</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#semaphore-operation","title":"Semaphore Operation","text":"<pre><code>struct sembuf {\n    unsigned short sem_num;  /* semaphore index in set */\n    short   sem_op;          /* operation value */\n    short   sem_flg;         /* IPC_NOWAIT, SEM_UNDO */\n};\n</code></pre> <p>Defined in <code>sys/sys/sem.h:62-66</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#undo-structure","title":"Undo Structure","text":"<pre><code>struct sem_undo {\n    TAILQ_ENTRY(sem_undo) un_entry;  /* global list linkage */\n    struct  proc *un_proc;            /* owning process */\n    int     un_refs;                  /* reference count */\n    short   un_cnt;                   /* active undo entries */\n    struct undo {\n        short   un_adjval;  /* adjustment value */\n        short   un_num;     /* semaphore number */\n        int     un_id;      /* semaphore set id */\n    } un_ent[1];            /* variable-length array */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_sem.c:49-60</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#system-limits","title":"System Limits","text":"Parameter Default Description <code>SEMMNI</code> 1024 Max semaphore identifiers <code>SEMMNS</code> 32767 Max semaphores system-wide <code>SEMMSL</code> SEMMNS Max semaphores per set <code>SEMOPM</code> 100 Max operations per semop() <code>SEMUME</code> 25 Max undo entries per process <code>SEMVMX</code> 32767 Maximum semaphore value <code>SEMAEM</code> 16384 Max adjust-on-exit value <p>Defined in <code>sys/kern/sysv_sem.c:65-91</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/sysv-sem/#global-lock","title":"Global Lock","text":"<pre><code>static struct lock sema_lk;\n</code></pre> <p>Protects allocation of new semaphore sets and the global <code>semtot</code> counter.</p>"},{"location":"sys/kern/ipc/sysv-sem/#per-set-lock","title":"Per-Set Lock","text":"<pre><code>struct semid_pool {\n    struct lock lk;  /* exclusive lock for this set */\n    ...\n};\n</code></pre> <p>Each semaphore set has its own lock for operations.</p>"},{"location":"sys/kern/ipc/sysv-sem/#per-semaphore-token","title":"Per-Semaphore Token","text":"<p>Individual semaphores use pool tokens for fine-grained locking:</p> <pre><code>lwkt_getpooltoken(semptr);\n/* modify semptr-&gt;semval */\nlwkt_relpooltoken(semptr);\n</code></pre> <p>This allows concurrent operations on different semaphores within the same set.</p>"},{"location":"sys/kern/ipc/sysv-sem/#undo-list-token","title":"Undo List Token","text":"<pre><code>static struct lwkt_token semu_token;\n</code></pre> <p>Protects the global <code>semu_list</code> of undo structures.</p>"},{"location":"sys/kern/ipc/sysv-sem/#initialization","title":"Initialization","text":"<p><code>seminit()</code> runs at <code>SI_SUB_SYSV_SEM</code>:</p> <ol> <li>Allocates <code>sema[]</code> array (SEMMNI entries)</li> <li>Initializes global lock <code>sema_lk</code></li> <li>Initializes per-set locks and marks all slots as unallocated</li> </ol> <p>See <code>sys/kern/sysv_sem.c:164-181</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/sysv-sem/#semget-create-or-access-set","title":"semget - Create or Access Set","text":"<pre><code>int sys_semget(struct sysmsg *sysmsg, const struct semget_args *uap)\n</code></pre> <p>Arguments: <code>key</code>, <code>nsems</code>, <code>semflg</code></p> <p>Operation: 1. Check jail capabilities 2. If <code>key != IPC_PRIVATE</code>, search for existing set with matching key 3. Validate permissions and nsems count 4. If not found and <code>IPC_CREAT</code>, allocate new set:    - Acquire <code>sema_lk</code> exclusive    - Check system-wide semaphore limit (<code>semtot + nsems &lt;= semmns</code>)    - Find free slot, initialize descriptor    - Allocate <code>sem_base</code> array    - Set <code>SEM_ALLOC</code> flag, increment <code>semtot</code> 5. Return unique semid with sequence number</p> <p>See <code>sys/kern/sysv_sem.c:573-722</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#semop-perform-operations","title":"semop - Perform Operations","text":"<pre><code>int sys_semop(struct sysmsg *sysmsg, const struct semop_args *uap)\n</code></pre> <p>Arguments: <code>semid</code>, <code>sops</code>, <code>nsops</code></p> <p>Atomicity: All operations in a semop() call succeed or fail together.</p> <p>Operation: 1. Copy <code>sops[]</code> array from userspace (max <code>MAX_SOPS</code> = 5) 2. Acquire set lock shared 3. For each operation:    - <code>sem_op &lt; 0</code>: Decrement if <code>semval + sem_op &gt;= 0</code>    - <code>sem_op == 0</code>: Wait until <code>semval == 0</code>    - <code>sem_op &gt; 0</code>: Increment unconditionally 4. If any operation blocks:    - Increment <code>semncnt</code> or <code>semzcnt</code>    - Rollback all completed operations    - If <code>IPC_NOWAIT</code>, return <code>EAGAIN</code>    - Release set lock, <code>tsleep()</code> on semaphore address    - On wakeup, reacquire lock and retry from beginning 5. On success, record undo adjustments if <code>SEM_UNDO</code> set 6. Update <code>sempid</code> for each touched semaphore</p> <p>Rollback: If blocking occurs, previously applied operations are undone to maintain atomicity. See <code>sys/kern/sysv_sem.c:883-892</code>.</p> <p>See <code>sys/kern/sysv_sem.c:727-1050</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#semctl-control-operations","title":"semctl - Control Operations","text":"<pre><code>int sys___semctl(struct sysmsg *sysmsg, const struct __semctl_args *uap)\n</code></pre> <p>Arguments: <code>semid</code>, <code>semnum</code>, <code>cmd</code>, <code>arg</code></p> <p>Commands:</p> Command Description <code>IPC_STAT</code> Copy semid_ds to user buffer <code>IPC_SET</code> Update uid, gid, mode <code>IPC_RMID</code> Remove semaphore set <code>SEM_STAT</code> Like IPC_STAT but semid is array index <code>GETVAL</code> Get single semaphore value <code>SETVAL</code> Set single semaphore value <code>GETALL</code> Get all semaphore values <code>SETALL</code> Set all semaphore values <code>GETPID</code> Get last operation pid <code>GETNCNT</code> Get semncnt (waiters for increment) <code>GETZCNT</code> Get semzcnt (waiters for zero) <p>IPC_RMID Operation: 1. Decrement global <code>semtot</code> by <code>sem_nsems</code> 2. Free <code>sem_base</code> array 3. Clear <code>SEM_ALLOC</code> flag 4. Call <code>semundo_clear()</code> to purge undo entries</p> <p>See <code>sys/kern/sysv_sem.c:346-568</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#sem_undo-mechanism","title":"SEM_UNDO Mechanism","text":"<p>When <code>SEM_UNDO</code> flag is set on an operation, the kernel records an adjustment that will be applied when the process exits.</p>"},{"location":"sys/kern/ipc/sysv-sem/#recording-adjustments","title":"Recording Adjustments","text":"<p><code>semundo_adjust()</code> maintains per-process undo entries:</p> <pre><code>static int semundo_adjust(struct proc *p, int semid, int semnum, int adjval)\n</code></pre> <ul> <li>Allocates <code>sem_undo</code> structure on first use</li> <li>Stores negative of the operation value</li> <li>Entries are compacted when adjustment becomes zero</li> </ul> <p>See <code>sys/kern/sysv_sem.c:218-269</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#process-exit","title":"Process Exit","text":"<p><code>semexit()</code> is called when a process exits:</p> <ol> <li>Iterate through undo entries in reverse order</li> <li>For each entry, apply the recorded adjustment</li> <li>Wake up any waiters on affected semaphores</li> <li>Remove undo structure from global list</li> </ol> <p>See <code>sys/kern/sysv_sem.c:1058-1163</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#clearing-undos-on-set-removal","title":"Clearing Undos on Set Removal","text":"<p><code>semundo_clear()</code> removes undo entries for a deleted semaphore set:</p> <pre><code>static void semundo_clear(int semid, int semnum)\n</code></pre> <p>Called by <code>IPC_RMID</code> with <code>semnum = -1</code> to clear all entries for the set.</p> <p>See <code>sys/kern/sysv_sem.c:274-339</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#wakeup-optimization","title":"Wakeup Optimization","text":"<p>The implementation uses delayed wakeups for efficiency:</p> <pre><code>wakeup_start_delayed();\n/* ... operations that may cause wakeups ... */\nwakeup_end_delayed();\n</code></pre> <p>This batches wakeup signals to reduce context switch overhead.</p>"},{"location":"sys/kern/ipc/sysv-sem/#generation-counter","title":"Generation Counter","text":"<p>Each semaphore set has a generation counter:</p> <pre><code>struct semid_pool {\n    ...\n    long gen;\n};\n</code></pre> <p>Incremented on allocation, used to detect races where a set is destroyed and recreated while a process sleeps. The sleeping process compares the generation before and after sleep.</p>"},{"location":"sys/kern/ipc/sysv-sem/#jail-support","title":"Jail Support","text":"<p>All system calls check jail capabilities:</p> <pre><code>if (pr &amp;&amp; !PRISON_CAP_ISSET(pr-&gt;pr_caps, PRISON_CAP_SYS_SYSVIPC))\n    return (ENOSYS);\n</code></pre>"},{"location":"sys/kern/ipc/sysv-sem/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Type Description <code>kern.ipc.semmap</code> RW Entries in semaphore map <code>kern.ipc.semmni</code> RD Max semaphore identifiers <code>kern.ipc.semmns</code> RD Max semaphores in system <code>kern.ipc.semmnu</code> RD Undo structures in system <code>kern.ipc.semmsl</code> RW Max semaphores per id <code>kern.ipc.semopm</code> RD Max operations per semop <code>kern.ipc.semume</code> RD Max undo entries per process <code>kern.ipc.semusz</code> RD Size of undo structure <code>kern.ipc.semvmx</code> RW Semaphore maximum value <code>kern.ipc.semaem</code> RW Adjust on exit max value <p>All parameters tunable at boot via loader.</p> <p>See <code>sys/kern/sysv_sem.c:119-149</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#error-handling","title":"Error Handling","text":"Error Condition <code>ENOSYS</code> Jail lacks SYSVIPC capability <code>EINVAL</code> Invalid semid, semnum, or nsems <code>EEXIST</code> <code>IPC_CREAT|IPC_EXCL</code> and set exists <code>ENOENT</code> Set not found, no <code>IPC_CREAT</code> <code>ENOSPC</code> No free slots or semaphore limit reached <code>EAGAIN</code> <code>IPC_NOWAIT</code> and would block <code>EIDRM</code> Set deleted while waiting <code>EINTR</code> Signal received while waiting <code>EFBIG</code> sem_num &gt;= sem_nsems <code>E2BIG</code> Too many operations (nsops &gt; MAX_SOPS)"},{"location":"sys/kern/ipc/sysv-shm/","title":"System V Shared Memory","text":"<p>System V shared memory allows processes to share memory regions directly. DragonFly's implementation derives from FreeBSD and uses VM objects backed by either physical memory or the swap pager.</p> <p>Source files: - <code>sys/kern/sysv_shm.c</code> - Implementation - <code>sys/sys/shm.h</code> - Public interface</p>"},{"location":"sys/kern/ipc/sysv-shm/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/sysv-shm/#shared-memory-descriptor","title":"Shared Memory Descriptor","text":"<pre><code>struct shmid_ds {\n    struct ipc_perm shm_perm;   /* permission structure */\n    size_t    shm_segsz;        /* segment size in bytes */\n    pid_t     shm_lpid;         /* last shmat/shmdt pid */\n    pid_t     shm_cpid;         /* creator pid */\n    shmatt_t  shm_nattch;       /* current attach count */\n    time_t    shm_atime;        /* last shmat() time */\n    time_t    shm_dtime;        /* last shmdt() time */\n    time_t    shm_ctime;        /* last shmctl() time */\n    void     *shm_internal;     /* kernel-internal handle */\n};\n</code></pre> <p>Defined in <code>sys/sys/shm.h:74-84</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#internal-handle","title":"Internal Handle","text":"<pre><code>struct shm_handle {\n    vm_object_t shm_object;  /* backing VM object */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_shm.c:71-74</code>. The <code>shm_internal</code> field of <code>shmid_ds</code> points to this structure.</p>"},{"location":"sys/kern/ipc/sysv-shm/#per-process-mapping-state","title":"Per-Process Mapping State","text":"<pre><code>struct shmmap_state {\n    vm_offset_t va;     /* virtual address of mapping */\n    int shmid;          /* attached segment id, or -1 */\n    int reserved;       /* reservation flag for races */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_shm.c:76-80</code>. Each process has an array of these (size <code>shmseg</code>) stored in <code>p-&gt;p_vmspace-&gt;vm_shm</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#segment-state-flags","title":"Segment State Flags","text":"Flag Value Description <code>SHMSEG_FREE</code> 0x0200 Slot is available <code>SHMSEG_REMOVED</code> 0x0400 Marked for removal <code>SHMSEG_ALLOCATED</code> 0x0800 Segment is in use <code>SHMSEG_WANTED</code> 0x1000 Someone waiting for allocation <p>Defined in <code>sys/kern/sysv_shm.c:62-65</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#system-limits","title":"System Limits","text":"Parameter Default Description <code>SHMMIN</code> 1 Minimum segment size <code>SHMMNI</code> 512 Max segment identifiers <code>SHMSEG</code> 1024 Max segments per process <code>shmmax</code> 2/3 RAM Max segment size (auto-computed) <code>shmall</code> 2/3 RAM pages Max total pages (auto-computed) <p>If <code>shmall</code> is not set via tunable, it defaults to 2/3 of physical pages. <code>shmmax</code> is computed as <code>shmall * PAGE_SIZE</code>.</p> <p>Defined in <code>sys/kern/sysv_shm.c:92-108</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#configuration-options","title":"Configuration Options","text":""},{"location":"sys/kern/ipc/sysv-shm/#shm_use_phys","title":"shm_use_phys","text":"<pre><code>static int shm_use_phys = 1;\n</code></pre> <p>When enabled, uses <code>phys_pager_alloc()</code> instead of <code>swap_pager_alloc()</code>. Physical backing provides better performance for large segments by allowing pmap optimizations. Pages are effectively wired.</p> <p>When set to 2 or higher, pages are pre-allocated at segment creation time, improving database warm-up times by enabling concurrent page faults on already-existing pages.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shm_allow_removed","title":"shm_allow_removed","text":"<pre><code>static int shm_allow_removed = 1;\n</code></pre> <p>When enabled, allows <code>shmat()</code> to attach to segments marked for removal (<code>IPC_RMID</code>) as long as they still have references. Used by Chrome and other applications to ensure cleanup after unexpected termination.</p>"},{"location":"sys/kern/ipc/sysv-shm/#synchronization","title":"Synchronization","text":"<p>A single LWKT token protects all shared memory operations:</p> <pre><code>static struct lwkt_token shm_token = LWKT_TOKEN_INITIALIZER(shm_token);\n</code></pre> <p>The <code>reserved</code> field in <code>shmmap_state</code> prevents races when the token is released during blocking operations in <code>shmat()</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#initialization","title":"Initialization","text":"<p><code>shminit()</code> runs at <code>SI_SUB_SYSV_SHM</code>:</p> <ol> <li>If <code>shmall == 0</code>, set to 2/3 of <code>v_page_count</code></li> <li>Compute <code>shmmax = shmall * PAGE_SIZE</code></li> <li>Allocate <code>shmsegs[]</code> array (SHMMNI entries)</li> <li>Mark all slots as <code>SHMSEG_FREE</code></li> </ol> <p>See <code>sys/kern/sysv_shm.c:704-727</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/sysv-shm/#shmget-create-or-access-segment","title":"shmget - Create or Access Segment","text":"<pre><code>int sys_shmget(struct sysmsg *sysmsg, const struct shmget_args *uap)\n</code></pre> <p>Arguments: <code>key</code>, <code>size</code>, <code>shmflg</code></p> <p>Operation: 1. If <code>key != IPC_PRIVATE</code>, search for existing segment 2. If found, call <code>shmget_existing()</code>:    - If <code>SHMSEG_REMOVED</code> set, sleep and retry    - Check <code>IPC_CREAT|IPC_EXCL</code> conflict    - Validate permissions and size 3. If not found and <code>IPC_CREAT</code>, call <code>shmget_allocate_segment()</code>:    - Validate size against limits    - Check system-wide page commitment    - Find free slot (may call <code>shmrealloc()</code> to expand)    - Mark slot <code>ALLOCATED | REMOVED</code> during allocation    - Allocate <code>shm_handle</code> and backing VM object    - Choose <code>phys_pager</code> or <code>swap_pager</code> based on <code>shm_use_phys</code>    - Optionally pre-fault pages if <code>shm_use_phys &gt; 1</code>    - Wake waiters if <code>SHMSEG_WANTED</code> was set</p> <p>See <code>sys/kern/sysv_shm.c:610-644</code>, <code>464-605</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shmat-attach-segment","title":"shmat - Attach Segment","text":"<pre><code>int sys_shmat(struct sysmsg *sysmsg, const struct shmat_args *uap)\n</code></pre> <p>Arguments: <code>shmid</code>, <code>shmaddr</code>, <code>shmflg</code></p> <p>Operation: 1. Allocate per-process <code>shmmap_state[]</code> array if needed 2. Find segment by shmid (respects <code>shm_allow_removed</code>) 3. Check permissions (<code>IPC_R</code> or <code>IPC_R|IPC_W</code>) 4. Find free slot in per-process array, mark <code>reserved = 1</code> 5. Calculate attach address:    - If <code>shmaddr</code> given with <code>SHM_RND</code>, round down to <code>SHMLBA</code>    - If <code>shmaddr</code> given without <code>SHM_RND</code>, must be <code>SHMLBA</code>-aligned    - Otherwise, hint near end of data segment 6. For large segments aligned to <code>SEG_SIZE</code>, use <code>SEG_SIZE</code> alignment 7. Call <code>vm_map_find()</code> to map the VM object 8. Set <code>VM_INHERIT_SHARE</code> so mappings persist across fork 9. Update <code>shmmap_state</code>, increment <code>shm_nattch</code></p> <p>See <code>sys/kern/sysv_shm.c:260-395</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shmdt-detach-segment","title":"shmdt - Detach Segment","text":"<pre><code>int sys_shmdt(struct sysmsg *sysmsg, const struct shmdt_args *uap)\n</code></pre> <p>Arguments: <code>shmaddr</code></p> <p>Operation: 1. Find mapping in per-process array by address 2. Call <code>shm_delete_mapping()</code>:    - <code>vm_map_remove()</code> the mapping    - Clear the <code>shmmap_state</code> entry    - Decrement <code>shm_nattch</code>    - If <code>shm_nattch == 0</code> and <code>SHMSEG_REMOVED</code>, deallocate segment</p> <p>See <code>sys/kern/sysv_shm.c:222-255</code>, <code>196-217</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shmctl-control-operations","title":"shmctl - Control Operations","text":"<pre><code>int sys_shmctl(struct sysmsg *sysmsg, const struct shmctl_args *uap)\n</code></pre> <p>Commands:</p> Command Description <code>IPC_STAT</code> Copy <code>shmid_ds</code> to user buffer <code>IPC_SET</code> Update uid, gid, mode <code>IPC_RMID</code> Mark segment for removal <p>IPC_RMID Operation: 1. Set <code>shm_perm.key = IPC_PRIVATE</code> (prevents new lookups) 2. Set <code>SHMSEG_REMOVED</code> flag 3. If <code>shm_nattch == 0</code>, deallocate immediately 4. Otherwise, wait for all detaches</p> <p>See <code>sys/kern/sysv_shm.c:400-462</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#segment-deallocation","title":"Segment Deallocation","text":"<pre><code>static void shm_deallocate_segment(struct shmid_ds *shmseg)\n</code></pre> <ol> <li>Get <code>shm_handle</code> from <code>shm_internal</code></li> <li>Release VM object reference (<code>vm_object_deallocate()</code>)</li> <li>Free <code>shm_handle</code></li> <li>Decrease <code>shm_committed</code> by segment pages</li> <li>Decrement <code>shm_nused</code></li> <li>Mark slot as <code>SHMSEG_FREE</code></li> </ol> <p>See <code>sys/kern/sysv_shm.c:180-194</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#fork-and-exit-handling","title":"Fork and Exit Handling","text":""},{"location":"sys/kern/ipc/sysv-shm/#shmfork","title":"shmfork","text":"<p>Called when a process forks:</p> <pre><code>void shmfork(struct proc *p1, struct proc *p2)\n</code></pre> <ol> <li>Allocate new <code>shmmap_state[]</code> for child</li> <li>Copy parent's mappings</li> <li>Increment <code>shm_nattch</code> for each attached segment</li> </ol> <p>The <code>VM_INHERIT_SHARE</code> flag ensures the actual mappings are shared.</p> <p>See <code>sys/kern/sysv_shm.c:646-663</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shmexit","title":"shmexit","text":"<p>Called when a process exits or execs:</p> <pre><code>void shmexit(struct vmspace *vm)\n</code></pre> <ol> <li>Detach all attached segments via <code>shm_delete_mapping()</code></li> <li>Free the <code>shmmap_state[]</code> array</li> </ol> <p>See <code>sys/kern/sysv_shm.c:665-681</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#dynamic-array-expansion","title":"Dynamic Array Expansion","text":"<pre><code>static void shmrealloc(void)\n</code></pre> <p>If <code>shmalloced &lt; shmmni</code>, reallocates <code>shmsegs[]</code> to full size. Called during allocation when no free slots exist.</p> <p>See <code>sys/kern/sysv_shm.c:683-702</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#vm-object-backing","title":"VM Object Backing","text":"<p>Two pager types are supported:</p>"},{"location":"sys/kern/ipc/sysv-shm/#physical-pager-shm_use_phys-1","title":"Physical Pager (shm_use_phys = 1)","text":"<pre><code>shm_handle-&gt;shm_object = phys_pager_alloc(NULL, size, VM_PROT_DEFAULT, 0);\n</code></pre> <ul> <li>Pages are wired in physical memory</li> <li>Better pmap optimization for large segments</li> <li>No swap backing</li> </ul>"},{"location":"sys/kern/ipc/sysv-shm/#swap-pager-shm_use_phys-0","title":"Swap Pager (shm_use_phys = 0)","text":"<pre><code>shm_handle-&gt;shm_object = swap_pager_alloc(NULL, size, VM_PROT_DEFAULT, 0);\n</code></pre> <ul> <li>Pages can be swapped out</li> <li>More flexible memory usage</li> </ul> <p>Both set <code>OBJ_NOSPLIT</code> to prevent the object from being split.</p>"},{"location":"sys/kern/ipc/sysv-shm/#jail-support","title":"Jail Support","text":"<p>All system calls check jail capabilities:</p> <pre><code>if (pr &amp;&amp; !PRISON_CAP_ISSET(pr-&gt;pr_caps, PRISON_CAP_SYS_SYSVIPC))\n    return (ENOSYS);\n</code></pre>"},{"location":"sys/kern/ipc/sysv-shm/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Type Description <code>kern.ipc.shmmax</code> RW Max segment size <code>kern.ipc.shmmin</code> RW Min segment size <code>kern.ipc.shmmni</code> RD Max identifiers <code>kern.ipc.shmseg</code> RW Max segments per process <code>kern.ipc.shmall</code> RW Max total pages <code>kern.ipc.shm_use_phys</code> RW Use physical pager <code>kern.ipc.shm_allow_removed</code> RW Allow attach to removed <p>Boot tunables: <code>kern.ipc.shmmin</code>, <code>kern.ipc.shmmni</code>, <code>kern.ipc.shmseg</code>, <code>kern.ipc.shmmaxpgs</code>, <code>kern.ipc.shm_use_phys</code>.</p> <p>See <code>sys/kern/sysv_shm.c:126-146</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#error-handling","title":"Error Handling","text":"Error Condition <code>ENOSYS</code> Jail lacks SYSVIPC capability <code>EINVAL</code> Invalid shmid, size, or address <code>EEXIST</code> <code>IPC_CREAT|IPC_EXCL</code> and segment exists <code>ENOENT</code> Segment not found, no <code>IPC_CREAT</code> <code>ENOSPC</code> No free slots <code>ENOMEM</code> Page commitment exceeded or mapping failed <code>EMFILE</code> Per-process segment limit reached <code>EACCES</code> Permission denied"},{"location":"sys/kern/ipc/unix-sockets/","title":"Unix Domain Sockets","text":"<p>Unix domain sockets provide local inter-process communication using the familiar socket API. DragonFly BSD implements Unix domain sockets with LWKT token-based synchronization and reference counting for safe concurrent access across multiple processors.</p>"},{"location":"sys/kern/ipc/unix-sockets/#overview","title":"Overview","text":"<p>Unix domain sockets (also called local sockets) enable efficient communication between processes on the same machine without network protocol overhead. Key features include:</p> <ul> <li>Filesystem binding - Sockets can be bound to pathnames in the filesystem</li> <li>File descriptor passing - Transfer open file descriptors between processes</li> <li>Credential passing - Automatic sender credential transmission</li> <li>Three socket types - SOCK_STREAM, SOCK_DGRAM, and SOCK_SEQPACKET</li> <li>Direct mbuf transfer - Zero-copy data delivery to peer's receive buffer</li> <li>Garbage collection - Automatic cleanup of in-flight file descriptors</li> </ul>"},{"location":"sys/kern/ipc/unix-sockets/#source-files","title":"Source Files","text":"File Description <code>sys/kern/uipc_usrreq.c</code> Unix domain socket protocol implementation <code>sys/sys/unpcb.h</code> Unix domain protocol control block definitions <code>sys/sys/un.h</code> Unix domain socket address structure"},{"location":"sys/kern/ipc/unix-sockets/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/unix-sockets/#struct-unpcb","title":"struct unpcb","text":"<p>The Unix domain protocol control block (<code>sys/sys/unpcb.h:68</code>):</p> <pre><code>struct unpcb {\n    struct socket   *unp_socket;    /* pointer back to socket */\n    struct unpcb    *unp_conn;      /* control block of connected socket */\n    int             unp_flags;      /* flags */\n    int             unp_refcnt;     /* reference count */\n    struct unp_head unp_refs;       /* referencing socket linked list (DGRAM) */\n    LIST_ENTRY(unpcb) unp_reflink;  /* link in unp_refs list */\n    struct sockaddr_un *unp_addr;   /* bound address of socket */\n    struct xucred   unp_peercred;   /* peer credentials, if applicable */\n    int             unp_msgcount;   /* # of cmsgs this unp are in */\n    int             unp_gcflags;    /* flags reserved for unp GC to use */\n    struct file     *unp_fp;        /* corresponding fp if unp is in cmsg */\n    long            unp_unused01;\n    struct vnode    *unp_vnode;     /* if associated with file */\n    struct vnode    *unp_rvnode;    /* root vp for creating process (jail) */\n    TAILQ_ENTRY(unpcb) unp_link;    /* glue on list of all PCBs */\n    unp_gen_t       unp_gencnt;     /* generation count of this instance */\n};\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#struct-sockaddr_un","title":"struct sockaddr_un","text":"<p>The Unix domain socket address (<code>sys/sys/un.h</code>):</p> <pre><code>struct sockaddr_un {\n    uint8_t  sun_len;           /* total sockaddr length */\n    sa_family_t sun_family;     /* AF_LOCAL / AF_UNIX */\n    char     sun_path[104];     /* path name (null-terminated) */\n};\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#global-socket-lists","title":"Global Socket Lists","text":"<p>Unix domain sockets are organized by type (<code>sys/kern/uipc_usrreq.c:124</code>):</p> <pre><code>struct unp_global_head {\n    struct unpcb_qhead  list;   /* TAILQ of unpcbs */\n    int                 count;  /* number in list */\n};\n\nstatic struct unp_global_head unp_stream_head;   /* SOCK_STREAM sockets */\nstatic struct unp_global_head unp_dgram_head;    /* SOCK_DGRAM sockets */\nstatic struct unp_global_head unp_seqpkt_head;   /* SOCK_SEQPACKET sockets */\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#unp-flags","title":"UNP Flags","text":"<p>Protocol control block flags (<code>sys/sys/unpcb.h:102</code>):</p> Flag Value Description <code>UNP_HAVEPC</code> 0x001 <code>unp_peercred</code> contains connected peer credentials <code>UNP_HAVEPCCACHED</code> 0x002 <code>unp_peercred</code> cached from listen() <code>UNP_DETACHED</code> 0x004 Socket detached from global list <code>UNP_CONNECTING</code> 0x008 Connection in progress <code>UNP_DROPPED</code> 0x010 Socket has been dropped <code>UNP_MARKER</code> 0x020 Marker for list traversal"},{"location":"sys/kern/ipc/unix-sockets/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/unix-sockets/#token-hierarchy","title":"Token Hierarchy","text":"<p>Unix domain sockets use a multi-level token hierarchy (<code>sys/kern/uipc_usrreq.c:185</code>):</p> <pre><code>flowchart TD\n    A[\"unp_token(global)\"]\n    B[\"Per-unpcb pool token(lwkt_token_pool_lookup(unp))\"]\n    C[\"unp_rights_token(for file descriptor passing)\"]\n\n    A --&gt; B --&gt; C\n</code></pre> <p>Rules:</p> <ol> <li>Any change to <code>unp_conn</code> requires both <code>unp_token</code> and the per-unpcb pool token</li> <li>Access to <code>so_pcb</code> to obtain unp requires the pool token</li> <li>File descriptor tracking (<code>unp_rights</code>) requires <code>unp_rights_token</code></li> </ol>"},{"location":"sys/kern/ipc/unix-sockets/#reference-counting","title":"Reference Counting","text":"<pre><code>static __inline void\nunp_reference(struct unpcb *unp)\n{\n    KKASSERT(unp-&gt;unp_refcnt &gt; 0);\n    atomic_add_int(&amp;unp-&gt;unp_refcnt, 1);\n}\n\nstatic __inline void\nunp_free(struct unpcb *unp)\n{\n    KKASSERT(unp-&gt;unp_refcnt &gt; 0);\n    if (atomic_fetchadd_int(&amp;unp-&gt;unp_refcnt, -1) == 1)\n        unp_detach(unp);\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#socket-token-acquisition","title":"Socket Token Acquisition","text":"<p><code>unp_getsocktoken()</code> safely acquires the pool token (<code>sys/kern/uipc_usrreq.c:215</code>):</p> <pre><code>static __inline struct unpcb *\nunp_getsocktoken(struct socket *so)\n{\n    struct unpcb *unp;\n\n    /* The unp pointer is invalid until verified by re-checking\n     * so_pcb AFTER obtaining the token. */\n    while ((unp = so-&gt;so_pcb) != NULL) {\n        lwkt_getpooltoken(unp);\n        if (unp == so-&gt;so_pcb)\n            break;\n        lwkt_relpooltoken(unp);\n    }\n    return unp;\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#protocol-operations","title":"Protocol Operations","text":""},{"location":"sys/kern/ipc/unix-sockets/#uipc_usrreqs","title":"uipc_usrreqs","text":"<p>The protocol switch operations (<code>sys/kern/uipc_usrreq.c:905</code>):</p> <pre><code>struct pr_usrreqs uipc_usrreqs = {\n    .pru_abort = uipc_abort,\n    .pru_accept = uipc_accept,\n    .pru_attach = uipc_attach,\n    .pru_bind = uipc_bind,\n    .pru_connect = uipc_connect,\n    .pru_connect2 = uipc_connect2,\n    .pru_control = pr_generic_notsupp,\n    .pru_detach = uipc_detach,\n    .pru_disconnect = uipc_disconnect,\n    .pru_listen = uipc_listen,\n    .pru_peeraddr = uipc_peeraddr,\n    .pru_rcvd = uipc_rcvd,\n    .pru_rcvoob = pr_generic_notsupp,\n    .pru_send = uipc_send,\n    .pru_sense = uipc_sense,\n    .pru_shutdown = uipc_shutdown,\n    .pru_sockaddr = uipc_sockaddr,\n    .pru_sosend = sosend,\n    .pru_soreceive = soreceive\n};\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#socket-lifecycle","title":"Socket Lifecycle","text":""},{"location":"sys/kern/ipc/unix-sockets/#attach","title":"Attach","text":"<p><code>unp_attach()</code> creates the protocol control block (<code>sys/kern/uipc_usrreq.c:1025</code>):</p> <pre><code>flowchart TD\n    A[\"unp_attach(so, ai)\"] --&gt; B[\"Reserve buffer space based on socket type\"]\n    B --&gt; B1[\"SOCK_STREAM: unpst_sendspace/recvspace (65536/65536)\"]\n    B --&gt; B2[\"SOCK_DGRAM: unpdg_sendspace/recvspace (65536/65536)\"]\n    B --&gt; B3[\"SOCK_SEQPACKET: unpsp_sendspace/recvspace (65536/65536)\"]\n\n    B1 --&gt; C[\"For SOCK_STREAM: Set SSB_STOPSUPP on both buffers\"]\n    B2 --&gt; C\n    B3 --&gt; C\n\n    C --&gt; D[\"Allocate unpcb (kmalloc M_UNPCB)\"]\n\n    D --&gt; E[\"Initialize\"]\n    E --&gt; E1[\"unp_refcnt = 1\"]\n    E --&gt; E2[\"unp_gencnt = ++unp_gencnt\"]\n    E --&gt; E3[\"LIST_INIT(&amp;unp-&gt;unp_refs)\"]\n    E --&gt; E4[\"unp_socket = so\"]\n    E --&gt; E5[\"unp_rvnode = ai-&gt;fd_rdir (jail root)\"]\n    E --&gt; E6[\"so-&gt;so_pcb = unp\"]\n\n    E1 --&gt; F[\"Add to global type-specific list\"]\n    E2 --&gt; F\n    E3 --&gt; F\n    E4 --&gt; F\n    E5 --&gt; F\n    E6 --&gt; F\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#bind","title":"Bind","text":"<p><code>unp_bind()</code> binds a socket to a filesystem path (<code>sys/kern/uipc_usrreq.c:1124</code>):</p> <pre><code>flowchart TD\n    A[\"unp_bind(unp, nam, td)\"] --&gt; B[\"Check unp_vnode == NULL(not already bound)\"]\n    B --&gt; C[\"Extract and null-terminate path from sockaddr_un\"]\n    C --&gt; D[\"nlookup_init() withNLC_LOCKVP | NLC_CREATE | NLC_REFDVP\"]\n    D --&gt; E[\"nlookup() \u2192 Find parent directory\"]\n    E --&gt; F[\"Check path doesn't exist(EADDRINUSE)\"]\n    F --&gt; G[\"VOP_NCREATE() \u2192 Create VSOCK vnode\"]\n    G --&gt; G1[\"vattr.va_type = VSOCK\"]\n    G --&gt; G2[\"vattr.va_mode = ACCESSPERMS &amp; ~cmask\"]\n    G1 --&gt; H[\"Link vnode to socket\"]\n    G2 --&gt; H\n    H --&gt; H1[\"vp-&gt;v_socket = unp-&gt;unp_socket\"]\n    H --&gt; H2[\"unp-&gt;unp_vnode = vp\"]\n    H --&gt; H3[\"unp-&gt;unp_addr = dup_sockaddr(nam)\"]\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#listen","title":"Listen","text":"<p><code>unp_listen()</code> prepares for incoming connections (<code>sys/kern/uipc_usrreq.c:2322</code>):</p> <pre><code>static int\nunp_listen(struct unpcb *unp, struct thread *td)\n{\n    struct proc *p = td-&gt;td_proc;\n\n    KKASSERT(p);\n    cru2x(p-&gt;p_ucred, &amp;unp-&gt;unp_peercred);  /* Cache server credentials */\n    unp_setflags(unp, UNP_HAVEPCCACHED);\n    return (0);\n}\n</code></pre> <p>The cached credentials are later copied to connecting clients.</p>"},{"location":"sys/kern/ipc/unix-sockets/#connect","title":"Connect","text":"<p><code>unp_connect()</code> establishes a connection (<code>sys/kern/uipc_usrreq.c:1177</code>):</p> <pre><code>flowchart TD\n    A[\"unp_connect(so, nam, td)\"] --&gt; B[\"Get socket token, check attached\"]\n    B --&gt; C[\"Check not already connecting or connected\"]\n    C --&gt; D[\"Set UNP_CONNECTING flag\"]\n    D --&gt; E[\"unp_find_lockref() \u2192 Find and lock target unpcb\"]\n    E --&gt; E1[\"nlookup() the path\"]\n    E --&gt; E2[\"Check vnode type == VSOCK\"]\n    E --&gt; E3[\"VOP_EACCESS() for VWRITE permission\"]\n    E --&gt; E4[\"Lock and reference target unpcb\"]\n\n    E1 --&gt; F{Socket Type?}\n    E2 --&gt; F\n    E3 --&gt; F\n    E4 --&gt; F\n\n    F --&gt;|STREAM/SEQPACKET| G[\"Connection-oriented\"]\n    F --&gt;|DGRAM| H[\"Connectionless\"]\n\n    G --&gt; G1[\"Check target has SO_ACCEPTCONNand UNP_HAVEPCCACHED\"]\n    G1 --&gt; G2[\"sonewconn_faddr() \u2192Create new socket for connection\"]\n    G2 --&gt; G3[\"Copy server's bound address to new socket\"]\n    G3 --&gt; G4[\"Exchange credentials\"]\n    G4 --&gt; G4a[\"Client creds \u2192 new socket's unp_peercred\"]\n    G4 --&gt; G4b[\"Server's cached creds \u2192 client's unp_peercred\"]\n    G4a --&gt; G5[\"unp_connect_pair(unp, unp3)\"]\n    G4b --&gt; G5\n\n    H --&gt; H1[\"unp_connect_pair(unp, unp2)\"]\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#connect-pair","title":"Connect Pair","text":"<p><code>unp_connect_pair()</code> links two sockets (<code>sys/kern/uipc_usrreq.c:2480</code>):</p> <pre><code>static int\nunp_connect_pair(struct unpcb *unp, struct unpcb *unp2)\n{\n    unp-&gt;unp_conn = unp2;\n\n    switch (so-&gt;so_type) {\n    case SOCK_DGRAM:\n        /* DGRAM: one-way reference, unp2 keeps list of referrers */\n        LIST_INSERT_HEAD(&amp;unp2-&gt;unp_refs, unp, unp_reflink);\n        soisconnected(so);\n        break;\n\n    case SOCK_STREAM:\n    case SOCK_SEQPACKET:\n        /* STREAM/SEQPACKET: bidirectional connection */\n        unp2-&gt;unp_conn = unp;\n        soisconnected(so);\n        soisconnected(so2);\n        break;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#data-transfer","title":"Data Transfer","text":""},{"location":"sys/kern/ipc/unix-sockets/#send","title":"Send","text":"<p><code>uipc_send()</code> transmits data (<code>sys/kern/uipc_usrreq.c:603</code>):</p>"},{"location":"sys/kern/ipc/unix-sockets/#datagram-send","title":"Datagram Send","text":"<pre><code>flowchart TD\n    A[\"uipc_send() [SOCK_DGRAM]\"] --&gt; B{\"Address provided?\"}\n    B --&gt;|Yes| C[\"Check not already connected\"]\n    C --&gt; D[\"unp_find_lockref() \u2192 Find destination\"]\n    B --&gt;|No| E[\"Use existing unp_conn\"]\n    D --&gt; F{\"SO_PASSCRED on receiver?\"}\n    E --&gt; F\n    F --&gt;|Yes| G[\"Add SCM_CREDS control message\"]\n    F --&gt;|No| H[\"Get receiver's ssb_token\"]\n    G --&gt; H\n    H --&gt; I[\"ssb_appendaddr() \u2192 Append with source address\"]\n    I --&gt; J[\"sorwakeup(so2)\"]\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#streamseqpacket-send","title":"Stream/Seqpacket Send","text":"<pre><code>flowchart TD\n    A[\"uipc_send() [SOCK_STREAM/SOCK_SEQPACKET]\"] --&gt; B[\"Connect if not connected and address provided\"]\n    B --&gt; C[\"Check SS_CANTSENDMORE\"]\n    C --&gt; D[\"Get peer's so_rcv.ssb_token\"]\n    D --&gt; E[\"Transfer data directly to peer's receive buffer\"]\n    E --&gt; E1[\"With control: ssb_appendcontrol()\"]\n    E --&gt; E2[\"SEQPACKET: sbappendrecord()(preserve boundaries)\"]\n    E --&gt; E3[\"STREAM: sbappend()(byte stream)\"]\n    E1 --&gt; F{\"Backpressure needed?\"}\n    E2 --&gt; F\n    E3 --&gt; F\n    F --&gt;|\"so2-&gt;so_rcv.ssb_cc &gt;= so-&gt;so_snd.ssb_hiwat ORso2-&gt;so_rcv.ssb_mbcnt &gt;= so-&gt;so_snd.ssb_mbmax\"| G[\"atomic_set_int(&amp;so-&gt;so_snd.ssb_flags, SSB_STOP)\"]\n    F --&gt;|No| H[\"sorwakeup(so2)\"]\n    G --&gt; H\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#receive-notification","title":"Receive Notification","text":"<p><code>uipc_rcvd()</code> handles flow control after receive (<code>sys/kern/uipc_usrreq.c:539</code>):</p> <pre><code>case SOCK_STREAM:\ncase SOCK_SEQPACKET:\n    if (unp-&gt;unp_conn == NULL)\n        break;\n    unp2 = unp-&gt;unp_conn;\n    so2 = unp2-&gt;unp_socket;\n\n    unp_reference(unp2);\n    lwkt_gettoken(&amp;so2-&gt;so_rcv.ssb_token);\n\n    /* Clear backpressure if buffer space available */\n    if (so-&gt;so_rcv.ssb_cc &lt; so2-&gt;so_snd.ssb_hiwat &amp;&amp;\n        so-&gt;so_rcv.ssb_mbcnt &lt; so2-&gt;so_snd.ssb_mbmax) {\n        atomic_clear_int(&amp;so2-&gt;so_snd.ssb_flags, SSB_STOP);\n        sowwakeup(so2);  /* Wake sender */\n    }\n\n    lwkt_reltoken(&amp;so2-&gt;so_rcv.ssb_token);\n    unp_free(unp2);\n    break;\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#file-descriptor-passing","title":"File Descriptor Passing","text":"<p>Unix domain sockets can transfer file descriptors between processes using ancillary data (SCM_RIGHTS).</p>"},{"location":"sys/kern/ipc/unix-sockets/#internalize-send-side","title":"Internalize (Send Side)","text":"<p><code>unp_internalize()</code> converts user FDs to kernel file pointers (<code>sys/kern/uipc_usrreq.c:1701</code>):</p> <pre><code>flowchart TD\n    A[\"unp_internalize(control, td)\"] --&gt; B[\"Validate control message(SCM_RIGHTS or SCM_CREDS)\"]\n    B --&gt; C{\"Message type?\"}\n    C --&gt;|SCM_CREDS| D[\"Fill in sender credentials and return\"]\n    C --&gt;|SCM_RIGHTS| E[\"Calculate number of FDs:(cmsg_len - CMSG_LEN(0)) / sizeof(int)\"]\n    E --&gt; F[\"Expand mbuf if needed forlarger struct file pointers\"]\n    F --&gt; G[\"Lock unp_rights_token and fd_spin\"]\n    G --&gt; H[\"Validate all FDs\"]\n    H --&gt; H1[\"Check fd &lt; fd_nfiles\"]\n    H --&gt; H2[\"Check fd_files[fd].fp != NULL\"]\n    H --&gt; H3[\"Reject kqueues (EOPNOTSUPP)\"]\n    H1 --&gt; I[\"Convert FDs to file pointers (reverse order)\"]\n    H2 --&gt; I\n    H3 --&gt; I\n    I --&gt; I1[\"fp = fdescp-&gt;fd_files[fdp[i]].fp\"]\n    I --&gt; I2[\"rp[i] = fp\"]\n    I --&gt; I3[\"fhold(fp)\"]\n    I --&gt; I4[\"unp_add_right(fp) - Track in-flight FDs\"]\n    I1 --&gt; J[\"Update cmsg_len for pointer size\"]\n    I2 --&gt; J\n    I3 --&gt; J\n    I4 --&gt; J\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#externalize-receive-side","title":"Externalize (Receive Side)","text":"<p><code>unp_externalize()</code> converts kernel file pointers to user FDs (<code>sys/kern/uipc_usrreq.c:1542</code>):</p> <pre><code>flowchart TD\n    A[\"unp_externalize(rights, flags)\"] --&gt; B[\"Calculate number of file pointers\"]\n    B --&gt; C[\"Check fdavail() for enough FD slots\"]\n    C --&gt; D[\"Hold revoke_token (shared)to catch revoked files\"]\n    D --&gt; E[\"For each file pointer\"]\n    E --&gt; F[\"fdalloc() \u2192 Allocate new FD\"]\n    F --&gt; G[\"unp_fp_externalize()\"]\n    G --&gt; G1[\"Handle FREVOKED files specially\"]\n    G --&gt; G2[\"Apply MSG_CMSG_CLOEXEC \u2192 UF_EXCLOSE\"]\n    G --&gt; G3[\"Apply MSG_CMSG_CLOFORK \u2192 UF_FOCLOSE\"]\n    G --&gt; G4[\"fsetfd() \u2192 Install in process FD table\"]\n    G --&gt; G5[\"unp_del_right(fp) \u2192 Remove from in-flight count\"]\n    G --&gt; G6[\"fdrop(fp)\"]\n    G1 --&gt; H[\"Adjust cmsg_len for int size\"]\n    G2 --&gt; H\n    G3 --&gt; H\n    G4 --&gt; H\n    G5 --&gt; H\n    G6 --&gt; H\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#in-flight-tracking","title":"In-Flight Tracking","text":"<pre><code>static __inline void\nunp_add_right(struct file *fp)\n{\n    struct unpcb *unp;\n\n    ASSERT_LWKT_TOKEN_HELD(&amp;unp_rights_token);\n\n    unp = unp_fp2unpcb(fp);\n    if (unp != NULL) {\n        unp-&gt;unp_fp = fp;\n        unp-&gt;unp_msgcount++;\n    }\n    fp-&gt;f_msgcount++;\n    unp_rights++;  /* Global in-flight counter */\n}\n\nstatic __inline void\nunp_del_right(struct file *fp)\n{\n    struct unpcb *unp;\n\n    unp = unp_fp2unpcb(fp);\n    if (unp != NULL) {\n        unp-&gt;unp_msgcount--;\n        if (unp-&gt;unp_msgcount == 0)\n            unp-&gt;unp_fp = NULL;\n    }\n    fp-&gt;f_msgcount--;\n    unp_rights--;\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#credential-passing","title":"Credential Passing","text":""},{"location":"sys/kern/ipc/unix-sockets/#scm_creds","title":"SCM_CREDS","text":"<p>Sender credentials are passed via <code>unp_internalize()</code> when control message type is SCM_CREDS:</p> <pre><code>if (cm-&gt;cmsg_type == SCM_CREDS) {\n    cmcred = (struct cmsgcred *)CMSG_DATA(cm);\n    cmcred-&gt;cmcred_pid = p-&gt;p_pid;\n    cmcred-&gt;cmcred_uid = p-&gt;p_ucred-&gt;cr_ruid;\n    cmcred-&gt;cmcred_gid = p-&gt;p_ucred-&gt;cr_rgid;\n    cmcred-&gt;cmcred_euid = p-&gt;p_ucred-&gt;cr_uid;\n    cmcred-&gt;cmcred_ngroups = MIN(p-&gt;p_ucred-&gt;cr_ngroups, CMGROUP_MAX);\n    for (i = 0; i &lt; cmcred-&gt;cmcred_ngroups; i++)\n        cmcred-&gt;cmcred_groups[i] = p-&gt;p_ucred-&gt;cr_groups[i];\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#so_passcred","title":"SO_PASSCRED","text":"<p>When the receiving socket has <code>SO_PASSCRED</code> set, credentials are automatically included even if the sender didn't provide them:</p> <pre><code>if (so2-&gt;so_options &amp; SO_PASSCRED) {\n    /* Check if SCM_CREDS already present */\n    mp = &amp;control;\n    while ((ncon = *mp) != NULL) {\n        cm = mtod(ncon, struct cmsghdr *);\n        if (cm-&gt;cmsg_type == SCM_CREDS &amp;&amp; cm-&gt;cmsg_level == SOL_SOCKET)\n            break;\n        mp = &amp;ncon-&gt;m_next;\n    }\n    if (ncon == NULL) {\n        /* Create and internalize credentials */\n        ncon = sbcreatecontrol(&amp;cred, sizeof(cred), SCM_CREDS, SOL_SOCKET);\n        unp_internalize(ncon, msg-&gt;send.nm_td);\n        *mp = ncon;\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#local_peercred","title":"LOCAL_PEERCRED","text":"<p>Connected stream/seqpacket sockets can retrieve peer credentials:</p> <pre><code>case LOCAL_PEERCRED:\n    if (unp-&gt;unp_flags &amp; UNP_HAVEPC)\n        soopt_from_kbuf(sopt, &amp;unp-&gt;unp_peercred, sizeof(unp-&gt;unp_peercred));\n    else {\n        if (so-&gt;so_type == SOCK_STREAM || so-&gt;so_type == SOCK_SEQPACKET)\n            error = ENOTCONN;\n        else\n            error = EINVAL;\n    }\n    break;\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#garbage-collection","title":"Garbage Collection","text":""},{"location":"sys/kern/ipc/unix-sockets/#problem-statement","title":"Problem Statement","text":"<p>File descriptors passed over Unix domain sockets can form unreachable cycles:</p> <ol> <li>Socket A holds a reference to socket B in its receive buffer</li> <li>Socket B holds a reference to socket A in its receive buffer</li> <li>Both sockets are closed by their owning processes</li> <li>The file descriptors in flight keep each other alive indefinitely</li> </ol>"},{"location":"sys/kern/ipc/unix-sockets/#gc-algorithm","title":"GC Algorithm","text":"<p>The garbage collector runs when <code>unp_rights &gt; 0</code> and a socket is detached (<code>sys/kern/uipc_usrreq.c:2144</code>):</p> <pre><code>flowchart TD\n    A[\"unp_gc()\"] --&gt; B[\"Clear all gcflags from previous runs\"]\n    B --&gt; C[\"Mark phase (iterate until no new marks)\"]\n    C --&gt; D[\"For each unpcb\"]\n    D --&gt; E{\"Already has UNPGC_REF?\"}\n    E --&gt;|Yes| F[\"skip\"]\n    E --&gt;|No| G{\"Potentially in cycle?(all refs from messages)\"}\n    G --&gt;|Yes| H[\"Mark UNPGC_DEAD,increment unp_unreachable\"]\n    G --&gt;|No| I[\"Scan receive buffer\"]\n    I --&gt; J[\"Mark referenced sockets with UNPGC_REF\"]\n    F --&gt; K{\"unp_marked == 0?\"}\n    H --&gt; K\n    J --&gt; K\n    K --&gt;|No| D\n    K --&gt;|Yes| L[\"Sweep phase\"]\n    L --&gt; M[\"Collect sockets marked UNPGC_DEAD\"]\n    M --&gt; N[\"sorflush() each to release rights\"]\n    N --&gt; O[\"fdrop() to release extra reference\"]\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#gc-flags","title":"GC Flags","text":"Flag Value Description <code>UNPGC_REF</code> 0x1 Socket has external reference <code>UNPGC_DEAD</code> 0x2 Socket might be in unreachable cycle <code>UNPGC_SCANNED</code> 0x4 Socket's receive buffer has been scanned"},{"location":"sys/kern/ipc/unix-sockets/#deferred-discard","title":"Deferred Discard","text":"<p>To avoid deep recursion when discarding Unix domain sockets, disposal is deferred to a taskqueue:</p> <pre><code>static void\nunp_discard(struct file *fp, void *data __unused)\n{\n    unp_del_right(fp);\n    if (unp_fp2unpcb(fp) != NULL) {\n        /* This is a Unix socket - defer to avoid recursion */\n        struct unp_defdiscard *d;\n\n        d = kmalloc(sizeof(*d), M_UNPCB, M_WAITOK);\n        d-&gt;fp = fp;\n\n        spin_lock(&amp;unp_defdiscard_spin);\n        SLIST_INSERT_HEAD(&amp;unp_defdiscard_head, d, next);\n        spin_unlock(&amp;unp_defdiscard_spin);\n\n        taskqueue_enqueue(unp_taskqueue, &amp;unp_defdiscard_task);\n    } else {\n        fdrop(fp);\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#disconnect-and-cleanup","title":"Disconnect and Cleanup","text":""},{"location":"sys/kern/ipc/unix-sockets/#disconnect","title":"Disconnect","text":"<p><code>unp_disconnect()</code> breaks a connection (<code>sys/kern/uipc_usrreq.c:1353</code>):</p> <pre><code>static void\nunp_disconnect(struct unpcb *unp, int error)\n{\n    struct socket *so = unp-&gt;unp_socket;\n    struct unpcb *unp2;\n\n    if (error)\n        so-&gt;so_error = error;\n\n    /* Get peer's token */\n    while ((unp2 = unp-&gt;unp_conn) != NULL) {\n        lwkt_getpooltoken(unp2);\n        if (unp2 == unp-&gt;unp_conn)\n            break;\n        lwkt_relpooltoken(unp2);\n    }\n    if (unp2 == NULL)\n        return;\n\n    unp-&gt;unp_conn = NULL;\n\n    switch (so-&gt;so_type) {\n    case SOCK_DGRAM:\n        LIST_REMOVE(unp, unp_reflink);\n        soclrstate(so, SS_ISCONNECTED);\n        break;\n\n    case SOCK_STREAM:\n    case SOCK_SEQPACKET:\n        unp_reference(unp2);\n        unp2-&gt;unp_conn = NULL;\n        soisdisconnected(so);\n        soisdisconnected(unp2-&gt;unp_socket);\n        unp_free(unp2);\n        break;\n    }\n\n    lwkt_relpooltoken(unp2);\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#drop","title":"Drop","text":"<p><code>unp_drop()</code> removes a socket from the system (<code>sys/kern/uipc_usrreq.c:2521</code>):</p> <pre><code>flowchart TD\n    A[\"unp_drop(unp, error)\"] --&gt; B[\"Mark UNP_DETACHED\"]\n    B --&gt; C[\"Remove from global list(stream/dgram/seqpkt)\"]\n    C --&gt; D[\"unp_disconnect() \u2192 Break connection\"]\n    D --&gt; E[\"For each socket referencing us (DGRAM)\"]\n    E --&gt; F[\"unp_disconnect(unp2, ECONNRESET)\"]\n    F --&gt; G[\"Mark UNP_DROPPED\"]\n    G --&gt; H[\"unp_free() \u2192 Try to free\"]\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#detach","title":"Detach","text":"<p><code>unp_detach()</code> performs final cleanup (<code>sys/kern/uipc_usrreq.c:1087</code>):</p> <pre><code>static void\nunp_detach(struct unpcb *unp)\n{\n    struct socket *so;\n\n    lwkt_gettoken(&amp;unp_token);\n    lwkt_getpooltoken(unp);\n\n    so = unp-&gt;unp_socket;\n\n    unp-&gt;unp_gencnt = ++unp_gencnt;\n    if (unp-&gt;unp_vnode) {\n        unp-&gt;unp_vnode-&gt;v_socket = NULL;\n        vrele(unp-&gt;unp_vnode);\n        unp-&gt;unp_vnode = NULL;\n    }\n    soisdisconnected(so);\n    so-&gt;so_pcb = NULL;\n    unp-&gt;unp_socket = NULL;\n\n    lwkt_relpooltoken(unp);\n    lwkt_reltoken(&amp;unp_token);\n\n    sofree(so);\n\n    if (unp-&gt;unp_addr)\n        kfree(unp-&gt;unp_addr, M_SONAME);\n    kfree(unp, M_UNPCB);\n\n    /* Trigger GC if file descriptors still in flight */\n    if (unp_rights)\n        taskqueue_enqueue(unp_taskqueue, &amp;unp_gc_task);\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#sysctl-parameters","title":"Sysctl Parameters","text":"<p>Buffer size tunables (<code>sys/kern/uipc_usrreq.c:1005</code>):</p> Sysctl Default Description <code>net.local.stream.sendspace</code> 65536 Stream socket send buffer <code>net.local.stream.recvspace</code> 65536 Stream socket receive buffer <code>net.local.dgram.maxdgram</code> 65536 Maximum datagram size <code>net.local.dgram.recvspace</code> 65536 Datagram socket receive buffer <code>net.local.seqpacket.maxseqpacket</code> 65536 Maximum seqpacket size <code>net.local.seqpacket.recvspace</code> 65536 Seqpacket receive buffer <code>net.local.inflight</code> (read-only) File descriptors currently in flight <p>PCB list access for netstat:</p> Sysctl Description <code>net.local.stream.pcblist</code> List of active stream sockets <code>net.local.dgram.pcblist</code> List of active datagram sockets <code>net.local.seqpacket.pcblist</code> List of active seqpacket sockets"},{"location":"sys/kern/ipc/unix-sockets/#jail-support","title":"Jail Support","text":"<p>Unix domain sockets respect jail boundaries via <code>prison_unpcb()</code> (<code>sys/kern/uipc_usrreq.c:1416</code>):</p> <pre><code>static int\nprison_unpcb(struct thread *td, struct unpcb *unp)\n{\n    struct proc *p;\n\n    if (td == NULL)\n        return (0);\n    if ((p = td-&gt;td_proc) == NULL)\n        return (0);\n    if (!p-&gt;p_ucred-&gt;cr_prison)\n        return (0);\n    /* Check if unp's root matches jail root */\n    if (p-&gt;p_fd-&gt;fd_rdir == unp-&gt;unp_rvnode)\n        return (0);\n    return (1);  /* Reject - different jail */\n}\n</code></pre> <p>The <code>unp_rvnode</code> field stores the root vnode of the process that created the socket, allowing cross-jail access to be filtered.</p>"},{"location":"sys/kern/ipc/unix-sockets/#error-handling","title":"Error Handling","text":"<p>Common error codes returned by Unix domain socket operations:</p> Error Condition <code>EINVAL</code> Socket not attached, already bound, invalid address <code>EADDRINUSE</code> Bind path already exists <code>ECONNREFUSED</code> Target not listening, not attached, or connection rejected <code>EPROTOTYPE</code> Socket type mismatch <code>EISCONN</code> Already connected <code>ENOTCONN</code> Not connected (for connected operations) <code>ENOTSOCK</code> Path is not a socket <code>ENOBUFS</code> Buffer space exhausted <code>EPIPE</code> Cannot send (SS_CANTSENDMORE) <code>EOPNOTSUPP</code> Out-of-band data not supported <code>EMSGSIZE</code> Not enough FD slots for rights <code>E2BIG</code> Too many file descriptors in control message <code>EBADF</code> Invalid file descriptor in SCM_RIGHTS"},{"location":"sys/kern/ipc/unix-sockets/#see-also","title":"See Also","text":"<ul> <li>Socket Layer - Generic socket infrastructure</li> <li>Mbufs - Memory buffer management</li> <li>IPC Overview - Inter-process communication</li> <li>Synchronization - LWKT tokens and locking</li> </ul>"},{"location":"sys/kern/vfs/","title":"Virtual Filesystem (VFS)","text":""},{"location":"sys/kern/vfs/#overview","title":"Overview","text":"<p>The Virtual Filesystem (VFS) layer is DragonFly BSD's abstraction layer that provides a uniform interface between the kernel and concrete filesystem implementations. It allows the kernel to support multiple filesystem types (UFS, HAMMER, HAMMER2, NFS, tmpfs, devfs, etc.) through a common API.</p> <p>Architecture: - Vnodes - In-memory representation of files, directories, devices - VFS operations - Filesystem-level operations (mount, statfs, sync) - Vnode operations (VOPs) - File/directory-level operations (open, read, write, lookup) - Name cache - High-performance path component caching - Buffer cache - Disk block caching and I/O management</p> <p>Key source files (Phase 6a - Initialization and Core): - <code>vfs_init.c</code> (504 lines) \u2014 VFS subsystem initialization - <code>vfs_conf.c</code> (713 lines) \u2014 Filesystem type registration, root mounting - <code>vfs_subr.c</code> (2,650 lines) \u2014 Vnode lifecycle, buffer management utilities - <code>vfs_vfsops.c</code> (321 lines) \u2014 VFS operation wrappers (mount, unmount, sync, etc.) - <code>vfs_vnops.c</code> (1,352 lines) \u2014 High-level vnode operations (vn_open, vn_close, vn_rdwr) - <code>vfs_vopops.c</code> (2,227 lines) \u2014 Vnode operation dispatch layer - <code>vfs_default.c</code> (1,684 lines) \u2014 Default vnode operation implementations</p>"},{"location":"sys/kern/vfs/#vfs-initialization-vfs_initc","title":"VFS Initialization (vfs_init.c)","text":""},{"location":"sys/kern/vfs/#initialization-flow","title":"Initialization Flow","text":"<p>Called during kernel bootstrap via <code>SYSINIT(vfs, SI_SUB_VFS, SI_ORDER_FIRST, vfsinit, NULL)</code>:</p> <pre><code>vfsinit()\n\u251c\u2500 TAILQ_INIT(&amp;vnodeopv_list)           // Initialize vop vector list\n\u251c\u2500 namei_oc = objcache_create_simple()  // Create namei path buffer cache\n\u251c\u2500 vfs_subr_init()                      // Initialize vnode subsystem\n\u251c\u2500 vfs_mount_init()                     // Initialize mount structures\n\u251c\u2500 vfs_lock_init()                      // Initialize vnode locking\n\u251c\u2500 nchinit()                            // Initialize name cache\n\u2514\u2500 vattr_null(&amp;va_null)                 // Initialize null vattr template\n</code></pre>"},{"location":"sys/kern/vfs/#vnode-operations-vector-management","title":"Vnode Operations Vector Management","text":"<p>Key functions: - <code>vfs_add_vnodeops()</code> - Add/register vnode operations vector - <code>vfs_rm_vnodeops()</code> - Remove vnode operations vector - <code>vfs_calc_vnodeops()</code> - Fill in NULL entries with defaults</p> <p>Each filesystem provides a <code>struct vop_ops</code> with function pointers for file operations. The VFS layer ensures NULL entries are replaced with default implementations.</p>"},{"location":"sys/kern/vfs/#filesystem-registration-vfsconf","title":"Filesystem Registration (vfsconf)","text":"<p>Data structures:</p> <pre><code>struct vfsconf {\n    struct vfsops *vfc_vfsops;       // Filesystem operations\n    char          vfc_name[MFSNAMELEN]; // Filesystem type name (e.g., \"ufs\")\n    int           vfc_typenum;       // Unique type number\n    int           vfc_refcount;      // Active mount count\n    ...\n};\n</code></pre> <p>Global registry: - <code>vfsconf_list</code> - STAILQ of all registered filesystem types - <code>vfsconf_maxtypenum</code> - Highest assigned type number</p> <p>Key functions (vfs_init.c): - <code>vfs_register(struct vfsconf *)</code> - Register a filesystem type   - Assigns unique <code>vfc_typenum</code>   - Fills in default vfsops entries (vfs_root, vfs_statfs, vfs_sync, etc.)   - Calls filesystem's <code>vfs_init()</code> method   - Registers sysctl nodes under <code>vfs.&lt;fsname&gt;</code> - <code>vfs_unregister(struct vfsconf *)</code> - Unregister (checks refcount) - <code>vfsconf_find_by_name(const char *)</code> - Lookup filesystem by name - <code>vfsconf_find_by_typenum(int)</code> - Lookup by type number</p> <p>Module integration: <code>vfs_modevent()</code> handles MOD_LOAD/MOD_UNLOAD for filesystem kernel modules.</p>"},{"location":"sys/kern/vfs/#root-filesystem-mounting-vfs_confc","title":"Root Filesystem Mounting (vfs_conf.c)","text":""},{"location":"sys/kern/vfs/#boot-sequence","title":"Boot Sequence","text":"<p><code>SYSINIT(mountroot, SI_SUB_MOUNT_ROOT, SI_ORDER_SECOND, vfs_mountroot, NULL)</code> orchestrates root filesystem mounting:</p> <pre><code>vfs_mountroot()\n\u251c\u2500 sync_devs()                    // Wait for disk device probing\n\u251c\u2500 tsleep(..., hz * wakedelay)    // Default 2s delay (vfs.root.wakedelay)\n\u251c\u2500 Try boot-time root specifications:\n\u2502  \u251c\u2500 RB_CDROM flag \u2192 try cdrom_rootdevnames[] array\n\u2502  \u251c\u2500 ROOTDEVNAME (compile-time)\n\u2502  \u251c\u2500 kgetenv(\"vfs.root.mountfrom\") from loader\n\u2502  \u251c\u2500 rootdevnames[0], rootdevnames[1] (machine-dependent legacy)\n\u2502  \u2514\u2500 RB_ASKNAME \u2192 vfs_mountroot_ask() (interactive prompt)\n\u2514\u2500 Panic if all methods fail\n</code></pre> <p>Root mount string format: <code>&lt;fstype&gt;:&lt;device&gt;</code> (e.g., <code>\"hammer2:da0s1a\"</code>, <code>\"ufs:da0s1a\"</code>)</p>"},{"location":"sys/kern/vfs/#vfs_mountroot_try","title":"vfs_mountroot_try()","text":"<p>Attempts to mount a specified root:</p> <ol> <li>Parse <code>&lt;fstype&gt;:&lt;devname&gt;</code> string</li> <li>Call <code>vfs_rootmountalloc()</code> to allocate <code>struct mount</code></li> <li>Set <code>mp-&gt;mnt_flag |= MNT_ROOTFS</code></li> <li>Call <code>VFS_MOUNT(mp, NULL, NULL, cred)</code></li> <li>On success:</li> <li>Insert into <code>mountlist</code> (first position)</li> <li>Call <code>inittodr()</code> to sync system clock from fs timestamp</li> <li>Get root vnode via <code>VFS_ROOT()</code></li> <li>Setup <code>proc0</code>'s fd_cdir, fd_rdir, fd_ncdir, fd_nrdir</li> <li>Call <code>vfs_cache_setroot()</code> for global rootnch</li> <li>Allocate syncer vnode via <code>vfs_allocate_syncvnode()</code></li> <li>Call <code>VFS_START(mp, 0)</code></li> </ol>"},{"location":"sys/kern/vfs/#interactive-root-prompt","title":"Interactive Root Prompt","text":"<p><code>vfs_mountroot_ask()</code> - when RB_ASKNAME boot flag set:</p> <pre><code>mountroot&gt; ?           # List available disk devices\nmountroot&gt; ufs:da0s1a  # Try mount\nmountroot&gt; panic       # Panic the kernel\nmountroot&gt; abort       # Give up\n</code></pre> <p>Uses <code>devfs_scan_callback()</code> to enumerate disk devices.</p>"},{"location":"sys/kern/vfs/#devfs-mounting","title":"devfs Mounting","text":"<p><code>vfs_mountroot_devfs()</code> - Mounts <code>/dev</code> (or <code>&lt;init_chroot&gt;/dev</code>):</p> <ol> <li>nlookup <code>/dev</code> path</li> <li>Allocate mount structure</li> <li>Call <code>VFS_MOUNT(mp, \"/dev\", NULL, cred)</code></li> <li>Mark ncp with <code>NCF_ISMOUNTPT</code></li> <li>Insert into mountlist</li> </ol>"},{"location":"sys/kern/vfs/#vnode-lifecycle-vfs_subrc","title":"Vnode Lifecycle (vfs_subr.c)","text":""},{"location":"sys/kern/vfs/#vnode-subsystem-initialization","title":"Vnode Subsystem Initialization","text":"<p>vfs_subr_init() (called from vfsinit):</p> <p>Calculates <code>maxvnodes</code> based on available RAM: - Base formula: <code>maxvnodes = freemem / (80 * (sizeof(struct vm_object) + sizeof(struct vnode)))</code> - Non-linear scaling for systems &gt; 1GB and &gt; 8GB - Minimum: max(MINVNODES=2000, maxproc * 8) - Maximum: MAXVNODES=4000000 - Bounded by kernel VA space (KvaSize)</p> <p>Global state: - <code>numvnodes</code> - Current vnode count (sysctl <code>debug.numvnodes</code>) - <code>maxvnodes</code> - Maximum vnodes (sysctl <code>kern.maxvnodes</code>) - <code>spechash_token</code> - Token protecting device vnode hash</p>"},{"location":"sys/kern/vfs/#vnode-buffer-tree-management","title":"Vnode Buffer Tree Management","text":"<p>Vnodes maintain red-black trees for buffer management (defined in vfs_subr.c:133):</p> <pre><code>struct vnode {\n    struct buf_rb_tree v_rbclean_tree;  // Clean buffers\n    struct buf_rb_tree v_rbdirty_tree;  // Dirty buffers\n    struct buf_rb_hash v_rbhash_tree;   // All buffers (hash by b_loffset)\n    ...\n};\n</code></pre> <p>Buffer-vnode association: - <code>bgetvp(struct vnode *, struct buf *)</code> - Associate buffer with vnode (vfs_subr.c:964)   - Inserts into <code>v_rbhash_tree</code>   - Diagnostics check for overlapping buffers - <code>brelvp(struct buf *)</code> - Disassociate buffer from vnode</p>"},{"location":"sys/kern/vfs/#buffer-invalidation","title":"Buffer Invalidation","text":"<p>vinvalbuf() (vfs_subr.c:313) - Flush and invalidate all buffers for a vnode:</p> <pre><code>vinvalbuf(struct vnode *vp, int flags, int slpflag, int slptimeo)\n</code></pre> <p>Flags: - <code>V_SAVE</code> - Call <code>VOP_FSYNC()</code> before invalidating</p> <p>Algorithm: 1. If V_SAVE: wait for write I/O, then <code>VOP_FSYNC()</code> 2. Loop:    - Scan <code>v_rbclean_tree</code> and <code>v_rbdirty_tree</code> with <code>vinvalbuf_bp()</code>    - Wait for all I/O completion (<code>bio_track_wait()</code>)    - Wait for VM paging I/O 3. Remove VM pages via <code>vm_object_page_remove()</code> 4. Panic if any buffers remain</p> <p>Used during: - Vnode reclamation - Truncation/unmount operations</p>"},{"location":"sys/kern/vfs/#buffer-truncation","title":"Buffer Truncation","text":"<p>vtruncbuf() (vfs_subr.c:475) - Truncate file buffers to new length:</p> <pre><code>vtruncbuf(struct vnode *vp, off_t length, int blksize)\n</code></pre> <ol> <li>Round <code>length</code> up to next block boundary</li> <li>Scan clean/dirty trees with <code>vtruncbuf_bp_trunc_cmp()</code></li> <li>Destroy buffers with <code>b_loffset &gt;= truncloffset</code></li> <li>For non-zero truncation: fsync remaining metadata buffers</li> <li>Call <code>vnode_pager_setsize()</code> to truncate VM backing store</li> <li>Wait for I/O completion</li> </ol>"},{"location":"sys/kern/vfs/#filesystem-sync-vfsync","title":"Filesystem Sync (vfsync)","text":"<p>vfsync() (vfs_subr.c:680) - Sync dirty buffers for a vnode:</p> <pre><code>vfsync(struct vnode *vp, int waitfor, int passes,\n       int (*checkdef)(struct buf *),\n       int (*waitoutput)(struct vnode *, struct thread *))\n</code></pre> <p>Wait modes: - <code>MNT_LAZY</code> - Lazy flush (limit to 1MB data), used by syncer - <code>MNT_NOWAIT</code> - Asynchronous flush (one data pass, one metadata pass) - <code>MNT_WAIT</code> - Synchronous (multiple passes until clean)</p> <p>Algorithm for MNT_WAIT: 1. Data-only pass (fast, no waiting) 2. Wait for I/O 3. Full pass (data + metadata) 4. Additional passes (up to <code>passes</code> count) until no dirty buffers remain 5. On final pass: set <code>info.synchronous = 1</code> to force blocking writes</p> <p>Lazy mode (<code>MNT_LAZY</code>): - Scan from <code>vp-&gt;v_lazyw</code> offset - Stop after flushing 1MB (<code>info.lazylimit</code>) - Updates <code>v_lazyw</code> to track progress - Reschedules vnode for syncer if incomplete</p> <p>Used by: - <code>VOP_FSYNC()</code> implementations - Periodic sync daemon</p>"},{"location":"sys/kern/vfs/#timestamp-precision","title":"Timestamp Precision","text":"<p>vfs_timestamp() (vfs_subr.c:238) - Get current timestamp with configurable precision:</p> <p>Sysctl <code>vfs.timestamp_precision</code>: - 0 = Seconds only - 1 = Microseconds (tick precision, default if hz &gt;= 100) - 2 = Microseconds (tick precision) - 3 = Nanoseconds (tick precision) - 4 = Microseconds (maximum precision, default if hz &lt; 100) - 5 = Nanoseconds (maximum precision)</p>"},{"location":"sys/kern/vfs/#vfs-operations-vfs_vfsopsc","title":"VFS Operations (vfs_vfsops.c)","text":""},{"location":"sys/kern/vfs/#mpsafe-wrapper-layer","title":"MPSAFE Wrapper Layer","text":"<p><code>vfs_vfsops.c</code> provides MPSAFE wrappers for all <code>struct vfsops</code> methods. Each wrapper:</p> <ol> <li>Acquires mount's MP lock (<code>VFS_MPLOCK(mp)</code>)</li> <li>Calls filesystem's method via function pointer</li> <li>Releases MP lock (<code>VFS_MPUNLOCK()</code>)</li> </ol> <p>Key wrappers:</p> <pre><code>int vfs_mount(struct mount *mp, char *path, caddr_t data, struct ucred *cred)\nint vfs_start(struct mount *mp, int flags)\nint vfs_unmount(struct mount *mp, int mntflags)\nint vfs_root(struct mount *mp, struct vnode **vpp)\nint vfs_sync(struct mount *mp, int waitfor)\nint vfs_statfs(struct mount *mp, struct statfs *sbp, struct ucred *cred)\nint vfs_statvfs(struct mount *mp, struct statvfs *sbp, struct ucred *cred)\nint vfs_vget(struct mount *mp, struct vnode *dvp, ino_t ino, struct vnode **vpp)\nint vfs_fhtovp(struct mount *mp, struct vnode *rootvp, struct fid *fhp, struct vnode **vpp)\nint vfs_vptofh(struct vnode *vp, struct fid *fhp)\nint vfs_checkexp(struct mount *mp, struct sockaddr *nam, int *extflagsp, struct ucred **credanonp)\nint vfs_extattrctl(struct mount *mp, int cmd, struct vnode *vp, int attrnamespace, \n                   const char *attrname, struct ucred *cred)\n</code></pre> <p>Quota integration: - <code>vfs_start()</code> calls <code>VFS_ACINIT()</code> on successful start - <code>vfs_unmount()</code> calls <code>VFS_ACDONE()</code> before unmounting</p> <p>Mount point locking strategy: - Most operations use exclusive MP lock - Some operations conditionally use MP lock via <code>VFS_MPLOCK_FLAG()</code> - Filesystems can opt-in to MPSAFE via <code>MNTK_*_MPSAFE</code> flags</p>"},{"location":"sys/kern/vfs/#high-level-vnode-operations-vfs_vnopsc","title":"High-Level Vnode Operations (vfs_vnops.c)","text":""},{"location":"sys/kern/vfs/#vn_open-opencreate-files","title":"vn_open() - Open/Create Files","text":"<p>Signature:</p> <pre><code>int vn_open(struct nlookupdata *nd, struct file **fpp, int fmode, int cmode)\n</code></pre> <p>Purpose: Unified entry point for opening and creating files/directories.</p> <p>Flow:</p> <pre><code>vn_open()\n\u251c\u2500 Setup nd-&gt;nl_flags (NLC_OPEN, NLC_APPEND, NLC_READ, NLC_WRITE, etc.)\n\u251c\u2500 if (fmode &amp; O_CREAT):\n\u2502  \u251c\u2500 Set NLC_CREATE, NLC_REFDVP, call nlookup()\n\u2502  \u2514\u2500 bwillinode(1)  // Reserve inode space\n\u251c\u2500 else:\n\u2502  \u2514\u2500 nlookup()  // Normal lookup\n\u251c\u2500 Check filesystem modification stall (ncp_writechk())\n\u251c\u2500 if (O_CREAT and ncp-&gt;nc_vp == NULL):\n\u2502  \u2514\u2500 VOP_NCREATE(&amp;nl_nch, nl_dvp, &amp;vp, cred, vap)  // Create new file\n\u251c\u2500 else:\n\u2502  \u2514\u2500 cache_vget(&amp;nl_nch, cred, LK_EXCLUSIVE/LK_SHARED, &amp;vp)  // Get existing\n\u251c\u2500 Validate vnode type (reject VLNK, VSOCK; check O_DIRECTORY)\n\u251c\u2500 Check write permission (vn_writechk()) if FWRITE|O_TRUNC\n\u251c\u2500 if (O_TRUNC):\n\u2502  \u251c\u2500 VOP_SETATTR_FP(vp, vap-&gt;va_size=0, cred, fp)\n\u2502  \u2514\u2500 VFS_ACCOUNT()  // Quota adjustment\n\u251c\u2500 Setup VNSWAPCACHE flags based on NCF_UF_CACHE/NCF_SF_NOCACHE\n\u251c\u2500 if (fp):\n\u2502  \u2514\u2500 fp-&gt;f_nchandle = nd-&gt;nl_nch  // Store namecache handle\n\u251c\u2500 VOP_OPEN(vp, fmode, cred, fpp)  // Call filesystem's open method\n\u2514\u2500 Return vnode in nd-&gt;nl_open_vp (if fp == NULL) or fp-&gt;f_data (if fp != NULL)\n</code></pre> <p>Key features: - Shared locking optimization: Uses LK_SHARED for read-only opens (when appropriate) - ESTALE handling: Re-resolves namecache on ESTALE errors - Quota integration: Checks/accounts for size changes on O_TRUNC - Swapcache control: Propagates NCF_UF_CACHE flags to VNSWAPCACHE vnode flag</p> <p>Error cases: - <code>EACCES</code> - Permission denied - <code>EEXIST</code> - O_CREAT | O_EXCL and file exists - <code>EISDIR</code> - Attempting to write/truncate a directory - <code>ENOTDIR</code> - O_DIRECTORY on non-directory - <code>EMLINK</code> - Opened a symlink (shouldn't happen with NLC_FOLLOW) - <code>ETXTBSY</code> - File is executing, cannot write - <code>EROFS</code> - Read-only filesystem - <code>ESTALE</code> - NFS stale file handle (triggers retry)</p>"},{"location":"sys/kern/vfs/#vn_close-close-files","title":"vn_close() - Close Files","text":"<pre><code>int vn_close(struct vnode *vp, int flags, struct file *fp)\n</code></pre> <ol> <li>Lock vnode (LK_SHARED | LK_RETRY | LK_FAILRECLAIM)</li> <li>Call <code>VOP_CLOSE(vp, flags, fp)</code></li> <li>Unlock vnode</li> </ol> <p>Flags: - <code>FREAD</code>, <code>FWRITE</code> - Indicating how file was opened - <code>FNONBLOCK</code> - Non-blocking close</p>"},{"location":"sys/kern/vfs/#vn_rdwr-kernel-file-io","title":"vn_rdwr() - Kernel File I/O","text":"<pre><code>int vn_rdwr(enum uio_rw rw, struct vnode *vp, caddr_t base, int len,\n            off_t offset, enum uio_seg segflg, int ioflags,\n            struct ucred *cred, int *aresid)\n</code></pre> <p>Purpose: Synchronous read/write from kernel context.</p> <p>Used by: - Executable loading (imgact_elf.c) - Core dumps (kern_sig.c) - Swap pager - Kernel module loading</p> <p>Steps: 1. Setup struct uio with I/O parameters 2. If UIO_SYSSPACE and vnode has VM object: use <code>vn_cache_strategy()</code> 3. Else: call <code>VOP_READ()</code> or <code>VOP_WRITE()</code> 4. Return residual count in <code>*aresid</code></p>"},{"location":"sys/kern/vfs/#vn_writechk-write-permission-check","title":"vn_writechk() - Write Permission Check","text":"<pre><code>int vn_writechk(struct vnode *vp)\n</code></pre> <p>Checks: - <code>VTEXT</code> flag - File is executing (returns ETXTBSY) - <code>MNT_RDONLY</code> - Filesystem is read-only (returns EROFS)</p> <p>Called after vnode is locked.</p>"},{"location":"sys/kern/vfs/#ncp_writechk-namecache-write-check","title":"ncp_writechk() - Namecache Write Check","text":"<pre><code>int ncp_writechk(struct nchandle *nch)\n</code></pre> <p>Checks: - <code>MNT_RDONLY</code> - Associated mount is read-only (returns EROFS) - Calls <code>VFS_MODIFYING()</code> if filesystem has special modifying callback</p> <p>Called BEFORE vnodes are locked (allows filesystem to stall modifications).</p>"},{"location":"sys/kern/vfs/#file-descriptor-operations","title":"File Descriptor Operations","text":"<p>vnode_fileops structure (vfs_vnops.c:77) - Provides file operations for vnodes:</p> <pre><code>struct fileops vnode_fileops = {\n    .fo_read     = vn_read,\n    .fo_write    = vn_write,\n    .fo_ioctl    = vn_ioctl,\n    .fo_kqfilter = vn_kqfilter,\n    .fo_stat     = vn_statfile,\n    .fo_close    = vn_closefile,\n    .fo_shutdown = nofo_shutdown,\n    .fo_seek     = vn_seek\n};\n</code></pre> <p>These functions bridge between file descriptor operations (<code>read(2)</code>, <code>write(2)</code>, etc.) and vnode operations (<code>VOP_READ()</code>, <code>VOP_WRITE()</code>).</p> <p>vn_read(): - Validates file is open for reading - For VREG: updates <code>f_offset</code> optimistically - Calls <code>VOP_READ(vp, uio, ioflag, cred, fp)</code> - Handles <code>f_offset</code> races</p> <p>vn_write(): - Validates file is open for writing - Handles <code>IO_APPEND</code> flag - Quota checks for regular files - Calls <code>VOP_WRITE(vp, uio, ioflag, cred, fp)</code> - Quota accounting on success</p> <p>vn_ioctl(): - Validates vnode type (reject directories) - Calls <code>VOP_IOCTL(vp, cmd, data, fflag, cred, msg)</code></p> <p>vn_statfile(): - Calls <code>VOP_GETATTR(vp, &amp;vattr)</code> - Converts <code>struct vattr</code> to <code>struct stat</code> - Fills in st_dev, st_ino, st_mode, st_size, timestamps, etc.</p> <p>vn_seek(): - Validates seek offset (no negative offsets) - For VREG: allows seeks beyond EOF - For VDIR: offset must be &lt;= current size</p>"},{"location":"sys/kern/vfs/#vnode-operation-dispatch-vfs_vopopsc","title":"Vnode Operation Dispatch (vfs_vopops.c)","text":""},{"location":"sys/kern/vfs/#purpose","title":"Purpose","text":"<p><code>vfs_vopops.c</code> provides MPSAFE dispatch wrappers for all vnode operations. Similar to <code>vfs_vfsops.c</code> but for per-vnode operations rather than per-mount operations.</p>"},{"location":"sys/kern/vfs/#wrapper-pattern","title":"Wrapper Pattern","text":"<p>Each VOP wrapper:</p> <ol> <li>Initializes <code>struct vop_*_args</code> with operation parameters</li> <li>Sets <code>a_head.a_desc</code> (operation descriptor)</li> <li>Sets <code>a_head.a_ops</code> (vnode's ops vector)</li> <li>Acquires mount's MP lock (<code>VFS_MPLOCK(vp-&gt;v_mount)</code>)</li> <li>Calls operation via <code>DO_OPS(ops, error, &amp;ap, vop_field)</code></li> <li>Releases MP lock (<code>VFS_MPUNLOCK()</code>)</li> <li>Returns error</li> </ol> <p>Example - VOP_OPEN():</p> <pre><code>int vop_open(struct vop_ops *ops, struct vnode *vp, int mode,\n             struct ucred *cred, struct file **fpp)\n{\n    struct vop_open_args ap;\n    VFS_MPLOCK_DECLARE;\n    int error;\n\n    // Decrement VAGE0/VAGE1 flags (aging mechanism)\n    if (vp-&gt;v_flag &amp; VAGE0) {\n        vclrflags(vp, VAGE0);\n    } else if (vp-&gt;v_flag &amp; VAGE1) {\n        vclrflags(vp, VAGE1);\n        vsetflags(vp, VAGE0);\n    }\n\n    ap.a_head.a_desc = &amp;vop_open_desc;\n    ap.a_head.a_ops = ops;\n    ap.a_vp = vp;\n    ap.a_fpp = fpp;\n    ap.a_mode = mode;\n    ap.a_cred = cred;\n\n    VFS_MPLOCK(vp-&gt;v_mount);\n    DO_OPS(ops, error, &amp;ap, vop_open);\n    VFS_MPUNLOCK();\n\n    return(error);\n}\n</code></pre>"},{"location":"sys/kern/vfs/#key-vnode-operations","title":"Key Vnode Operations","text":"<p>Namespace operations (new API): - <code>vop_nresolve()</code> - Resolve namecache entry to vnode - <code>vop_nlookupdotdot()</code> - Lookup parent directory (..) - <code>vop_ncreate()</code> - Create file via namecache - <code>vop_nmkdir()</code> - Create directory via namecache - <code>vop_nmknod()</code> - Create device node via namecache - <code>vop_nlink()</code> - Create hard link via namecache - <code>vop_nsymlink()</code> - Create symbolic link via namecache - <code>vop_nwhiteout()</code> - Create/delete whiteout entry - <code>vop_nremove()</code> - Remove file via namecache - <code>vop_nrmdir()</code> - Remove directory via namecache - <code>vop_nrename()</code> - Rename file/directory via namecache</p> <p>File operations: - <code>vop_open()</code> - Open file - <code>vop_close()</code> - Close file - <code>vop_read()</code> - Read data - <code>vop_write()</code> - Write data (with quota accounting) - <code>vop_ioctl()</code> - I/O control operations - <code>vop_poll()</code> - Poll for events - <code>vop_kqfilter()</code> - Register kqueue filter - <code>vop_fsync()</code> - Sync dirty data/metadata - <code>vop_fdatasync()</code> - Sync data only</p> <p>Metadata operations: - <code>vop_getattr()</code> - Get vnode attributes - <code>vop_getattr_lite()</code> - Get lightweight attributes - <code>vop_setattr()</code> - Set vnode attributes - <code>vop_access()</code> - Check access permissions</p> <p>I/O operations: - <code>vop_bmap()</code> - Map logical block to physical - <code>vop_strategy()</code> - Perform I/O strategy - <code>vop_getpages()</code> - Get VM pages - <code>vop_putpages()</code> - Flush VM pages</p> <p>Directory operations: - <code>vop_readdir()</code> - Read directory entries - <code>vop_readlink()</code> - Read symbolic link target</p> <p>Lifecycle operations: - <code>vop_inactive()</code> - Vnode is no longer referenced - <code>vop_reclaim()</code> - Reclaim vnode resources</p> <p>Special operations: - <code>vop_mmap()</code> - Memory-map file - <code>vop_advlock()</code> - Advisory locking - <code>vop_balloc()</code> - Allocate blocks - <code>vop_freeblks()</code> - Free blocks (sparse files/truncation) - <code>vop_pathconf()</code> - Get filesystem path configuration - <code>vop_markatime()</code> - Mark access time (deferred atime updates) - <code>vop_allocate()</code> - Preallocate space (fallocate)</p> <p>Extended attributes: - <code>vop_getacl()</code> - Get ACL - <code>vop_setacl()</code> - Set ACL - <code>vop_aclcheck()</code> - Check ACL validity - <code>vop_getextattr()</code> - Get extended attribute - <code>vop_setextattr()</code> - Set extended attribute</p>"},{"location":"sys/kern/vfs/#mpsafe-optimization","title":"MPSAFE Optimization","text":"<p>Some operations support conditional locking for better concurrency:</p> <p>VFS_MPLOCK_FLAG() variants: - <code>MNTK_GA_MPSAFE</code> - Getattr is MP-safe - <code>MNTK_RD_MPSAFE</code> - Read is MP-safe - <code>MNTK_WR_MPSAFE</code> - Write is MP-safe - <code>MNTK_ST_MPSAFE</code> - Start is MP-safe</p> <p>If the mount has the appropriate flag set, the wrapper skips MP lock acquisition.</p>"},{"location":"sys/kern/vfs/#quota-integration","title":"Quota Integration","text":"<p>vop_write() wrapper includes comprehensive quota handling:</p> <ol> <li>Before write: <code>VOP_GETATTR()</code> to get current size and ownership</li> <li>Calculate potential new size (accounting for IO_APPEND)</li> <li>Check quota: <code>vq_write_ok(mp, uid, gid, delta)</code></li> <li>Perform write via filesystem's method</li> <li>On success: <code>VFS_ACCOUNT(mp, uid, gid, actual_delta)</code></li> </ol>"},{"location":"sys/kern/vfs/#default-vnode-operations-vfs_defaultc","title":"Default Vnode Operations (vfs_default.c)","text":""},{"location":"sys/kern/vfs/#default-operations-table","title":"Default Operations Table","text":"<p><code>default_vnode_vops</code> provides fallback implementations when filesystems don't implement specific operations:</p> <p>Common defaults: - <code>.vop_default = vop_eopnotsupp</code> - Return EOPNOTSUPP for unimplemented ops - <code>.vop_advlock = vop_einval</code> - Advisory locking not supported - <code>.vop_fsync = vop_null</code> - Successful no-op (for filesystems with no dirty buffers) - <code>.vop_fdatasync = vop_stdfdatasync</code> - Calls vop_fsync - <code>.vop_open = vop_stdopen</code> - Standard open logic - <code>.vop_close = vop_stdclose</code> - Standard close logic - <code>.vop_mmap = vop_einval</code> - Memory-mapping not supported by default - <code>.vop_readlink = vop_einval</code> - Not a symlink - <code>.vop_markatime = vop_stdmarkatime</code> - Standard atime marking</p> <p>Compatibility wrappers (old namespace API \u2192 new): - <code>.vop_nresolve = vop_compat_nresolve</code> - <code>.vop_ncreate = vop_compat_ncreate</code> - <code>.vop_nmkdir = vop_compat_nmkdir</code> - <code>.vop_nremove = vop_compat_nremove</code> - <code>.vop_nrename = vop_compat_nrename</code></p> <p>These wrappers translate new-style namecache operations (VOPs taking <code>struct nchandle *</code>) to old-style operations (VOPs taking <code>struct componentname *</code>), allowing legacy filesystems to work with modern code.</p>"},{"location":"sys/kern/vfs/#standard-error-returns","title":"Standard Error Returns","text":"<pre><code>int vop_eopnotsupp(struct vop_generic_args *ap) { return EOPNOTSUPP; }\nint vop_ebadf(struct vop_generic_args *ap)      { return EBADF; }\nint vop_enotty(struct vop_generic_args *ap)     { return ENOTTY; }\nint vop_einval(struct vop_generic_args *ap)     { return EINVAL; }\nint vop_null(struct vop_generic_args *ap)       { return 0; }\n</code></pre>"},{"location":"sys/kern/vfs/#standard-implementations","title":"Standard Implementations","text":"<p>vop_stdopen(): - For VCHR (character device): calls <code>spec_open()</code> - For VFIFO: calls <code>fifo_open()</code> - Otherwise: returns 0</p> <p>vop_stdclose(): - For VCHR: calls <code>spec_close()</code> - For VFIFO: calls <code>fifo_close()</code> - Otherwise: returns 0</p> <p>vop_stdgetattr_lite(): - Calls <code>VOP_GETATTR()</code> and extracts lightweight fields - Used for stat-like operations that don't need full vattr</p> <p>vop_stdmarkatime(): - Sets <code>VN_ATIME</code> flag on vnode - Actual atime update deferred until vnode is written back</p> <p>vop_stdpathconf(): - Returns standard POSIX path configuration values - <code>_PC_LINK_MAX</code>, <code>_PC_NAME_MAX</code>, <code>_PC_PATH_MAX</code>, etc.</p> <p>vop_stdallocate(): - Default fallocate(2) implementation - Simply extends file size via <code>VOP_SETATTR()</code> (non-sparse)</p>"},{"location":"sys/kern/vfs/#compatibility-layer-old-new-namespace-api","title":"Compatibility Layer (Old \u2192 New Namespace API)","text":"<p>vop_compat_nresolve(): Translates <code>VOP_NRESOLVE(nch, dvp, cred)</code> to: 1. Extract componentname from nch 2. Call <code>VOP_OLD_LOOKUP(dvp, &amp;vp, cnp)</code> 3. Cache result in nch</p> <p>vop_compat_ncreate(): Translates <code>VOP_NCREATE(nch, dvp, &amp;vp, cred, vap)</code> to: 1. Lock parent directory exclusively 2. Call <code>VOP_OLD_CREATE(dvp, &amp;vp, cnp, vap)</code> 3. Cache new vnode in nch</p> <p>vop_compat_nremove(): Translates <code>VOP_NREMOVE(nch, dvp, cred)</code> to: 1. Extract componentname 2. Lock parent and target 3. Call <code>VOP_OLD_REMOVE(dvp, vp, cnp)</code></p> <p>Similar wrappers exist for nmkdir, nmknod, nlink, nsymlink, nrmdir, nrename, nwhiteout.</p> <p>Purpose: Allows old filesystems (written for the componentname API) to work transparently with the modern namecache-centric API.</p>"},{"location":"sys/kern/vfs/#summary-phase-6a","title":"Summary: Phase 6a","text":"<p>Files analyzed (7 files, 9,451 lines): 1. vfs_init.c (504 lines) - VFS/vfsconf initialization, vnode ops registration 2. vfs_conf.c (713 lines) - Root filesystem mounting, interactive prompt 3. vfs_subr.c (2,650 lines) - Vnode lifecycle, buffer management, sync operations 4. vfs_vfsops.c (321 lines) - MPSAFE VFS operation wrappers 5. vfs_vnops.c (1,352 lines) - High-level vnode operations (vn_open, vn_close, vn_rdwr) 6. vfs_vopops.c (2,227 lines) - Vnode operation dispatch layer 7. vfs_default.c (1,684 lines) - Default VOP implementations, compatibility layer</p> <p>Key Concepts: - Vnode - In-memory file/directory representation - VFS operations - Filesystem-level (mount, statfs, sync) - Vnode operations - File-level (open, read, write, lookup) - vfsconf - Filesystem type registry - MPSAFE wrappers - Per-mount/vnode operation locking - Namespace API - Modern namecache-centric operations (VOP_NRESOLVE, VOP_NCREATE) - Compatibility layer - Old componentname API \u2192 new namecache API</p> <p>Next Phase 6 Steps: - 6b: Name lookup and caching (vfs_cache.c, vfs_nlookup.c) - 6c: Mounting and syscalls (vfs_mount.c, vfs_syscalls.c) - 6d: Buffer cache and I/O (vfs_bio.c, vfs_cluster.c) - 6e: Extensions (locking, journaling, quota, AIO)</p>"},{"location":"sys/kern/vfs/buffer-cache/","title":"VFS Buffer Cache and I/O","text":""},{"location":"sys/kern/vfs/buffer-cache/#overview","title":"Overview","text":"<p>The VFS buffer cache is a critical subsystem that mediates between the filesystem layer and the underlying storage devices while integrating tightly with the VM system. It provides caching for filesystem metadata and file data, manages asynchronous and synchronous I/O, implements read-ahead and write-behind optimizations, and coordinates between buffer-based and VM-page-based I/O.</p> <p>Key source files: - <code>sys/kern/vfs_bio.c</code> (4,659 lines) - Buffer cache management - <code>sys/kern/vfs_cluster.c</code> (1,814 lines) - Cluster I/O optimization - <code>sys/kern/vfs_vm.c</code> (503 lines) - VM integration - <code>sys/sys/buf.h</code> - Buffer structure definitions - <code>sys/sys/bio.h</code> - BIO layer structures</p> <p>Core responsibilities: - Cache filesystem blocks in memory - Manage dirty data and write-behind - Implement read-ahead for sequential access - Cluster I/O operations for performance - Integrate with VM system for unified caching - Provide async/sync I/O primitives</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-structure","title":"Buffer Structure","text":""},{"location":"sys/kern/vfs/buffer-cache/#struct-buf","title":"struct buf","text":"<p>The <code>struct buf</code> (sys/sys/buf.h:153) is the central data structure representing a cached filesystem block:</p> <pre><code>struct buf {\n    /* Tree linkages */\n    RB_ENTRY(buf) b_rbnode;         /* RB node in vnode clean/dirty tree */\n    RB_ENTRY(buf) b_rbhash;         /* RB node in vnode hash tree */\n    TAILQ_ENTRY(buf) b_freelist;    /* Free list position if not active */\n    struct buf *b_cluster_next;     /* Next buffer (cluster code) */\n\n    /* Vnode association */\n    struct vnode *b_vp;             /* Vnode for this buffer */\n\n    /* BIO translation layers */\n    struct bio b_bio_array[NBUF_BIO]; /* Typically 6 layers */\n\n    /* State and control */\n    u_int32_t b_flags;              /* B_* flags */\n    unsigned int b_qindex;          /* Buffer queue index */\n    unsigned int b_qcpu;            /* Buffer queue CPU */\n    unsigned char b_act_count;      /* Activity count (like vm_page) */\n    unsigned char b_swindex;        /* Swap index */\n    cpumask_t b_cpumask;            /* KVABIO API CPU mask */\n    struct lock b_lock;             /* Buffer lock */\n    buf_cmd_t b_cmd;                /* I/O command */\n\n    /* Size fields */\n    int b_bufsize;                  /* Allocated buffer size (filesystem block) */\n    int b_runningbufspace;          /* When I/O is running, pipelining */\n    int b_bcount;                   /* Valid bytes in buffer */\n    int b_resid;                    /* Remaining I/O */\n    int b_error;                    /* Error return */\n\n    /* Data pointers */\n    caddr_t b_data;                 /* Data pointer (KVA) */\n    caddr_t b_kvabase;              /* Base KVA for buffer */\n    int b_kvasize;                  /* Size of KVA for buffer */\n\n    /* Dirty tracking */\n    int b_dirtyoff;                 /* Offset in buffer of dirty region */\n    int b_dirtyend;                 /* Offset of end of dirty region */\n\n    /* Reference counting */\n    int b_refs;                     /* FINDBLK_REF/bqhold()/bqdrop() */\n\n    /* Page list management */\n    struct xio b_xio;               /* Data buffer page list management */\n\n    /* Filesystem dependencies */\n    struct bio_ops *b_ops;          /* Bio_ops used w/ b_dep */\n    union {\n        struct workhead b_dep;      /* List of filesystem dependencies */\n        void *b_priv;               /* Filesystem private data */\n    };\n};\n</code></pre> <p>Key field groups:</p> <ol> <li>Indexing: b_rbnode, b_rbhash organize buffers by (vnode, offset)</li> <li>Bio layers: b_bio_array[] provides I/O address translation</li> <li>State: b_flags, b_cmd, b_error track buffer state</li> <li>Sizing: b_bufsize (allocation), b_bcount (valid data), b_resid (remaining)</li> <li>Data: b_data points to KVA, b_xio manages VM pages</li> <li>Dirty: b_dirtyoff/b_dirtyend track partial dirty ranges</li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#bio-layer-fields","title":"BIO Layer Fields","text":"<p>b_bio1 (b_bio_array[0]) - Logical layer: - Contains logical offset (b_loffset = b_bio1.bio_offset) - Used with primary vnode (bp-&gt;b_vp) - Operations: <code>vn_strategy(bp-&gt;b_vp, &amp;bp-&gt;b_bio1)</code></p> <p>b_bio2 (b_bio_array[1]) - Physical layer: - Contains device-relative offset (translated from logical) - Used with device vnode in filesystems - Set by VOP_BMAP() call</p> <p>Additional layers: Allocated from object cache for device stacking (RAID, encryption, etc.)</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-flags-b_flags","title":"Buffer Flags (b_flags)","text":"<p>Buffer state flags (sys/sys/buf.h:304):</p> <p>Cache state: - <code>B_CACHE</code> (0x00000020) - Buffer found in cache, data valid - <code>B_INVAL</code> (0x00002000) - Buffer does not contain valid info - <code>B_DELWRI</code> (0x00000080) - Delayed write (dirty, needs flush) - <code>B_DIRTY</code> (0x00200000) - Needs writing later</p> <p>I/O state: - <code>B_ERROR</code> (0x00000800) - I/O error occurred - <code>B_EINTR</code> (0x00000400) - I/O was interrupted - <code>B_IOISSUED</code> (0x00001000) - I/O has been issued (vfs can clear)</p> <p>VM integration: - <code>B_VMIO</code> (0x20000000) - Buffer tied to VM object - <code>B_PAGING</code> (0x04000000) - Volatile paging I/O, bypass VMIO - <code>B_RAM</code> (0x10000000) - Read-ahead mark</p> <p>Clustering: - <code>B_CLUSTER</code> (0x40000000) - Part of a cluster operation - <code>B_CLUSTEROK</code> (0x00020000) - May be clustered with adjacent buffers</p> <p>Locking: - <code>B_LOCKED</code> (0x00004000) - Locked in core (not reusable) - <code>B_KVABIO</code> (0x00010000) - Lockholder uses KVABIO API</p> <p>Lifecycle: - <code>B_AGE</code> (0x00000001) - Reuse more quickly - <code>B_RELBUF</code> (0x00400000) - Release VMIO buffer - <code>B_NOCACHE</code> (0x00008000) - Destroy buffer AND backing store</p> <p>Special: - <code>B_HEAVY</code> (0x00100000) - Heavy-weight buffer (needs special handling) - <code>B_BNOCLIP</code> (0x00000100) - EOF clipping not allowed - <code>B_NOTMETA</code> (0x00000004) - Not metadata (affects VM page handling) - <code>B_MARKER</code> (0x00040000) - Special marker buffer in queue - <code>B_HASHED</code> (0x00000040) - Indexed via v_rbhash_tree</p> <p>Tree linkage: - <code>B_VNCLEAN</code> (0x01000000) - On vnode clean list - <code>B_VNDIRTY</code> (0x02000000) - On vnode dirty list</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-commands-buf_cmd_t","title":"Buffer Commands (buf_cmd_t)","text":"<p>I/O operation types (sys/sys/buf.h:87):</p> <pre><code>typedef enum buf_cmd {\n    BUF_CMD_DONE = 0,      /* I/O completed */\n    BUF_CMD_READ,          /* Read operation */\n    BUF_CMD_WRITE,         /* Write operation */\n    BUF_CMD_FREEBLKS,      /* Free blocks */\n    BUF_CMD_FORMAT,        /* Format operation */\n    BUF_CMD_FLUSH,         /* Cache flush */\n    BUF_CMD_SEEK,          /* Seek operation */\n} buf_cmd_t;\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-queues","title":"Buffer Queues","text":""},{"location":"sys/kern/vfs/buffer-cache/#per-cpu-queue-structure","title":"Per-CPU Queue Structure","text":"<p>Buffers are organized into per-CPU queues to reduce lock contention (vfs_bio.c:72):</p> <pre><code>enum bufq_type {\n    BQUEUE_NONE,        /* not on any queue */\n    BQUEUE_LOCKED,      /* locked buffers */\n    BQUEUE_CLEAN,       /* non-B_DELWRI buffers */\n    BQUEUE_DIRTY,       /* B_DELWRI buffers */\n    BQUEUE_DIRTY_HW,    /* B_DELWRI buffers - heavy weight */\n    BQUEUE_EMPTY,       /* empty buffer headers */\n\n    BUFFER_QUEUES       /* number of buffer queues */\n};\n\nstruct bufpcpu {\n    struct spinlock spin;\n    struct bqueues bufqueues[BUFFER_QUEUES];\n} __cachealign;\n\nstruct bufpcpu bufpcpu[MAXCPU];\n</code></pre> <p>Queue semantics:</p> <ul> <li>BQUEUE_NONE: Buffer is actively in use (locked, doing I/O)</li> <li>BQUEUE_LOCKED: Buffer explicitly locked with B_LOCKED flag</li> <li>BQUEUE_CLEAN: Clean cached buffers eligible for reuse</li> <li>BQUEUE_DIRTY: Dirty buffers awaiting flush</li> <li>BQUEUE_DIRTY_HW: Heavy-weight dirty buffers (special flushing)</li> <li>BQUEUE_EMPTY: Empty buffer headers (no data allocated)</li> </ul>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-lifecycle-through-queues","title":"Buffer Lifecycle Through Queues","text":"<ol> <li>Allocation (getnewbuf):</li> <li>Pull from BQUEUE_EMPTY or BQUEUE_CLEAN</li> <li> <p>State: BQUEUE_NONE (in use)</p> </li> <li> <p>Active use:</p> </li> <li>Buffer locked via BUF_LOCK()</li> <li>State: BQUEUE_NONE</li> <li> <p>I/O operations performed</p> </li> <li> <p>Release (brelse/bqrelse):</p> </li> <li>Unlock buffer</li> <li> <p>Move to appropriate queue:</p> <ul> <li>B_DELWRI \u2192 BQUEUE_DIRTY or BQUEUE_DIRTY_HW</li> <li>B_LOCKED \u2192 BQUEUE_LOCKED</li> <li>Clean \u2192 BQUEUE_CLEAN</li> </ul> </li> <li> <p>Reuse (getnewbuf):</p> </li> <li>Scan BQUEUE_CLEAN for victims</li> <li> <p>Invalidate and reallocate</p> </li> <li> <p>Flush (buf_daemon):</p> </li> <li>Scan BQUEUE_DIRTY/BQUEUE_DIRTY_HW</li> <li>Write dirty buffers asynchronously</li> <li>Move to BQUEUE_CLEAN after write completes</li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-cache-tuning-parameters","title":"Buffer Cache Tuning Parameters","text":""},{"location":"sys/kern/vfs/buffer-cache/#sysctl-tunables","title":"Sysctl Tunables","text":"<p>Space management (vfs_bio.c:162-210):</p> <pre><code>/* Operational control */\nlong maxbufspace;          /* Hard limit on buffer space */\nlong hibufspace;           /* Soft limit (high watermark) */\nlong lobufspace;           /* Low watermark */\nlong bufspace;             /* Current buffer space used */\n\nlong lodirtybufspace;      /* Trigger buf_daemon activation */\nlong hidirtybufspace;      /* High watermark for dirty buffers */\nlong dirtybufspace;        /* Current dirty buffer space */\nlong dirtybufcount;        /* Number of dirty buffers */\nlong dirtybufspacehw;      /* Dirty space (heavy-weight) */\nlong dirtybufcounthw;      /* Dirty count (heavy-weight) */\n\nlong lorunningspace;       /* Minimum space for active I/O */\nlong hirunningspace;       /* Maximum space for active I/O */\nlong runningbufspace;      /* Currently running I/O space */\nlong runningbufcount;      /* Currently running I/O count */\n\nu_int flushperqueue;       /* Buffers to flush per queue (default: 1024) */\nlong bufcache_bw;          /* Buffer\u2192VM transfer bandwidth (200 MB/s) */\n</code></pre> <p>Watermark behavior:</p> <ul> <li>lobufspace \u2192 hibufspace: Normal operation, allocate freely</li> <li>&gt; hibufspace: Trigger aggressive buffer reclamation</li> <li>lodirtybufspace \u2192 hidirtybufspace: Normal dirty buffer accumulation</li> <li>&gt; hidirtybufspace: Wake buf_daemon to flush aggressively</li> <li>lorunningspace \u2192 hirunningspace: Control I/O pipeline depth</li> </ul>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-daemon-threads","title":"Buffer Daemon Threads","text":"<p>Two kernel threads manage buffer flushing (vfs_bio.c:155-156):</p> <p>bufdaemon_td - Standard buffer daemon: - Flushes BQUEUE_DIRTY when dirtybufspace &gt; lodirtybufspace - Triggered via bd_request atomic flag - Writes dirty buffers asynchronously - Moves flushed buffers to BQUEUE_CLEAN</p> <p>bufdaemonhw_td - Heavy-weight buffer daemon: - Flushes BQUEUE_DIRTY_HW - Separate thread prevents deadlock (heavy buffers may need more buffers to flush) - Triggered via bd_request_hw atomic flag</p> <p>bd_signal() (vfs_bio.c:4522): Wakes buffer daemons based on dirty space:</p> <pre><code>static void bd_signal(long totalspace)\n{\n    if (totalspace &gt; 0 &amp;&amp;\n        runningbufspace + dirtykvaspace &gt;= lodirtybufspace) {\n        atomic_set_int(&amp;bd_request, 1);\n        wakeup(&amp;bd_request);\n\n        if (dirtybufspacehw &gt; lodirtybufspace / 2) {\n            atomic_set_int(&amp;bd_request_hw, 1);\n            wakeup(&amp;bd_request_hw);\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-allocation","title":"Buffer Allocation","text":""},{"location":"sys/kern/vfs/buffer-cache/#getnewbuf-core-allocation","title":"getnewbuf() - Core Allocation","text":"<p>getnewbuf() (vfs_bio.c:1885)</p> <p>Allocates a buffer for use, reusing existing buffers when necessary:</p> <pre><code>struct buf *getnewbuf(int blkflags, int slptimeo, int size, int maxsize)\n</code></pre> <p>Allocation strategy:</p> <ol> <li> <p>Check space limits:    <pre><code>while (bufspace + maxsize &gt; hibufspace)\n    bufspacewakeup();  /* wait for space */\n</code></pre></p> </li> <li> <p>Try BQUEUE_EMPTY (fast path):</p> </li> <li>Pull empty buffer header</li> <li> <p>No data backing store yet</p> </li> <li> <p>Scan BQUEUE_CLEAN (reuse path):</p> </li> <li>Look for buffers with B_AGE (prefer aged buffers)</li> <li>Check lock availability (LK_NOWAIT)</li> <li> <p>Validate buffer can be reused:</p> <ul> <li>Not locked (B_LOCKED clear)</li> <li>Not in I/O (B_IOISSUED clear)</li> <li>Not referenced (b_refs == 0)</li> </ul> </li> <li> <p>Flush and wait (pressure path):</p> </li> <li>If no buffers available, flush BQUEUE_DIRTY</li> <li>Wait for buffers to become available</li> <li> <p>Retry allocation</p> </li> <li> <p>Initialize buffer:    <pre><code>bp-&gt;b_flags = B_CACHE;  /* Start cached */\nbp-&gt;b_cmd = BUF_CMD_DONE;\nbp-&gt;b_qindex = BQUEUE_NONE;\nbp-&gt;b_error = 0;\n</code></pre></p> </li> </ol> <p>Heavy-weight handling:</p> <p>Heavy-weight buffers (B_HEAVY) have restrictions: - May need additional buffers to complete write - Prevented from being allocated when dirty space is high - Separate daemon thread (bufdaemonhw_td) for flushing</p>"},{"location":"sys/kern/vfs/buffer-cache/#getblk-cached-block-access","title":"getblk() - Cached Block Access","text":"<p>getblk() (vfs_bio.c:2729)</p> <p>Main entry point for accessing cached filesystem blocks:</p> <pre><code>struct buf *getblk(struct vnode *vp, off_t loffset, int size, \n                   int blkflags, int slptimeo)\n</code></pre> <p>Lookup and allocation workflow:</p> <ol> <li> <p>Search vnode's buffer tree:    <pre><code>bp = findblk(vp, loffset, FINDBLK_NBLOCK);\nif (bp) {\n    /* Found in cache */\n    if (BUF_LOCK(bp, LK_EXCLUSIVE | LK_NOWAIT)) {\n        /* Lock contention, retry or wait */\n    }\n    return bp;  /* Cache hit */\n}\n</code></pre></p> </li> <li> <p>Allocate new buffer:    <pre><code>bp = getnewbuf(blkflags, slptimeo, size, maxsize);\n</code></pre></p> </li> <li> <p>Insert into vnode's tree:    <pre><code>lwkt_gettoken(&amp;vp-&gt;v_token);\n/* Re-check for race (someone else inserted) */\nbp2 = findblk(vp, loffset, 0);\nif (bp2) {\n    /* Lost race, use bp2 */\n    brelse(bp);\n    bp = bp2;\n} else {\n    /* Won race, insert bp */\n    bp-&gt;b_vp = vp;\n    bp-&gt;b_loffset = loffset;\n    buf_rb_tree_RB_INSERT(&amp;vp-&gt;v_rbclean_tree, bp);\n    buf_rb_hash_RB_INSERT(&amp;vp-&gt;v_rbhash_tree, bp);\n    bp-&gt;b_flags |= B_HASHED | B_VNCLEAN;\n}\nlwkt_reltoken(&amp;vp-&gt;v_token);\n</code></pre></p> </li> <li> <p>Handle size changes:</p> </li> <li> <p>If buffer exists but size doesn't match:</p> <ul> <li>GETBLK_SZMATCH: Return NULL</li> <li>Otherwise: Reallocate buffer with new size</li> </ul> </li> <li> <p>Initialize for use:    <pre><code>if ((bp-&gt;b_flags &amp; B_CACHE) == 0) {\n    /* Not valid, caller must issue I/O */\n    bp-&gt;b_flags &amp;= ~(B_ERROR | B_INVAL);\n    /* Don't set BUF_CMD_READ here, caller does it */\n}\n</code></pre></p> </li> </ol> <p>Flags: - <code>GETBLK_PCATCH</code>: Allow signals (can return NULL) - <code>GETBLK_BHEAVY</code>: Mark as heavy-weight buffer - <code>GETBLK_SZMATCH</code>: Fail if size doesn't match - <code>GETBLK_NOWAIT</code>: Non-blocking lock - <code>GETBLK_KVABIO</code>: Request KVABIO buffer</p> <p>Return states: - Buffer locked and ready for use - B_CACHE set if data valid - B_CACHE clear if I/O needed</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-io-operations","title":"Buffer I/O Operations","text":""},{"location":"sys/kern/vfs/buffer-cache/#bread-synchronous-read","title":"bread() - Synchronous Read","text":"<p>bread() (vfs_bio.c:857)</p> <p>Reads a single block synchronously:</p> <pre><code>int bread(struct vnode *vp, off_t loffset, int size, struct buf **bpp)\n</code></pre> <p>Workflow:</p> <pre><code>/* Get buffer (from cache or allocate) */\nbp = getblk(vp, loffset, size, 0, 0);\n\n/* If not in cache, issue I/O */\nif ((bp-&gt;b_flags &amp; B_CACHE) == 0) {\n    bp-&gt;b_flags &amp;= ~(B_ERROR | B_EINTR | B_INVAL);\n    bp-&gt;b_cmd = BUF_CMD_READ;\n    bp-&gt;b_bio1.bio_done = biodone_sync;\n    bp-&gt;b_bio1.bio_flags |= BIO_SYNC;\n    vfs_busy_pages(vp, bp);\n    vn_strategy(vp, &amp;bp-&gt;b_bio1);\n    error = biowait(&amp;bp-&gt;b_bio1, \"biord\");\n}\n\n*bpp = bp;  /* Return locked buffer */\nreturn error;\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#breadn-read-with-read-ahead","title":"breadn() - Read with Read-ahead","text":"<p>breadn() (vfs_bio.c:892)</p> <p>Reads a block with optional read-ahead:</p> <pre><code>int breadn(struct vnode *vp, off_t loffset, int size, int bflags,\n           off_t *raoffset, int *rabsize, int cnt, struct buf **bpp)\n</code></pre> <p>Read-ahead strategy:</p> <ol> <li> <p>Issue primary read (synchronous):    <pre><code>bp = getblk(vp, loffset, size, 0, 0);\nif ((bp-&gt;b_flags &amp; B_CACHE) == 0) {\n    /* Issue sync I/O */\n    vn_strategy(vp, &amp;bp-&gt;b_bio1);\n    readwait = 1;\n}\n</code></pre></p> </li> <li> <p>Issue read-ahead requests (asynchronous):    <pre><code>for (i = 0; i &lt; cnt; i++) {\n    if (inmem(vp, raoffset[i]))\n        continue;  /* Already cached */\n\n    rabp = getblk(vp, raoffset[i], rabsize[i], 0, 0);\n    if ((rabp-&gt;b_flags &amp; B_CACHE) == 0) {\n        rabp-&gt;b_cmd = BUF_CMD_READ;\n        BUF_KERNPROC(rabp);  /* Async, owned by kernel */\n        vn_strategy(vp, &amp;rabp-&gt;b_bio1);\n    } else {\n        brelse(rabp);  /* Already cached */\n    }\n}\n</code></pre></p> </li> <li> <p>Wait for primary read:    <pre><code>if (readwait)\n    error = biowait(&amp;bp-&gt;b_bio1, \"biord\");\n</code></pre></p> </li> </ol> <p>Read-ahead benefits: - Overlap disk I/O with CPU processing - Exploit sequential access patterns - Improve throughput for streaming reads</p>"},{"location":"sys/kern/vfs/buffer-cache/#bwrite-synchronous-write","title":"bwrite() - Synchronous Write","text":"<p>bwrite() (vfs_bio.c:963)</p> <p>Writes a buffer synchronously:</p> <pre><code>int bwrite(struct buf *bp)\n</code></pre> <p>Workflow:</p> <pre><code>if (bp-&gt;b_flags &amp; B_INVAL) {\n    brelse(bp);\n    return 0;\n}\n\n/* Clear errors, mark cached */\nbp-&gt;b_flags &amp;= ~(B_ERROR | B_EINTR);\nbp-&gt;b_flags |= B_CACHE;\nbp-&gt;b_cmd = BUF_CMD_WRITE;\nbp-&gt;b_bio1.bio_done = biodone_sync;\nbp-&gt;b_bio1.bio_flags |= BIO_SYNC;\n\nvfs_busy_pages(bp-&gt;b_vp, bp);\nbsetrunningbufspace(bp, bp-&gt;b_bufsize);  /* Account running space */\nvn_strategy(bp-&gt;b_vp, &amp;bp-&gt;b_bio1);\n\nerror = biowait(&amp;bp-&gt;b_bio1, \"biows\");\nbrelse(bp);\nreturn error;\n</code></pre> <p>Key points: - Waits for I/O completion - Sets B_CACHE (data valid after write) - Tracks running I/O space - Always releases buffer after completion</p>"},{"location":"sys/kern/vfs/buffer-cache/#bawrite-asynchronous-write","title":"bawrite() - Asynchronous Write","text":"<p>bawrite() (vfs_bio.c:1014)</p> <p>Writes a buffer asynchronously:</p> <pre><code>void bawrite(struct buf *bp)\n</code></pre> <p>Differences from bwrite(): - Does NOT wait for completion (no biowait) - Uses default biodone (not biodone_sync) - Marks buffer as kernel-owned (BUF_KERNPROC) - Returns immediately</p> <pre><code>bp-&gt;b_flags &amp;= ~(B_ERROR | B_EINTR);\nbp-&gt;b_flags |= B_CACHE;\nbp-&gt;b_cmd = BUF_CMD_WRITE;\nbp-&gt;b_bio1.bio_done = NULL;  /* Use default */\n\nvfs_busy_pages(bp-&gt;b_vp, bp);\nbsetrunningbufspace(bp, bp-&gt;b_bufsize);\nBUF_KERNPROC(bp);  /* Transfer to kernel */\nvn_strategy(bp-&gt;b_vp, &amp;bp-&gt;b_bio1);\n/* Returns immediately, I/O in progress */\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#bdwrite-delayed-write","title":"bdwrite() - Delayed Write","text":"<p>bdwrite() (vfs_bio.c:1060)</p> <p>Marks a buffer dirty for later writing:</p> <pre><code>void bdwrite(struct buf *bp)\n</code></pre> <p>Delayed write behavior:</p> <pre><code>bdirty(bp);  /* Mark B_DELWRI, move to dirty tree */\nbp-&gt;b_flags |= B_CACHE;\n\n/* Pre-map physical block to avoid deadlock during sync */\nif (bp-&gt;b_bio2.bio_offset == NOOFFSET) {\n    VOP_BMAP(bp-&gt;b_vp, bp-&gt;b_loffset, \n             &amp;bp-&gt;b_bio2.bio_offset, NULL, NULL, \n             BUF_CMD_WRITE);\n}\n\n/* Mark pages clean (earmarked for buffer flush) */\nvfs_clean_pages(bp);\nbqrelse(bp);  /* Release to dirty queue */\n</code></pre> <p>Why delay writes? - Batch multiple writes together - Allow cancellation (if file deleted) - Enable write clustering - Avoid synchronous delays</p> <p>VOP_BMAP call importance: - Pre-translates logical\u2192physical block mapping - Avoids needing buffers during sync (prevents deadlock) - Memory for indirect blocks may not be available during sync</p>"},{"location":"sys/kern/vfs/buffer-cache/#bdirty-mark-buffer-dirty","title":"bdirty() - Mark Buffer Dirty","text":"<p>bdirty() (vfs_bio.c:1163)</p> <p>Core function to mark a buffer dirty:</p> <pre><code>void bdirty(struct buf *bp)\n{\n    KASSERT(bp-&gt;b_qindex == BQUEUE_NONE, ...);\n\n    bp-&gt;b_flags &amp;= ~(B_RELBUF | B_NOCACHE);\n\n    if ((bp-&gt;b_flags &amp; B_DELWRI) == 0) {\n        lwkt_gettoken(&amp;bp-&gt;b_vp-&gt;v_token);\n        bp-&gt;b_flags |= B_DELWRI;\n        reassignbuf(bp);  /* Move from clean\u2192dirty tree */\n        lwkt_reltoken(&amp;bp-&gt;b_vp-&gt;v_token);\n\n        /* Update global counters */\n        atomic_add_long(&amp;dirtybufcount, 1);\n        atomic_add_long(&amp;dirtykvaspace, bp-&gt;b_kvasize);\n        atomic_add_long(&amp;dirtybufspace, bp-&gt;b_bufsize);\n        if (bp-&gt;b_flags &amp; B_HEAVY) {\n            atomic_add_long(&amp;dirtybufcounthw, 1);\n            atomic_add_long(&amp;dirtybufspacehw, bp-&gt;b_bufsize);\n        }\n\n        bd_heatup();  /* Signal buffer daemon */\n    }\n}\n</code></pre> <p>reassignbuf(): Moves buffer between vnode trees: - From: <code>vp-&gt;v_rbclean_tree</code>, <code>B_VNCLEAN</code> - To: <code>vp-&gt;v_rbdirty_tree</code>, <code>B_VNDIRTY</code></p>"},{"location":"sys/kern/vfs/buffer-cache/#buwrite-fake-write-tmpfs","title":"buwrite() - Fake Write (tmpfs)","text":"<p>buwrite() (vfs_bio.c:1127)</p> <p>Used by tmpfs to mark pages dirty without writing to disk:</p> <pre><code>void buwrite(struct buf *bp)\n{\n    /* Only for VMIO buffers */\n    if ((bp-&gt;b_flags &amp; B_VMIO) == 0 || (bp-&gt;b_flags &amp; B_DELWRI)) {\n        bdwrite(bp);\n        return;\n    }\n\n    /* Mark VM pages as needing commit */\n    for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n        m = bp-&gt;b_xio.xio_pages[i];\n        vm_page_need_commit(m);\n    }\n\n    bqrelse(bp);  /* Release without marking buffer dirty */\n}\n</code></pre> <p>Use case: - tmpfs stores data in VM pages, not disk - Pages need marking dirty for VM system - Buffer itself doesn't need writing</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-release","title":"Buffer Release","text":""},{"location":"sys/kern/vfs/buffer-cache/#brelse-standard-release","title":"brelse() - Standard Release","text":"<p>brelse() (vfs_bio.c:1268)</p> <p>Releases a buffer back to the cache:</p> <pre><code>void brelse(struct buf *bp)\n</code></pre> <p>Release workflow:</p> <ol> <li> <p>Clear transient flags:    <pre><code>bp-&gt;b_flags &amp;= ~(B_IOISSUED | B_EINTR | B_NOTMETA | B_KVABIO);\n</code></pre></p> </li> <li> <p>Handle B_NOCACHE (destroy request):    <pre><code>if (bp-&gt;b_flags &amp; B_NOCACHE) {\n    bp-&gt;b_flags |= B_INVAL;\n}\n</code></pre></p> </li> <li> <p>Handle B_INVAL (invalidate):    <pre><code>if (bp-&gt;b_flags &amp; B_INVAL) {\n    if (bp-&gt;b_flags &amp; (B_DELWRI | B_VNDIRTY))\n        bundirty(bp);  /* Remove from dirty tree */\n    if (bp-&gt;b_flags &amp; B_HASHED)\n        buf_rb_hash_RB_REMOVE(&amp;vp-&gt;v_rbhash_tree, bp);\n    if (bp-&gt;b_flags &amp; (B_VNCLEAN | B_VNDIRTY))\n        buf_rb_tree_RB_REMOVE(..., bp);\n    bp-&gt;b_vp = NULL;\n    bp-&gt;b_flags &amp;= ~(B_HASHED | B_VNCLEAN | B_VNDIRTY);\n}\n</code></pre></p> </li> <li> <p>Determine destination queue:    <pre><code>if (bp-&gt;b_flags &amp; B_LOCKED)\n    qindex = BQUEUE_LOCKED;\nelse if (bp-&gt;b_flags &amp; B_DELWRI)\n    qindex = (bp-&gt;b_flags &amp; B_HEAVY) ? \n             BQUEUE_DIRTY_HW : BQUEUE_DIRTY;\nelse if (bp-&gt;b_vp)\n    qindex = BQUEUE_CLEAN;\nelse\n    qindex = BQUEUE_EMPTY;\n</code></pre></p> </li> <li> <p>Insert into queue:    <pre><code>spin_lock(&amp;bufqspin);\nTAILQ_INSERT_TAIL(&amp;bufqueues[qindex], bp, b_freelist);\nspin_unlock(&amp;bufqspin);\nbp-&gt;b_qindex = qindex;\n</code></pre></p> </li> <li> <p>Unlock buffer:    <pre><code>BUF_UNLOCK(bp);\n</code></pre></p> </li> <li> <p>Wake waiters:    <pre><code>bufcountwakeup();  /* Wake anyone waiting for buffers */\nbufspacewakeup();  /* Wake anyone waiting for space */\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#bqrelse-quick-release","title":"bqrelse() - Quick Release","text":"<p>bqrelse() (vfs_bio.c:1564)</p> <p>Optimized release for buffers expected to be reused:</p> <pre><code>void bqrelse(struct buf *bp)\n</code></pre> <p>Difference from brelse(): - Doesn't set B_AGE flag - Leaves buffer in favorable position for reuse - Used for metadata that's likely to be accessed again soon</p>"},{"location":"sys/kern/vfs/buffer-cache/#vm-integration-vmio","title":"VM Integration (VMIO)","text":""},{"location":"sys/kern/vfs/buffer-cache/#vmio-buffers","title":"VMIO Buffers","text":"<p>Buffers can be backed by VM pages instead of pure KVA (B_VMIO flag):</p> <p>Benefits: - Unified buffer cache and page cache - Pages shared between mmap() and read()/write() - Better memory utilization - Supports direct I/O to user pages</p> <p>b_xio structure (sys/sys/xio.h): Manages the list of VM pages backing the buffer:</p> <pre><code>struct xio {\n    int xio_npages;                    /* Number of pages */\n    int xio_flags;                     /* Flags */\n    vm_page_t xio_pages[XIO_INTERNAL_PAGES];  /* Page array */\n    struct vm_page *xio_internal_pages;        /* Internal storage */\n};\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_vmio_alloc-allocate-vm-pages","title":"vfs_vmio_alloc() - Allocate VM Pages","text":"<p>vfs_vmio_alloc() (vfs_bio.c:2247)</p> <p>Allocates VM pages to back a buffer:</p> <pre><code>static int vfs_vmio_alloc(struct buf *bp, off_t loffset, \n                          int size, int bsize)\n</code></pre> <p>Allocation workflow:</p> <ol> <li> <p>Calculate page range:    <pre><code>vm_object_t obj = vp-&gt;v_object;\noff_t pgoff = loffset &amp; PAGE_MASK;\nsize_t npages = btoc(round_page(size + pgoff));\n</code></pre></p> </li> <li> <p>Allocate/lookup pages:    <pre><code>for (i = 0; i &lt; npages; i++) {\n    vm_pindex_t pg = btop(loffset) + i;\n    m = bio_page_alloc(bp, obj, pg, ...);\n    bp-&gt;b_xio.xio_pages[i] = m;\n}\n</code></pre></p> </li> <li> <p>Set buffer properties:    <pre><code>bp-&gt;b_flags |= B_VMIO;\nbp-&gt;b_data = (caddr_t)(pgoff + (vm_offset_t)m);\nbp-&gt;b_xio.xio_npages = npages;\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_bio_clrbuf-clear-buffer","title":"vfs_bio_clrbuf() - Clear Buffer","text":"<p>vfs_bio_clrbuf() (vfs_bio.c:729)</p> <p>Zeros a buffer and validates all pages:</p> <pre><code>void vfs_bio_clrbuf(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_VMIO) {\n        /* Zero VM pages */\n        for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n            m = bp-&gt;b_xio.xio_pages[i];\n            if (m-&gt;valid != VM_PAGE_BITS_ALL) {\n                pmap_zero_page(m-&gt;phys_addr);\n                m-&gt;valid = VM_PAGE_BITS_ALL;\n                m-&gt;dirty = 0;\n            }\n        }\n        bp-&gt;b_resid = 0;\n    } else {\n        /* Zero KVA */\n        clrbuf(bp);\n    }\n    bp-&gt;b_flags |= B_CACHE;\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_busy_pages-vfs_unbusy_pages","title":"vfs_busy_pages() / vfs_unbusy_pages()","text":"<p>vfs_busy_pages() (vfs_bio.c:4051)</p> <p>Prepares VM pages for I/O:</p> <pre><code>void vfs_busy_pages(struct vnode *vp, struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_VMIO) {\n        for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n            m = bp-&gt;b_xio.xio_pages[i];\n\n            if (bp-&gt;b_cmd == BUF_CMD_READ) {\n                m-&gt;flags &amp;= ~PG_ZERO;\n                vm_page_io_start(m);\n            } else {\n                vm_page_protect(m, VM_PROT_READ);\n                vm_page_io_start(m);\n            }\n        }\n    }\n}\n</code></pre> <p>vfs_unbusy_pages() (vfs_bio.c:4096)</p> <p>Completes I/O on VM pages:</p> <pre><code>void vfs_unbusy_pages(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_VMIO) {\n        for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n            m = bp-&gt;b_xio.xio_pages[i];\n            vm_page_io_finish(m);\n\n            if (bp-&gt;b_cmd == BUF_CMD_READ &amp;&amp; !error) {\n                /* Mark page valid after successful read */\n                m-&gt;valid = VM_PAGE_BITS_ALL;\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_vmio_release-release-vm-pages","title":"vfs_vmio_release() - Release VM Pages","text":"<p>vfs_vmio_release() (vfs_bio.c:1233)</p> <p>Releases VM pages when buffer destroyed:</p> <pre><code>static void vfs_vmio_release(struct buf *bp)\n{\n    for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n        m = bp-&gt;b_xio.xio_pages[i];\n        bp-&gt;b_xio.xio_pages[i] = NULL;\n\n        vm_page_busy_wait(m, FALSE, \"vmiorl\");\n\n        /* Free page if appropriate */\n        if (bp-&gt;b_flags &amp; (B_NOCACHE | B_DIRECT)) {\n            vm_page_try_to_free(m);\n        } else {\n            vm_page_try_to_cache(m);\n        }\n        vm_page_wakeup(m);\n    }\n    bp-&gt;b_xio.xio_npages = 0;\n    bp-&gt;b_flags &amp;= ~B_VMIO;\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#cluster-io","title":"Cluster I/O","text":""},{"location":"sys/kern/vfs/buffer-cache/#overview_1","title":"Overview","text":"<p>Cluster I/O optimization groups contiguous filesystem blocks into single I/O operations for improved throughput. Implemented in <code>sys/kern/vfs_cluster.c</code>.</p> <p>Benefits: - Reduces per-I/O overhead - Better utilizes disk bandwidth - Exploits spatial locality - Amortizes seek time over multiple blocks</p>"},{"location":"sys/kern/vfs/buffer-cache/#cluster-cache","title":"Cluster Cache","text":"<p>cluster_cache_t structure (vfs_cluster.c:70):</p> <pre><code>typedef struct cluster_cache {\n    off_t cc_loffset;          /* Logical offset (cluster start) */\n    off_t cc_lastloffset;      /* Last offset in cluster */\n    int cc_flags;              /* Flags */\n    struct vnode *cc_vp;       /* Vnode */\n    int cc_refs;               /* Reference count */\n} cluster_cache_t;\n\n#define CLUSTER_CACHE_SIZE 16\ncluster_cache_t cluster_array[CLUSTER_CACHE_SIZE];\n</code></pre> <p>Per-vnode cluster state: - Tracks sequential access patterns - Maintains read-ahead context - Cached in global array indexed by vnode</p> <p>cluster_getcache() / cluster_putcache(): Manage cluster cache entries</p>"},{"location":"sys/kern/vfs/buffer-cache/#cluster_read-clustered-read","title":"cluster_read() - Clustered Read","text":"<p>cluster_read() (vfs_cluster.c:224)</p> <p>Main entry point for clustered read operations:</p> <pre><code>int cluster_read(struct vnode *vp, off_t filesize, off_t loffset,\n                 int blksize, int totalbytes, int seqcount, \n                 struct buf **bpp)\n</code></pre> <p>Parameters: - <code>filesize</code>: Total file size - <code>loffset</code>: Logical offset to read - <code>blksize</code>: Filesystem block size - <code>totalbytes</code>: Total read size - <code>seqcount</code>: Sequential access count (for read-ahead) - <code>bpp</code>: Returns buffer pointer</p> <p>Read clustering workflow:</p> <ol> <li> <p>Check for existing buffer:    <pre><code>bp = getblk(vp, loffset, blksize, 0, 0);\nif (bp-&gt;b_flags &amp; B_CACHE) {\n    *bpp = bp;\n    return 0;  /* Cache hit */\n}\n</code></pre></p> </li> <li> <p>Determine cluster size:    <pre><code>/* Use seqcount to scale read-ahead */\nmaxra = seqcount * blksize;\nmaxra = min(maxra, MAXPHYS);  /* Cap at MAXPHYS (128KB) */\n</code></pre></p> </li> <li> <p>Build read-ahead list:    <pre><code>for (i = 1; i &lt; maxblocks; i++) {\n    if (inmem(vp, loffset + i * blksize))\n        break;  /* Stop at cached block */\n    raoffset[rablks] = loffset + i * blksize;\n    rabsize[rablks] = blksize;\n    rablks++;\n}\n</code></pre></p> </li> <li> <p>Issue clustered I/O:    <pre><code>error = cluster_rbuild(vp, filesize, bp, \n                       loffset, raoffset, rabsize, rablks);\n</code></pre></p> </li> <li> <p>Update cluster cache:    <pre><code>cc = cluster_getcache(NULL, vp, loffset);\ncc-&gt;cc_lastloffset = loffset + rablks * blksize;\ncluster_putcache(cc);\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#cluster_rbuild-build-read-cluster","title":"cluster_rbuild() - Build Read Cluster","text":"<p>cluster_rbuild() (vfs_cluster.c:893)</p> <p>Constructs a clustered read I/O operation:</p> <pre><code>static int cluster_rbuild(struct vnode *vp, off_t filesize, \n                          struct buf *bp, off_t loffset,\n                          off_t *raoffset, int *rabsize, int rablks)\n</code></pre> <p>Clustering strategy:</p> <ol> <li> <p>Allocate read-ahead buffers:    <pre><code>for (i = 0; i &lt; rablks; i++) {\n    rabp = getblk(vp, raoffset[i], rabsize[i], 0, 0);\n    if (rabp-&gt;b_flags &amp; B_CACHE) {\n        brelse(rabp);\n        continue;  /* Skip cached */\n    }\n    rabp-&gt;b_flags |= B_RAM;  /* Mark read-ahead */\n    rabp-&gt;b_cmd = BUF_CMD_READ;\n    /* Link into cluster chain */\n    cluster_append(&amp;bp-&gt;b_bio1, rabp);\n}\n</code></pre></p> </li> <li> <p>Issue parent I/O:    <pre><code>bp-&gt;b_cmd = BUF_CMD_READ;\nbp-&gt;b_bio1.bio_done = cluster_callback;\nbp-&gt;b_flags |= B_CLUSTER;\nvfs_busy_pages(vp, bp);\nvn_strategy(vp, &amp;bp-&gt;b_bio1);\n</code></pre></p> </li> <li> <p>Cluster callback (cluster_callback):</p> </li> <li>Completes parent bio</li> <li>Iterates child bios (read-ahead buffers)</li> <li>Calls biodone() on each child</li> <li>Releases buffers</li> </ol> <p>Chain structure: - Parent buffer has bio chain (bio-&gt;bio_caller_info.cluster_head) - Child buffers linked via cluster_next - All complete when parent I/O finishes</p>"},{"location":"sys/kern/vfs/buffer-cache/#cluster_write-clustered-write","title":"cluster_write() - Clustered Write","text":"<p>cluster_write() (vfs_cluster.c:1244)</p> <p>Attempts to cluster a write operation with adjacent dirty buffers:</p> <pre><code>void cluster_write(struct buf *bp, off_t filesize, int blksize, int seqcount)\n</code></pre> <p>Write clustering workflow:</p> <ol> <li> <p>Check if clustering allowed:    <pre><code>if ((bp-&gt;b_flags &amp; B_CLUSTEROK) == 0)\n    goto out;  /* Filesystem doesn't allow clustering */\nif (bp-&gt;b_flags &amp; B_LOCKED)\n    goto out;  /* Locked buffer */\n</code></pre></p> </li> <li> <p>Scan for adjacent dirty buffers:    <pre><code>/* Scan backwards */\nfor (i = 1; i &lt;= maxback; i++) {\n    tbp = findblk(vp, loffset - i * blksize, FINDBLK_TEST);\n    if (!tbp || !(tbp-&gt;b_flags &amp; B_DELWRI))\n        break;\n    /* Collect buffer */\n}\n\n/* Scan forwards */\nfor (i = 1; i &lt;= maxahead; i++) {\n    tbp = findblk(vp, loffset + i * blksize, FINDBLK_TEST);\n    if (!tbp || !(tbp-&gt;b_flags &amp; B_DELWRI))\n        break;\n    /* Collect buffer */\n}\n</code></pre></p> </li> <li> <p>Build cluster:    <pre><code>cluster_wbuild(vp, bpp, numblks, start_loffset, blksize);\n</code></pre></p> </li> <li> <p>Issue clustered write:    <pre><code>if (nblocks == 1) {\n    /* Single block, issue normally */\n    bawrite(bp);\n} else {\n    /* Multi-block cluster */\n    for each buffer in cluster:\n        cluster_append(&amp;parent-&gt;b_bio1, child);\n    vn_strategy(vp, &amp;parent-&gt;b_bio1);\n}\n</code></pre></p> </li> </ol> <p>Write clustering benefits: - Reduces write overhead - Better disk utilization - Elevator seeking optimization - Improved metadata write performance</p>"},{"location":"sys/kern/vfs/buffer-cache/#cluster_awrite-asynchronous-cluster-write","title":"cluster_awrite() - Asynchronous Cluster Write","text":"<p>cluster_awrite() (vfs_cluster.c:1418)</p> <p>Called during buffer flushing to attempt clustering:</p> <pre><code>void cluster_awrite(struct buf *bp)\n{\n    /* If already doing I/O, skip */\n    if (bp-&gt;b_flags &amp; B_IOISSUED)\n        return;\n\n    /* Try to build cluster */\n    cluster_write(bp, filesize, blksize, seqcount);\n}\n</code></pre> <p>Called by buf_daemon when flushing BQUEUE_DIRTY.</p>"},{"location":"sys/kern/vfs/buffer-cache/#sequential-access-detection","title":"Sequential Access Detection","text":"<p>sequential_heuristic() (vfs_vnops.c):</p> <p>Filesystem code tracks sequential access via VOP_READ/VOP_WRITE:</p> <pre><code>int seqcount = (bp-&gt;b_flags &amp; B_SEQMASK) &gt;&gt; B_SEQSHIFT;\nif (loffset == last_loffset + blksize)\n    seqcount = min(seqcount + 1, B_SEQMAX);  /* 127 max */\nelse\n    seqcount = 0;  /* Reset on non-sequential */\n</code></pre> <p>seqcount usage: - 0: Random access, minimal read-ahead - &gt;0: Sequential, scale read-ahead proportionally - Max (127): Aggressive read-ahead (127 * blksize)</p>"},{"location":"sys/kern/vfs/buffer-cache/#bio-operations","title":"BIO Operations","text":""},{"location":"sys/kern/vfs/buffer-cache/#bio-layer-overview","title":"BIO Layer Overview","text":"<p>The BIO (Block I/O) layer provides a flexible mechanism for I/O request transformation and stacking.</p> <p>struct bio (sys/sys/bio.h):</p> <pre><code>struct bio {\n    struct bio *bio_next;          /* Next bio in chain */\n    struct bio *bio_prev;          /* Previous bio */\n    off_t bio_offset;              /* Logical offset (or block number) */\n    struct buf *bio_buf;           /* Associated buffer */\n    bio_track_t *bio_track;        /* I/O tracking */\n    void (*bio_done)(struct bio *); /* Completion callback */\n    void *bio_caller_info1;        /* Caller private data */\n    union {\n        void *cluster_head;        /* Cluster I/O head */\n        void *cluster_parent;      /* Cluster parent */\n    } bio_caller_info;\n    u_int bio_flags;               /* BIO flags */\n    ...\n};\n</code></pre> <p>BIO flags: - <code>BIO_SYNC</code> - Synchronous I/O - <code>BIO_WANT</code> - Want notification on completion - <code>BIO_DONE</code> - I/O completed</p>"},{"location":"sys/kern/vfs/buffer-cache/#bio-callbacks","title":"Bio Callbacks","text":"<p>biodone() (vfs_bio.c:4201)</p> <p>Default I/O completion handler:</p> <pre><code>void biodone(struct bio *bio)\n{\n    struct buf *bp = bio-&gt;bio_buf;\n\n    /* Call bio-specific done function if present */\n    if (bio-&gt;bio_done) {\n        bio-&gt;bio_done(bio);\n        return;\n    }\n\n    /* Default completion */\n    bufdone(bp);  /* Complete buffer I/O */\n}\n</code></pre> <p>biodone_sync() (vfs_bio.c:4226)</p> <p>Synchronous I/O completion:</p> <pre><code>void biodone_sync(struct bio *bio)\n{\n    bio-&gt;bio_flags |= BIO_DONE;\n    wakeup(bio);  /* Wake biowait() */\n}\n</code></pre> <p>biowait() (vfs_bio.c:4243)</p> <p>Wait for synchronous I/O completion:</p> <pre><code>int biowait(struct bio *bio, const char *wmesg)\n{\n    while ((bio-&gt;bio_flags &amp; BIO_DONE) == 0)\n        tsleep(bio, 0, wmesg, 0);\n\n    if (bio-&gt;bio_flags &amp; BIO_ERROR)\n        return bio-&gt;bio_buf-&gt;b_error;\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#bio-stacking","title":"Bio Stacking","text":"<p>Device drivers and filesystems can push additional BIO layers:</p> <pre><code>/* Filesystem layer (logical offset) */\nvn_strategy(vp, &amp;bp-&gt;b_bio1);\n    \u2193\n/* Filesystem translates bio1 \u2192 bio2 */\nVOP_STRATEGY(devvp, &amp;bp-&gt;b_bio2);\n    \u2193\n/* Device driver handles bio2 (physical offset) */\ndev_dstrategy(...);\n</code></pre> <p>Example: HAMMER filesystem 1. bio1: File logical offset 2. bio2: HAMMER volume offset 3. bio3: Device physical offset</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-flushing","title":"Buffer Flushing","text":""},{"location":"sys/kern/vfs/buffer-cache/#buf_daemon-main-flush-thread","title":"buf_daemon() - Main Flush Thread","text":"<p>buf_daemon() (vfs_bio.c:4566)</p> <p>Kernel thread that flushes dirty buffers:</p> <pre><code>static void buf_daemon(void)\n{\n    for (;;) {\n        /* Sleep until work needed */\n        tsleep(&amp;bd_request, 0, \"psleep\", hz);\n\n        /* Check if flushing needed */\n        if (runningbufspace + dirtykvaspace &lt; lodirtybufspace)\n            continue;\n\n        /* Flush dirty buffers */\n        flushbufqueues(NULL, BQUEUE_DIRTY);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#flushbufqueues-queue-flusher","title":"flushbufqueues() - Queue Flusher","text":"<p>flushbufqueues() (vfs_bio.c:4330)</p> <p>Scans a buffer queue and flushes dirty buffers:</p> <pre><code>static int flushbufqueues(struct buf *marker, bufq_type_t q)\n{\n    int flushed = 0;\n\n    /* Scan per-CPU queues */\n    for (cpu = 0; cpu &lt; ncpus; cpu++) {\n        TAILQ_FOREACH(bp, &amp;bufpcpu[cpu].bufqueues[q], b_freelist) {\n            if (bp-&gt;b_flags &amp; B_MARKER)\n                continue;\n            if (bp-&gt;b_flags &amp; B_DELWRI) {\n                /* Remove from queue */\n                bremfree(bp);\n\n                /* Try to lock */\n                if (BUF_LOCK(bp, LK_EXCLUSIVE | LK_NOWAIT))\n                    continue;  /* Skip if locked */\n\n                /* Cluster and write */\n                cluster_awrite(bp);\n                flushed++;\n\n                if (flushed &gt;= flushperqueue)\n                    break;  /* Flushed enough */\n            }\n        }\n    }\n\n    return flushed;\n}\n</code></pre> <p>Flush triggers: - <code>dirtybufspace &gt; hidirtybufspace</code> - High watermark exceeded - System sync operation (sync(2) system call) - Vnode reclamation (vnode has dirty buffers) - Filesystem-specific sync (VFS_SYNC)</p>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_sync-filesystem-sync","title":"VFS_SYNC - Filesystem Sync","text":"<p>VFS_SYNC() vnode operation:</p> <p>Called to sync a filesystem:</p> <pre><code>int VFS_SYNC(struct mount *mp, int waitfor)\n</code></pre> <p>Typical implementation:</p> <pre><code>static int myfs_sync(struct mount *mp, int waitfor)\n{\n    /* Sync inodes */\n    myfs_sync_inodes(mp, waitfor);\n\n    /* Scan dirty vnodes */\n    sync_info.waitfor = waitfor;\n    vmntvnodescan(mp, VMSC_GETVP, NULL, myfs_sync_callback, &amp;sync_info);\n\n    /* Write superblock */\n    myfs_write_superblock(mp);\n\n    return 0;\n}\n</code></pre> <p>waitfor values: - <code>MNT_WAIT</code>: Wait for all I/O to complete - <code>MNT_NOWAIT</code>: Initiate I/O but don't wait - <code>MNT_LAZY</code>: Lazy sync (metadata only)</p>"},{"location":"sys/kern/vfs/buffer-cache/#page-cleaning","title":"Page Cleaning","text":""},{"location":"sys/kern/vfs/buffer-cache/#vfs_clean_pages-mark-pages-clean","title":"vfs_clean_pages() - Mark Pages Clean","text":"<p>vfs_clean_pages() (vfs_bio.c:3980)</p> <p>Marks VM pages clean after buffer written:</p> <pre><code>static void vfs_clean_pages(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_VMIO) {\n        for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n            m = bp-&gt;b_xio.xio_pages[i];\n            vfs_clean_one_page(bp, i, m);\n        }\n    }\n}\n</code></pre> <p>vfs_clean_one_page() (vfs_bio.c:4002):</p> <pre><code>static void vfs_clean_one_page(struct buf *bp, int pageno, vm_page_t m)\n{\n    int soff, eoff;\n\n    /* Calculate page-relative dirty range */\n    soff = max(bp-&gt;b_dirtyoff - pageno * PAGE_SIZE, 0);\n    eoff = min(bp-&gt;b_dirtyend - pageno * PAGE_SIZE, PAGE_SIZE);\n\n    if (eoff &gt; soff) {\n        /* Mark page range clean */\n        vm_page_set_valid(m, soff, eoff - soff);\n        vm_page_clear_dirty(m, soff, eoff - soff);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#page-validity","title":"Page Validity","text":"<p>VM pages have validity bits tracking which parts contain valid data:</p> <p>vm_page-&gt;valid bitmask: - One bit per 512-byte sector (DEV_BSIZE) - <code>VM_PAGE_BITS_ALL</code> (0xFF): Entire page valid - Partial validity supported</p> <p>vm_page_set_valid(): Sets validity bits for byte range</p> <p>vm_page_clear_dirty(): Clears dirty bits for byte range</p>"},{"location":"sys/kern/vfs/buffer-cache/#kvabio-api","title":"KVABIO API","text":""},{"location":"sys/kern/vfs/buffer-cache/#overview_2","title":"Overview","text":"<p>KVABIO (Kernel Virtual Address BIO) allows efficient buffer access across CPUs without explicit synchronization.</p> <p>Problem: Regular buffers (b_data) may be accessed via different CPUs: - Data written on CPU0 - Buffer passed to CPU1 - CPU1 reads stale data from cache</p> <p>Solution: KVABIO API provides: - Explicit synchronization primitives - Per-CPU data mapping tracking - Automatic cache coherency</p>"},{"location":"sys/kern/vfs/buffer-cache/#kvabio-functions","title":"KVABIO Functions","text":"<p>bkvasync() (vfs_bio.c):</p> <p>Synchronizes buffer data for current CPU:</p> <pre><code>void bkvasync(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_KVABIO) {\n        /* Ensure data visible to current CPU */\n        cpu_lfence();  /* Load fence */\n    }\n}\n</code></pre> <p>bkvasync_all() (vfs_bio.c):</p> <p>Synchronizes buffer data for all CPUs:</p> <pre><code>void bkvasync_all(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_KVABIO) {\n        /* Flush to all CPUs */\n        cpu_sfence();  /* Store fence */\n        /* IPI to other CPUs if needed */\n    }\n}\n</code></pre> <p>Usage: - Call bkvasync() before reading bp-&gt;b_data - Call bkvasync_all() after writing bp-&gt;b_data - Only needed for B_KVABIO buffers</p>"},{"location":"sys/kern/vfs/buffer-cache/#performance-considerations","title":"Performance Considerations","text":""},{"location":"sys/kern/vfs/buffer-cache/#buffer-cache-sizing","title":"Buffer Cache Sizing","text":"<p>Optimal buffer cache size: - Default: ~10% of physical RAM - Minimum (lobufspace): 4 MB - Maximum (maxbufspace): Computed from available RAM - Adjust via sysctl: <code>vfs.maxbufspace</code>, <code>vfs.hibufspace</code></p> <p>Trade-offs: - Larger cache: Better hit rate, less I/O - Smaller cache: More RAM for VM page cache - Balance based on workload</p>"},{"location":"sys/kern/vfs/buffer-cache/#dirty-buffer-limits","title":"Dirty Buffer Limits","text":"<p>Configure dirty limits: - <code>vfs.lodirtybufspace</code>: When to start flushing (default: ~5% RAM) - <code>vfs.hidirtybufspace</code>: Aggressive flushing (default: ~10% RAM) - <code>vfs.dirtybufspace</code>: Current dirty space (read-only)</p> <p>Why limit dirty buffers? - Prevent memory exhaustion - Bound data loss on crash - Ensure write progress - Avoid sync stalls</p>"},{"location":"sys/kern/vfs/buffer-cache/#read-ahead-tuning","title":"Read-ahead Tuning","text":"<p>Sequential detection: - Tracked via seqcount (0-127) - Scales read-ahead: seqcount * blksize - Maximum read-ahead: MAXPHYS (128KB typically)</p> <p>Read-ahead benefits: - Hides disk latency - Improves streaming performance - Minimal overhead for random access</p> <p>Disable read-ahead: - For random workloads - Flash storage with fast random access - Set <code>vfs.read_max</code> lower</p>"},{"location":"sys/kern/vfs/buffer-cache/#write-clustering","title":"Write Clustering","text":"<p>Enable clustering: - Filesystem sets B_CLUSTEROK on buffers - bdwrite() for delayed writes - buf_daemon clusters during flush</p> <p>Benefits: - Reduces write overhead - Larger I/O sizes - Better disk scheduling</p> <p>When not to cluster: - Synchronous writes (bwrite) - Small files (&lt;128KB) - Random write patterns</p>"},{"location":"sys/kern/vfs/buffer-cache/#error-handling","title":"Error Handling","text":""},{"location":"sys/kern/vfs/buffer-cache/#io-errors","title":"I/O Errors","text":"<p>Error propagation:</p> <pre><code>/* I/O completes with error */\nbiodone() {\n    bio-&gt;bio_flags |= BIO_ERROR;\n    bp-&gt;b_error = EIO;  /* Or specific error */\n    bp-&gt;b_flags |= B_ERROR;\n    bufdone(bp);\n}\n\n/* Sync I/O: Error returned */\nerror = biowait(bio, \"biord\");\nif (error)\n    return error;\n\n/* Async I/O: Error logged, buffer marked invalid */\nbiodone() {\n    if (bp-&gt;b_flags &amp; B_ERROR) {\n        bp-&gt;b_flags |= B_INVAL;  /* Invalidate */\n        /* Error may be logged by filesystem */\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#retry-strategies","title":"Retry Strategies","text":"<p>Filesystem-level retry: - VFS layers don't retry automatically - Filesystem must detect error and retry - Example: NFS retries on EIO</p> <p>User-level retry: - read(2)/write(2) return -1 with errno - Application decides retry policy</p>"},{"location":"sys/kern/vfs/buffer-cache/#debugging","title":"Debugging","text":""},{"location":"sys/kern/vfs/buffer-cache/#buffer-state-inspection","title":"Buffer State Inspection","text":"<p>DDB commands (when kernel debugger active):</p> <pre><code>db&gt; show buffer &lt;addr&gt;       # Display buffer state\ndb&gt; show allbufs             # List all buffers\ndb&gt; show lockedbufs          # List locked buffers\ndb&gt; show dirtybufs           # List dirty buffers\n</code></pre> <p>Sysctl inspection:</p> <pre><code># Buffer cache statistics\nsysctl vfs.nbuf              # Total buffers\nsysctl vfs.bufspace          # Current space used\nsysctl vfs.dirtybufspace     # Dirty buffer space\nsysctl vfs.dirtybufcount     # Dirty buffer count\nsysctl vfs.runningbufspace   # Running I/O space\nsysctl vfs.runningbufcount   # Running I/O count\n\n# Tuning parameters\nsysctl vfs.maxbufspace       # Max buffer space\nsysctl vfs.hibufspace        # High watermark\nsysctl vfs.lodirtybufspace   # Dirty low watermark\nsysctl vfs.hidirtybufspace   # Dirty high watermark\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#common-issues","title":"Common Issues","text":"<p>Issue: System hangs during sync - Cause: Deadlock in buffer allocation - Debug: Check runningbufspace vs hirunningspace - Solution: Increase vfs.hirunningspace</p> <p>Issue: Poor write performance - Cause: Not clustering writes - Debug: Check if B_CLUSTEROK set on buffers - Solution: Enable write clustering in filesystem</p> <p>Issue: Excessive read-ahead - Cause: High seqcount on random workload - Debug: Monitor vfs.lowmempgallocs - Solution: Reduce MAXPHYS or tune read-ahead</p> <p>Issue: Buffer cache thrashing - Cause: Working set larger than cache - Debug: Monitor vfs.getnewbufcalls - Solution: Increase vfs.maxbufspace</p>"},{"location":"sys/kern/vfs/buffer-cache/#summary","title":"Summary","text":"<p>The VFS buffer cache is a sophisticated subsystem providing:</p> <ol> <li>Caching: Filesystem block caching with LRU eviction</li> <li>I/O Management: Sync/async I/O primitives (bread, bwrite, bdwrite)</li> <li>VM Integration: Unified buffer/page cache via VMIO</li> <li>Clustering: Read-ahead and write clustering for performance</li> <li>Dirty Tracking: Write-behind with configurable watermarks</li> <li>Multi-threading: Per-CPU queues and dedicated flush threads</li> <li>Flexibility: BIO layer enables device stacking and transformation</li> </ol> <p>The buffer cache sits at a critical junction between filesystems, the VM system, and device drivers, providing high-performance cached I/O while maintaining data consistency and integrity.</p> <p>Key design principles: - Lock-free fast paths: Atomic operations and per-CPU structures - Unified caching: VMIO integrates buffer and page caches - Asynchronous I/O: Pipeline writes, overlap operations - Adaptive behavior: Sequential detection, watermark-based flushing - Layered I/O: BIO translation enables complex storage stacks</p>"},{"location":"sys/kern/vfs/journaling/","title":"VFS Journaling System","text":""},{"location":"sys/kern/vfs/journaling/#overview","title":"Overview","text":"<p>The DragonFly BSD VFS journaling system provides a flexible infrastructure for recording filesystem operations to a journal stream. This enables features like:</p> <ul> <li>Transaction logging - Record all filesystem changes</li> <li>Replication - Stream changes to remote systems</li> <li>Crash recovery - Replay operations after system failures</li> <li>Auditing - Track all filesystem modifications</li> <li>Two-way acknowledgement - Full-duplex journaling with commit confirmation</li> </ul> <p>The journaling layer sits between VOP wrappers and the underlying filesystem, transparently intercepting and recording operations before passing them through to the actual filesystem implementation.</p> <p>Key components: - Memory FIFO - Circular buffer for batching journal records - Worker threads - Asynchronous write-out to journal targets - Stream records - Structured format for journal data - Transaction IDs - Sequencing and acknowledgement - Subrecords - Nested transaction structure</p> <p>Key files: - <code>sys/kern/vfs_journal.c</code> - Core journaling infrastructure and FIFO management - <code>sys/kern/vfs_jops.c</code> - Journal VOP implementations - <code>sys/sys/journal.h</code> - Journaling data structures and protocol definitions</p>"},{"location":"sys/kern/vfs/journaling/#architecture","title":"Architecture","text":""},{"location":"sys/kern/vfs/journaling/#layered-design","title":"Layered Design","text":"<pre><code>flowchart TD\n    USER[\"User/Kernel VFS calls\"] --&gt; VOP[\"VOP Wrappers(vfs_vopops.c)\"]\n    VOP --&gt; JOURNAL{\"Journalingenabled?\"}\n    JOURNAL --&gt;|Yes| JLAYER[\"Journal Layer(vfs_jops.c)\"]\n    JOURNAL --&gt;|No| FS\n    JLAYER --&gt; FS[\"Underlying Filesystem\"]\n    FS --&gt; DISK[\"Disk/Storage\"]\n</code></pre> <p>When journaling is enabled for a mount point: 1. Mount point's <code>mnt_vn_journal_ops</code> is set to <code>journal_vnode_vops</code> 2. VOP operations are intercepted by journal functions 3. Journal records operation details to memory FIFO 4. Worker thread writes FIFO contents to journal target 5. Operation is passed to underlying filesystem</p>"},{"location":"sys/kern/vfs/journaling/#memory-fifo-structure","title":"Memory FIFO Structure","text":"<p>The memory FIFO is a circular buffer that batches journal records before writing to the target:</p> <pre><code>struct journal_fifo {\n    char *membase;      // Base of circular buffer\n    size_t size;        // Total buffer size (power of 2)\n    size_t mask;        // Size - 1 (for wrapping)\n    int64_t windex;     // Write index (monotonically increasing)\n    int64_t rindex;     // Read index (what's been written out)\n    int64_t xindex;     // Acknowledgement index (what's been committed)\n};\n</code></pre> <p>Index relationships: - <code>windex &gt;= rindex &gt;= xindex</code> (always) - Available write space: <code>size - (windex - xindex)</code> - Unwritten data: <code>windex - rindex</code> - Unacknowledged data: <code>rindex - xindex</code></p> <p>Key properties: - Indices never decrease (monotonically increasing) - Wrapping uses mask: <code>physicaloffset = index &amp; mask</code> - 16-byte alignment for all records - Incomplete records block worker thread progress</p>"},{"location":"sys/kern/vfs/journaling/#journal-records","title":"Journal Records","text":""},{"location":"sys/kern/vfs/journaling/#stream-record-structure","title":"Stream Record Structure","text":"<p>Stream records are the fundamental unit of journaling:</p> <pre><code>struct journal_rawrecbeg {\n    u_int16_t begmagic;     // JREC_BEGMAGIC (0x1234) or INCOMPLETE\n    u_int16_t streamid;     // Stream ID + control bits\n    int32_t recsize;        // Total record size (includes header/trailer)\n    int64_t transid;        // Sequence/transaction ID\n    // ... payload data ...\n};\n\nstruct journal_rawrecend {\n    u_int16_t endmagic;     // JREC_ENDMAGIC (0xCDEF)\n    u_int16_t check;        // Checksum (0 = disabled)\n    int32_t recsize;        // Same as rawrecbeg-&gt;recsize\n};\n</code></pre> <p>Record layout:</p> <pre><code>block-beta\n    columns 1\n    block:header[\"rawrecbeg (16B) - Header\"]\n    end\n    block:payload[\"Payload data (Variable size)(subrecords)\"]\n    end\n    block:trailer[\"rawrecend (8B) - Trailer\"]\n    end\n</code></pre> <p>Total: 16-byte aligned</p> <p>Magic numbers: - <code>JREC_BEGMAGIC (0x1234)</code> - Valid record ready to write - <code>JREC_INCOMPLETEMAGIC (0xFFFF)</code> - Reserved but not yet committed - <code>JREC_ENDMAGIC (0xCDEF)</code> - End marker for reverse scanning</p>"},{"location":"sys/kern/vfs/journaling/#stream-control-bits","title":"Stream Control Bits","text":"<p>The <code>streamid</code> field combines control bits and stream identifier:</p> <pre><code>#define JREC_STREAMCTL_BEGIN    0x8000  // Start of logical stream\n#define JREC_STREAMCTL_END      0x4000  // End of logical stream\n#define JREC_STREAMCTL_ABORTED  0x2000  // Stream was aborted\n#define JREC_STREAMID_MASK      0x1FFF  // Actual stream ID (bits 0-12)\n</code></pre> <p>Stream lifecycle: - Single record: <code>BEGIN | END</code> set (complete transaction in one record) - Multi-record: First has <code>BEGIN</code>, intermediate have neither, last has <code>END</code> - Aborted: Last record has <code>ABORTED | END</code></p>"},{"location":"sys/kern/vfs/journaling/#special-stream-ids","title":"Special Stream IDs","text":"<pre><code>#define JREC_STREAMID_PAD       0x0001  // Padding (FIFO wrap-around)\n#define JREC_STREAMID_SYNCPT    0x0000  // Synchronization point\n#define JREC_STREAMID_DISCONT   0x0002  // Discontinuity marker\n#define JREC_STREAMID_ACK       0x0004  // Acknowledgement record\n#define JREC_STREAMID_RESTART   0x0005  // Journal restart marker\n\n// Filesystem operation streams: 0x0100 - 0x1FFF\n</code></pre>"},{"location":"sys/kern/vfs/journaling/#subrecords","title":"Subrecords","text":"<p>Within a stream record, operations are broken down into subrecords:</p> <pre><code>struct journal_subrecord {\n    u_int16_t rectype;      // Control bits + type\n    int16_t reserved;       // Future use\n    int32_t recsize;        // Subrecord size\n    // ... type-specific data ...\n};\n</code></pre> <p>Subrecord control bits: <pre><code>#define JMASK_NESTED    0x8000  // Contains nested subrecords\n#define JMASK_LAST      0x4000  // Last subrecord in group\n</code></pre></p> <p>Common subrecord types: - <code>JTYPE_SETATTR</code> - Attribute changes (nested) - <code>JTYPE_WRITE</code> - File write operation (nested) - <code>JTYPE_CREATE</code> - File creation (nested) - <code>JTYPE_REMOVE</code> - File removal (nested) - <code>JTYPE_RENAME</code> - File rename (nested) - <code>JTYPE_UNDO</code> - Undo information (nested) - <code>JLEAF_FILEDATA</code> - File content data (leaf) - <code>JLEAF_PATH1/2/3/4</code> - Pathname components (leaf) - <code>JLEAF_UID/GID</code> - User/group IDs (leaf)</p>"},{"location":"sys/kern/vfs/journaling/#fifo-management","title":"FIFO Management","text":""},{"location":"sys/kern/vfs/journaling/#reservation-process","title":"Reservation Process","text":"<p>Located at <code>sys/kern/vfs_journal.c:496</code>.</p> <p>Function: <code>journal_reserve()</code></p> <p>The reservation process ensures thread-safe allocation of FIFO space:</p> <ol> <li> <p>Calculate required space: <pre><code>total_bytes = header_size + payload_size + trailer_size;\naligned_bytes = (total_bytes + 15) &amp; ~15;  // 16-byte align\n</code></pre></p> </li> <li> <p>Check for wrap-around: <pre><code>availtoend = fifo_size - (windex &amp; fifo_mask);\nif (bytes &gt; availtoend) {\n    req = bytes + availtoend;  // Need pad record at end\n}\n</code></pre></p> </li> <li> <p>Wait for space if needed: <pre><code>avail = fifo_size - (windex - xindex);\nif (avail &lt; req) {\n    jo-&gt;flags |= MC_JOURNAL_WWAIT;\n    tsleep(&amp;jo-&gt;fifo.windex, 0, \"jwrite\", 0);\n}\n</code></pre></p> </li> <li> <p>Create pad record if wrapping:</p> </li> <li>Pad record fills dead space at end of FIFO</li> <li>Has valid transaction ID for sequencing</li> <li> <p>Worker thread skips pad records</p> </li> <li> <p>Reserve space: <pre><code>rawp-&gt;begmagic = JREC_INCOMPLETEMAGIC;  // Blocks worker thread\nrawp-&gt;recsize = bytes;\nrawp-&gt;streamid = streamid | JREC_STREAMCTL_BEGIN;\nrawp-&gt;transid = jo-&gt;transid;\nwindex += aligned_bytes;\n</code></pre></p> </li> <li> <p>Return pointer to payload area: <pre><code>return (rawp + 1);  // Skip header\n</code></pre></p> </li> </ol> <p>Key insight: The incomplete magic prevents the worker thread from writing past this record until it's committed, allowing the caller to populate the record at leisure.</p>"},{"location":"sys/kern/vfs/journaling/#extension-and-truncation","title":"Extension and Truncation","text":"<p>Located at <code>sys/kern/vfs_journal.c:610</code>.</p> <p>Function: <code>journal_extend()</code></p> <p>Streams can be extended after initial reservation:</p> <p>Case 1: Simple extension (no size class change) <pre><code>if (new_aligned_size == old_aligned_size) {\n    rawp-&gt;recsize += bytes;  // Just update size\n    return (payload + truncbytes);\n}\n</code></pre></p> <p>Case 2: FIFO still at our record (can adjust windex) <pre><code>if (windex is still at end of our record) {\n    windex += (new_size - old_size);\n    rawp-&gt;recsize += bytes;\n    return (payload + truncbytes);\n}\n</code></pre></p> <p>Case 3: Must create new stream record <pre><code>// Commit current record (marked END)\njournal_commit(jo, rawpp, truncbytes, 0);\n\n// Create new continuing record (no BEGIN mark)\nrptr = journal_reserve(jo, rawpp, streamid, bytes);\nrawp-&gt;streamid &amp;= ~JREC_STREAMCTL_BEGIN;\n</code></pre></p> <p>This creates a multi-record stream where records share the same stream ID but only the first has <code>BEGIN</code> and only the last has <code>END</code>.</p>"},{"location":"sys/kern/vfs/journaling/#commit-process","title":"Commit Process","text":"<p>Located at <code>sys/kern/vfs_journal.c:712</code>.</p> <p>Function: <code>journal_commit()</code></p> <p>Committing a record makes it visible to the worker thread:</p> <ol> <li> <p>Truncate if requested: <pre><code>if (bytes &gt;= 0) {\n    new_recsize = bytes + header_size + trailer_size;\n    new_aligned = (new_recsize + 15) &amp; ~15;\n}\n</code></pre></p> </li> <li> <p>Handle freed space:</p> </li> <li>If windex still at our record: Back-index windex</li> <li> <p>Otherwise: Create pad record in dead space</p> </li> <li> <p>Fill in trailer: <pre><code>rendp = (char *)rawp + aligned_size - sizeof(*rendp);\nrendp-&gt;endmagic = JREC_ENDMAGIC;\nrendp-&gt;recsize = rawp-&gt;recsize;\nrendp-&gt;check = 0;  // Checksum (currently disabled)\n</code></pre></p> </li> <li> <p>Mark stream end if closeout: <pre><code>if (closeout)\n    rawp-&gt;streamid |= JREC_STREAMCTL_END;\n</code></pre></p> </li> <li> <p>Commit with memory barrier: <pre><code>cpu_sfence();  // Ensure trailer written before magic\nrawp-&gt;begmagic = JREC_BEGMAGIC;  // Makes record visible\n</code></pre></p> </li> <li> <p>Wake worker if needed:</p> </li> <li>If FIFO more than half full</li> <li>If threads waiting for space (<code>MC_JOURNAL_WWAIT</code>)</li> </ol>"},{"location":"sys/kern/vfs/journaling/#abort-process","title":"Abort Process","text":"<p>Located at <code>sys/kern/vfs_journal.c:678</code>.</p> <p>Function: <code>journal_abort()</code></p> <p>Aborts can optimize away uncommitted records:</p> <p>Case 1: Can reverse windex (record at end of FIFO) <pre><code>if (is_begin &amp;&amp; windex == end_of_our_record) {\n    windex -= aligned_size;  // Completely remove record\n    *rawpp = NULL;\n}\n</code></pre></p> <p>Case 2: Must mark as aborted <pre><code>else {\n    rawp-&gt;streamid |= JREC_STREAMCTL_ABORTED;\n    journal_commit(jo, rawpp, 0, 1);  // Commit with 0 payload\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#worker-threads","title":"Worker Threads","text":""},{"location":"sys/kern/vfs/journaling/#write-worker-thread","title":"Write Worker Thread","text":"<p>Located at <code>sys/kern/vfs_journal.c:165</code>.</p> <p>Function: <code>journal_wthread()</code></p> <p>The write worker drains the FIFO to the journal target:</p> <p>Main loop: <pre><code>for (;;) {\n    // Calculate writable bytes\n    bytes = windex - rindex;\n\n    // Sleep if nothing to write\n    if (bytes == 0) {\n        if (stop_requested) break;\n        tsleep(&amp;jo-&gt;fifo, 0, \"jfifo\", hz);\n        continue;\n    }\n\n    // Block on incomplete records\n    rawp = membase + (rindex &amp; mask);\n    if (rawp-&gt;begmagic == JREC_INCOMPLETEMAGIC) {\n        tsleep(&amp;jo-&gt;fifo, 0, \"jpad\", hz);\n        continue;\n    }\n\n    // Skip pad records\n    if (rawp-&gt;streamid == JREC_STREAMID_PAD) {\n        rindex += aligned_recsize;\n        xindex += aligned_recsize;  // (if not full-duplex)\n        continue;\n    }\n\n    // Calculate contiguous writable region\n    res = 0;\n    avail = fifo_size - (rindex &amp; mask);  // To end of buffer\n    while (res &lt; bytes &amp;&amp; rawp-&gt;begmagic == JREC_BEGMAGIC) {\n        res += aligned_recsize;\n        if (res &gt;= avail) break;  // Hit end of buffer\n        rawp = next_record(rawp);\n    }\n\n    // Write to target\n    rindex += bytes;  // Advance BEFORE write (for ack racing)\n    error = fp_write(fp, membase + old_rindex, bytes, &amp;written);\n\n    // Advance acknowledgement index (if not full-duplex)\n    if (!full_duplex) {\n        xindex += bytes;\n        wakeup_waiters();\n    }\n}\n</code></pre></p> <p>Key aspects: - Never writes incomplete records (blocks until committed) - Writes contiguous regions up to buffer wrap - Advances rindex before writing (allows acks to race) - Handles full-duplex vs simplex differently</p>"},{"location":"sys/kern/vfs/journaling/#read-worker-thread-full-duplex","title":"Read Worker Thread (Full-Duplex)","text":"<p>Located at <code>sys/kern/vfs_journal.c:301</code>.</p> <p>Function: <code>journal_rthread()</code></p> <p>For two-way journaling streams, reads acknowledgements from target:</p> <p>Main loop: <pre><code>for (;;) {\n    if (stop_requested) break;\n\n    // Read acknowledgement record\n    if (transid == 0) {\n        error = fp_read(fp, &amp;ack, sizeof(ack), &amp;count, 1, UIO_SYSSPACE);\n        if (error || count != sizeof(ack)) break;\n\n        // Validate magic numbers\n        if (ack.rbeg.begmagic != JREC_BEGMAGIC) break;\n        if (ack.rend.endmagic != JREC_ENDMAGIC) break;\n\n        transid = ack.rbeg.transid;\n    }\n\n    // Check for unacknowledged data\n    bytes = rindex - xindex;\n    if (bytes == 0) {\n        // Unsent data acknowledged - protocol error\n        kprintf(\"warning: unsent data acknowledged\\n\");\n        transid = 0;\n        continue;\n    }\n\n    // Get record at xindex\n    rawp = membase + (xindex &amp; mask);\n\n    // Advance xindex for all records up to transid\n    if (rawp-&gt;transid &lt; transid) {\n        xindex += aligned_recsize;\n        total_acked += aligned_recsize;\n        wakeup_waiters();\n        continue;\n    }\n\n    // Found matching transid\n    if (rawp-&gt;transid == transid) {\n        xindex += aligned_recsize;\n        total_acked += aligned_recsize;\n        wakeup_waiters();\n        transid = 0;\n        continue;\n    }\n\n    // Unsent data acknowledged - protocol error\n    transid = 0;\n}\n</code></pre></p> <p>Key aspects: - Target can acknowledge multiple records at once - Target sends back transaction IDs that are committed - Advances xindex to free up FIFO space - Wakes up threads waiting for space</p>"},{"location":"sys/kern/vfs/journaling/#journal-vop-operations","title":"Journal VOP Operations","text":"<p>The file <code>sys/kern/vfs_jops.c</code> implements journal-aware VOP operations that intercept filesystem operations to record them before passing through to the underlying filesystem.</p>"},{"location":"sys/kern/vfs/journaling/#operation-interception","title":"Operation Interception","text":"<p>When journaling is enabled:</p> <pre><code>struct vop_ops journal_vnode_vops = {\n    .vop_default =      vop_journal_operate_ap,\n    .vop_setattr =      journal_setattr,\n    .vop_write =        journal_write,\n    .vop_fsync =        journal_fsync,\n    .vop_ncreate =      journal_ncreate,\n    .vop_nremove =      journal_nremove,\n    .vop_nrename =      journal_nrename,\n    // ... etc\n};\n</code></pre> <p>Each intercepted operation follows this pattern:</p> <ol> <li>Start journal record</li> <li>Record UNDO information (if journaling is reversible)</li> <li>Call underlying VOP</li> <li>Record REDO information (operation details)</li> <li>Commit journal record</li> <li>Return result</li> </ol>"},{"location":"sys/kern/vfs/journaling/#example-journal_write","title":"Example: journal_write()","text":"<p>Located at <code>sys/kern/vfs_jops.c:400</code> (approximate).</p> <p>Simplified flow: <pre><code>static int\njournal_write(struct vop_write_args *ap)\n{\n    struct mount *mp = ap-&gt;a_vp-&gt;v_mount;\n    struct journal *jo;\n    struct jrecord jrec;\n    int error;\n\n    // For each journal on this mount\n    TAILQ_FOREACH(jo, &amp;mp-&gt;mnt_jlist, jentry) {\n        // Initialize journal record\n        jrecord_init(jo, &amp;jrec, JTYPE_WRITE);\n\n        // Record UNDO data (old file contents) if reversible\n        if (jo-&gt;flags &amp; MC_JOURNAL_WANT_REVERSIBLE) {\n            jrecord_undo_file(&amp;jrec, ap-&gt;a_vp, JRUNDO_FILEDATA,\n                            ap-&gt;a_uio-&gt;uio_offset, \n                            ap-&gt;a_uio-&gt;uio_resid);\n        }\n\n        // Record write operation details\n        jrecord_write(&amp;jrec, ap-&gt;a_vp, ap-&gt;a_uio, ap-&gt;a_ioflag);\n    }\n\n    // Call underlying filesystem VOP\n    error = vop_write_ap(ap);\n\n    // Commit all journal records\n    TAILQ_FOREACH(jo, &amp;mp-&gt;mnt_jlist, jentry) {\n        jrecord_done(&amp;jrec, error);\n    }\n\n    return error;\n}\n</code></pre></p> <p>Key steps: 1. Loop through all journals on mount point (can have multiple) 2. Create <code>JTYPE_WRITE</code> stream record 3. Record UNDO if reversible (old file data) 4. Record REDO (write parameters + new data) 5. Perform actual write via underlying VOP 6. Commit journal records with result</p>"},{"location":"sys/kern/vfs/journaling/#undo-recording","title":"UNDO Recording","text":"<p>Located at <code>sys/kern/vfs_jops.c:600</code> (approximate).</p> <p>Function: <code>jrecord_undo_file()</code></p> <p>For reversible journals, UNDO information allows replaying backwards:</p> <pre><code>static void\njrecord_undo_file(struct jrecord *jrec, struct vnode *vp,\n                  int jrflags, off_t off, off_t bytes)\n{\n    struct vattr vat;\n    struct uio uio;\n\n    // Start UNDO subrecord\n    jrecord_push(jrec, JTYPE_UNDO);\n\n    // Record file attributes\n    if (jrflags &amp; JRUNDO_VATTR) {\n        VOP_GETATTR(vp, &amp;vat);\n        jrecord_write_vattr(jrec, &amp;vat);\n    }\n\n    // Record file data\n    if (jrflags &amp; JRUNDO_FILEDATA) {\n        // Read old file contents\n        uio.uio_offset = off;\n        uio.uio_resid = bytes;\n        VOP_READ(vp, &amp;uio, IO_NODELOCKED, cred);\n\n        // Write to journal\n        jrecord_write_uio(jrec, &amp;uio);\n    }\n\n    // End UNDO subrecord\n    jrecord_pop(jrec);\n}\n</code></pre> <p>UNDO flags: - <code>JRUNDO_SIZE</code> - File size - <code>JRUNDO_UID/GID</code> - Ownership - <code>JRUNDO_MODES</code> - Permissions - <code>JRUNDO_MTIME/ATIME/CTIME</code> - Timestamps - <code>JRUNDO_FILEDATA</code> - File contents - <code>JRUNDO_NLINK</code> - Link count - <code>JRUNDO_VATTR</code> - All vattr fields</p>"},{"location":"sys/kern/vfs/journaling/#redo-recording","title":"REDO Recording","text":"<p>REDO information describes the operation being performed:</p> <p>For JTYPE_WRITE: <pre><code>jrecord_push(jrec, JTYPE_WRITE);\njrecord_leaf(jrec, JLEAF_PATH1, pathname, pathlen);\njrecord_leaf(jrec, JLEAF_FILEDATA, data, datalen);\njrecord_leaf(jrec, JLEAF_OFFSET, &amp;offset, sizeof(offset));\njrecord_pop(jrec);\n</code></pre></p> <p>For JTYPE_RENAME: <pre><code>jrecord_push(jrec, JTYPE_RENAME);\njrecord_push(jrec, JTYPE_UNDO);\n    jrecord_leaf(jrec, JLEAF_PATH1, oldpath, oldlen);\njrecord_pop(jrec);\njrecord_leaf(jrec, JLEAF_PATH1, oldpath, oldlen);\njrecord_leaf(jrec, JLEAF_PATH2, newpath, newlen);\njrecord_pop(jrec);\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#journal-management","title":"Journal Management","text":""},{"location":"sys/kern/vfs/journaling/#installing-a-journal","title":"Installing a Journal","text":"<p>Located at <code>sys/kern/vfs_jops.c:250</code> (approximate).</p> <p>Function: <code>journal_install_vfs_journal()</code></p> <p>Journals are installed via <code>mountctl()</code> system call:</p> <pre><code>static int\njournal_install_vfs_journal(struct mount *mp, struct file *fp,\n                           const struct mountctl_install_journal *info)\n{\n    struct journal *jo;\n\n    // Check for duplicate journal ID\n    TAILQ_FOREACH(jo, &amp;mp-&gt;mnt_jlist, jentry) {\n        if (strcmp(jo-&gt;id, info-&gt;id) == 0)\n            return EALREADY;\n    }\n\n    // Allocate journal structure\n    jo = kmalloc(sizeof(*jo), M_JOURNAL, M_WAITOK | M_ZERO);\n\n    // Initialize fields\n    strlcpy(jo-&gt;id, info-&gt;id, sizeof(jo-&gt;id));\n    jo-&gt;fp = fp;  // File/socket to write journal to\n    jo-&gt;flags = info-&gt;flags;\n\n    // Allocate memory FIFO\n    jo-&gt;fifo.size = info-&gt;fifo_size;\n    jo-&gt;fifo.mask = jo-&gt;fifo.size - 1;\n    jo-&gt;fifo.membase = kmalloc(jo-&gt;fifo.size, M_JFIFO, M_WAITOK);\n\n    // Initialize indices\n    jo-&gt;fifo.windex = 0;\n    jo-&gt;fifo.rindex = 0;\n    jo-&gt;fifo.xindex = 0;\n    jo-&gt;transid = 1;\n\n    // Create worker threads\n    journal_create_threads(jo);\n\n    // Add to mount point's journal list\n    TAILQ_INSERT_TAIL(&amp;mp-&gt;mnt_jlist, jo, jentry);\n\n    return 0;\n}\n</code></pre> <p>Installation flags: - <code>MC_JOURNAL_WANT_AUDIT</code> - Audit trail mode - <code>MC_JOURNAL_WANT_REVERSIBLE</code> - Record UNDO information - <code>MC_JOURNAL_WANT_FULLDUPLEX</code> - Two-way acknowledgement</p>"},{"location":"sys/kern/vfs/journaling/#journal-lifecycle","title":"Journal Lifecycle","text":"<p>Attach: <code>journal_attach(mp)</code> - Switches mount point's vnops to <code>journal_vnode_vops</code> - Sets <code>mp-&gt;mnt_vn_journal_ops</code></p> <p>Install: <code>journal_install_vfs_journal()</code> - Creates journal structure - Allocates FIFO - Starts worker threads - Adds to <code>mp-&gt;mnt_jlist</code></p> <p>Operate: Normal VFS operations are intercepted and journaled</p> <p>Detach: <code>journal_detach(mp)</code> - Stops worker threads - Flushes FIFO - Frees resources - Restores normal vnops</p>"},{"location":"sys/kern/vfs/journaling/#transaction-structure","title":"Transaction Structure","text":""},{"location":"sys/kern/vfs/journaling/#nested-subrecords","title":"Nested Subrecords","text":"<p>Journal records use nested subrecord structure:</p> <pre><code>Stream Record (JTYPE_RENAME)\n\u251c\u2500 JTYPE_UNDO (nested)\n\u2502  \u251c\u2500 JLEAF_PATH1 (old source path) [leaf]\n\u2502  \u2514\u2500 JLEAF_PATH2 (old dest path if exists) [leaf, LAST]\n\u251c\u2500 JLEAF_PATH1 (source path) [leaf]\n\u2514\u2500 JLEAF_PATH2 (destination path) [leaf, LAST]\n</code></pre> <p>Subrecord traversal: <pre><code>void traverse_subrecords(char *data, int size) {\n    struct journal_subrecord *sub = (void *)data;\n\n    while ((char *)sub &lt; data + size) {\n        if (sub-&gt;rectype &amp; JMASK_NESTED) {\n            // Recurse into nested subrecord\n            traverse_subrecords(sub + 1, sub-&gt;recsize - 8);\n        } else {\n            // Process leaf subrecord\n            process_leaf(sub);\n        }\n\n        // Check for last subrecord\n        if (sub-&gt;rectype &amp; JMASK_LAST)\n            break;\n\n        // Advance to next subrecord\n        sub = (char *)sub + sub-&gt;recsize;\n    }\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#jrecord-api","title":"jrecord API","text":"<p>High-level API for building journal records:</p> <p>Initialize: <pre><code>void jrecord_init(struct journal *jo, struct jrecord *jrec, \n                  int16_t streamid);\n</code></pre></p> <p>Push/pop nested subrecords: <pre><code>void jrecord_push(struct jrecord *jrec, int16_t rectype);\nvoid jrecord_pop(struct jrecord *jrec);\n</code></pre></p> <p>Write leaf data: <pre><code>void jrecord_leaf(struct jrecord *jrec, int16_t rectype, \n                  void *data, int bytes);\nvoid jrecord_data(struct jrecord *jrec, void *buf, int bytes, int dtype);\n</code></pre></p> <p>Commit: <pre><code>void jrecord_done(struct jrecord *jrec, int error);\n</code></pre></p> <p>Example usage: <pre><code>struct jrecord jrec;\n\njrecord_init(jo, &amp;jrec, JTYPE_WRITE);\n\njrecord_push(&amp;jrec, JTYPE_UNDO);\n    jrecord_leaf(&amp;jrec, JLEAF_FILEDATA, oldbuf, oldsize);\njrecord_pop(&amp;jrec);\n\njrecord_leaf(&amp;jrec, JLEAF_PATH1, path, pathlen);\njrecord_leaf(&amp;jrec, JLEAF_FILEDATA, newbuf, newsize);\n\njrecord_done(&amp;jrec, error);\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#synchronization-and-locking","title":"Synchronization and Locking","text":""},{"location":"sys/kern/vfs/journaling/#fifo-concurrency","title":"FIFO Concurrency","text":"<p>The memory FIFO supports concurrent operations:</p> <p>Multiple writers:  - Each thread reserves its own space via <code>journal_reserve()</code> - Incomplete magic blocks worker thread from writing past incomplete records - Records can be completed out-of-order - Worker thread writes in reservation order</p> <p>Single writer thread: - One worker thread per journal - Reads from FIFO via rindex - Blocks on incomplete records</p> <p>Acknowledgement: - Single reader thread (full-duplex only) - Advances xindex based on acknowledgements - Frees up FIFO space</p> <p>Wait conditions: - Writers wait on <code>&amp;jo-&gt;fifo.windex</code> when FIFO full (<code>MC_JOURNAL_WWAIT</code>) - Worker wakes writers when space available - Worker waits on <code>&amp;jo-&gt;fifo</code> when nothing to write</p>"},{"location":"sys/kern/vfs/journaling/#memory-barriers","title":"Memory Barriers","text":"<p>Critical use of memory barriers for correctness:</p> <p>Reserve: <pre><code>// Initialize record header and trailer\nrawp-&gt;begmagic = JREC_INCOMPLETEMAGIC;\nrawp-&gt;recsize = bytes;\n// ... fill in fields ...\n\ncpu_sfence();  // Ensure writes complete before advancing windex\njo-&gt;fifo.windex += aligned_bytes;\n</code></pre></p> <p>Commit: <pre><code>// Fill in trailer\nrendp-&gt;endmagic = JREC_ENDMAGIC;\nrendp-&gt;recsize = rawp-&gt;recsize;\n\ncpu_sfence();  // Ensure trailer written before magic\nrawp-&gt;begmagic = JREC_BEGMAGIC;  // Make visible to worker\n</code></pre></p> <p>Pad record: <pre><code>rawp-&gt;streamid = JREC_STREAMID_PAD;\nrawp-&gt;recsize = recsize;\nrendp-&gt;endmagic = JREC_ENDMAGIC;\n\ncpu_sfence();  // Ensure complete before making visible\nrawp-&gt;begmagic = JREC_BEGMAGIC;\n</code></pre></p> <p>These barriers prevent: - Worker thread seeing incomplete records - Reordered writes corrupting record structure - CPU/compiler optimization breaking protocol</p>"},{"location":"sys/kern/vfs/journaling/#journal-targets","title":"Journal Targets","text":""},{"location":"sys/kern/vfs/journaling/#filesocket-support","title":"File/Socket Support","text":"<p>Journals can write to:</p> <p>Regular files: <pre><code>fp_write(jo-&gt;fp, buf, size, &amp;written, UIO_SYSSPACE);\n</code></pre></p> <p>Sockets (network journaling): - TCP sockets for remote replication - Can span network boundaries - Two-way acknowledgement over socket</p> <p>Special devices: - Raw disk partitions for fast local journaling - Block devices</p>"},{"location":"sys/kern/vfs/journaling/#full-duplex-journaling","title":"Full-Duplex Journaling","text":"<p>For two-way acknowledgement:</p> <p>Setup: <pre><code>info-&gt;flags |= MC_JOURNAL_WANT_FULLDUPLEX;\njournal_install_vfs_journal(mp, fp, info);\n</code></pre></p> <p>Operation: - Write worker sends journal records - Read worker receives acknowledgements - Target must implement ack protocol - xindex only advances on ack receipt</p> <p>Acknowledgement record format: <pre><code>struct journal_ackrecord {\n    struct journal_rawrecbeg rbeg;\n    int32_t filler0;\n    int32_t filler1;\n    struct journal_rawrecend rend;\n};\n</code></pre></p> <p>Target sends back transaction ID when committed to stable storage.</p>"},{"location":"sys/kern/vfs/journaling/#restart-and-resync","title":"Restart and Resync","text":"<p>Journals support interruption and restart:</p> <p>Restart marker: <pre><code>JREC_STREAMID_RESTART  // Marks journal restart after interruption\n</code></pre></p> <p>Resync operation: - Target can request resync to transaction ID - Journal fast-forwards xindex - Allows recovery after link interruption</p> <p>Use cases: - Network failure recovery - Target system restart - Catching up after outage</p>"},{"location":"sys/kern/vfs/journaling/#performance-considerations","title":"Performance Considerations","text":""},{"location":"sys/kern/vfs/journaling/#fifo-sizing","title":"FIFO Sizing","text":"<p>FIFO size affects performance and stall behavior:</p> <p>Too small: - Frequent stalls waiting for space - <code>fifostalls</code> counter increments - Threads block in <code>journal_reserve()</code></p> <p>Too large: - Memory overhead - Longer recovery window on restart - Delayed error detection</p> <p>Recommended: - 1-4 MB for local journaling - 8-32 MB for network journaling - Power of 2 for efficient masking</p>"},{"location":"sys/kern/vfs/journaling/#batching-efficiency","title":"Batching Efficiency","text":"<p>Worker thread batching reduces overhead:</p> <p>Wakeup policy: <pre><code>if (fifo &gt; 50% full || waiters_present)\n    wakeup(&amp;jo-&gt;fifo);\n</code></pre></p> <p>Benefits: - Amortizes thread switch overhead - Better CPU cache utilization - Reduces syscall/write overhead - Batches related operations</p> <p>Heartbeat: - Worker wakes periodically (HZ) - Flushes small amounts of data - Prevents indefinite delay</p>"},{"location":"sys/kern/vfs/journaling/#zero-copy-optimization","title":"Zero-Copy Optimization","text":"<p>Journal records are built directly in FIFO:</p> <ol> <li>Reserve space in FIFO</li> <li>Write data directly to reserved space</li> <li>Commit when complete</li> <li>Worker writes directly from FIFO to target</li> </ol> <p>No intermediate buffering or copying required.</p>"},{"location":"sys/kern/vfs/journaling/#error-handling","title":"Error Handling","text":""},{"location":"sys/kern/vfs/journaling/#filesystem-operation-errors","title":"Filesystem Operation Errors","text":"<p>If underlying VOP fails:</p> <pre><code>error = vop_write_ap(ap);\njrecord_done(&amp;jrec, error);  // Records error in journal\n</code></pre> <p>Journal records operation and result, even on failure.</p>"},{"location":"sys/kern/vfs/journaling/#journal-write-errors","title":"Journal Write Errors","text":"<p>If worker thread encounters write error:</p> <pre><code>error = fp_write(jo-&gt;fp, buf, bytes, &amp;res);\nif (error) {\n    kprintf(\"journal_thread(%s) write, error %d\\n\", jo-&gt;id, error);\n    // XXX: Error policy TBD\n    // Options: pause, abort, mark journal failed\n}\n</code></pre> <p>Current behavior: Log error and continue Future: Configurable error policies</p>"},{"location":"sys/kern/vfs/journaling/#fifo-overflow","title":"FIFO Overflow","text":"<p>When FIFO fills up:</p> <pre><code>avail = fifo_size - (windex - xindex);\nif (avail &lt; required) {\n    jo-&gt;flags |= MC_JOURNAL_WWAIT;\n    ++jo-&gt;fifostalls;\n    tsleep(&amp;jo-&gt;fifo.windex, 0, \"jwrite\", 0);\n}\n</code></pre> <p>Blocking behavior: - Thread sleeps until space available - Worker thread wakes waiters - <code>fifostalls</code> tracks frequency</p> <p>Implications: - Filesystem operations block - System slows to journal speed - Prevents memory exhaustion</p>"},{"location":"sys/kern/vfs/journaling/#use-cases","title":"Use Cases","text":""},{"location":"sys/kern/vfs/journaling/#replication","title":"Replication","text":"<p>Stream filesystem changes to remote system:</p> <ol> <li>Install journal with socket to remote host</li> <li>All filesystem operations recorded</li> <li>Remote system applies changes</li> <li>Full-duplex acks ensure durability</li> </ol> <p>Benefits: - Near real-time replication - Crash recovery via replay - Bandwidth efficient (operation-level)</p>"},{"location":"sys/kern/vfs/journaling/#auditing","title":"Auditing","text":"<p>Record all filesystem access:</p> <pre><code>info-&gt;flags = MC_JOURNAL_WANT_AUDIT;\n</code></pre> <p>Captures: - All file creates/deletes - All writes and modifications - User credentials - Timestamps - Process information</p> <p>Use: Security auditing, compliance</p>"},{"location":"sys/kern/vfs/journaling/#reversible-journaling","title":"Reversible Journaling","text":"<p>Enable undo capability:</p> <pre><code>info-&gt;flags = MC_JOURNAL_WANT_REVERSIBLE;\n</code></pre> <p>Records: - UNDO information (old data) - REDO information (new data) - Complete state for rollback</p> <p>Use: Snapshot-like functionality, experimental</p>"},{"location":"sys/kern/vfs/journaling/#local-fast-journal","title":"Local Fast Journal","text":"<p>For crash recovery:</p> <ol> <li>Journal to fast SSD/NVMe</li> <li>Async write to slower main storage</li> <li>Replay journal on crash</li> <li>Discard journal when committed</li> </ol> <p>Benefits: - Fast write acknowledgement - Large write coalescing - Crash consistency</p>"},{"location":"sys/kern/vfs/journaling/#debugging","title":"Debugging","text":""},{"location":"sys/kern/vfs/journaling/#statistics","title":"Statistics","text":"<p>Each journal maintains statistics:</p> <pre><code>struct journal {\n    int64_t total_acked;    // Total bytes acknowledged\n    int fifostalls;         // FIFO full stall count\n    // ...\n};\n</code></pre> <p>Access via mountctl: <pre><code>MOUNTCTL_STATUS_VFS_JOURNAL\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#tracing","title":"Tracing","text":"<p>Enable debugging output:</p> <pre><code>#if 1\nkprintf(\"ackskip %08llx/%08llx\\n\", rawp-&gt;transid, transid);\n#endif\n</code></pre> <p>Traces: - Acknowledgement processing - Record sequencing - Error conditions</p>"},{"location":"sys/kern/vfs/journaling/#common-issues","title":"Common Issues","text":"<p>FIFO stalls: - <code>fifostalls</code> counter high - Increase FIFO size - Check target write performance</p> <p>Incomplete record hangs: - Thread crashed while populating record - Incomplete magic left in FIFO - Worker thread blocked forever - Solution: Timeout + recovery logic (TBD)</p> <p>Acknowledgement protocol errors: - \"warning: unsent data acknowledged\" - Target acknowledging wrong transid - Check target implementation</p>"},{"location":"sys/kern/vfs/journaling/#limitations-and-future-work","title":"Limitations and Future Work","text":""},{"location":"sys/kern/vfs/journaling/#current-limitations","title":"Current Limitations","text":"<ol> <li>MPLOCK dependency:</li> <li>Worker threads still use MPLOCK</li> <li> <p>Not fully SMP-optimized</p> </li> <li> <p>Error handling:</p> </li> <li>Limited error policies</li> <li> <p>No automatic journal failure handling</p> </li> <li> <p>Checksum disabled:</p> </li> <li><code>check</code> field in records unused</li> <li> <p>No data integrity verification</p> </li> <li> <p>No encryption:</p> </li> <li>All data in clear text</li> <li>Security via transport layer only</li> </ol>"},{"location":"sys/kern/vfs/journaling/#planned-enhancements","title":"Planned Enhancements","text":"<p>From <code>vfs_journal.c:35-60</code> comments:</p> <ol> <li>Two-way acknowledgement:</li> <li>Transaction ID acknowledgement (partially implemented)</li> <li>Explicit and implicit ack schemes</li> <li>Resynchronization support</li> <li> <p>Restart after interruption</p> </li> <li> <p>Swap space spooling:</p> </li> <li>Use swap to absorb long interruptions</li> <li>Prevent slow links from blocking local ops</li> <li> <p>Larger buffer capacity</p> </li> <li> <p>Per-CPU FIFOs:</p> </li> <li>Remove locking requirements</li> <li>Better SMP scalability</li> <li> <p>Reduce contention</p> </li> <li> <p>Filesystem integration:</p> </li> <li>Allow filesystems to use journal layer directly</li> <li>Avoid rolling their own journaling</li> <li>Leverage kernel infrastructure</li> </ol>"},{"location":"sys/kern/vfs/journaling/#summary","title":"Summary","text":"<p>The VFS journaling system provides a sophisticated infrastructure for recording filesystem operations. Key aspects:</p> <p>Architecture: - Interception layer between VOP wrappers and filesystem - Memory FIFO batches records before writing - Asynchronous worker threads for write-out - Optional two-way acknowledgement</p> <p>Record structure: - Stream records with headers/trailers - Nested subrecord hierarchy - Transaction IDs for sequencing - Extensible operation types</p> <p>Concurrency: - Lock-free reservation with incomplete magic - Multiple writers, single worker thread - Memory barriers for correctness - Wait/wakeup for flow control</p> <p>Features: - Multiple journals per mount point - Full-duplex acknowledgement - Reversible journals (UNDO) - Network and local targets</p> <p>Use cases: - Replication to remote systems - Security auditing trails - Crash recovery journals - Experimental undo functionality</p> <p>The journaling system is mature but retains areas for optimization, particularly in SMP scalability and error handling sophistication.</p> <p>Related documentation: - VFS Operations - VOP dispatch mechanism - Buffer Cache - Block I/O infrastructure - Mounting - Mount point management</p>"},{"location":"sys/kern/vfs/mounting/","title":"VFS Mounting and Unmounting","text":""},{"location":"sys/kern/vfs/mounting/#overview","title":"Overview","text":"<p>The VFS mounting subsystem manages filesystem mount points throughout their lifecycle, from initial allocation through mounting, operation, and eventual unmounting. This documentation covers the mount point management infrastructure in <code>vfs_mount.c</code> and the mount/unmount system calls in <code>vfs_syscalls.c</code>.</p> <p>Key source files: - <code>sys/kern/vfs_mount.c</code> (1,249 lines) - Mount point lifecycle and management - <code>sys/kern/vfs_syscalls.c</code> (5,512+ lines) - VFS system calls including mount/unmount - <code>sys/sys/mount.h</code> - Mount structure and flag definitions</p>"},{"location":"sys/kern/vfs/mounting/#mount-structure","title":"Mount Structure","text":""},{"location":"sys/kern/vfs/mounting/#struct-mount","title":"struct mount","text":"<p>The <code>struct mount</code> (defined in sys/sys/mount.h:216) is the central data structure representing a mounted filesystem instance:</p> <pre><code>struct mount {\n    TAILQ_ENTRY(mount) mnt_list;        /* mount list linkage */\n    struct vfsops      *mnt_op;         /* filesystem operations */\n    struct vfsconf     *mnt_vfc;        /* filesystem configuration */\n    u_int              mnt_namecache_gen; /* negative cache invalidation */\n    u_int              mnt_pbuf_count;  /* pbuf usage limit */\n    struct vnode       *mnt_syncer;     /* syncer vnode */\n    struct syncer_ctx  *mnt_syncer_ctx; /* syncer process context */\n    struct vnodelst    mnt_nvnodelist;  /* list of vnodes on this mount */\n    TAILQ_HEAD(,vmntvnodescan_info) mnt_vnodescan_list;\n    struct lock        mnt_lock;        /* mount structure lock */\n    int                mnt_flag;        /* user-visible flags */\n    int                mnt_kern_flag;   /* kernel-only flags */\n    int                mnt_maxsymlinklen; /* max short symlink size */\n    struct statfs      mnt_stat;        /* filesystem statistics */\n    struct statvfs     mnt_vstat;       /* extended statistics */\n    qaddr_t            mnt_data;        /* filesystem-private data */\n    time_t             mnt_time;        /* last write time */\n    u_int              mnt_iosize_max;  /* max IO request size */\n    struct vnodelst    mnt_reservedvnlist; /* reserved/dirty vnode list */\n    int                mnt_nvnodelistsize; /* vnode list size */\n\n    /* VFS operations vectors (stacked) */\n    struct vop_ops     *mnt_vn_use_ops;      /* current ops */\n    struct vop_ops     *mnt_vn_coherency_ops; /* cache coherency */\n    struct vop_ops     *mnt_vn_journal_ops;   /* journaling */\n    struct vop_ops     *mnt_vn_norm_ops;      /* normal ops */\n    struct vop_ops     *mnt_vn_spec_ops;      /* special files */\n    struct vop_ops     *mnt_vn_fifo_ops;      /* FIFOs */\n\n    /* Namecache integration */\n    struct nchandle    mnt_ncmountpt;   /* mount point (root of fs) */\n    struct nchandle    mnt_ncmounton;   /* mounted on (directory) */\n\n    /* Reference counting */\n    struct ucred       *mnt_cred;       /* credentials */\n    int                mnt_refs;        /* nchandle references */\n    int                mnt_hold;        /* prevent premature free */\n    struct lwkt_token  mnt_token;       /* token lock if !MPSAFE */\n\n    /* Journaling support */\n    struct journallst  mnt_jlist;       /* active journals */\n    u_int8_t           *mnt_jbitmap;    /* streamid bitmap */\n    int16_t            mnt_streamid;    /* last streamid */\n\n    /* Buffer I/O operations */\n    struct bio_ops     *mnt_bioops;     /* BIO ops (HAMMER, softupd) */\n    struct lock        mnt_renlock;     /* rename directory lock */\n\n    /* Quota accounting */\n    struct vfs_acct    mnt_acct;        /* space accounting */\n    RB_ENTRY(mount)    mnt_node;        /* mounttree RB-tree node */\n};\n</code></pre> <p>Key nchandle fields: - <code>mnt_ncmountpt</code>: Points to the root of the mounted filesystem (created during mount) - <code>mnt_ncmounton</code>: Points to the directory where the filesystem is mounted</p>"},{"location":"sys/kern/vfs/mounting/#mount-flags-mnt_flag","title":"Mount Flags (mnt_flag)","text":"<p>User-visible flags in <code>mnt_flag</code> (sys/sys/mount.h:274):</p> <p>Access control: - <code>MNT_RDONLY</code> (0x00000001) - Read-only filesystem - <code>MNT_NOSUID</code> (0x00000008) - Ignore setuid/setgid bits - <code>MNT_NOEXEC</code> (0x00000004) - Disallow program execution - <code>MNT_NODEV</code> (0x00000010) - Ignore device files</p> <p>Performance: - <code>MNT_SYNCHRONOUS</code> (0x00000002) - Synchronous writes - <code>MNT_ASYNC</code> (0x00000040) - Asynchronous writes - <code>MNT_NOATIME</code> (0x10000000) - Don't update access times - <code>MNT_NOCLUSTERR</code> (0x40000000) - Disable cluster read - <code>MNT_NOCLUSTERW</code> (0x80000000) - Disable cluster write</p> <p>Special behavior: - <code>MNT_NOSYMFOLLOW</code> (0x00400000) - Don't follow symlinks - <code>MNT_SUIDDIR</code> (0x00100000) - Special SUID directory handling - <code>MNT_TRIM</code> (0x01000000) - Enable online FS trimming - <code>MNT_AUTOMOUNTED</code> (0x00000020) - Mounted by automountd(8)</p> <p>NFS export: - <code>MNT_EXPORTED</code> (0x00000100) - Filesystem is exported - <code>MNT_DEFEXPORTED</code> (0x00000200) - Exported to the world - <code>MNT_EXRDONLY</code> (0x00000080) - Exported read-only</p> <p>Internal: - <code>MNT_LOCAL</code> (0x00001000) - Stored locally - <code>MNT_QUOTA</code> (0x00002000) - Quotas enabled - <code>MNT_ROOTFS</code> (0x00004000) - Root filesystem - <code>MNT_USER</code> (0x00008000) - Mounted by non-root user - <code>MNT_IGNORE</code> (0x00800000) - Hide from df output</p> <p>Command flags (transient): - <code>MNT_UPDATE</code> (0x00010000) - Update existing mount - <code>MNT_RELOAD</code> (0x00040000) - Reload filesystem data - <code>MNT_FORCE</code> (0x00080000) - Force unmount/readonly change</p>"},{"location":"sys/kern/vfs/mounting/#kernel-flags-mnt_kern_flag","title":"Kernel Flags (mnt_kern_flag)","text":"<p>Kernel-only flags in <code>mnt_kern_flag</code> (sys/sys/mount.h:348):</p> <p>Unmounting: - <code>MNTK_UNMOUNT</code> (0x01000000) - Unmount in progress - <code>MNTK_UNMOUNTF</code> (0x00000001) - Forced unmount in progress - <code>MNTK_MWAIT</code> (0x02000000) - Waiting for unmount to finish - <code>MNTK_QUICKHALT</code> (0x00008000) - Quick unmount on system halt</p> <p>MPSAFE operation flags: - <code>MNTK_ALL_MPSAFE</code> - All VFS operations are MPSAFE - <code>MNTK_MPSAFE</code> (0x00010000) - VFS operations don't need mnt_token - <code>MNTK_RD_MPSAFE</code> (0x00020000) - vop_read is MPSAFE - <code>MNTK_WR_MPSAFE</code> (0x00040000) - vop_write is MPSAFE - <code>MNTK_GA_MPSAFE</code> (0x00080000) - vop_getattr is MPSAFE - <code>MNTK_IN_MPSAFE</code> (0x00100000) - vop_inactive is MPSAFE - <code>MNTK_SG_MPSAFE</code> (0x00200000) - vop_strategy is MPSAFE - <code>MNTK_ST_MPSAFE</code> (0x80000000) - vfs_start is MPSAFE</p> <p>Other: - <code>MNTK_WANTRDWR</code> (0x04000000) - Upgrade to read/write requested - <code>MNTK_NOSTKMNT</code> (0x10000000) - No stacked mounts allowed - <code>MNTK_NCALIASED</code> (0x00800000) - Namecache is aliased - <code>MNTK_NOMSYNC</code> (0x20000000) - Used by tmpfs - <code>MNTK_THR_SYNC</code> (0x40000000) - FS sync thread requested</p>"},{"location":"sys/kern/vfs/mounting/#mount-point-lifecycle","title":"Mount Point Lifecycle","text":""},{"location":"sys/kern/vfs/mounting/#initialization","title":"Initialization","text":"<p>vfs_mount_init() (vfs_mount.c:154)</p> <p>Called from <code>vfsinit()</code> during system initialization:</p> <pre><code>void vfs_mount_init(void)\n{\n    lwkt_token_init(&amp;mountlist_token, \"mntlist\");\n    lwkt_token_init(&amp;mntid_token, \"mntid\");\n    TAILQ_INIT(&amp;mountscan_list);\n    mount_init(&amp;dummymount, NULL);\n    dummymount.mnt_flag |= MNT_RDONLY;\n    dummymount.mnt_kern_flag |= MNTK_ALL_MPSAFE;\n}\n</code></pre> <p>Creates a dummy mount used for vnodes that have no filesystem (e.g., early boot devices).</p> <p>mount_init() (vfs_mount.c:373)</p> <p>Initializes a mount structure:</p> <pre><code>void mount_init(struct mount *mp, struct vfsops *ops)\n{\n    lockinit(&amp;mp-&gt;mnt_lock, \"vfslock\", hz*5, 0);\n    lockinit(&amp;mp-&gt;mnt_renlock, \"renamlk\", hz*5, 0);\n    lwkt_token_init(&amp;mp-&gt;mnt_token, \"permnt\");\n\n    TAILQ_INIT(&amp;mp-&gt;mnt_vnodescan_list);\n    TAILQ_INIT(&amp;mp-&gt;mnt_nvnodelist);\n    TAILQ_INIT(&amp;mp-&gt;mnt_reservedvnlist);\n    TAILQ_INIT(&amp;mp-&gt;mnt_jlist);\n\n    mp-&gt;mnt_nvnodelistsize = 0;\n    mp-&gt;mnt_flag = 0;\n    mp-&gt;mnt_hold = 1;  /* hold for umount last drop */\n    mp-&gt;mnt_iosize_max = MAXPHYS;\n    mp-&gt;mnt_op = ops;\n\n    if (ops == NULL || (ops-&gt;vfs_flags &amp; VFSOPSF_NOSYNCERTHR) == 0)\n        vn_syncer_thr_create(mp);  /* create syncer thread */\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#reference-counting","title":"Reference Counting","text":"<p>Mount structures use two reference counters:</p> <p>mnt_refs - nchandle references: - Incremented when nchandles reference this mount - Managed automatically by the namecache system - Must reach 1 (only mnt_ncmountpt reference) before unmount</p> <p>mnt_hold - Hold count: - Prevents premature kfree of the mount structure - Initialized to 1 in mount_init() - Used when mount might be accessed without holding mountlist_token</p> <p>mount_hold() / mount_drop() (vfs_mount.c:393, 399):</p> <pre><code>void mount_hold(struct mount *mp)\n{\n    atomic_add_int(&amp;mp-&gt;mnt_hold, 1);\n}\n\nvoid mount_drop(struct mount *mp)\n{\n    if (atomic_fetchadd_int(&amp;mp-&gt;mnt_hold, -1) == 1) {\n        KKASSERT(mp-&gt;mnt_refs == 0);\n        kfree(mp, M_MOUNT);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#filesystem-id-fsid-management","title":"Filesystem ID (FSID) Management","text":"<p>vfs_getnewfsid() (vfs_mount.c:441)</p> <p>Generates a unique filesystem identifier based on the mount path:</p> <pre><code>void vfs_getnewfsid(struct mount *mp)\n{\n    fsid_t tfsid;\n    int mtype;\n    char *retbuf, *freebuf;\n\n    mtype = mp-&gt;mnt_vfc-&gt;vfc_typenum;\n    tfsid.val[1] = mtype;\n\n    /* Hash the mount point path to create unique FSID */\n    error = cache_fullpath(NULL, &amp;mp-&gt;mnt_ncmounton, NULL,\n                          &amp;retbuf, &amp;freebuf, 0);\n    if (error) {\n        tfsid.val[0] = makeudev(255, 0);\n    } else {\n        tfsid.val[0] = makeudev(255,\n                               iscsi_crc32(retbuf, strlen(retbuf)) &amp;\n                               ~makeudev(255, 0));\n        kfree(freebuf, M_TEMP);\n    }\n\n    mp-&gt;mnt_stat.f_fsid.val[0] = tfsid.val[0];\n    mp-&gt;mnt_stat.f_fsid.val[1] = tfsid.val[1];\n}\n</code></pre> <p>The FSID will be adjusted automatically during <code>mountlist_insert()</code> if collisions occur.</p>"},{"location":"sys/kern/vfs/mounting/#mount-lists-and-trees","title":"Mount Lists and Trees","text":""},{"location":"sys/kern/vfs/mounting/#global-mount-data-structures","title":"Global Mount Data Structures","text":"<p>mountlist (vfs_mount.c:143) - Ordered list of all mounts: <pre><code>struct mntlist mountlist = TAILQ_HEAD_INITIALIZER(mountlist);\n</code></pre></p> <p>mounttree (vfs_mount.c:144) - Red-black tree indexed by FSID: <pre><code>struct mount_rb_tree mounttree = RB_INITIALIZER(dev_tree_mounttree);\n</code></pre></p> <p>mountlist_token (vfs_mount.c:146) - Protects both structures</p>"},{"location":"sys/kern/vfs/mounting/#mount-list-operations","title":"Mount List Operations","text":"<p>mountlist_insert() (vfs_mount.c:590)</p> <p>Adds a mount to the global mount list and tree:</p> <pre><code>void mountlist_insert(struct mount *mp, int how)\n{\n    int lim = 0x01000000;\n\n    lwkt_gettoken(&amp;mountlist_token);\n\n    /* Add to ordered list */\n    if (how == MNTINS_FIRST)\n        TAILQ_INSERT_HEAD(&amp;mountlist, mp, mnt_list);\n    else\n        TAILQ_INSERT_TAIL(&amp;mountlist, mp, mnt_list);\n\n    /* Add to RB-tree, adjusting FSID on collision */\n    while (mount_rb_tree_RB_INSERT(&amp;mounttree, mp)) {\n        int32_t val = mp-&gt;mnt_stat.f_fsid.val[0];\n        val = ((val &amp; 0xFFFF0000) &gt;&gt; 8) | (val &amp; 0x000000FF);\n        ++val;\n        val = ((val &lt;&lt; 8) &amp; 0xFFFF0000) | (val &amp; 0x000000FF);\n        mp-&gt;mnt_stat.f_fsid.val[0] = val;\n\n        if (--lim == 0) {\n            lim = 0x01000000;\n            mp-&gt;mnt_stat.f_fsid.val[1] += 0x0100;\n            kprintf(\"mountlist_insert: fsid collision, \"\n                   \"too many mounts\\n\");\n        }\n    }\n\n    lwkt_reltoken(&amp;mountlist_token);\n}\n</code></pre> <p>mountlist_remove() (vfs_mount.c:663)</p> <p>Removes a mount from both structures:</p> <pre><code>void mountlist_remove(struct mount *mp)\n{\n    struct mountscan_info *msi;\n\n    lwkt_gettoken(&amp;mountlist_token);\n\n    /* Adjust any active scans past this mount */\n    TAILQ_FOREACH(msi, &amp;mountscan_list, msi_entry) {\n        if (msi-&gt;msi_node == mp) {\n            if (msi-&gt;msi_how &amp; MNTSCAN_FORWARD)\n                msi-&gt;msi_node = TAILQ_NEXT(mp, mnt_list);\n            else\n                msi-&gt;msi_node = TAILQ_PREV(mp, mntlist, mnt_list);\n        }\n    }\n\n    TAILQ_REMOVE(&amp;mountlist, mp, mnt_list);\n    mount_rb_tree_RB_REMOVE(&amp;mounttree, mp);\n\n    lwkt_reltoken(&amp;mountlist_token);\n}\n</code></pre> <p>vfs_getvfs() (vfs_mount.c:414)</p> <p>Looks up a mount by FSID:</p> <pre><code>struct mount *vfs_getvfs(fsid_t *fsid)\n{\n    struct mount *mp;\n\n    lwkt_gettoken_shared(&amp;mountlist_token);\n    mp = mount_rb_tree_RB_LOOKUP_FSID(&amp;mounttree, fsid);\n    if (mp)\n        mount_hold(mp);  /* caller must mount_drop() */\n    lwkt_reltoken(&amp;mountlist_token);\n\n    return (mp);\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#scanning-the-mount-list","title":"Scanning the Mount List","text":"<p>mountlist_scan() (vfs_mount.c:736)</p> <p>Safely iterates over all mounts with a callback:</p> <pre><code>int mountlist_scan(int (*callback)(struct mount *, void *),\n                   void *data, int how)\n</code></pre> <p>Scan flags: - <code>MNTSCAN_FORWARD</code> - Forward iteration - <code>MNTSCAN_REVERSE</code> - Reverse iteration - <code>MNTSCAN_NOBUSY</code> - Don't call vfs_busy() before callback - <code>MNTSCAN_NOUNLOCK</code> - Keep mountlist_token held during callback</p> <p>The scanner: 1. Registers scan state with mountscan_list 2. Iterates mount list calling callback for each mount 3. Calls vfs_busy() unless MNTSCAN_NOBUSY is set 4. Unlocks mountlist_token during callback (unless MNTSCAN_NOUNLOCK) 5. Handles mount removal during iteration 6. Aggregates callback return values</p> <p>Example usage (sys_sync): <pre><code>int sys_sync(struct sysmsg *sysmsg, const struct sync_args *uap)\n{\n    mountlist_scan(sync_callback, NULL, MNTSCAN_FORWARD);\n    return (0);\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#mount-busy-protocol","title":"Mount Busy Protocol","text":""},{"location":"sys/kern/vfs/mounting/#vfs_busy-vfs_unbusy","title":"vfs_busy() / vfs_unbusy()","text":"<p>The busy protocol prevents a mount from being unmounted while operations are in progress.</p> <p>vfs_busy() (vfs_mount.c:271)</p> <p>Acquires a shared lock on the mount, incrementing mnt_refs:</p> <pre><code>int vfs_busy(struct mount *mp, int flags)\n{\n    atomic_add_int(&amp;mp-&gt;mnt_refs, 1);\n    lwkt_gettoken(&amp;mp-&gt;mnt_token);\n\n    /* Check if unmount is in progress */\n    if (mp-&gt;mnt_kern_flag &amp; MNTK_UNMOUNT) {\n        if (flags &amp; LK_NOWAIT) {\n            lwkt_reltoken(&amp;mp-&gt;mnt_token);\n            atomic_add_int(&amp;mp-&gt;mnt_refs, -1);\n            return (ENOENT);\n        }\n        /* Wait for unmount to complete */\n        mp-&gt;mnt_kern_flag |= MNTK_MWAIT;\n        tsleep((caddr_t)mp, 0, \"vfs_busy\", 0);\n        lwkt_reltoken(&amp;mp-&gt;mnt_token);\n        atomic_add_int(&amp;mp-&gt;mnt_refs, -1);\n        return (ENOENT);\n    }\n\n    /* Acquire shared lock */\n    if (lockmgr(&amp;mp-&gt;mnt_lock, LK_SHARED))\n        panic(\"vfs_busy: unexpected lock failure\");\n\n    lwkt_reltoken(&amp;mp-&gt;mnt_token);\n    return (0);\n}\n</code></pre> <p>vfs_unbusy() (vfs_mount.c:317)</p> <p>Releases the busy lock:</p> <pre><code>void vfs_unbusy(struct mount *mp)\n{\n    mount_hold(mp);  /* prevent race with final unmount */\n    atomic_add_int(&amp;mp-&gt;mnt_refs, -1);\n    lockmgr(&amp;mp-&gt;mnt_lock, LK_RELEASE);\n    mount_drop(mp);\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#vnode-mount-integration","title":"Vnode-Mount Integration","text":""},{"location":"sys/kern/vfs/mounting/#associating-vnodes-with-mounts","title":"Associating Vnodes with Mounts","text":"<p>insmntque() (vfs_mount.c:835)</p> <p>Moves a vnode to a mount's vnode list:</p> <pre><code>void insmntque(struct vnode *vp, struct mount *mp)\n{\n    struct mount *omp;\n\n    /* Remove from old mount if present */\n    if ((omp = vp-&gt;v_mount) != NULL) {\n        lwkt_gettoken(&amp;omp-&gt;mnt_token);\n        vremovevnodemnt(vp);\n        omp-&gt;mnt_nvnodelistsize--;\n        lwkt_reltoken(&amp;omp-&gt;mnt_token);\n    }\n\n    if (mp == NULL) {\n        vp-&gt;v_mount = NULL;\n        return;\n    }\n\n    /* Insert into new mount's vnode list */\n    lwkt_gettoken(&amp;mp-&gt;mnt_token);\n    vp-&gt;v_mount = mp;\n\n    /* Insert before syncer vnode if present, else at tail */\n    if (mp-&gt;mnt_syncer) {\n        TAILQ_INSERT_BEFORE(mp-&gt;mnt_syncer, vp, v_nmntvnodes);\n    } else {\n        TAILQ_INSERT_TAIL(&amp;mp-&gt;mnt_nvnodelist, vp, v_nmntvnodes);\n    }\n\n    mp-&gt;mnt_nvnodelistsize++;\n    lwkt_reltoken(&amp;mp-&gt;mnt_token);\n}\n</code></pre> <p>getnewvnode() (vfs_mount.c:194)</p> <p>Allocates a new vnode and associates it with a mount:</p> <pre><code>int getnewvnode(enum vtagtype tag, struct mount *mp,\n                struct vnode **vpp, int lktimeout, int lkflags)\n{\n    struct vnode *vp;\n\n    KKASSERT(mp != NULL);\n\n    vp = allocvnode(lktimeout, lkflags);\n    vp-&gt;v_tag = tag;\n    vp-&gt;v_data = NULL;\n\n    /* Assign mount's normal operations vector */\n    vp-&gt;v_ops = &amp;mp-&gt;mnt_vn_use_ops;\n    vp-&gt;v_pbuf_count = nswbuf_kva / NSWBUF_SPLIT;\n\n    /* Make vnode visible on mount */\n    insmntque(vp, mp);\n\n    *vpp = vp;  /* VX locked &amp; refd */\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#scanning-mount-vnodes","title":"Scanning Mount Vnodes","text":"<p>vmntvnodescan() (vfs_mount.c:894)</p> <p>Scans vnodes on a mount point with fast and slow callbacks:</p> <pre><code>int vmntvnodescan(struct mount *mp, int flags,\n                  int (*fastfunc)(...),\n                  int (*slowfunc)(...),\n                  void *data)\n</code></pre> <p>Flags: - <code>VMSC_GETVP</code> - Lock vnode with vget() before slowfunc - <code>VMSC_GETVX</code> - Lock vnode with vx_get() before slowfunc - <code>VMSC_NOWAIT</code> - Use LK_NOWAIT when locking - <code>VMSC_ONEPASS</code> - Stop after one pass through list</p> <p>Callback semantics: - <code>fastfunc()</code>: Called with only mnt_token held, vnode not locked   - Return &lt; 0: Skip slowfunc, continue   - Return 0: Call slowfunc   - Return &gt; 0: Terminate scan</p> <ul> <li><code>slowfunc()</code>: Called with vnode locked</li> <li>Return 0: Continue</li> <li>Return != 0: Terminate scan</li> </ul> <p>Used by vflush(), filesystem sync operations, and vnode reclamation.</p>"},{"location":"sys/kern/vfs/mounting/#mounting-a-filesystem","title":"Mounting a Filesystem","text":""},{"location":"sys/kern/vfs/mounting/#the-sys_mount-system-call","title":"The sys_mount() System Call","text":"<p>sys_mount() (vfs_syscalls.c:118)</p> <p>Main entry point for the mount(2) system call:</p> <pre><code>int sys_mount(struct sysmsg *sysmsg, const struct mount_args *uap)\n{\n    /* uap-&gt;type:  filesystem type name */\n    /* uap-&gt;path:  mount point path */\n    /* uap-&gt;flags: mount flags */\n    /* uap-&gt;data:  filesystem-specific data */\n}\n</code></pre> <p>Mount workflow:</p> <ol> <li>Permission and type checks (lines 136-156):</li> <li>Deny user mounts inside jails</li> <li>Copy in filesystem type name</li> <li>Check capabilities (get_fscap)</li> <li> <p>Enforce MNT_NOSUID|MNT_NODEV for non-root</p> </li> <li> <p>Path lookup (lines 174-207):</p> </li> <li>Use nlookup() to resolve mount point path</li> <li>Extract nchandle and vnode</li> <li> <p>Check if already mounted (cache_findmount)</p> </li> <li> <p>Update vs. new mount (lines 223-279):</p> </li> <li>For MNT_UPDATE: verify VROOT|VPFSROOT, check ownership</li> <li> <p>For new mount: check ownership, validate directory</p> </li> <li> <p>Find or load VFS (lines 309-339):</p> </li> <li>Look up vfsconf by name</li> <li> <p>Auto-load kernel module if not found (root only)</p> </li> <li> <p>Allocate mount structure (lines 350-361):</p> </li> <li>Allocate and initialize struct mount</li> <li>Call mount_init()</li> <li> <p>Set initial flags from vfsconf</p> </li> <li> <p>Call VFS_MOUNT (lines 395-412):</p> </li> <li>For update: call with MNT_UPDATE flag</li> <li> <p>For new: call to initialize filesystem</p> </li> <li> <p>Finalize mount (lines 426-451):</p> </li> <li>Create mnt_ncmountpt if needed</li> <li>Mark directory as mount point (NCF_ISMOUNTPT)</li> <li>Insert into mountlist</li> <li>Update process directories (checkdirs)</li> <li>Allocate syncer vnode</li> <li> <p>Call VFS_START()</p> </li> <li> <p>Error handling (lines 452-470):</p> </li> <li>On failure: stop syncer, free mount structure</li> <li>Clean up nchandles and vnode</li> </ol>"},{"location":"sys/kern/vfs/mounting/#mount-point-namecache-integration","title":"Mount Point Namecache Integration","text":"<p>Two nchandles connect a mount to the namecache:</p> <p>mnt_ncmounton - The directory being mounted on: - Set to the mount point directory - Used to navigate \"up\" from the filesystem</p> <p>mnt_ncmountpt - The root of the mounted filesystem: - Created with cache_allocroot() during mount - Marked with NCF_ISMOUNTPT flag - Used to navigate \"down\" into the filesystem</p> <p>checkdirs() (vfs_syscalls.c:494)</p> <p>After mounting, updates process current/root directories:</p> <pre><code>static void checkdirs(struct nchandle *old_nch,\n                      struct nchandle *new_nch)\n{\n    struct vnode *olddp = old_nch-&gt;ncp-&gt;nc_vp;\n    struct vnode *newdp;\n    struct mount *mp = new_nch-&gt;mount;\n\n    /* Skip if no processes reference the old vnode */\n    if (olddp == NULL || VREFCNT(olddp) == 1)\n        return;\n\n    /* Resolve new mount's root vnode */\n    VFS_ROOT(mp, &amp;newdp);\n    cache_setvp(new_nch, newdp);\n\n    /* Update rootvnode if mounting over root */\n    if (rootvnode == olddp) {\n        vref(newdp);\n        vfs_cache_setroot(newdp, cache_hold(new_nch));\n    }\n\n    /* Scan all processes, updating fd_ncdir/fd_nrdir */\n    allproc_scan(checkdirs_callback, &amp;info, 0);\n\n    vput(newdp);\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#filesystem-specific-mount-data","title":"Filesystem-Specific Mount Data","text":"<p>VFS calls VFS_MOUNT() to let the filesystem: 1. Parse filesystem-specific mount options from uap-&gt;data 2. Read filesystem superblock/metadata 3. Initialize mp-&gt;mnt_data with private data 4. Set mp-&gt;mnt_stat fields (f_bsize, f_blocks, etc.) 5. Optionally create root vnode</p> <p>Example (from a typical VFS_MOUNT): <pre><code>static int myfs_mount(struct mount *mp, char *path,\n                      caddr_t data, struct ucred *cred)\n{\n    struct myfs_mount *mmp;\n    struct vnode *devvp;\n    int error;\n\n    /* Parse mount options */\n    error = myfs_parse_opts(data, &amp;opts);\n\n    /* Open device */\n    error = nlookup_init(&amp;nd, opts.fspec, ...);\n    devvp = nd.nl_nch.ncp-&gt;nc_vp;\n\n    /* Read superblock */\n    error = myfs_read_super(devvp, &amp;sb);\n\n    /* Allocate private mount data */\n    mmp = kmalloc(sizeof(*mmp), M_MYFS, M_WAITOK | M_ZERO);\n    mmp-&gt;devvp = devvp;\n    mp-&gt;mnt_data = (qaddr_t)mmp;\n\n    /* Fill in mount stats */\n    mp-&gt;mnt_stat.f_bsize = sb.s_blocksize;\n    mp-&gt;mnt_stat.f_blocks = sb.s_blocks;\n    mp-&gt;mnt_stat.f_bfree = sb.s_free_blocks;\n\n    /* Set max symlink length */\n    mp-&gt;mnt_maxsymlinklen = sb.s_symlink_max;\n\n    vfs_getnewfsid(mp);  /* generate unique FSID */\n    return (0);\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#unmounting-a-filesystem","title":"Unmounting a Filesystem","text":""},{"location":"sys/kern/vfs/mounting/#the-sys_unmount-system-call","title":"The sys_unmount() System Call","text":"<p>sys_unmount() (vfs_syscalls.c:610)</p> <p>Entry point for umount(2):</p> <pre><code>int sys_unmount(struct sysmsg *sysmsg,\n                const struct unmount_args *uap)\n{\n    /* uap-&gt;path:  mount point path */\n    /* uap-&gt;flags: MNT_FORCE, etc. */\n}\n</code></pre> <p>Unmount workflow:</p> <ol> <li>Permission checks (lines 625-657):</li> <li>Deny user unmounts in jails</li> <li>Resolve path with nlookup()</li> <li>Check filesystem type capabilities</li> <li> <p>Verify ownership or root privilege</p> </li> <li> <p>Validation (lines 660-682):</p> </li> <li>Reject unmounting root filesystem</li> <li>Verify unmounting at mount point root</li> <li> <p>Check jail ownership</p> </li> <li> <p>Call dounmount() (lines 690-694):</p> </li> <li>Hold mount to prevent races</li> <li>Release nlookup resources</li> <li>Perform actual unmount</li> </ol>"},{"location":"sys/kern/vfs/mounting/#the-dounmount-function","title":"The dounmount() Function","text":"<p>dounmount() (vfs_syscalls.c:793)</p> <p>The core unmount implementation:</p> <pre><code>int dounmount(struct mount *mp, int flags, int halting)\n</code></pre> <p>Unmount phases:</p> <p>1. Interlock and lock (lines 806-842): <pre><code>/* Check for quickhalt (devfs, tmpfs, procfs on shutdown) */\nif (halting &amp;&amp; (mp-&gt;mnt_kern_flag &amp; MNTK_QUICKHALT))\n    quickhalt = 1;\n\n/* Set MNTK_UNMOUNT flag atomically */\nmountlist_interlock(dounmount_interlock, mp);\n\n/* Set MNTK_UNMOUNTF if forced */\nif (flags &amp; MNT_FORCE)\n    mp-&gt;mnt_kern_flag |= MNTK_UNMOUNTF;\n\n/* Acquire exclusive mount lock */\nlflags = LK_EXCLUSIVE | ((flags &amp; MNT_FORCE) ? 0 : LK_TIMELOCK);\nerror = lockmgr(&amp;mp-&gt;mnt_lock, lflags);\n</code></pre></p> <p>2. Sync and stop syncer (lines 847-871): <pre><code>/* Sync dirty data */\nvfs_msync(mp, MNT_WAIT);\nmp-&gt;mnt_flag &amp;= ~MNT_ASYNC;\n\n/* Stop syncer vnode */\nif ((vp = mp-&gt;mnt_syncer) != NULL) {\n    mp-&gt;mnt_syncer = NULL;\n    vrele(vp);\n}\n\n/* Final sync (unless quickhalt) */\nif (quickhalt == 0) {\n    if ((mp-&gt;mnt_flag &amp; MNT_RDONLY) == 0)\n        VFS_SYNC(mp, MNT_WAIT);\n}\n</code></pre></p> <p>3. Wait for references to drain (lines 880-955): <pre><code>for (retry = 0; retry &lt; UMOUNTF_RETRIES; ++retry) {\n    /* Invalidate namecache under mount point */\n    if ((mp-&gt;mnt_kern_flag &amp; MNTK_NCALIASED) == 0) {\n        cache_inval(&amp;mp-&gt;mnt_ncmountpt,\n                   CINV_DESTROY | CINV_CHILDREN);\n    }\n\n    /* Clear per-CPU caches */\n    cache_unmounting(mp);\n    if (mp-&gt;mnt_refs != 1)\n        cache_clearmntcache(mp);\n\n    /* Check if ready to unmount */\n    ncp = mp-&gt;mnt_ncmountpt.ncp;\n    if (mp-&gt;mnt_refs == 1 &amp;&amp;\n        (ncp == NULL || (ncp-&gt;nc_refs == 1 &amp;&amp;\n                        TAILQ_FIRST(&amp;ncp-&gt;nc_list) == NULL))) {\n        break;  /* Success! */\n    }\n\n    /* Force unmount: kill processes using the mount */\n    if (flags &amp; MNT_FORCE) {\n        switch(retry) {\n        case 3:  info.sig = SIGINT;  break;\n        case 7:  info.sig = SIGKILL; break;\n        default: info.sig = 0;       break;\n        }\n        allproc_scan(&amp;unmount_allproc_cb, &amp;info, 0);\n    }\n\n    /* Sleep and retry */\n    tsleep(&amp;dummy, 0, \"mntbsy\", hz / 4 + 1);\n}\n</code></pre></p> <p>The retry loop: - Invalidates the namecache tree under the mount - Clears per-CPU namecache entries - Checks if mnt_refs dropped to 1 (only mnt_ncmountpt left) - For forced unmount: sends SIGINT (retry 3) then SIGKILL (retry 7) to processes - Retries up to 50 times (12.5 seconds)</p> <p>4. Call VFS_UNMOUNT (lines 990-1007): <pre><code>if (error == 0 &amp;&amp; quickhalt == 0) {\n    if (mp-&gt;mnt_flag &amp; MNT_RDONLY) {\n        error = VFS_UNMOUNT(mp, flags);\n    } else {\n        error = VFS_SYNC(mp, MNT_WAIT);\n        if (error == 0 || error == EOPNOTSUPP ||\n            (flags &amp; MNT_FORCE)) {\n            error = VFS_UNMOUNT(mp, flags);\n        }\n    }\n}\n</code></pre></p> <p>5. Handle errors or finalize (lines 1010-1120):</p> <p>On error: <pre><code>if (error) {\n    /* Re-allocate syncer if needed */\n    if (mp-&gt;mnt_syncer == NULL &amp;&amp; hadsyncer)\n        vfs_allocate_syncvnode(mp);\n\n    /* Clear unmount flags */\n    mp-&gt;mnt_kern_flag &amp;= ~(MNTK_UNMOUNT | MNTK_UNMOUNTF);\n    mp-&gt;mnt_flag |= async_flag;\n\n    /* Release lock and wakeup waiters */\n    lockmgr(&amp;mp-&gt;mnt_lock, LK_RELEASE);\n    if (mp-&gt;mnt_kern_flag &amp; MNTK_MWAIT)\n        wakeup(mp);\n\n    goto out;\n}\n</code></pre></p> <p>On success: <pre><code>/* Remove journals */\njournal_remove_all_journals(mp, ...);\n\n/* Remove from mountlist */\nmountlist_remove(mp);\n\n/* Remove vnode ops (unless quickhalt) */\nif (quickhalt == 0) {\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_coherency_ops);\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_journal_ops);\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_norm_ops);\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_spec_ops);\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_fifo_ops);\n}\n\n/* Drop nchandle references */\nif (mp-&gt;mnt_ncmountpt.ncp != NULL) {\n    nch = mp-&gt;mnt_ncmountpt;\n    cache_zero(&amp;mp-&gt;mnt_ncmountpt);\n    cache_clrmountpt(&amp;nch);\n    cache_drop(&amp;nch);\n}\nif (mp-&gt;mnt_ncmounton.ncp != NULL) {\n    nch = mp-&gt;mnt_ncmounton;\n    cache_zero(&amp;mp-&gt;mnt_ncmounton);\n    cache_clrmountpt(&amp;nch);\n    cache_drop(&amp;nch);\n}\n\n/* Release credentials */\nif (mp-&gt;mnt_cred) {\n    crfree(mp-&gt;mnt_cred);\n    mp-&gt;mnt_cred = NULL;\n}\n\n/* Decrement vfsconf refcount */\nmp-&gt;mnt_vfc-&gt;vfc_refcount--;\n\n/* Verify no vnodes remain (unless quickhalt) */\nif (quickhalt == 0 &amp;&amp; !TAILQ_EMPTY(&amp;mp-&gt;mnt_nvnodelist))\n    panic(\"unmount: dangling vnode\");\n\n/* Release lock */\nlockmgr(&amp;mp-&gt;mnt_lock, LK_RELEASE);\n\n/* Free mount structure if freeok */\nif (freeok) {\n    /* Wait for mnt_refs to drop to 0 */\n    while (mp-&gt;mnt_refs &gt; 0) {\n        cache_clearmntcache(mp);\n        tsleep(&amp;mp-&gt;mnt_refs, 0, \"umntrwait\", hz / 10 + 1);\n    }\n    mount_drop(mp);  /* Final free */\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#forced-unmount","title":"Forced Unmount","text":"<p>When <code>MNT_FORCE</code> is specified:</p> <ol> <li>MNTK_UNMOUNTF flag alerts filesystem of forced unmount</li> <li>Process termination: Processes using the mount are killed:</li> <li>Retry 3: Send SIGINT</li> <li>Retry 7: Send SIGKILL</li> <li>Checks: p_textnch, fd_ncdir, fd_nrdir, fd_njdir, open files</li> <li>Ignore busy errors: Unmount proceeds even if refs remain</li> <li>vflush FORCECLOSE: Vnodes are forcibly reclaimed</li> </ol> <p>unmount_allproc_cb() (vfs_syscalls.c:761): <pre><code>static int unmount_allproc_cb(struct proc *p, void *arg)\n{\n    struct unmount_allproc_info *info = arg;\n    struct mount *mp = info-&gt;mp;\n\n    /* Drop text reference */\n    if (p-&gt;p_textnch.mount == mp)\n        cache_drop(&amp;p-&gt;p_textnch);\n\n    /* Signal if using mount */\n    if (info-&gt;sig &amp;&amp; process_uses_mount(p, mp)) {\n        p-&gt;p_flags |= P_MUSTKILL;\n        ksignal(p, info-&gt;sig);\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#vflush-flushing-mount-vnodes","title":"vflush() - Flushing Mount Vnodes","text":"<p>vflush() (vfs_mount.c:1070)</p> <p>Removes vnodes from a mount during unmount:</p> <pre><code>int vflush(struct mount *mp, int rootrefs, int flags)\n{\n    /* flags:\n     *   FORCECLOSE - forcibly close active vnodes\n     *   WRITECLOSE - close vnodes open for writing\n     *   SKIPSYSTEM - skip VSYSTEM vnodes\n     */\n}\n</code></pre> <p>Uses vmntvnodescan() to iterate vnodes:</p> <p>vflush_scan() (vfs_mount.c:1124): <pre><code>static int vflush_scan(struct mount *mp, struct vnode *vp,\n                       void *data)\n{\n    struct vflush_info *info = data;\n    int flags = info-&gt;flags;\n\n    /* Mark for finalization */\n    atomic_set_int(&amp;vp-&gt;v_refcnt, VREF_FINALIZE);\n\n    /* Skip VSYSTEM vnodes if requested */\n    if ((flags &amp; SKIPSYSTEM) &amp;&amp; (vp-&gt;v_flag &amp; VSYSTEM))\n        return (0);\n\n    /* Don't force-close VCHR/VBLK */\n    if (vp-&gt;v_type == VCHR || vp-&gt;v_type == VBLK)\n        flags &amp;= ~(WRITECLOSE | FORCECLOSE);\n\n    /* WRITECLOSE: only flush writable regular files */\n    if ((flags &amp; WRITECLOSE) &amp;&amp; ...) {\n        return (0);\n    }\n\n    /* If only holder, reclaim vnode */\n    if (VREFCNT(vp) &lt;= 1) {\n        vgone_vxlocked(vp);\n        return (0);\n    }\n\n    /* FORCECLOSE: forcibly destroy vnode */\n    if (flags &amp; FORCECLOSE) {\n        vhold(vp);\n        vgone_vxlocked(vp);\n        if (vp-&gt;v_mount == NULL)\n            insmntque(vp, &amp;dummymount);  /* orphan vnode */\n        vdrop(vp);\n        return (0);\n    }\n\n    /* Vnode is busy */\n    ++info-&gt;busy;\n    return (0);\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#bio-operations-integration","title":"Bio Operations Integration","text":""},{"location":"sys/kern/vfs/mounting/#struct-bio_ops","title":"struct bio_ops","text":"<p>Mount points can register bio_ops for buffer I/O interception (used by HAMMER, soft updates):</p> <pre><code>struct bio_ops {\n    TAILQ_ENTRY(bio_ops) entry;\n    void (*io_start)(struct buf *);         /* I/O initiated */\n    void (*io_complete)(struct buf *);      /* I/O completed */\n    void (*io_deallocate)(struct buf *);    /* buffer freed */\n    int  (*io_fsync)(struct vnode *);       /* fsync vnode */\n    int  (*io_sync)(struct mount *);        /* sync filesystem */\n    void (*io_movedeps)(struct buf *, struct buf *);  /* move deps */\n    int  (*io_countdeps)(struct buf *, int); /* count deps */\n    int  (*io_checkread)(struct buf *);     /* check read */\n    int  (*io_checkwrite)(struct buf *);    /* check write */\n};\n</code></pre> <p>add_bio_ops() / rem_bio_ops() (vfs_mount.c:1199, 1205):</p> <pre><code>void add_bio_ops(struct bio_ops *ops)\n{\n    TAILQ_INSERT_TAIL(&amp;bio_ops_list, ops, entry);\n}\n\nvoid rem_bio_ops(struct bio_ops *ops)\n{\n    TAILQ_REMOVE(&amp;bio_ops_list, ops, entry);\n}\n</code></pre> <p>bio_ops_sync() (vfs_mount.c:1218):</p> <p>Called during sync operations:</p> <pre><code>void bio_ops_sync(struct mount *mp)\n{\n    struct bio_ops *ops;\n\n    if (mp) {\n        /* Sync specific mount */\n        if ((ops = mp-&gt;mnt_bioops) != NULL)\n            ops-&gt;io_sync(mp);\n    } else {\n        /* Sync all registered bio_ops */\n        TAILQ_FOREACH(ops, &amp;bio_ops_list, entry) {\n            ops-&gt;io_sync(NULL);\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#root-filesystem-mounting","title":"Root Filesystem Mounting","text":""},{"location":"sys/kern/vfs/mounting/#vfs_rootmountalloc","title":"vfs_rootmountalloc()","text":"<p>vfs_rootmountalloc() (vfs_mount.c:332)</p> <p>Allocates a mount structure for the root filesystem:</p> <pre><code>int vfs_rootmountalloc(char *fstypename, char *devname,\n                       struct mount **mpp)\n{\n    struct vfsconf *vfsp;\n    struct mount *mp;\n\n    /* Find filesystem type */\n    vfsp = vfsconf_find_by_name(fstypename);\n    if (vfsp == NULL)\n        return (ENODEV);\n\n    /* Allocate and initialize mount */\n    mp = kmalloc(sizeof(struct mount), M_MOUNT,\n                 M_WAITOK | M_ZERO);\n    mount_init(mp, vfsp-&gt;vfc_vfsops);\n    lockinit(&amp;mp-&gt;mnt_lock, \"vfslock\", VLKTIMEOUT, 0);\n    lockinit(&amp;mp-&gt;mnt_renlock, \"renamlk\", VLKTIMEOUT, 0);\n\n    /* Mark as busy */\n    vfs_busy(mp, 0);\n\n    /* Set initial state */\n    mp-&gt;mnt_vfc = vfsp;\n    mp-&gt;mnt_pbuf_count = nswbuf_kva / NSWBUF_SPLIT;\n    vfsp-&gt;vfc_refcount++;\n    mp-&gt;mnt_stat.f_type = vfsp-&gt;vfc_typenum;\n    mp-&gt;mnt_flag |= MNT_RDONLY;\n    mp-&gt;mnt_flag |= vfsp-&gt;vfc_flags &amp; MNT_VISFLAGMASK;\n\n    /* Set names */\n    strncpy(mp-&gt;mnt_stat.f_fstypename, vfsp-&gt;vfc_name,\n            MFSNAMELEN);\n    copystr(devname, mp-&gt;mnt_stat.f_mntfromname,\n            MNAMELEN - 1, 0);\n\n    /* Pre-set MPSAFE flags */\n    if (vfsp-&gt;vfc_flags &amp; VFCF_MPSAFE)\n        mp-&gt;mnt_kern_flag |= MNTK_ALL_MPSAFE;\n\n    *mpp = mp;\n    return (0);\n}\n</code></pre> <p>Called early in kernel initialization before the full VFS is available.</p>"},{"location":"sys/kern/vfs/mounting/#summary","title":"Summary","text":""},{"location":"sys/kern/vfs/mounting/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Mount lifecycle: init \u2192 busy \u2192 insert \u2192 operate \u2192 unmount \u2192 free</li> <li>Two reference counters: mnt_refs (nchandle) and mnt_hold (kfree prevention)</li> <li>Two nchandles: mnt_ncmounton (mounting on) and mnt_ncmountpt (root of fs)</li> <li>Busy protocol: vfs_busy/unbusy prevents unmount during operations</li> <li>Mount lists: mountlist (ordered) and mounttree (RB-tree by FSID)</li> <li>Safe iteration: mountlist_scan() and vmntvnodescan() handle concurrent modifications</li> <li>Forced unmount: SIGINT \u2192 SIGKILL for processes, FORCECLOSE for vnodes</li> <li>FSID generation: Based on mount path hash, auto-adjusted on collision</li> <li>Syncer integration: Each mount has a syncer vnode for dirty data tracking</li> <li>Bio ops: Extensible buffer I/O hooks for soft updates, journaling</li> </ol>"},{"location":"sys/kern/vfs/mounting/#important-invariants","title":"Important Invariants","text":"<ul> <li>Mount on mountlist \u21d4 visible to lookups</li> <li>MNTK_UNMOUNT set \u21d4 vfs_busy() will fail</li> <li>mnt_refs == 1 at unmount \u21d4 only mnt_ncmountpt reference remains</li> <li>mnt_lock exclusive \u21d4 unmounting or updating</li> <li>mnt_lock shared \u21d4 normal operations (via vfs_busy)</li> </ul>"},{"location":"sys/kern/vfs/mounting/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Forgetting vfs_unbusy(): Prevents unmounting</li> <li>Not checking MNTK_UNMOUNT: Can race with unmount</li> <li>Holding mnt_token too long: Blocks all mount operations</li> <li>Not using mount_hold/drop: Can cause use-after-free</li> <li>Modifying mount without mnt_token: Race conditions</li> </ol>"},{"location":"sys/kern/vfs/mounting/#related-documentation","title":"Related Documentation","text":"<ul> <li>VFS Name Lookup and Caching - Namecache integration</li> <li>VFS Initialization - System initialization</li> <li>VFS Vnodes - Vnode lifecycle</li> </ul>"},{"location":"sys/kern/vfs/mounting/#source-code-locations","title":"Source Code Locations","text":"<ul> <li><code>sys/kern/vfs_mount.c</code> - Mount point management</li> <li><code>sys/kern/vfs_syscalls.c:118</code> - sys_mount()</li> <li><code>sys/kern/vfs_syscalls.c:610</code> - sys_unmount()</li> <li><code>sys/kern/vfs_syscalls.c:793</code> - dounmount()</li> <li><code>sys/sys/mount.h:216</code> - struct mount definition</li> <li><code>sys/sys/mount.h:274</code> - Mount flags (MNT_*)</li> <li><code>sys/sys/mount.h:348</code> - Kernel flags (MNTK_*)</li> </ul>"},{"location":"sys/kern/vfs/namecache/","title":"VFS Name Lookup and Caching","text":""},{"location":"sys/kern/vfs/namecache/#overview","title":"Overview","text":"<p>The DragonFly BSD namecache subsystem provides a high-performance layer between pathname lookup operations and the underlying filesystem. It consists of three main components:</p> <ol> <li>Namecache (<code>vfs_cache.c</code>) - The core caching infrastructure that maintains mappings between directory entries and vnodes</li> <li>New Lookup API (<code>vfs_nlookup.c</code>) - Modern path resolution based on namecache records instead of vnode locking</li> <li>Legacy Lookup (<code>vfs_lookup.c</code>) - Traditional BSD <code>namei()</code> compatibility for old code</li> </ol> <p>The namecache is fundamental to DragonFly's VFS design and all filesystem operations must interact with it, even filesystems that don't want to cache.</p>"},{"location":"sys/kern/vfs/namecache/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/kern/vfs/namecache/#struct-namecache-sysnamecacheh125","title":"<code>struct namecache</code> (<code>sys/namecache.h:125</code>)","text":"<p>The core namecache entry representing a single pathname component:</p> <pre><code>struct namecache {\n    TAILQ_ENTRY(namecache) nc_hash;      /* hash chain (nc_parent,name) */\n    TAILQ_ENTRY(namecache) nc_entry;     /* scan via nc_parent-&gt;nc_list */\n    TAILQ_ENTRY(namecache) nc_vnode;     /* scan via vnode-&gt;v_namecache */\n    struct namecache_list  nc_list;      /* list of children */\n    struct nchash_head    *nc_head;\n    struct namecache      *nc_parent;    /* namecache entry for parent */\n    struct vnode          *nc_vp;        /* vnode representing name or NULL */\n    u_short               nc_flag;\n    u_char                nc_nlen;       /* name length, 255 max */\n    u_char                nc_unused;\n    char                  *nc_name;      /* separately allocated segment name */\n    int                   nc_error;\n    int                   nc_timeout;    /* compared against ticks, or 0 */\n    int                   nc_negcpu;     /* which ncneg list are we on? */\n    struct {\n        u_int             nc_namecache_gen; /* mount generation (autoclear) */\n        u_int             nc_generation;    /* see notes below */\n        int               nc_refs;          /* ref count prevents deletion */\n    } __cachealign;\n    struct {\n        struct lock       nc_lock;\n    } __cachealign;\n};\n</code></pre> <p>Key fields:</p> <ul> <li><code>nc_parent</code> - Points to parent directory's namecache entry, forming a tree from leaf to root</li> <li><code>nc_vp</code> - The vnode this entry represents; NULL for negative cache entries (non-existent files)</li> <li><code>nc_name</code> - The filename component (allocated separately)</li> <li><code>nc_list</code> - Children of this directory entry</li> <li><code>nc_refs</code> - Reference count (naturally 1, +1 if resolved, +1 for each child)</li> <li><code>nc_generation</code> - Incremented by 2 when entry changes, allowing lock-free detection of modifications</li> <li><code>nc_flag</code> - See flags below</li> </ul> <p>Important flags (<code>NCF_*</code>):</p> <ul> <li><code>NCF_UNRESOLVED</code> (0x0004) - Entry not yet resolved or invalidated</li> <li><code>NCF_WHITEOUT</code> (0x0002) - Negative entry is a whiteout (for union mounts)</li> <li><code>NCF_DESTROYED</code> (0x0400) - Name association considered destroyed</li> <li><code>NCF_ISMOUNTPT</code> (0x0008) - Someone may have mounted here</li> <li><code>NCF_ISSYMLINK</code> (0x0100) - Entry is a symlink</li> <li><code>NCF_ISDIR</code> (0x0200) - Entry is a directory</li> </ul>"},{"location":"sys/kern/vfs/namecache/#struct-nchandle-sysnamecacheh154","title":"<code>struct nchandle</code> (<code>sys/namecache.h:154</code>)","text":"<p>A handle to a namecache entry with associated mount point:</p> <pre><code>struct nchandle {\n    struct namecache *ncp;    /* ncp in underlying filesystem */\n    struct mount     *mount;  /* mount pt (possible overlay) */\n};\n</code></pre> <p>The mount reference allows topologies to be replicated across mount overlays (nullfs, unionfs, etc.). This is DragonFly's key innovation for handling stacked filesystems.</p>"},{"location":"sys/kern/vfs/namecache/#struct-nlookupdata-sysnlookuph68","title":"<code>struct nlookupdata</code> (<code>sys/nlookup.h:68</code>)","text":"<p>Encapsulates all state for a path lookup operation:</p> <pre><code>struct nlookupdata {\n    struct nchandle  nl_nch;       /* result */\n    struct nchandle *nl_basench;   /* start-point directory */\n    struct nchandle  nl_rootnch;   /* root directory */\n    struct nchandle  nl_jailnch;   /* jail directory */\n\n    char            *nl_path;      /* path buffer */\n    struct thread   *nl_td;        /* thread requesting the nlookup */\n    struct ucred    *nl_cred;      /* credentials for nlookup */\n    struct vnode    *nl_dvp;       /* NLC_REFDVP */\n\n    int              nl_flags;     /* operations flags */\n    int              nl_loopcnt;   /* symlinks encountered */\n    int              nl_dir_error; /* error assoc w/intermediate dir */\n    int              nl_elmno;     /* iteration# to help caches */\n\n    struct vnode    *nl_open_vp;\n    int              nl_vp_fmode;\n};\n</code></pre> <p>Key flags (<code>NLC_*</code>):</p> <ul> <li><code>NLC_FOLLOW</code> (0x00000001) - Follow leaf symlink</li> <li><code>NLC_CREATE</code> (0x00000080) - Do create checks</li> <li><code>NLC_DELETE</code> (0x00000100) - Do delete checks</li> <li><code>NLC_RENAME_SRC</code> (0x00002000) - Do rename checks (source)</li> <li><code>NLC_RENAME_DST</code> (0x00000200) - Do rename checks (target)</li> <li><code>NLC_OPEN</code> (0x00000400) - Do open checks</li> <li><code>NLC_SHAREDLOCK</code> (0x00004000) - Allow shared ncp &amp; vp lock</li> <li><code>NLC_REFDVP</code> (0x00040000) - Set ref'd/unlocked nl_dvp</li> <li><code>NLC_READ</code> (0x00400000) - Require read access</li> <li><code>NLC_WRITE</code> (0x00800000) - Require write access</li> <li><code>NLC_EXEC</code> (0x01000000) - Require execute access</li> </ul>"},{"location":"sys/kern/vfs/namecache/#struct-nlcomponent-sysnlookuph55","title":"<code>struct nlcomponent</code> (<code>sys/nlookup.h:55</code>)","text":"<p>Represents a single path component during lookup:</p> <pre><code>struct nlcomponent {\n    char *nlc_nameptr;\n    int   nlc_namelen;\n};\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#architecture","title":"Architecture","text":""},{"location":"sys/kern/vfs/namecache/#namecache-topology","title":"Namecache Topology","text":"<p>The DragonFly namecache maintains a complete path from any active vnode to the root (except for NFS server and removed files). This is a key difference from traditional BSD systems:</p> <pre><code>Root (\"/\")\n  \u2514\u2500 bin/\n      \u2514\u2500 ls (vnode)\n  \u2514\u2500 usr/\n      \u2514\u2500 local/\n          \u2514\u2500 bin/\n              \u2514\u2500 bash (vnode)\n</code></pre> <p>Each <code>namecache</code> entry points to its <code>nc_parent</code>, and parents maintain a list of children in <code>nc_list</code>. This bidirectional tree enables:</p> <ol> <li>Efficient forward lookups (parent \u2192 child)</li> <li>Efficient reverse path reconstruction (child \u2192 root for <code>getcwd()</code>)</li> <li>Efficient subtree invalidation (e.g., when unmounting)</li> </ol>"},{"location":"sys/kern/vfs/namecache/#positive-vs-negative-caching","title":"Positive vs Negative Caching","text":"<p>Positive entries: - <code>nc_vp</code> != NULL - Represent files/directories that exist - May be unresolved (NCF_UNRESOLVED) if not yet looked up</p> <p>Negative entries: - <code>nc_vp</code> == NULL - Represent failed lookups (file doesn't exist) - Stored in per-CPU negative lists for quick reclamation - May be whiteouts (NCF_WHITEOUT) for union filesystems</p> <p>Negative caching is crucial for performance, avoiding expensive filesystem lookups for common cases like: - PATH searches in shell (<code>/bin/foo</code>, <code>/usr/bin/foo</code>, etc.) - Probing for config files (<code>.bashrc</code>, <code>.config/app</code>, etc.)</p>"},{"location":"sys/kern/vfs/namecache/#hash-table-organization","title":"Hash Table Organization","text":"<p>The namecache uses a hash table indexed by <code>(nc_parent, name)</code>:</p> <pre><code>#define NCHHASH(hash)  (&amp;nchashtbl[(hash) &amp; nchash])\n</code></pre> <p>Each hash bucket has its own spinlock. The implementation uses an update counter (<code>nc_generation</code>) to allow lock-free lookups in common cases - the code can detect if an entry changed during access and retry with locks if needed.</p>"},{"location":"sys/kern/vfs/namecache/#reference-counting","title":"Reference Counting","text":"<p>Namecache entries use <code>nc_refs</code> for lifecycle management:</p> <ul> <li>Base ref: 1 (entry exists)</li> <li>Resolved ref: +1 if entry is resolved (positive or negative)</li> <li>Child refs: +1 for each child in <code>nc_list</code></li> <li>Lookup refs: +1 while held by threads or mountcache</li> </ul> <p>On the 1\u21920 transition, the entry must be destroyed immediately. The entry cannot be on any list at this point.</p> <p>Reference management functions:</p> <ul> <li><code>cache_hold()</code> / <code>cache_get()</code> - Acquire reference</li> <li><code>cache_put()</code> - Release reference (drop lock + drop ref)</li> <li><code>cache_drop()</code> - Release reference (no lock, just drop ref)</li> <li><code>_cache_drop()</code> - Internal version that handles 1\u21920 transition</li> </ul>"},{"location":"sys/kern/vfs/namecache/#locking-strategy","title":"Locking Strategy","text":"<p>DragonFly uses child-to-parent lock ordering:</p> <ol> <li>Lock child first, then parent</li> <li>Allows forward scans (parent \u2192 child) to hold parent unlocked</li> <li>Deletions propagate bottom-up naturally</li> </ol> <p>Lock types:</p> <ul> <li>Exclusive locks - Required for modifications, last element of path (unless NLC_SHAREDLOCK)</li> <li>Shared locks - Allowed for intermediate path components, read-only operations</li> </ul> <p>The <code>nc_generation</code> field enables optimistic lock-free access:</p> <pre><code>static __inline void\n_cache_ncp_gen_enter(struct namecache *ncp)\n{\n    ncp-&gt;nc_generation += 2;  /* Odd = in-progress */\n    cpu_sfence();\n}\n\nstatic __inline void\n_cache_ncp_gen_exit(struct namecache *ncp)\n{\n    cpu_sfence();\n    ncp-&gt;nc_generation += 2;  /* Even = stable */\n    cpu_sfence();\n}\n</code></pre> <p>If <code>(nc_generation &amp; 1)</code> is set, modification is in progress. Readers check generation before and after access, retrying if it changed.</p>"},{"location":"sys/kern/vfs/namecache/#core-namecache-operations","title":"Core Namecache Operations","text":""},{"location":"sys/kern/vfs/namecache/#path-lookup-cache_nlookup-vfs_cachec3228","title":"Path Lookup: <code>cache_nlookup()</code> (<code>vfs_cache.c:3228</code>)","text":"<p>Looks up a single path component in the cache:</p> <pre><code>struct nchandle cache_nlookup(struct nchandle *par_nch, \n                              struct nlcomponent *nlc);\n</code></pre> <p>Algorithm:</p> <ol> <li>Hash lookup - Compute <code>hash(parent, name)</code> and search bucket</li> <li>Lock-free scan - Check each entry's generation before/after reading</li> <li>Match found - Return referenced nchandle</li> <li>Miss - Call <code>cache_nlookup_create()</code> to create unresolved entry</li> <li>Resolve - Call <code>cache_resolve()</code> to resolve via VOP_NRESOLVE()</li> </ol> <p>Variants:</p> <ul> <li><code>cache_nlookup_nonblock()</code> - Returns immediately if lock unavailable</li> <li><code>cache_nlookup_nonlocked()</code> - Optimistic lock-free lookup</li> <li><code>cache_nlookup_maybe_shared()</code> - Allows shared locks if <code>!excl</code></li> </ul>"},{"location":"sys/kern/vfs/namecache/#resolution-cache_resolve-vfs_cachec4273","title":"Resolution: <code>cache_resolve()</code> (<code>vfs_cache.c:4273</code>)","text":"<p>Resolves an unresolved namecache entry by calling into the filesystem:</p> <pre><code>int cache_resolve(struct nchandle *nch, u_int *genp, struct ucred *cred);\n</code></pre> <p>Steps:</p> <ol> <li>Check if already resolved (fast path)</li> <li>Check generation counter for races</li> <li>Call <code>VOP_NRESOLVE(dvp, ncp, cred)</code> on parent directory</li> <li>Filesystem fills in <code>ncp-&gt;nc_vp</code> or leaves NULL for negative entry</li> <li>Clear <code>NCF_UNRESOLVED</code> flag</li> </ol> <p>This is the critical bridge between namecache and filesystem-specific code.</p>"},{"location":"sys/kern/vfs/namecache/#invalidation-cache_inval-vfs_cachec1692","title":"Invalidation: <code>cache_inval()</code> (<code>vfs_cache.c:1692</code>)","text":"<p>Invalidates a namecache entry:</p> <pre><code>int cache_inval(struct nchandle *nch, int flags);\n</code></pre> <p>Flags:</p> <ul> <li><code>CINV_DESTROY</code> (0x0001) - Mark destroyed so lookups ignore it</li> <li><code>CINV_CHILDREN</code> (0x0004) - Recursively invalidate all children</li> </ul> <p>Use cases:</p> <ul> <li>File/directory deletion</li> <li>Filesystem unmount</li> <li>Stale NFS entries</li> </ul>"},{"location":"sys/kern/vfs/namecache/#reference-management","title":"Reference Management","text":"<p><code>cache_get()</code> (<code>vfs_cache.c:1293</code>)</p> <p>Acquires a reference and returns a handle:</p> <pre><code>void cache_get(struct nchandle *nch, struct nchandle *target);\n</code></pre> <p><code>cache_put()</code> (<code>vfs_cache.c:1322</code>)</p> <p>Drops lock and reference:</p> <pre><code>void cache_put(struct nchandle *nch);\n</code></pre> <p><code>cache_drop()</code> (<code>vfs_cache.c:1090</code>)</p> <p>Drops reference without affecting lock:</p> <pre><code>void cache_drop(struct nchandle *nch);\n</code></pre> <p><code>cache_lock()</code> (<code>vfs_cache.c:1111</code>)</p> <p>Acquires exclusive lock on namecache entry:</p> <pre><code>void cache_lock(struct nchandle *nch);\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#mount-point-caching","title":"Mount Point Caching","text":"<p>DragonFly caches mount point references to reduce atomic operations on <code>mnt_refs</code>:</p> <p>Per-CPU mount cache (<code>pcpu_mntcache</code>):</p> <pre><code>struct mntcache_elm {\n    struct namecache *ncp;\n    struct mount     *mp;\n    int              ticks;\n    int              unused01;\n};\n</code></pre> <ul> <li>32 entries per CPU, 8-way set associative</li> <li>LRU replacement based on <code>ticks</code></li> <li>Avoids cache-line ping-ponging in multi-socket systems</li> </ul> <p>Functions:</p> <ul> <li><code>_cache_mntref()</code> - Cache a mount ref</li> <li><code>_cache_mntrel()</code> - Release a cached mount ref</li> <li><code>cache_findmount()</code> - Find mount point for a namecache entry</li> </ul>"},{"location":"sys/kern/vfs/namecache/#new-lookup-api-nlookup","title":"New Lookup API (nlookup)","text":"<p>The nlookup API is DragonFly's modern path resolution interface, replacing the traditional <code>namei()</code>.</p>"},{"location":"sys/kern/vfs/namecache/#key-advantages-over-namei","title":"Key Advantages Over namei()","text":"<ol> <li>Namecache-centric - Operations work on namecache records, not vnodes</li> <li>Better parallelism - Lock granularity is per-entry, not per-vnode</li> <li>Cleaner semantics - Locked/unlocked state is explicit</li> <li>Overlay-aware - Native support for nullfs, unionfs via <code>nchandle</code></li> <li>Negative caching - First-class support for non-existent entries</li> </ol>"},{"location":"sys/kern/vfs/namecache/#usage-pattern","title":"Usage Pattern","text":"<pre><code>struct nlookupdata nd;\nint error;\n\n/* Initialize lookup */\nerror = nlookup_init(&amp;nd, path, UIO_USERSPACE, NLC_FOLLOW);\nif (error == 0) {\n    /* Perform lookup */\n    error = nlookup(&amp;nd);\n    if (error == 0) {\n        /* Use nd.nl_nch result */\n        struct nchandle *nch = &amp;nd.nl_nch;\n        struct vnode *vp;\n\n        error = cache_vget(nch, cred, LK_EXCLUSIVE, &amp;vp);\n        if (error == 0) {\n            /* ... use vp ... */\n            vput(vp);\n        }\n    }\n    /* Cleanup */\n    nlookup_done(&amp;nd);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#core-functions","title":"Core Functions","text":""},{"location":"sys/kern/vfs/namecache/#nlookup_init-vfs_nlookupc116","title":"<code>nlookup_init()</code> (<code>vfs_nlookup.c:116</code>)","text":"<p>Initialize a lookup operation:</p> <pre><code>int nlookup_init(struct nlookupdata *nd, const char *path, \n                 enum uio_seg seg, int flags);\n</code></pre> <p>Setup:</p> <ol> <li>Allocate path buffer from <code>namei_oc</code> objcache</li> <li>Copy path from userspace/kernelspace</li> <li>Initialize <code>nl_nch</code> to current working directory (or root)</li> <li>Copy root directory to <code>nl_rootnch</code></li> <li>Copy jail directory to <code>nl_jailnch</code> (if jailed)</li> <li>Set credentials from current thread</li> </ol>"},{"location":"sys/kern/vfs/namecache/#nlookup-vfs_nlookupc530","title":"<code>nlookup()</code> (<code>vfs_nlookup.c:530</code>)","text":"<p>Perform the actual path lookup:</p> <pre><code>int nlookup(struct nlookupdata *nd);\n</code></pre> <p>Main loop algorithm:</p> <pre><code>for each path component:\n    1. Skip leading '/' characters\n    2. Check for root directory replacement\n    3. Check execute permission on current directory (naccess)\n    4. Extract next component (up to 255 chars)\n    5. Handle special cases:\n       - \".\" = current directory (no-op)\n       - \"..\" = parent directory (traverse mounts)\n       - regular name = cache_nlookup()\n    6. If unresolved, call cache_resolve()\n    7. Handle symlinks (if NLC_FOLLOW and nc_flag &amp; NCF_ISSYMLINK)\n    8. Handle mount point crossings\n    9. Perform access checks based on nl_flags\n    10. Update nl_nch to point to new entry\n</code></pre> <p>Symlink handling:</p> <ul> <li>Detect via <code>NCF_ISSYMLINK</code> flag</li> <li>Read symlink contents via <code>VOP_READLINK()</code></li> <li>Restart lookup from symlink target</li> <li>Limit to <code>MAXSYMLINKS</code> (typically 32) to prevent loops</li> </ul> <p>Mount point traversal:</p> <pre><code>/* Cross into mounted filesystem */\nif (nch.ncp-&gt;nc_flag &amp; NCF_ISMOUNTPT) {\n    mp = cache_findmount(&amp;nch);\n    if (mp) {\n        /* Replace with mount point's root */\n        cache_dropmount(mp);\n    }\n}\n</code></pre> <p>Generation tracking:</p> <p>The code carefully tracks <code>nc_generation</code> throughout:</p> <pre><code>nl_gen = nd-&gt;nl_nch.ncp-&gt;nc_generation &amp; ~3;\n...\nif (gen_changed || (nl_gen &amp; 1)) {\n    /* Retry lookup */\n    goto nlookup_start;\n}\n</code></pre> <p>This allows detection of concurrent modifications and automatic retry.</p>"},{"location":"sys/kern/vfs/namecache/#nlookup_done-vfs_nlookupc289","title":"<code>nlookup_done()</code> (<code>vfs_nlookup.c:289</code>)","text":"<p>Cleanup after lookup:</p> <pre><code>void nlookup_done(struct nlookupdata *nd);\n</code></pre> <ul> <li>Release all nchandles</li> <li>Free path buffer</li> <li>Drop credential reference</li> <li>Close <code>nl_open_vp</code> if set</li> </ul>"},{"location":"sys/kern/vfs/namecache/#access-checking-naccess-vfs_nlookupc1531","title":"Access Checking: <code>naccess()</code> (<code>vfs_nlookup.c:1531</code>)","text":"<p>Check permissions during path traversal:</p> <pre><code>static int naccess(struct nlookupdata *nd, struct nchandle *nch,\n                   u_int *genp, int vmode, struct ucred *cred, \n                   int *stickyp, int nchislocked);\n</code></pre> <p>Optimizations:</p> <ol> <li>Cached permissions - Check <code>NCF_WXOK</code> flag for world-searchable dirs</li> <li>Lock-free - Avoids locking if cached perms are sufficient</li> <li>Generation tracking - Detect races with <code>nc_generation</code></li> </ol>"},{"location":"sys/kern/vfs/namecache/#legacy-lookup-api-namei","title":"Legacy Lookup API (namei)","text":""},{"location":"sys/kern/vfs/namecache/#relookup-vfs_lookupc75","title":"<code>relookup()</code> (<code>vfs_lookup.c:75</code>)","text":"<p>Old API function used only by legacy <code>*_rename()</code> code:</p> <pre><code>int relookup(struct vnode *dvp, struct vnode **vpp, \n             struct componentname *cnp);\n</code></pre> <p>This is a compatibility shim. New code should use nlookup exclusively.</p> <p>Key differences from nlookup:</p> <ul> <li>Works with vnodes directly (requires vnode locks)</li> <li>Uses <code>componentname</code> instead of <code>nlcomponent</code></li> <li>Less efficient due to vnode-based locking</li> <li>Does not support overlay mounts as cleanly</li> </ul>"},{"location":"sys/kern/vfs/namecache/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"sys/kern/vfs/namecache/#lock-free-lookups","title":"Lock-Free Lookups","text":"<p>The core innovation is optimistic lock-free access:</p> <pre><code>/* Fast path - no locks */\ndo {\n    gen_before = ncp-&gt;nc_generation;\n    cpu_lfence();\n    /* ... read fields ... */\n    cpu_lfence();\n    gen_after = ncp-&gt;nc_generation;\n} while (gen_before != gen_after || (gen_after &amp; 1));\n</code></pre> <p>If generation matches and is even (not in-progress), the read is consistent.</p>"},{"location":"sys/kern/vfs/namecache/#per-cpu-negative-lists","title":"Per-CPU Negative Lists","text":"<p>Negative entries are stored in per-CPU lists (<code>pcpu_ncache[cpu].neg_list</code>):</p> <pre><code>struct pcpu_ncache {\n    struct spinlock       umount_spin;\n    struct spinlock       neg_spin;\n    struct namecache_list neg_list;\n    long                  neg_count;\n    long                  vfscache_negs;\n    long                  vfscache_count;\n    /* ... statistics ... */\n} __cachealign;\n</code></pre> <p>Benefits:</p> <ul> <li>No inter-CPU contention on negative entry allocation/free</li> <li>Cache-line alignment prevents false sharing</li> <li>Quick reclamation when memory pressure hits</li> </ul>"},{"location":"sys/kern/vfs/namecache/#mount-reference-caching","title":"Mount Reference Caching","text":"<p>The per-CPU mount cache (<code>pcpu_mntcache</code>) is critical for performance:</p> <ul> <li>Problem: Atomic ops on <code>mp-&gt;mnt_refs</code> cause cache-line bouncing</li> <li>Solution: Cache mount refs per-CPU, only updating global ref periodically</li> <li>Result: 10-100x reduction in cache misses on multi-socket systems</li> </ul>"},{"location":"sys/kern/vfs/namecache/#namecache-size-limits","title":"Namecache Size Limits","text":"<p>DragonFly dynamically balances cache sizes:</p> <pre><code>__read_mostly static int ncnegfactor = 16;   /* ratio of negative entries */\n__read_mostly static int ncposfactor = 16;   /* ratio of unres+leaf entries */\n</code></pre> <p>Functions like <code>_cache_cleanneg()</code> and <code>_cache_cleanpos()</code> trim caches when:</p> <ol> <li>Memory pressure increases</li> <li>Ratios exceed configured factors</li> <li>Mount/unmount operations occur</li> </ol>"},{"location":"sys/kern/vfs/namecache/#filesystem-integration","title":"Filesystem Integration","text":""},{"location":"sys/kern/vfs/namecache/#required-vop-operations","title":"Required VOP Operations","text":"<p>Filesystems must implement these to integrate with namecache:</p>"},{"location":"sys/kern/vfs/namecache/#vop_nresolve","title":"<code>VOP_NRESOLVE()</code>","text":"<p>Resolve an unresolved namecache entry:</p> <pre><code>int VOP_NRESOLVE(struct vnode *dvp, struct namecache *ncp, \n                 struct ucred *cred);\n</code></pre> <p>Responsibilities:</p> <ol> <li>Search directory <code>dvp</code> for name <code>ncp-&gt;nc_name</code></li> <li>If found: Call <code>cache_setvp(nch, vp)</code> to set <code>ncp-&gt;nc_vp</code></li> <li>If not found: Leave <code>ncp-&gt;nc_vp</code> as NULL (negative entry)</li> <li>Return 0 on success, error otherwise</li> </ol>"},{"location":"sys/kern/vfs/namecache/#vop_ncreate","title":"<code>VOP_NCREATE()</code>","text":"<p>Create file via namecache:</p> <pre><code>int VOP_NCREATE(struct vnode *dvp, struct vnode **vpp,\n                struct nchandle *nch, struct vattr *vap,\n                struct ucred *cred);\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#vop_nremove","title":"<code>VOP_NREMOVE()</code>","text":"<p>Remove file via namecache:</p> <pre><code>int VOP_NREMOVE(struct vnode *dvp, struct nchandle *nch,\n                struct ucred *cred);\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#vop_nrename","title":"<code>VOP_NRENAME()</code>","text":"<p>Rename via namecache:</p> <pre><code>int VOP_NRENAME(struct nchandle *fnch, struct nchandle *tnch,\n                struct vnode *fdvp, struct vnode *tdvp,\n                struct ucred *cred);\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#invalidation-requirements","title":"Invalidation Requirements","text":"<p>Filesystems must invalidate namecache entries when:</p> <ol> <li>File/directory deleted - <code>cache_inval(nch, CINV_DESTROY)</code></li> <li>Directory modified - <code>cache_inval_vp(dvp, CINV_CHILDREN)</code></li> <li>Vnode recycled - <code>cache_inval_vp(vp, CINV_DESTROY)</code></li> <li>Mount/unmount - <code>cache_purgevfs(mp)</code></li> </ol> <p>Example (tmpfs):</p> <pre><code>/* After unlinking a file */\nerror = tmpfs_remove_dirent(dvp, node, nch);\nif (error == 0) {\n    cache_inval(nch, CINV_DESTROY);\n    cache_inval_vp(vp, CINV_DESTROY);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#special-cases","title":"Special Cases","text":""},{"location":"sys/kern/vfs/namecache/#mount-point-handling","title":"Mount Point Handling","text":"<p>When a lookup encounters a mount point:</p> <ol> <li>Detect via <code>NCF_ISMOUNTPT</code> flag</li> <li>Call <code>cache_findmount(nch)</code> to get mounted filesystem</li> <li>Replace <code>nch</code> with mount point's root <code>mp-&gt;mnt_ncmountpt</code></li> <li>Continue lookup in new filesystem</li> </ol> <p>Reverse traversal (`..'):</p> <pre><code>while (nctmp.ncp == nctmp.mount-&gt;mnt_ncmountpt.ncp) {\n    /* Traverse to mounted-on directory */\n    nctmp = nctmp.mount-&gt;mnt_ncmounton;\n}\nnctmp.ncp = nctmp.ncp-&gt;nc_parent;\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#jail-and-chroot","title":"Jail and Chroot","text":"<p>The lookup code respects process jails and chroot:</p> <ul> <li><code>nl_rootnch</code> - Process's root (may be chrooted)</li> <li><code>nl_jailnch</code> - Jail's root (if jailed)</li> </ul> <p>Root clamping:</p> <pre><code>if (nd-&gt;nl_nch.mount == nd-&gt;nl_rootnch.mount &amp;&amp;\n    nd-&gt;nl_nch.ncp == nd-&gt;nl_rootnch.ncp) {\n    /* At root, \"..\" returns root */\n    cache_copy(&amp;nd-&gt;nl_rootnch, &amp;nch);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#whiteouts-union-mounts","title":"Whiteouts (Union Mounts)","text":"<p>Whiteouts represent explicitly deleted entries in union filesystems:</p> <ul> <li>Negative entry with <code>NCF_WHITEOUT</code> flag set</li> <li>Prevents lower layers from showing through</li> <li>Created by <code>VOP_NWHITEOUT()</code></li> </ul>"},{"location":"sys/kern/vfs/namecache/#common-operations","title":"Common Operations","text":""},{"location":"sys/kern/vfs/namecache/#get-vnode-from-nchandle","title":"Get vnode from nchandle","text":"<pre><code>struct vnode *vp;\nerror = cache_vget(nch, cred, LK_EXCLUSIVE, &amp;vp);\nif (error == 0) {\n    /* ... use vp ... */\n    vput(vp);  /* Release lock + ref */\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#get-nchandle-from-vnode","title":"Get nchandle from vnode","text":"<pre><code>struct nchandle nch;\nerror = cache_fromdvp(vp, cred, 1, &amp;nch);\nif (error == 0) {\n    /* ... use nch ... */\n    cache_drop(&amp;nch);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#full-path-from-nchandle","title":"Full path from nchandle","text":"<pre><code>char *freebuf;\nchar *fullpath;\nerror = cache_fullpath(p, nch, NULL, &amp;fullpath, &amp;freebuf, 0);\nif (error == 0) {\n    kprintf(\"Path: %s\\n\", fullpath);\n    kfree(freebuf, M_TEMP);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#check-if-path-is-open","title":"Check if path is open","text":"<pre><code>if (cache_isopen(nch)) {\n    /* Entry has open file descriptors */\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#statistics-and-debugging","title":"Statistics and Debugging","text":""},{"location":"sys/kern/vfs/namecache/#sysctl-variables","title":"Sysctl Variables","text":"<pre><code>vfs.cache.numneg       - Number of negative entries\nvfs.cache.numcache     - Total namecache entries\nvfs.cache.numleafs     - Leaf entries (no children)\nvfs.cache.numunres     - Unresolved leaf entries\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#per-cpu-statistics-struct-nchstats","title":"Per-CPU Statistics (<code>struct nchstats</code>)","text":"<p>Exported via <code>vfs.cache.nchstats</code> sysctl:</p> <ul> <li><code>ncs_goodhits</code> - Successful cache hits</li> <li><code>ncs_neghits</code> - Negative cache hits</li> <li><code>ncs_badhits</code> - Hits that required locking</li> <li><code>ncs_miss</code> - Cache misses requiring VOP_NRESOLVE</li> <li><code>ncs_longhits</code> - Long name hits (&gt; 32 chars)</li> </ul>"},{"location":"sys/kern/vfs/namecache/#debug-variables","title":"Debug Variables","text":"<pre><code>debug.ncvp_debug       - Namecache debug level (0-3)\ndebug.ncnegflush       - Batch flush negative entries\ndebug.ncposflush       - Batch flush positive entries\ndebug.nclockwarn       - Warn on locked entries in ticks\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#example-creating-a-file","title":"Example: Creating a File","text":"<pre><code>struct nlookupdata nd;\nstruct vnode *vp;\nstruct vattr vat;\nint error;\n\n/* Lookup parent directory and target name */\nerror = nlookup_init(&amp;nd, \"/tmp/newfile\", UIO_SYSSPACE, NLC_FOLLOW);\nif (error == 0) {\n    nd.nl_flags |= NLC_CREATE;\n    error = nlookup(&amp;nd);\n    if (error == 0) {\n        /* Setup attributes */\n        VATTR_NULL(&amp;vat);\n        vat.va_type = VREG;\n        vat.va_mode = 0644;\n\n        /* Get parent directory vnode */\n        error = cache_vget(&amp;nd.nl_nch, cred, LK_EXCLUSIVE, &amp;vp);\n        if (error == 0) {\n            /* Create the file */\n            error = VOP_NCREATE(vp, &amp;vp, &amp;nd.nl_nch, &amp;vat, cred);\n            if (error == 0) {\n                /* ... file created, vp is new file ... */\n                vput(vp);\n            } else {\n                vput(vp);\n            }\n        }\n    }\n    nlookup_done(&amp;nd);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#summary","title":"Summary","text":"<p>The DragonFly namecache and nlookup system provides:</p> <ol> <li>High-performance caching with lock-free reads</li> <li>Negative caching to avoid redundant lookups</li> <li>Clean API separating pathname lookup from vnode operations</li> <li>Native overlay support via nchandle abstraction</li> <li>Fine-grained locking for better SMP scalability</li> <li>Complete path maintenance from leaf to root</li> </ol> <p>This design is a significant improvement over traditional BSD namecache, enabling better performance on modern multi-core systems while simplifying filesystem implementation.</p>"},{"location":"sys/kern/vfs/namecache/#related-documentation","title":"Related Documentation","text":"<ul> <li>VFS Core - VFS subsystem overview</li> <li>VFS Mounting - Mount point management (Phase 6c)</li> <li>Process File Descriptors - <code>fd_ncdir</code>, <code>fd_nrdir</code> usage</li> </ul>"},{"location":"sys/kern/vfs/namecache/#source-files","title":"Source Files","text":"<ul> <li><code>sys/kern/vfs_cache.c</code> (~5,000 lines) - Namecache implementation</li> <li><code>sys/kern/vfs_nlookup.c</code> (~2,300 lines) - New lookup API</li> <li><code>sys/kern/vfs_lookup.c</code> (~160 lines) - Legacy lookup API</li> <li><code>sys/sys/namecache.h</code> - Namecache structures and API</li> <li><code>sys/sys/nlookup.h</code> - nlookup structures and flags</li> </ul>"},{"location":"sys/kern/vfs/vfs-extensions/","title":"VFS Extensions and Helpers","text":""},{"location":"sys/kern/vfs/vfs-extensions/#overview","title":"Overview","text":"<p>This document covers several VFS subsystems that extend the core VFS functionality:</p> <ul> <li>VFS Helper Functions (<code>vfs_helper.c</code>) - UNIX permission checking and attribute modification</li> <li>Filesystem Synchronization (<code>vfs_sync.c</code>) - Per-mount syncer daemon and dirty vnode management</li> <li>Synthetic Filesystem (<code>vfs_synth.c</code>) - Early boot devfs access for root device lookup</li> <li>VFS Quota System (<code>vfs_quota.c</code>) - Per-user/group space accounting and limits</li> <li>Asynchronous I/O (<code>vfs_aio.c</code>) - POSIX AIO stubs (not implemented)</li> </ul>"},{"location":"sys/kern/vfs/vfs-extensions/#vfs-helper-functions","title":"VFS Helper Functions","text":"<p>The <code>vfs_helper.c</code> file provides filesystem-agnostic implementations of common UNIX operations. These helpers allow filesystems to share code for permission checking, ownership changes, and file attribute modification.</p> <p>Key files: - <code>sys/kern/vfs_helper.c</code> - Helper function implementations</p>"},{"location":"sys/kern/vfs/vfs-extensions/#permission-checking-vop_helper_access","title":"Permission Checking (vop_helper_access)","text":"<p>Standard UNIX permission semantics for <code>VOP_ACCESS</code>:</p> <pre><code>int vop_helper_access(struct vop_access_args *ap, uid_t ino_uid, gid_t ino_gid,\n                      mode_t ino_mode, u_int32_t ino_flags)\n</code></pre> <p>Permission check order: 1. Check read-only filesystem for write attempts 2. Check immutable flag (<code>IMMUTABLE</code>) for write attempts 3. Allow root (uid 0) unconditional access 4. Check owner permissions if <code>proc_uid == ino_uid</code> 5. Check group permissions if <code>proc_gid == ino_gid</code> or user is in supplementary group 6. Check \"other\" permissions</p> <p>AT_EACCESS support: <pre><code>if (ap-&gt;a_flags &amp; AT_EACCESS) {\n    proc_uid = cred-&gt;cr_uid;   /* effective uid */\n    proc_gid = cred-&gt;cr_gid;   /* effective gid */\n} else {\n    proc_uid = cred-&gt;cr_ruid;  /* real uid */\n    proc_gid = cred-&gt;cr_rgid;  /* real gid */\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-extensions/#file-attribute-modification","title":"File Attribute Modification","text":"<p><code>vop_helper_setattr_flags()</code> - Modify file flags (chflags):</p> <pre><code>int vop_helper_setattr_flags(u_int32_t *ino_flags, u_int32_t vaflags,\n                             uid_t uid, struct ucred *cred)\n</code></pre> <ul> <li>Non-owner requires <code>SYSCAP_NOVFS_SYSFLAGS</code> capability</li> <li>Root can set system flags (<code>SF_*</code>) unless securelevel &gt; 0</li> <li>Regular users can only modify user flags (<code>UF_SETTABLE</code>)</li> <li>Jail restrictions apply via <code>PRISON_CAP_VFS_CHFLAGS</code></li> </ul> <p><code>vop_helper_chmod()</code> - Change file mode:</p> <pre><code>int vop_helper_chmod(struct vnode *vp, mode_t new_mode, struct ucred *cred,\n                     uid_t cur_uid, gid_t cur_gid, mode_t *cur_modep)\n</code></pre> <ul> <li>Non-owner requires <code>SYSCAP_NOVFS_CHMOD</code> capability</li> <li>Non-root users cannot set sticky bit on non-directories</li> <li>Non-root users cannot set SGID if not in file's group</li> </ul> <p><code>vop_helper_chown()</code> - Change file ownership:</p> <pre><code>int vop_helper_chown(struct vnode *vp, uid_t new_uid, gid_t new_gid,\n                     struct ucred *cred,\n                     uid_t *cur_uidp, gid_t *cur_gidp, mode_t *cur_modep)\n</code></pre> <ul> <li>Non-owner requires <code>SYSCAP_NOVFS_CHOWN</code> capability</li> <li>Changing owner or group clears SUID/SGID bits (unless root)</li> <li>Validates group membership for non-privileged users</li> </ul> <p><code>vop_helper_create_uid()</code> - Determine new file ownership:</p> <pre><code>uid_t vop_helper_create_uid(struct mount *mp, mode_t dmode, uid_t duid,\n                            struct ucred *cred, mode_t *modep)\n</code></pre> <ul> <li>Supports <code>SUIDDIR</code> mount option (files inherit directory owner)</li> <li>Otherwise returns creator's uid</li> </ul>"},{"location":"sys/kern/vfs/vfs-extensions/#vm-read-shortcut","title":"VM Read Shortcut","text":"<p><code>vop_helper_read_shortcut()</code> - Bypass VFS for cached reads:</p> <p>When <code>LWBUF_IS_OPTIMAL</code> is defined, this function attempts to read directly from VM pages without going through the buffer cache:</p> <pre><code>int vop_helper_read_shortcut(struct vop_read_args *ap)\n{\n    // Check prerequisites\n    if (vp-&gt;v_object == NULL || uio-&gt;uio_segflg == UIO_NOCOPY)\n        return 0;  // Fall back to normal path\n\n    vm_object_hold_shared(obj);\n\n    while (uio-&gt;uio_resid &amp;&amp; error == 0) {\n        // Look up page in VM object\n        m = vm_page_lookup_sbusy_try(obj, OFF_TO_IDX(uio-&gt;uio_offset), ...);\n        if (m == NULL || (m-&gt;valid &amp; VM_PAGE_BITS_ALL) != VM_PAGE_BITS_ALL)\n            break;  // Fall back to normal path\n\n        // Copy directly from page\n        lwb = lwbuf_alloc(m, &amp;lwb_cache);\n        error = uiomove_nofault((char *)lwbuf_kva(lwb) + offset, n, uio);\n        lwbuf_free(lwb);\n        vm_page_sbusy_drop(m);\n    }\n\n    vm_object_drop(obj);\n    return error;\n}\n</code></pre> <p>Controlled via sysctl <code>vm.read_shortcut_enable</code>.</p>"},{"location":"sys/kern/vfs/vfs-extensions/#filesystem-synchronization","title":"Filesystem Synchronization","text":"<p>The <code>vfs_sync.c</code> file implements per-filesystem syncer daemons that periodically flush dirty data to disk.</p> <p>Key files: - <code>sys/kern/vfs_sync.c</code> - Syncer daemon and worklist management</p>"},{"location":"sys/kern/vfs/vfs-extensions/#architecture","title":"Architecture","text":"<p>Each mounted filesystem with <code>MNTK_THR_SYNC</code> gets its own syncer thread:</p> <pre><code>flowchart TB\n    SYNCER[\"syncer_thread(per mount)\"]\n\n    W0[\"workitem[0](sync now)\"]\n    W1[\"workitem[1](sync +1s)\"]\n    WN[\"workitem[N](sync +Ns)\"]\n\n    SYNCER --&gt; W0\n    SYNCER --&gt; W1\n    SYNCER --&gt; WN\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#syncer-context-structure","title":"Syncer Context Structure","text":"<pre><code>struct syncer_ctx {\n    struct mount        *sc_mp;\n    struct lwkt_token   sc_token;\n    struct thread       *sc_thread;\n    int                 sc_flags;\n    struct synclist     *syncer_workitem_pending;  /* Hash table */\n    long                syncer_mask;               /* Hash mask */\n    int                 syncer_delayno;            /* Current slot */\n    int                 syncer_forced;             /* Force sync mode */\n    int                 syncer_rushjob;            /* Rush sync mode */\n    int                 syncer_trigger;            /* Trigger full sync */\n    long                syncer_count;              /* Vnodes on lists */\n};\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#delay-parameters","title":"Delay Parameters","text":"<p>Tunable via sysctl:</p> Parameter Default Sysctl Description <code>syncdelay</code> 30s <code>kern.syncdelay</code> Max delay for data sync <code>filedelay</code> 30s <code>kern.filedelay</code> File data sync delay <code>dirdelay</code> 29s <code>kern.dirdelay</code> Directory sync delay <code>metadelay</code> 28s <code>kern.metadelay</code> Metadata sync delay <code>retrydelay</code> 1s <code>kern.retrydelay</code> Retry delay after failure"},{"location":"sys/kern/vfs/vfs-extensions/#worklist-management","title":"Worklist Management","text":"<p>Adding vnodes to syncer worklist:</p> <pre><code>void vn_syncer_add(struct vnode *vp, int delay)\n{\n    ctx = vp-&gt;v_mount-&gt;mnt_syncer_ctx;\n    lwkt_gettoken(&amp;ctx-&gt;sc_token);\n\n    if (vp-&gt;v_flag &amp; VONWORKLST) {\n        LIST_REMOVE(vp, v_synclist);\n        --ctx-&gt;syncer_count;\n    }\n\n    slot = (ctx-&gt;syncer_delayno + delay) &amp; ctx-&gt;syncer_mask;\n    LIST_INSERT_HEAD(&amp;ctx-&gt;syncer_workitem_pending[slot], vp, v_synclist);\n    vsetflags(vp, VONWORKLST);\n    ++ctx-&gt;syncer_count;\n\n    lwkt_reltoken(&amp;ctx-&gt;sc_token);\n}\n</code></pre> <p>Removing vnodes from worklist:</p> <pre><code>void vn_syncer_remove(struct vnode *vp, int force)\n</code></pre> <p>Called when vnode is no longer dirty or during forced unmount.</p>"},{"location":"sys/kern/vfs/vfs-extensions/#dirty-vnode-tracking","title":"Dirty Vnode Tracking","text":"<p><code>vsetisdirty(vp)</code> - Mark vnode as having dirty inode data:</p> <pre><code>void vsetisdirty(struct vnode *vp)\n{\n    if ((vp-&gt;v_flag &amp; VISDIRTY) == 0) {\n        vsetflags(vp, VISDIRTY);\n        if ((vp-&gt;v_flag &amp; VONWORKLST) == 0)\n            vn_syncer_add(vp, syncdelay);\n    }\n}\n</code></pre> <p><code>vsetobjdirty(vp)</code> - Mark vnode as having dirty VM object:</p> <pre><code>void vsetobjdirty(struct vnode *vp)\n{\n    if ((vp-&gt;v_flag &amp; VOBJDIRTY) == 0) {\n        vsetflags(vp, VOBJDIRTY);\n        if ((vp-&gt;v_flag &amp; VONWORKLST) == 0)\n            vn_syncer_add(vp, syncdelay);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#syncer-thread-operation","title":"Syncer Thread Operation","text":"<p>The syncer thread runs a continuous loop:</p> <pre><code>static void syncer_thread(void *_ctx)\n{\n    for (;;) {\n        // 1. Handle triggered full sync\n        if (ctx-&gt;syncer_trigger) {\n            VOP_FSYNC(ctx-&gt;sc_mp-&gt;mnt_syncer, MNT_LAZY, 0);\n            atomic_clear_int(&amp;ctx-&gt;syncer_trigger, 1);\n        }\n\n        // 2. Process current time slot\n        slp = &amp;ctx-&gt;syncer_workitem_pending[ctx-&gt;syncer_delayno];\n        while ((vp = LIST_FIRST(slp)) != NULL) {\n            vn_syncer_add(vp, retrydelay);  // Move to retry slot\n            if (vget(vp, LK_EXCLUSIVE | LK_NOWAIT) == 0) {\n                VOP_FSYNC(vp, MNT_LAZY, 0);\n                vput(vp);\n            }\n        }\n\n        // 3. Advance to next slot\n        ctx-&gt;syncer_delayno = (ctx-&gt;syncer_delayno + 1) &amp; ctx-&gt;syncer_mask;\n\n        // 4. Sleep until next second (or wakeup)\n        tsleep(ctx, PINTERLOCKED, \"syncer\", hz);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#syncer-control-functions","title":"Syncer Control Functions","text":"<p><code>speedup_syncer(mp)</code> - Request faster sync processing:</p> <pre><code>void speedup_syncer(struct mount *mp)\n{\n    atomic_add_int(&amp;rushjob, 1);\n    if (mp &amp;&amp; mp-&gt;mnt_syncer_ctx)\n        wakeup(mp-&gt;mnt_syncer_ctx);\n}\n</code></pre> <p><code>trigger_syncer(mp)</code> - Request immediate full sync:</p> <pre><code>void trigger_syncer(struct mount *mp)\n{\n    if (mp &amp;&amp; (ctx = mp-&gt;mnt_syncer_ctx) != NULL) {\n        atomic_set_int(&amp;ctx-&gt;syncer_trigger, 1);\n        wakeup(ctx);\n    }\n}\n</code></pre> <p><code>trigger_syncer_start(mp)</code> / <code>trigger_syncer_stop(mp)</code> - Continuous sync mode:</p> <p>Used by filesystems that need guaranteed sync progress (e.g., waiting for dirty data flush).</p>"},{"location":"sys/kern/vfs/vfs-extensions/#syncer-vnode","title":"Syncer Vnode","text":"<p>Each mount point has a special syncer vnode (<code>mp-&gt;mnt_syncer</code>) that: - Is always on the syncer worklist - Triggers <code>VFS_SYNC()</code> when its turn comes - Has minimal vnode operations (just fsync, inactive, reclaim)</p> <pre><code>int vfs_allocate_syncvnode(struct mount *mp)\n{\n    error = getspecialvnode(VT_VFS, mp, &amp;sync_vnode_vops_p, &amp;vp, 0, 0);\n    vp-&gt;v_type = VNON;\n    vn_syncer_add(vp, next % syncdelay);\n    mp-&gt;mnt_syncer = vp;\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#vsyncscan-efficient-dirty-vnode-iteration","title":"vsyncscan() - Efficient Dirty Vnode Iteration","text":"<p>For filesystems with many vnodes, iterating all mount vnodes is expensive. <code>vsyncscan()</code> iterates only vnodes on the syncer worklist:</p> <pre><code>int vsyncscan(struct mount *mp, int vmsc_flags,\n              int (*slowfunc)(struct mount *mp, struct vnode *vp, void *data),\n              void *data)\n</code></pre> <p>Flags: - <code>VMSC_NOWAIT</code> - Use non-blocking vnode acquisition - <code>VMSC_GETVP</code> - Acquire vnode lock before callback - <code>VMSC_GETVX</code> - Acquire VX lock before callback</p>"},{"location":"sys/kern/vfs/vfs-extensions/#synthetic-filesystem","title":"Synthetic Filesystem","text":"<p>The <code>vfs_synth.c</code> file provides a synthetic devfs mount used during early boot to locate root devices.</p> <p>Key files: - <code>sys/kern/vfs_synth.c</code> - Synthetic filesystem initialization</p>"},{"location":"sys/kern/vfs/vfs-extensions/#purpose","title":"Purpose","text":"<p>During boot, the kernel needs to find the root device before the normal filesystem hierarchy is mounted. The synthetic filesystem:</p> <ol> <li>Creates an internal devfs mount (<code>synth_mp</code>)</li> <li>Provides <code>getsynthvnode()</code> to look up device nodes by name</li> <li>Triggers <code>sync_devs()</code> to ensure devices are enumerated</li> </ol>"},{"location":"sys/kern/vfs/vfs-extensions/#initialization","title":"Initialization","text":"<pre><code>static void synthinit(void *arg __unused)\n{\n    // Create internal devfs mount\n    error = vfs_rootmountalloc(\"devfs\", \"dummy\", &amp;synth_mp);\n    error = VFS_MOUNT(synth_mp, NULL, NULL, proc0.p_ucred);\n    error = VFS_ROOT(synth_mp, &amp;synth_vp);\n\n    // Set up namecache for lookups\n    cache_allocroot(&amp;synth_mp-&gt;mnt_ncmountpt, synth_mp, synth_vp);\n    cache_unlock(&amp;synth_mp-&gt;mnt_ncmountpt);\n    vput(synth_vp);\n\n    synth_inited = 1;\n}\n\nSYSINIT(synthinit, SI_SUB_VFS, SI_ORDER_ANY, synthinit, NULL);\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#device-lookup","title":"Device Lookup","text":"<pre><code>struct vnode *getsynthvnode(const char *devname)\n{\n    KKASSERT(synth_inited != 0);\n\n    // Sync devfs twice to ensure devices are present\n    if (synth_synced &lt; 2) {\n        sync_devs();\n        ++synth_synced;\n    }\n\n    // Look up device in synthetic devfs\n    error = nlookup_init_root(&amp;nd, devname, UIO_SYSSPACE, NLC_FOLLOW,\n                              cred, &amp;synth_mp-&gt;mnt_ncmountpt,\n                              &amp;synth_mp-&gt;mnt_ncmountpt);\n    error = nlookup(&amp;nd);\n\n    if (error == 0) {\n        vp = nch.ncp-&gt;nc_vp;\n        error = vget(vp, LK_EXCLUSIVE);\n    }\n\n    nlookup_done(&amp;nd);\n    return vp;  // Returns VX-locked, referenced vnode\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#vfs-quota-system","title":"VFS Quota System","text":"<p>The <code>vfs_quota.c</code> file implements per-user and per-group space accounting and limits.</p> <p>Key files: - <code>sys/kern/vfs_quota.c</code> - Quota implementation - <code>sys/sys/vfs_quota.h</code> - Quota data structures</p>"},{"location":"sys/kern/vfs/vfs-extensions/#enabling-quotas","title":"Enabling Quotas","text":"<p>Quotas are disabled by default and controlled via: - Boot-time tunable: <code>vfs.quota_enabled</code> - Sysctl: <code>vfs.quota_enabled</code> (read-only)</p>"},{"location":"sys/kern/vfs/vfs-extensions/#data-structures","title":"Data Structures","text":"<p>Per-mount accounting uses red-black trees for efficient uid/gid lookup:</p> <pre><code>struct ac_unode {\n    RB_ENTRY(ac_unode) rb_entry;\n    uint32_t left_bits;        /* uid &gt;&gt; ACCT_CHUNK_BITS */\n    struct {\n        int64_t space;         /* bytes used */\n        int64_t limit;         /* byte limit (0 = unlimited) */\n    } uid_chunk[ACCT_CHUNK_NIDS];\n};\n\nstruct ac_gnode {\n    RB_ENTRY(ac_gnode) rb_entry;\n    uint32_t left_bits;        /* gid &gt;&gt; ACCT_CHUNK_BITS */\n    struct {\n        int64_t space;\n        int64_t limit;\n    } gid_chunk[ACCT_CHUNK_NIDS];\n};\n</code></pre> <p>Chunked storage reduces tree nodes: each node handles <code>ACCT_CHUNK_NIDS</code> consecutive uid/gids.</p>"},{"location":"sys/kern/vfs/vfs-extensions/#initialization_1","title":"Initialization","text":"<pre><code>void vq_init(struct mount *mp)\n{\n    if (!vfs_quota_enabled)\n        return;\n\n    RB_INIT(&amp;mp-&gt;mnt_acct.ac_uroot);\n    RB_INIT(&amp;mp-&gt;mnt_acct.ac_groot);\n    spin_init(&amp;mp-&gt;mnt_acct.ac_spin, \"vqinit\");\n\n    mp-&gt;mnt_acct.ac_bytes = 0;\n    mp-&gt;mnt_op-&gt;vfs_account = vfs_stdaccount;\n    mp-&gt;mnt_flag |= MNT_QUOTA;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#accounting-callback","title":"Accounting Callback","text":"<p>Filesystems call <code>vfs_account()</code> on space changes:</p> <pre><code>void vfs_stdaccount(struct mount *mp, uid_t uid, gid_t gid, int64_t delta)\n{\n    spin_lock(&amp;mp-&gt;mnt_acct.ac_spin);\n\n    mp-&gt;mnt_acct.ac_bytes += delta;\n\n    // Find or create uid node\n    ufind.left_bits = (uid &gt;&gt; ACCT_CHUNK_BITS);\n    if ((unp = RB_FIND(ac_utree, &amp;mp-&gt;mnt_acct.ac_uroot, &amp;ufind)) == NULL)\n        unp = unode_insert(mp, uid);\n\n    // Find or create gid node\n    gfind.left_bits = (gid &gt;&gt; ACCT_CHUNK_BITS);\n    if ((gnp = RB_FIND(ac_gtree, &amp;mp-&gt;mnt_acct.ac_groot, &amp;gfind)) == NULL)\n        gnp = gnode_insert(mp, gid);\n\n    // Update usage\n    unp-&gt;uid_chunk[(uid &amp; ACCT_CHUNK_MASK)].space += delta;\n    gnp-&gt;gid_chunk[(gid &amp; ACCT_CHUNK_MASK)].space += delta;\n\n    spin_unlock(&amp;mp-&gt;mnt_acct.ac_spin);\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#quota-enforcement","title":"Quota Enforcement","text":"<p><code>vq_write_ok()</code> - Check if write is allowed:</p> <pre><code>int vq_write_ok(struct mount *mp, uid_t uid, gid_t gid, uint64_t delta)\n{\n    spin_lock(&amp;mp-&gt;mnt_acct.ac_spin);\n\n    // Check filesystem limit\n    if (mp-&gt;mnt_acct.ac_limit &amp;&amp;\n        (mp-&gt;mnt_acct.ac_bytes + delta) &gt; mp-&gt;mnt_acct.ac_limit) {\n        rv = 0;\n        goto done;\n    }\n\n    // Check uid limit\n    if (unp &amp;&amp; unp-&gt;uid_chunk[...].limit &amp;&amp;\n        (space + delta) &gt; limit) {\n        rv = 0;\n        goto done;\n    }\n\n    // Check gid limit\n    if (gnp &amp;&amp; gnp-&gt;gid_chunk[...].limit &amp;&amp;\n        (space + delta) &gt; limit) {\n        rv = 0;\n    }\n\ndone:\n    spin_unlock(&amp;mp-&gt;mnt_acct.ac_spin);\n    return rv;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#system-call-interface","title":"System Call Interface","text":"<p><code>sys_vquotactl()</code> - Quota control system call:</p> <p>Commands (via proplib dictionary): - <code>\"get usage all\"</code> - Return all usage statistics - <code>\"set usage all\"</code> - Set usage statistics (for restore) - <code>\"set limit\"</code> - Set filesystem-wide limit - <code>\"set limit uid\"</code> - Set per-uid limit - <code>\"set limit gid\"</code> - Set per-gid limit</p>"},{"location":"sys/kern/vfs/vfs-extensions/#pfs-support","title":"PFS Support","text":"<p>For pseudo-filesystems (nullfs, etc.), <code>vq_vptomp()</code> returns the real mount point:</p> <pre><code>struct mount *vq_vptomp(struct vnode *vp)\n{\n    if ((vp-&gt;v_pfsmp != NULL) &amp;&amp; (mountlist_exists(vp-&gt;v_pfsmp)))\n        return vp-&gt;v_pfsmp;  /* Real mount for PFS */\n    else\n        return vp-&gt;v_mount;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#asynchronous-io-stubs","title":"Asynchronous I/O (Stubs)","text":"<p>The <code>vfs_aio.c</code> file contains stub implementations of POSIX AIO functions. These system calls are not implemented in DragonFly BSD and return <code>ENOSYS</code>:</p> <pre><code>int sys_aio_read(struct sysmsg *sysmsg, const struct aio_read_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_write(struct sysmsg *sysmsg, const struct aio_write_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_return(struct sysmsg *sysmsg, const struct aio_return_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_suspend(struct sysmsg *sysmsg, const struct aio_suspend_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_cancel(struct sysmsg *sysmsg, const struct aio_cancel_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_error(struct sysmsg *sysmsg, const struct aio_error_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_lio_listio(struct sysmsg *sysmsg, const struct lio_listio_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_waitcomplete(struct sysmsg *sysmsg, const struct aio_waitcomplete_args *uap)\n{\n    return ENOSYS;\n}\n</code></pre> <p>The kevent filter for AIO also returns <code>ENXIO</code>:</p> <pre><code>static int filt_aioattach(struct knote *kn)\n{\n    return ENXIO;\n}\n\nstruct filterops aio_filtops =\n    { FILTEROP_MPSAFE, filt_aioattach, NULL, NULL };\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#summary","title":"Summary","text":"Subsystem File Lines Purpose VFS Helpers <code>vfs_helper.c</code> 405 UNIX permission/attribute helpers Syncer <code>vfs_sync.c</code> 890 Per-mount syncer daemon Synthetic FS <code>vfs_synth.c</code> 137 Early boot devfs access Quotas <code>vfs_quota.c</code> 486 Space accounting and limits AIO <code>vfs_aio.c</code> 83 POSIX AIO stubs (not implemented) <p>Key design points:</p> <ol> <li>Helper functions provide consistent UNIX semantics across filesystems</li> <li>Per-mount syncers scale better than a single global syncer</li> <li>Quota system uses chunked RB-trees for efficient uid/gid tracking</li> <li>Synthetic filesystem enables device lookup before root mount</li> <li>AIO is unimplemented - applications should use alternatives</li> </ol>"},{"location":"sys/kern/vfs/vfs-locking/","title":"VFS Vnode Locking and Lifecycle","text":""},{"location":"sys/kern/vfs/vfs-locking/#overview","title":"Overview","text":"<p>The DragonFly BSD vnode locking subsystem (<code>vfs_lock.c</code>) manages vnode lifecycle states, reference counting, and synchronization. Unlike traditional BSD systems that use simple reference counts, DragonFly implements a sophisticated state machine with multiple vnode states and specialized locking primitives designed for SMP scalability.</p> <p>The system provides:</p> <ul> <li>State-based lifecycle management - Vnodes transition through CACHED, ACTIVE, INACTIVE, and DYING states</li> <li>Two-tier reference counting - Regular refs (<code>v_refcnt</code>) and auxiliary refs (<code>v_auxrefs</code>)</li> <li>Per-CPU vnode lists - Reduces lock contention by partitioning vnodes across CPUs</li> <li>Lockless fast paths - Many operations use atomic operations without acquiring locks</li> <li>VX locking - Special exclusive locks for reclamation and deactivation</li> </ul> <p>Key files: - <code>sys/kern/vfs_lock.c</code> - Vnode locking, state transitions, and recycling - <code>sys/kern/vfs_vnops.c</code> - Standard vnode lock functions (<code>vn_lock</code>, <code>vn_unlock</code>) - <code>sys/sys/vnode.h</code> - Vnode structure and state definitions</p>"},{"location":"sys/kern/vfs/vfs-locking/#vnode-states","title":"Vnode States","text":"<p>Vnodes exist in one of four states, managed through the <code>v_state</code> field:</p> <pre><code>flowchart LR\n    CACHED[\"VS_CACHED\"]\n    ACTIVE[\"VS_ACTIVE\"]\n    INACTIVE[\"VS_INACTIVE\"]\n    DYING[\"VS_DYING\"]\n    FREE[\"kfree()\"]\n\n    CACHED --&gt;|\"vget()\"| ACTIVE\n    ACTIVE --&gt;|\"vrele()\"| INACTIVE\n    ACTIVE --&gt;|\"vget()\"| CACHED\n    INACTIVE --&gt;|\"reclaim\"| DYING\n    DYING --&gt; FREE\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#state-definitions","title":"State Definitions","text":"State Value Description <code>VS_CACHED</code> 0 Vnode exists but has no references; eligible for reuse <code>VS_ACTIVE</code> 1 Vnode is actively referenced and in use <code>VS_INACTIVE</code> 2 Vnode has been deactivated; on inactive list awaiting reclamation <code>VS_DYING</code> 3 Vnode is being destroyed; removed from all lists"},{"location":"sys/kern/vfs/vfs-locking/#state-transition-locking-requirements","title":"State Transition Locking Requirements","text":"<p>From <code>vfs_lock.c:38-53</code>:</p> <pre><code>INACTIVE -&gt; CACHED|DYING    vx_lock(excl) + vi-&gt;spin\nDYING    -&gt; CACHED          vx_lock(excl)\nACTIVE   -&gt; INACTIVE        (none) + v_spin + vi-&gt;spin\nINACTIVE -&gt; ACTIVE          vn_lock(any) + v_spin + vi-&gt;spin\nCACHED   -&gt; ACTIVE          vn_lock(any) + v_spin + vi-&gt;spin\n</code></pre> <p>Key observations: - Transitions into ACTIVE can use shared locks - Transitions into CACHED or DYING require exclusive VX locks - The <code>v_spin</code> spinlock protects per-vnode state - The <code>vi-&gt;spin</code> spinlock protects the per-CPU vnode lists</p>"},{"location":"sys/kern/vfs/vfs-locking/#reference-counting","title":"Reference Counting","text":"<p>DragonFly uses a sophisticated reference counting scheme with special flags embedded in the reference count field.</p>"},{"location":"sys/kern/vfs/vfs-locking/#reference-count-fields","title":"Reference Count Fields","text":"<pre><code>struct vnode {\n    int v_refcnt;      /* vget/vput refs + flags */\n    int v_auxrefs;     /* vhold/vdrop refs */\n    // ...\n};\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#reference-count-flags","title":"Reference Count Flags","text":"<p>From <code>sys/vnode.h:252-256</code>:</p> <pre><code>#define VREF_TERMINATE  0x80000000  /* termination in progress */\n#define VREF_FINALIZE   0x40000000  /* deactivate on last vrele */\n#define VREF_MASK       0xBFFFFFFF  /* includes VREF_TERMINATE */\n\n#define VREFCNT(vp)     ((int)((vp)-&gt;v_refcnt &amp; VREF_MASK))\n</code></pre> Flag Purpose <code>VREF_TERMINATE</code> Set when vnode is undergoing termination; prevents reactivation <code>VREF_FINALIZE</code> Requests deactivation on the 1-&gt;0 ref transition <code>VREF_MASK</code> Extracts actual reference count (includes TERMINATE bit)"},{"location":"sys/kern/vfs/vfs-locking/#regular-references-vrefvrele","title":"Regular References (vref/vrele)","text":"<p><code>vref(vp)</code> - Add a reference to an active vnode:</p> <pre><code>void vref(struct vnode *vp)\n{\n    KASSERT((VREFCNT(vp) &gt; 0 &amp;&amp; vp-&gt;v_state != VS_INACTIVE),\n            (\"vref: bad refcnt %08x %d\", vp-&gt;v_refcnt, vp-&gt;v_state));\n    atomic_add_int(&amp;vp-&gt;v_refcnt, 1);\n}\n</code></pre> <ul> <li>Caller must already hold a reference</li> <li>Cannot be called on inactive vnodes (use <code>vget()</code> instead)</li> <li>Lock-free atomic operation</li> </ul> <p><code>vrele(vp)</code> - Release a reference:</p> <p>The 1-&gt;0 transition is critical and handles finalization:</p> <pre><code>void vrele(struct vnode *vp)\n{\n    // For refs &gt; 1: simple atomic decrement\n    if ((count &amp; VREF_MASK) &gt; 1) {\n        atomic_fcmpset_int(&amp;vp-&gt;v_refcnt, &amp;count, count - 1);\n        return;\n    }\n\n    // For 1-&gt;0 with VREF_FINALIZE: trigger termination\n    if (count &amp; VREF_FINALIZE) {\n        vx_lock(vp);\n        if (atomic_fcmpset_int(&amp;vp-&gt;v_refcnt, &amp;count, VREF_TERMINATE)) {\n            vnode_terminate(vp);  // Calls VOP_INACTIVE, moves to inactive list\n        }\n        vx_unlock(vp);\n    } else {\n        // Simple 1-&gt;0: vnode becomes cached\n        atomic_fcmpset_int(&amp;vp-&gt;v_refcnt, &amp;count, 0);\n        atomic_add_int(&amp;mycpu-&gt;gd_cachedvnodes, 1);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#auxiliary-references-vholdvdrop","title":"Auxiliary References (vhold/vdrop)","text":"<p>Auxiliary references prevent vnode destruction but don't affect state:</p> <pre><code>void vhold(struct vnode *vp)\n{\n    atomic_add_int(&amp;vp-&gt;v_auxrefs, 1);\n}\n\nvoid vdrop(struct vnode *vp)\n{\n    atomic_add_int(&amp;vp-&gt;v_auxrefs, -1);\n}\n</code></pre> <p>Use cases: - Namecache entries holding references to vnodes - VM objects associated with vnodes - Temporary holds during complex operations</p> <p>A vnode cannot be freed (<code>kfree()</code>'d) while <code>v_auxrefs &gt; 0</code>.</p>"},{"location":"sys/kern/vfs/vfs-locking/#vx-locking","title":"VX Locking","text":"<p>VX locks are special exclusive locks used for vnode reclamation and deactivation. They combine the standard vnode lock with a spin lock update.</p>"},{"location":"sys/kern/vfs/vfs-locking/#vx-lock-functions","title":"VX Lock Functions","text":"<pre><code>void vx_lock(struct vnode *vp)\n{\n    lockmgr(&amp;vp-&gt;v_lock, LK_EXCLUSIVE);\n    spin_lock_update_only(&amp;vp-&gt;v_spin);\n}\n\nvoid vx_unlock(struct vnode *vp)\n{\n    spin_unlock_update_only(&amp;vp-&gt;v_spin);\n    lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n}\n</code></pre> <p>The <code>spin_lock_update_only()</code> is a special spinlock mode that: - Prevents readers from proceeding - Allows the holder to make atomic state changes - Is lighter weight than a full exclusive spinlock</p>"},{"location":"sys/kern/vfs/vfs-locking/#vx-vs-vn-locking","title":"VX vs VN Locking","text":"Aspect VN Lock (<code>vn_lock</code>) VX Lock (<code>vx_lock</code>) Lock type Can be shared or exclusive Always exclusive Spin lock Not held Holds <code>v_spin</code> in update mode Use case Normal vnode operations Reclamation, deactivation Reactivation Allowed Not applicable"},{"location":"sys/kern/vfs/vfs-locking/#downgrading-vx-to-vn","title":"Downgrading VX to VN","text":"<p>After allocating a new vnode, callers typically downgrade from VX to VN:</p> <pre><code>void vx_downgrade(struct vnode *vp)\n{\n    spin_unlock_update_only(&amp;vp-&gt;v_spin);\n    // Lock remains EXCLUSIVE, just without spin update mode\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#vnode-acquisition-vget","title":"Vnode Acquisition (vget)","text":"<p>The <code>vget()</code> function acquires a reference and lock on a vnode, potentially reactivating it:</p> <pre><code>int vget(struct vnode *vp, int flags)\n{\n    // 1. Add reference (may remove from cached count)\n    if ((atomic_fetchadd_int(&amp;vp-&gt;v_refcnt, 1) &amp; VREF_MASK) == 0)\n        atomic_add_int(&amp;mycpu-&gt;gd_cachedvnodes, -1);\n\n    // 2. Acquire lock (shared or exclusive based on flags)\n    if ((error = vn_lock(vp, flags | LK_FAILRECLAIM)) != 0) {\n        vrele(vp);\n        return error;\n    }\n\n    // 3. Check for reclaimed vnode\n    if (vp-&gt;v_flag &amp; VRECLAIMED) {\n        vn_unlock(vp);\n        vrele(vp);\n        return ENOENT;\n    }\n\n    // 4. Reactivate if needed\n    if (vp-&gt;v_state != VS_ACTIVE) {\n        _vclrflags(vp, VINACTIVE);\n        spin_lock(&amp;vp-&gt;v_spin);\n        _vactivate(vp);\n        atomic_clear_int(&amp;vp-&gt;v_refcnt, VREF_TERMINATE | VREF_FINALIZE);\n        spin_unlock(&amp;vp-&gt;v_spin);\n    }\n\n    return 0;\n}\n</code></pre> <p>Key points: - Can use shared locks for reactivation (important for scalability) - Clears <code>VREF_TERMINATE</code> and <code>VREF_FINALIZE</code> on success - Updates <code>v_act</code> activity counter for LRU decisions</p>"},{"location":"sys/kern/vfs/vfs-locking/#vput-combined-unlock-and-release","title":"vput() - Combined Unlock and Release","text":"<pre><code>void vput(struct vnode *vp)\n{\n    vn_unlock(vp);\n    vrele(vp);\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#per-cpu-vnode-lists","title":"Per-CPU Vnode Lists","text":"<p>Vnodes are distributed across per-CPU lists to reduce contention:</p> <pre><code>struct vnode_index {\n    struct freelst  active_list;     /* Active vnodes */\n    struct vnode    active_rover;    /* Rover for deactivation scan */\n    struct freelst  inactive_list;   /* Inactive vnodes awaiting reclaim */\n    struct spinlock spin;            /* Protects this CPU's lists */\n    int deac_rover;                  /* Deactivation scan position */\n    int free_rover;                  /* Free scan position */\n} __cachealign;\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#list-assignment","title":"List Assignment","text":"<p>Vnodes are assigned to lists using a hash of their address:</p> <pre><code>#define VLIST_HASH(vp)  (((uintptr_t)vp ^ VLIST_XOR) % \\\n                         VLIST_PRIME2 % (unsigned)ncpus)\n</code></pre> <p>This ensures: - Consistent list assignment for a given vnode - Even distribution across CPUs - Cache-line alignment for each <code>vnode_index</code></p>"},{"location":"sys/kern/vfs/vfs-locking/#activeinactive-list-management","title":"Active/Inactive List Management","text":"<p><code>_vactivate(vp)</code> - Move vnode to active list:</p> <pre><code>static void _vactivate(struct vnode *vp)\n{\n    struct vnode_index *vi = &amp;vnode_list_hash[VLIST_HASH(vp)];\n\n    spin_lock(&amp;vi-&gt;spin);\n\n    switch(vp-&gt;v_state) {\n    case VS_INACTIVE:\n        TAILQ_REMOVE(&amp;vi-&gt;inactive_list, vp, v_list);\n        atomic_add_int(&amp;mycpu-&gt;gd_inactivevnodes, -1);\n        break;\n    case VS_CACHED:\n    case VS_DYING:\n        break;\n    }\n\n    TAILQ_INSERT_TAIL(&amp;vi-&gt;active_list, vp, v_list);\n    vp-&gt;v_state = VS_ACTIVE;\n    spin_unlock(&amp;vi-&gt;spin);\n    atomic_add_int(&amp;mycpu-&gt;gd_activevnodes, 1);\n}\n</code></pre> <p><code>_vinactive(vp)</code> - Move vnode to inactive list:</p> <pre><code>static void _vinactive(struct vnode *vp)\n{\n    struct vnode_index *vi = &amp;vnode_list_hash[VLIST_HASH(vp)];\n\n    spin_lock(&amp;vi-&gt;spin);\n\n    if (vp-&gt;v_state == VS_ACTIVE) {\n        TAILQ_REMOVE(&amp;vi-&gt;active_list, vp, v_list);\n        atomic_add_int(&amp;mycpu-&gt;gd_activevnodes, -1);\n    }\n\n    // Reclaimed vnodes go to head (recycled first)\n    if (vp-&gt;v_flag &amp; VRECLAIMED) {\n        TAILQ_INSERT_HEAD(&amp;vi-&gt;inactive_list, vp, v_list);\n    } else {\n        TAILQ_INSERT_TAIL(&amp;vi-&gt;inactive_list, vp, v_list);\n    }\n    vp-&gt;v_state = VS_INACTIVE;\n    spin_unlock(&amp;vi-&gt;spin);\n    atomic_add_int(&amp;mycpu-&gt;gd_inactivevnodes, 1);\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#vnode-allocation-and-recycling","title":"Vnode Allocation and Recycling","text":""},{"location":"sys/kern/vfs/vfs-locking/#allocvnode-allocate-a-new-vnode","title":"allocvnode() - Allocate a New Vnode","text":"<pre><code>struct vnode *allocvnode(int lktimeout, int lkflags)\n{\n    struct vnode *vp;\n    struct vnode_index *vi = &amp;vnode_list_hash[mycpuid];\n\n    // 1. Try to reuse a reclaimed vnode from local inactive list\n    spin_lock(&amp;vi-&gt;spin);\n    vp = TAILQ_FIRST(&amp;vi-&gt;inactive_list);\n    if (vp &amp;&amp; (vp-&gt;v_flag &amp; VRECLAIMED)) {\n        // Fast path: reuse existing vnode structure\n        if (vx_get_nonblock(vp) == 0) {\n            // ... validation checks ...\n            TAILQ_REMOVE(&amp;vi-&gt;inactive_list, vp, v_list);\n            vp-&gt;v_state = VS_DYING;\n            spin_unlock(&amp;vi-&gt;spin);\n\n            bzero(vp, sizeof(*vp));  // Reuse structure\n            goto initialize;\n        }\n    }\n    spin_unlock(&amp;vi-&gt;spin);\n\n    // 2. Slow path: allocate new vnode\n    vp = kmalloc_obj(sizeof(*vp), M_VNODE, M_ZERO | M_WAITOK);\n    atomic_add_int(&amp;numvnodes, 1);\n\ninitialize:\n    // Initialize vnode fields\n    lwkt_token_init(&amp;vp-&gt;v_token, \"vnode\");\n    lockinit(&amp;vp-&gt;v_lock, \"vnode\", lktimeout, lkflags);\n    spin_init(&amp;vp-&gt;v_spin, \"allocvnode\");\n    // ... other initialization ...\n\n    vx_lock(vp);\n    vp-&gt;v_refcnt = 1;\n    vp-&gt;v_state = VS_CACHED;\n    _vactivate(vp);\n\n    return vp;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#cleanfreevnode-recycle-inactive-vnodes","title":"cleanfreevnode() - Recycle Inactive Vnodes","text":"<p>The <code>cleanfreevnode()</code> function scans inactive lists to find vnodes suitable for recycling:</p> <ol> <li>Deactivation scan: Moves vnodes from active to inactive list based on activity</li> <li>Reclamation scan: Finds fully reclaimable vnodes on inactive list</li> </ol> <pre><code>static struct vnode *cleanfreevnode(int maxcount)\n{\n    // Phase 1: Try to deactivate active vnodes\n    for (count = 0; count &lt; maxcount * 2; ++count) {\n        vi = &amp;vnode_list_hash[((unsigned)ri &gt;&gt; 4) % ncpus];\n        vp = TAILQ_NEXT(&amp;vi-&gt;active_rover, v_list);\n\n        // Skip if referenced\n        if ((vp-&gt;v_refcnt &amp; VREF_MASK) != 0)\n            continue;\n\n        // Decay activity counter\n        if (vp-&gt;v_act &gt; 0) {\n            vp-&gt;v_act -= VACT_INC;\n            continue;\n        }\n\n        // Trigger deactivation via finalize\n        atomic_set_int(&amp;vp-&gt;v_refcnt, VREF_FINALIZE);\n        vrele(vp);\n    }\n\n    // Phase 2: Find reclaimable inactive vnode\n    for (count = 0; count &lt; maxcount; ++count) {\n        vp = TAILQ_FIRST(&amp;vi-&gt;inactive_list);\n\n        // Must have no refs or auxrefs\n        if (vp-&gt;v_auxrefs != vp-&gt;v_namecache_count ||\n            (vp-&gt;v_refcnt &amp; ~VREF_FINALIZE) != VREF_TERMINATE + 1)\n            continue;\n\n        // Reclaim and return\n        if ((vp-&gt;v_flag &amp; VRECLAIMED) == 0) {\n            cache_inval_vp_nonblock(vp);\n            vgone_vxlocked(vp);\n        }\n\n        vp-&gt;v_state = VS_DYING;\n        return vp;\n    }\n    return NULL;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#activity-counter-v_act","title":"Activity Counter (v_act)","text":"<p>The <code>v_act</code> field implements LRU-like behavior:</p> <pre><code>#define VACT_MAX    10\n#define VACT_INC    2\n</code></pre> <ul> <li>Incremented on <code>vget()</code> (up to <code>VACT_MAX</code>)</li> <li>Decremented during deactivation scans</li> <li>Vnodes with <code>v_act == 0</code> are candidates for deactivation</li> <li>VM-heavy vnodes decay slower (based on <code>v_object-&gt;resident_page_count</code>)</li> </ul>"},{"location":"sys/kern/vfs/vfs-locking/#standard-vnode-lock-functions","title":"Standard Vnode Lock Functions","text":"<p>Located in <code>vfs_vnops.c</code>, these wrap the lockmgr:</p>"},{"location":"sys/kern/vfs/vfs-locking/#vn_lock-acquire-vnode-lock","title":"vn_lock() - Acquire Vnode Lock","text":"<pre><code>int vn_lock(struct vnode *vp, int flags)\n{\n    int error;\n\n    do {\n        error = lockmgr(&amp;vp-&gt;v_lock, flags);\n        if (error == 0)\n            break;\n    } while (flags &amp; LK_RETRY);\n\n    // Handle reclaimed vnodes\n    if (error == 0 &amp;&amp; (vp-&gt;v_flag &amp; VRECLAIMED)) {\n        if (flags &amp; LK_FAILRECLAIM) {\n            lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n            error = ENOENT;\n        }\n    }\n    return error;\n}\n</code></pre> <p>Flags: - <code>LK_SHARED</code> - Shared (read) lock - <code>LK_EXCLUSIVE</code> - Exclusive (write) lock - <code>LK_NOWAIT</code> - Don't block if unavailable - <code>LK_RETRY</code> - Retry on failure - <code>LK_FAILRECLAIM</code> - Fail if vnode is reclaimed</p>"},{"location":"sys/kern/vfs/vfs-locking/#vn_unlock-release-vnode-lock","title":"vn_unlock() - Release Vnode Lock","text":"<pre><code>void vn_unlock(struct vnode *vp)\n{\n    lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#statistics-and-monitoring","title":"Statistics and Monitoring","text":"<p>Global vnode statistics are tracked per-CPU and aggregated:</p> <pre><code>int activevnodes;    /* sysctl debug.activevnodes */\nint cachedvnodes;    /* sysctl debug.cachedvnodes */\nint inactivevnodes;  /* sysctl debug.inactivevnodes */\n\nvoid synchronizevnodecount(void)\n{\n    for (i = 0; i &lt; ncpus; ++i) {\n        globaldata_t gd = globaldata_find(i);\n        nca += gd-&gt;gd_cachedvnodes;\n        act += gd-&gt;gd_activevnodes;\n        ina += gd-&gt;gd_inactivevnodes;\n    }\n    cachedvnodes = nca;\n    activevnodes = act;\n    inactivevnodes = ina;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#initialization","title":"Initialization","text":"<p>Called from <code>vfsinit()</code>:</p> <pre><code>void vfs_lock_init(void)\n{\n    kmalloc_obj_raise_limit(M_VNODE, 0);  /* unlimited */\n\n    vnode_list_hash = kmalloc(sizeof(*vnode_list_hash) * ncpus,\n                              M_VNODE_HASH, M_ZERO | M_WAITOK);\n\n    for (i = 0; i &lt; ncpus; ++i) {\n        struct vnode_index *vi = &amp;vnode_list_hash[i];\n        TAILQ_INIT(&amp;vi-&gt;inactive_list);\n        TAILQ_INIT(&amp;vi-&gt;active_list);\n        TAILQ_INSERT_TAIL(&amp;vi-&gt;active_list, &amp;vi-&gt;active_rover, v_list);\n        spin_init(&amp;vi-&gt;spin, \"vfslock\");\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#summary","title":"Summary","text":"<p>The DragonFly vnode locking system achieves scalability through:</p> <ol> <li>State machine design - Clear state transitions with well-defined locking requirements</li> <li>Embedded flags in refcount - Atomic flag manipulation without separate locks</li> <li>Per-CPU partitioning - Reduces cross-CPU cache line bouncing</li> <li>Activity-based LRU - Intelligent recycling decisions</li> <li>Lockless fast paths - Most operations use atomic CAS without locks</li> <li>Shared lock reactivation - Multiple readers can reactivate simultaneously</li> </ol> <p>Key invariants: - <code>v_refcnt &gt; 0</code> implies vnode cannot be recycled - <code>v_auxrefs &gt; 0</code> prevents <code>kfree()</code> of vnode structure - <code>VRECLAIMED</code> flag prevents reactivation - State transitions follow defined locking protocol</p>"},{"location":"sys/kern/vfs/vfs-operations/","title":"VFS Operations Framework","text":""},{"location":"sys/kern/vfs/vfs-operations/#overview","title":"Overview","text":"<p>The VFS operations framework provides a flexible, extensible architecture for implementing filesystem operations in DragonFly BSD. At its core is the VOP (Vnode Operation) dispatch mechanism, which allows different filesystem implementations to provide their own handlers for standard operations like open, read, write, and close.</p> <p>The framework consists of multiple layers:</p> <ol> <li>VOP Wrapper Layer (<code>vfs_vopops.c</code>) - High-level entry points with locking and journaling</li> <li>Journaling Layer (<code>vfs_journal.c</code>) - Optional transaction recording</li> <li>Filesystem Implementation Layer - Filesystem-specific operation handlers</li> <li>Compatibility Layer (<code>vfs_default.c</code>) - Default implementations and API translation</li> </ol> <p>This architecture enables: - Uniform interface across different filesystem types - Transparent journaling support - Gradual migration from legacy APIs - Per-mount locking strategies (MPLOCK vs fine-grained)</p> <p>Key files: - <code>sys/kern/vfs_vopops.c</code> - VOP wrapper functions and dispatch - <code>sys/kern/vfs_vnops.c</code> - High-level vnode operations - <code>sys/kern/vfs_default.c</code> - Default implementations and compatibility - <code>sys/sys/vfsops.h</code> - Operation structures and argument definitions</p>"},{"location":"sys/kern/vfs/vfs-operations/#vop-architecture","title":"VOP Architecture","text":""},{"location":"sys/kern/vfs/vfs-operations/#operation-vectors","title":"Operation Vectors","text":"<p>Every vnode has an associated <code>vop_ops</code> structure that defines handlers for filesystem operations:</p> <pre><code>struct vop_ops {\n    struct vop_generic_args **vop_ops_first_p;\n    struct vop_generic_args **vop_ops_last_p;\n\n    int (*vop_default)(struct vop_generic_args *);\n    int (*vop_old_lookup)(struct vop_old_lookup_args *);\n    int (*vop_old_create)(struct vop_old_create_args *);\n    // ... many more operations\n    int (*vop_nresolve)(struct vop_nresolve_args *);\n    int (*vop_nlookupdotdot)(struct vop_nlookupdotdot_args *);\n    int (*vop_ncreate)(struct vop_ncreate_args *);\n    int (*vop_nmkdir)(struct vop_nmkdir_args *);\n    // ... etc\n};\n</code></pre> <p>Key characteristics: - Each operation is a function pointer taking a typed argument structure - The <code>vop_default</code> handler catches unimplemented operations - Two API generations coexist: old (componentname) and new (nchandle) - Operation vectors are typically defined statically per filesystem type</p>"},{"location":"sys/kern/vfs/vfs-operations/#argument-structures","title":"Argument Structures","text":"<p>All VOP argument structures inherit from <code>vop_generic_args</code>:</p> <pre><code>struct vop_generic_args {\n    struct vop_ops *a_ops;\n    int a_reserved[3];\n    int a_desc_offset;\n};\n</code></pre> <p>Example operation-specific structures:</p> <pre><code>struct vop_open_args {\n    struct vop_ops *a_ops;\n    int a_reserved[3];\n    int a_desc_offset;\n    struct vnode *a_vp;\n    int a_mode;\n    struct ucred *a_cred;\n    struct file *a_fp;\n};\n\nstruct vop_read_args {\n    struct vop_ops *a_ops;\n    int a_reserved[3];\n    int a_desc_offset;\n    struct vnode *a_vp;\n    struct uio *a_uio;\n    int a_ioflag;\n    struct ucred *a_cred;\n};\n</code></pre> <p>Important fields: - <code>a_ops</code> - Points to the operation vector (used for dispatch) - <code>a_desc_offset</code> - Offset within vop_ops to find the correct handler - Operation-specific arguments follow the header</p>"},{"location":"sys/kern/vfs/vfs-operations/#dispatch-mechanism","title":"Dispatch Mechanism","text":"<p>VOP dispatch follows this path:</p> <ol> <li>Caller invokes wrapper (e.g., <code>vop_open()</code>)</li> <li>Wrapper handles MPLOCK if needed, sets up arguments</li> <li>Journal layer (if enabled) records operation</li> <li>Filesystem handler performs actual operation</li> <li>Return path releases locks, cleans up</li> </ol> <p>The actual dispatch uses the <code>a_desc_offset</code> to index into the <code>vop_ops</code> structure and call the appropriate handler.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vop-wrapper-functions","title":"VOP Wrapper Functions","text":""},{"location":"sys/kern/vfs/vfs-operations/#purpose-and-design","title":"Purpose and Design","text":"<p>The VOP wrapper functions in <code>vfs_vopops.c</code> provide:</p> <ol> <li>MPLOCK handling - Acquire/release Giant lock for non-MPSAFE filesystems</li> <li>Argument marshalling - Set up typed argument structures</li> <li>Journal integration - Optional operation recording</li> <li>Error checking - Validate return values</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#mplock-management","title":"MPLOCK Management","text":"<p>DragonFly supports both traditional (MPLOCK-protected) and modern (fine-grained locking) filesystems. Per-mount flags control locking behavior:</p> <p>Mount flags (from <code>sys/mount.h</code>): - <code>MNTK_MPSAFE</code> - Filesystem is fully SMP-safe - <code>MNTK_RD_MPSAFE</code> - Reads are SMP-safe, writes need MPLOCK - <code>MNTK_WR_MPSAFE</code> - Writes are SMP-safe, reads need MPLOCK - <code>MNTK_GA_MPSAFE</code> - Getattr is SMP-safe - <code>MNTK_IN_MPSAFE</code> - Inactive is SMP-safe - <code>MNTK_SG_MPSAFE</code> - Strategy is SMP-safe - <code>MNTK_NCALIASED</code> - Nchandle aliasing enabled</p> <p>Macros for MPLOCK handling:</p> <pre><code>#define VFS_MPLOCK_FLAG(MP, FLAG) \\\n    ((MP) == NULL || ((MP)-&gt;mnt_kern_flag &amp; (FLAG)))\n\n#define VFS_MPLOCK1(MP) \\\n    if (VFS_NEEDMPLOCK(MP)) get_mplock()\n\n#define VFS_MPLOCK2(MP) \\\n    if (VFS_NEEDMPLOCK(MP)) rel_mplock()\n</code></pre>"},{"location":"sys/kern/vfs/vfs-operations/#common-vop-wrappers","title":"Common VOP Wrappers","text":"<p>File access operations: - <code>vop_open()</code> - Open file/device - <code>vop_close()</code> - Close file/device - <code>vop_access()</code> - Check access permissions - <code>vop_read()</code> - Read from vnode - <code>vop_write()</code> - Write to vnode - <code>vop_ioctl()</code> - Device/file control - <code>vop_fsync()</code> - Sync file data to disk</p> <p>Namespace operations (old API): - <code>vop_old_lookup()</code> - Look up name in directory - <code>vop_old_create()</code> - Create regular file - <code>vop_old_mkdir()</code> - Create directory - <code>vop_old_rmdir()</code> - Remove directory - <code>vop_old_unlink()</code> - Remove file</p> <p>Namespace operations (new API): - <code>vop_nresolve()</code> - Resolve name to vnode (nchandle-based) - <code>vop_ncreate()</code> - Create file (nchandle-based) - <code>vop_nmkdir()</code> - Create directory (nchandle-based) - <code>vop_nremove()</code> - Remove file (nchandle-based) - <code>vop_nrmdir()</code> - Remove directory (nchandle-based) - <code>vop_nrename()</code> - Rename file (nchandle-based)</p> <p>Metadata operations: - <code>vop_getattr()</code> - Get file attributes - <code>vop_setattr()</code> - Set file attributes - <code>vop_getpages()</code> - Get VM pages for file - <code>vop_putpages()</code> - Write VM pages back</p> <p>Directory operations: - <code>vop_readdir()</code> - Read directory entries - <code>vop_readlink()</code> - Read symbolic link target</p>"},{"location":"sys/kern/vfs/vfs-operations/#example-vop_open","title":"Example: vop_open()","text":"<p>From <code>sys/kern/vfs_vopops.c:148</code>:</p> <pre><code>int\nvop_open(struct vop_ops *ops, struct vnode *vp, int mode,\n         struct ucred *cred, struct file *file)\n{\n    struct vop_open_args ap;\n    int error;\n\n    ap.a_head.a_ops = ops;\n    ap.a_head.a_desc = &amp;vop_open_desc;\n    ap.a_vp = vp;\n    ap.a_mode = mode;\n    ap.a_cred = cred;\n    ap.a_fp = file;\n\n    VFS_MPLOCK1(vp-&gt;v_mount);\n    error = vop_open_ap(&amp;ap);\n    VFS_MPLOCK2(vp-&gt;v_mount);\n\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-operations/#high-level-vnode-operations","title":"High-Level Vnode Operations","text":"<p>The file <code>sys/kern/vfs_vnops.c</code> provides high-level operations built on top of VOP primitives. These functions are used by system calls and kernel subsystems.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_open-complex-file-opening","title":"vn_open() - Complex File Opening","text":"<p>Located at <code>sys/kern/vfs_vnops.c:80</code>.</p> <p>Purpose: Open a file given a namecache path, handling permissions, device special files, and various flags.</p> <p>Key responsibilities: 1. Resolve path via namecache (<code>ncp-&gt;nc_vp</code>) 2. Check access permissions 3. Handle special cases (directories, block devices) 4. Call <code>VOP_OPEN()</code> on underlying filesystem 5. Set up sequential I/O heuristics if appropriate 6. Handle <code>O_TRUNC</code> flag for truncation after open</p> <p>Important checks: - Block opening directories for writing - Enforce read-only mounts - Handle device opens specially (pass to device driver) - Manage vnode reference counts</p> <p>Sequential I/O heuristics: - If opened with <code>FREAD | FWRITE</code> and not <code>FAPPEND</code>, sets <code>VSEQIO</code> flag - Helps buffer cache optimize for sequential access patterns</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_close-file-closing","title":"vn_close() - File Closing","text":"<p>Located at <code>sys/kern/vfs_vnops.c:229</code>.</p> <p>Purpose: Close an open file, synchronizing if needed and releasing resources.</p> <p>Operations: 1. Call <code>VOP_CLOSE()</code> on filesystem 2. Clear sequential I/O flag if set 3. Release vnode reference via <code>vrele()</code></p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_rdwr-and-vn_rdwr_inchunks-kernel-io","title":"vn_rdwr() and vn_rdwr_inchunks() - Kernel I/O","text":"<p>Located at <code>sys/kern/vfs_vnops.c:262</code> and <code>sys/kern/vfs_vnops.c:321</code>.</p> <p>Purpose: Perform read or write operations from kernel context.</p> <p>Key features: - Used for kernel-to-kernel I/O (e.g., loading executables, swap, core dumps) - <code>vn_rdwr_inchunks()</code> splits large I/O into manageable chunks (limited by <code>iosize_max()</code>) - Handles both UIO_USERSPACE and UIO_SYSSPACE addresses - Can perform synchronous I/O (<code>IO_SYNC</code> flag) - Manages file offset locking for concurrent access</p> <p>Common flags: - <code>IO_UNIT</code> - Atomic operation (all or nothing) - <code>IO_APPEND</code> - Append to end of file - <code>IO_SYNC</code> - Synchronous write (wait for disk) - <code>IO_NODELOCKED</code> - Node already locked - <code>IO_SEQMAX</code> - Maximum sequential I/O heuristic</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_read-read-from-vnode","title":"vn_read() - Read from Vnode","text":"<p>Located at <code>sys/kern/vfs_vnops.c:455</code>.</p> <p>Purpose: Read data from a vnode via VOP_READ.</p> <p>Sequential I/O detection: <pre><code>if ((fp-&gt;f_flag &amp; FSEQIO) || (vp-&gt;v_flag &amp; VSEQIO))\n    ioflag |= IO_SEQMAX;\n</code></pre></p> <p>If sequential I/O is detected, sets <code>IO_SEQMAX</code> flag to hint buffer cache to maximize read-ahead.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_write-write-to-vnode","title":"vn_write() - Write to Vnode","text":"<p>Located at <code>sys/kern/vfs_vnops.c:508</code>.</p> <p>Purpose: Write data to a vnode via VOP_WRITE.</p> <p>Key operations: 1. Check for read-only mount 2. Set <code>IO_SEQMAX</code> if sequential 3. Call <code>VOP_WRITE()</code> 4. Update access time if configured</p> <p>Mount-level write protection: <pre><code>if (vp-&gt;v_mount &amp;&amp; (vp-&gt;v_mount-&gt;mnt_flag &amp; MNT_RDONLY)) {\n    error = EROFS;\n    goto done;\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-operations/#file-offset-locking","title":"File Offset Locking","text":"<p>Located at <code>sys/kern/vfs_vnops.c:571</code> and <code>sys/kern/vfs_vnops.c:599</code>.</p> <p>Functions: - <code>vn_get_fpf_offset(struct file *fp, off_t *offset)</code> - Atomically read file offset - <code>vn_set_fpf_offset(struct file *fp, off_t offset)</code> - Atomically set file offset</p> <p>Purpose: Safely manipulate file position in multi-threaded contexts.</p> <p>Implementation: - Uses <code>spin_lock()</code> on <code>fp-&gt;f_spin</code> - Returns offset via pointer argument - Essential for concurrent file access</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_stat-get-file-statistics","title":"vn_stat() - Get File Statistics","text":"<p>Located at <code>sys/kern/vfs_vnops.c:625</code>.</p> <p>Purpose: Fill in a <code>struct stat</code> from vnode attributes.</p> <p>Operations: 1. Call <code>VOP_GETATTR()</code> to get vnode attributes 2. Translate <code>struct vattr</code> to <code>struct stat</code> 3. Handle special fields (st_dev, st_ino, st_blocks, st_blksize) 4. Compute optimal I/O size based on filesystem block size</p> <p>Device number handling: - For device special files, uses <code>vp-&gt;v_rdev</code> as <code>st_rdev</code> - Regular files use mount device number</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_ioctl-io-control","title":"vn_ioctl() - I/O Control","text":"<p>Located at <code>sys/kern/vfs_vnops.c:741</code>.</p> <p>Purpose: Perform ioctl operations on vnodes and underlying devices.</p> <p>Special handling: - <code>FIOSEEKDATA</code> / <code>FIOSEEKHOLE</code> - Sparse file support (find next data/hole) - <code>FIOASYNC</code> - Enable/disable async I/O notifications - <code>FIOSETOWN</code> / <code>FIOGETOWN</code> - Manage signal recipient for async I/O - Falls through to <code>VOP_IOCTL()</code> for filesystem-specific operations</p>"},{"location":"sys/kern/vfs/vfs-operations/#other-helper-functions","title":"Other Helper Functions","text":"<p>vn_islocked() / vn_lock() - Query and acquire vnode locks - Wrappers around <code>vn_lock_shared()</code> and <code>vn_lock_exclusive()</code></p> <p>vn_fullpath() / vn_fullpath_global() - Reconstruct full pathname from vnode - Uses namecache to traverse parent directories</p> <p>vn_touser() / vn_touser_pgcache() - Copy file data to user buffer - Used for sendfile() and similar operations</p>"},{"location":"sys/kern/vfs/vfs-operations/#old-vs-new-api","title":"Old vs New API","text":"<p>DragonFly BSD is transitioning from an older nameiop-based API to a newer nchandle-based API. The two APIs coexist for backward compatibility.</p>"},{"location":"sys/kern/vfs/vfs-operations/#old-api-componentname-based","title":"Old API (componentname-based)","text":"<p>Characteristics: - Uses <code>struct componentname</code> to represent path components - Operations like <code>vop_old_lookup()</code>, <code>vop_old_create()</code>, <code>vop_old_mkdir()</code> - Directory operations split into multiple steps - More complex locking requirements</p> <p>Example operations: <pre><code>vop_old_lookup(struct vnode *dvp, struct vnode **vpp,\n               struct componentname *cnp)\nvop_old_create(struct vnode *dvp, struct vnode **vpp,\n               struct componentname *cnp, struct vattr *vap)\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-operations/#new-api-nchandle-based","title":"New API (nchandle-based)","text":"<p>Characteristics: - Uses <code>struct nchandle</code> from namecache - Operations like <code>vop_nresolve()</code>, <code>vop_ncreate()</code>, <code>vop_nmkdir()</code> - Better integration with namecache - Cleaner locking semantics - Supports namecache aliasing</p> <p>Example operations: <pre><code>vop_nresolve(struct nchandle *nch, struct vnode *dvp,\n             struct ucred *cred)\nvop_ncreate(struct nchandle *nch, struct vnode *dvp,\n            struct vnode **vpp, struct ucred *cred,\n            struct vattr *vap)\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-operations/#api-translation","title":"API Translation","text":"<p>Modern filesystems should implement the new API. The compatibility layer in <code>vfs_default.c</code> provides automatic translation for filesystems that only implement the old API.</p> <p>Translation functions: - <code>vop_compat_nresolve()</code> \u2192 <code>vop_old_lookup()</code> - <code>vop_compat_ncreate()</code> \u2192 <code>vop_old_create()</code> - <code>vop_compat_nmkdir()</code> \u2192 <code>vop_old_mkdir()</code> - <code>vop_compat_nlink()</code> \u2192 <code>vop_old_link()</code> - <code>vop_compat_nremove()</code> \u2192 <code>vop_old_unlink()</code> - <code>vop_compat_nrmdir()</code> \u2192 <code>vop_old_rmdir()</code> - <code>vop_compat_nrename()</code> \u2192 <code>vop_old_rename()</code></p> <p>These shims extract <code>componentname</code> from <code>nchandle</code> and call the old API, then update the namecache with results.</p>"},{"location":"sys/kern/vfs/vfs-operations/#compatibility-layer","title":"Compatibility Layer","text":"<p>The file <code>sys/kern/vfs_default.c</code> provides default implementations and compatibility shims.</p>"},{"location":"sys/kern/vfs/vfs-operations/#default-operation-vector","title":"Default Operation Vector","text":"<p>Located at <code>sys/kern/vfs_default.c:110</code>:</p> <pre><code>struct vop_ops default_vnode_vops = {\n    .vop_default         = vop_defaultop,\n    .vop_old_lookup      = vop_eopnotsupp,\n    .vop_old_create      = vop_eopnotsupp,\n    .vop_open            = vop_stdopen,\n    .vop_close           = vop_stdclose,\n    .vop_access          = vop_eopnotsupp,\n    .vop_nresolve        = vop_compat_nresolve,\n    .vop_ncreate         = vop_compat_ncreate,\n    // ... many more\n};\n</code></pre> <p>Purpose: Provide fallback handlers for filesystems that don't implement all operations.</p>"},{"location":"sys/kern/vfs/vfs-operations/#error-return-functions","title":"Error Return Functions","text":"<p>vop_eopnotsupp() - Returns <code>EOPNOTSUPP</code> - Used for unimplemented optional operations - Indicates operation not supported by this filesystem</p> <p>vop_einval() - Returns <code>EINVAL</code> - Used for operations that should never be called - Indicates programming error</p> <p>vop_enotty() - Returns <code>ENOTTY</code> - Used for ioctl operations on non-tty vnodes</p>"},{"location":"sys/kern/vfs/vfs-operations/#standard-implementations","title":"Standard Implementations","text":"<p>vop_stdopen() / vop_stdclose() - Minimal open/close handlers - Just return success (0)</p> <p>vop_stdgetpages() / vop_stdputpages() - Standard VM integration - Delegates to <code>vnode_pager_generic_getpages()</code> / <code>vnode_pager_generic_putpages()</code></p> <p>vop_stdpathconf() - Returns standard pathconf values - Handles <code>_PC_LINK_MAX</code>, <code>_PC_NAME_MAX</code>, <code>_PC_PIPE_BUF</code>, etc.</p> <p>vop_stdioctl() - Handles standard ioctls - <code>FIOSEEKDATA</code> / <code>FIOSEEKHOLE</code> via <code>vop_helper_seek_hole()</code></p> <p>vop_stdmountctl() - Default mount control operations - Returns <code>EOPNOTSUPP</code> for unsupported operations</p>"},{"location":"sys/kern/vfs/vfs-operations/#compatibility-shims","title":"Compatibility Shims","text":""},{"location":"sys/kern/vfs/vfs-operations/#vop_compat_nresolve","title":"vop_compat_nresolve()","text":"<p>Located at <code>sys/kern/vfs_default.c:557</code>.</p> <p>Purpose: Translate new-style <code>vop_nresolve()</code> to old-style <code>vop_old_lookup()</code>.</p> <p>Algorithm: 1. Extract component name from nchandle 2. Allocate and populate <code>struct componentname</code> 3. Call <code>VOP_OLD_LOOKUP()</code> on parent directory 4. Update namecache with result 5. Release resources</p> <p>Key code: <pre><code>struct componentname cn;\ncn.cn_nameiop = NAMEI_LOOKUP;\ncn.cn_flags = 0;\ncn.cn_cred = ap-&gt;a_cred;\ncn.cn_nameptr = ap-&gt;a_nch-&gt;ncp-&gt;nc_name;\ncn.cn_namelen = ap-&gt;a_nch-&gt;ncp-&gt;nc_nlen;\n\nerror = VOP_OLD_LOOKUP(ap-&gt;a_dvp, &amp;vp, &amp;cn);\nif (error == 0)\n    cache_setvp(ap-&gt;a_nch, vp);\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-operations/#vop_compat_ncreate","title":"vop_compat_ncreate()","text":"<p>Located at <code>sys/kern/vfs_default.c:623</code>.</p> <p>Purpose: Translate <code>vop_ncreate()</code> to <code>vop_old_create()</code>.</p> <p>Algorithm: 1. Similar to nresolve, but uses <code>NAMEI_CREATE</code> flag 2. Calls <code>VOP_OLD_CREATE()</code> 3. Updates namecache with newly created vnode 4. Returns vnode via <code>ap-&gt;a_vpp</code></p>"},{"location":"sys/kern/vfs/vfs-operations/#vop_compat_nremove","title":"vop_compat_nremove()","text":"<p>Located at <code>sys/kern/vfs_default.c:779</code>.</p> <p>Purpose: Translate <code>vop_nremove()</code> to <code>vop_old_unlink()</code>.</p> <p>Algorithm: 1. Extract vnode from nchandle (if cached) 2. Call <code>VOP_OLD_UNLINK()</code> with parent and component name 3. Call <code>cache_unlink()</code> to remove from namecache 4. Release vnode</p> <p>Important: Must handle case where nchandle has no cached vnode yet.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vop_compat_nrename","title":"vop_compat_nrename()","text":"<p>Located at <code>sys/kern/vfs_default.c:917</code>.</p> <p>Purpose: Translate <code>vop_nrename()</code> to <code>vop_old_rename()</code>.</p> <p>Complexity: Most complex shim due to: - Four directory/file combinations (source dir/file, target dir/file) - Namecache updates for both source and target - Handling cross-directory renames - Updating parent directory links (..)</p>"},{"location":"sys/kern/vfs/vfs-operations/#vfs-standard-operations","title":"VFS Standard Operations","text":"<p>These handle filesystem-level (not vnode-level) operations:</p> <p>vfs_stdroot() - Get root vnode of filesystem vfs_stdstatfs() - Get filesystem statistics vfs_stdsync() - Sync all dirty data on filesystem vfs_stdvptofh() - Convert vnode to file handle vfs_stdfhtovp() - Convert file handle to vnode</p> <p>These are used when a filesystem doesn't provide custom implementations.</p>"},{"location":"sys/kern/vfs/vfs-operations/#fileops-integration","title":"Fileops Integration","text":"<p>VOP operations integrate with file descriptor operations via <code>struct fileops</code>. The structure <code>vnode_fileops</code> (defined in <code>sys/kern/vfs_vnops.c:1354</code>) provides the glue between file descriptor operations and VOP operations.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vnode_fileops-structure","title":"vnode_fileops Structure","text":"<pre><code>struct fileops vnode_fileops = {\n    .fo_read = vn_read,\n    .fo_write = vn_write,\n    .fo_ioctl = vn_ioctl,\n    .fo_kqfilter = vn_kqfilter,\n    .fo_stat = vn_stat,\n    .fo_close = vn_closefile,\n    .fo_shutdown = vn_shutdown\n};\n</code></pre> <p>Purpose: When a file descriptor refers to a vnode, these functions are called for file operations.</p>"},{"location":"sys/kern/vfs/vfs-operations/#mapping-fo_-to-vop_","title":"Mapping fo_ to VOP_","text":"<p>fo_read \u2192 vn_read() \u2192 VOP_READ() - Reads from file descriptor go through vnode read path</p> <p>fo_write \u2192 vn_write() \u2192 VOP_WRITE() - Writes to file descriptor go through vnode write path</p> <p>fo_ioctl \u2192 vn_ioctl() \u2192 VOP_IOCTL() - Ioctl operations on files/devices</p> <p>fo_stat \u2192 vn_stat() \u2192 VOP_GETATTR() - fstat() system call implementation</p> <p>fo_close \u2192 vn_closefile() \u2192 VOP_CLOSE() - Close file descriptor</p> <p>fo_shutdown \u2192 vn_shutdown() \u2192 VOP_SHUTDOWN() - Shutdown file descriptor (for socket-like operations)</p>"},{"location":"sys/kern/vfs/vfs-operations/#call-flow-examples","title":"Call Flow Examples","text":""},{"location":"sys/kern/vfs/vfs-operations/#opening-a-file-open-system-call","title":"Opening a File: open() System Call","text":"<ol> <li>System call handler calls <code>kern_open()</code> (<code>sys/kern/kern_descrip.c</code>)</li> <li>Namei lookup resolves path to namecache entry</li> <li>kern_open() calls <code>vn_open()</code> with nchandle</li> <li>vn_open() extracts vnode from namecache (<code>nch-&gt;ncp-&gt;nc_vp</code>)</li> <li>vn_open() performs access checks</li> <li>vn_open() calls <code>VOP_OPEN(vp-&gt;v_ops, ...)</code></li> <li>VOP wrapper handles MPLOCK, calls journal layer</li> <li>Filesystem handler (e.g., <code>hammer2_vop_open()</code>) performs open</li> <li>Return path propagates back to system call</li> <li>File descriptor set up with <code>vnode_fileops</code></li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#reading-from-a-file-read-system-call","title":"Reading from a File: read() System Call","text":"<ol> <li>System call handler calls <code>sys_read()</code> (<code>sys/kern/sys_generic.c</code>)</li> <li>sys_read() looks up file descriptor</li> <li>Calls <code>fo_read()</code> on file \u2192 <code>vnode_fileops.fo_read</code> \u2192 <code>vn_read()</code></li> <li>vn_read() sets up UIO structure with user buffer</li> <li>vn_read() detects sequential I/O, sets <code>IO_SEQMAX</code></li> <li>vn_read() calls <code>VOP_READ()</code></li> <li>VOP wrapper handles MPLOCK</li> <li>Filesystem handler (e.g., <code>hammer2_vop_read()</code>) performs read</li> <li>May call <code>cluster_read()</code> for buffer cache integration</li> <li>Data copied from kernel buffers to user space</li> <li>Return byte count to user</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#creating-a-file-open-with-o_creat","title":"Creating a File: open() with O_CREAT","text":"<ol> <li>System call handler calls <code>kern_open()</code> with <code>O_CREAT</code> flag</li> <li>Namei lookup with CREATE intent</li> <li>If file doesn't exist, calls <code>VOP_NCREATE()</code> via namecache</li> <li>VOP wrapper <code>vop_ncreate()</code> calls filesystem handler</li> <li>Filesystem (e.g., <code>hammer2_vop_ncreate()</code>) creates inode and directory entry</li> <li>Namecache updated with new vnode</li> <li>VOP_OPEN() called on newly created vnode</li> <li>File descriptor set up and returned to user</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#directory-lookup-stat-system-call","title":"Directory Lookup: stat() System Call","text":"<ol> <li>System call handler calls <code>kern_stat()</code> (<code>sys/kern/vfs_syscalls.c</code>)</li> <li>Namei lookup resolves path via namecache</li> <li>May call <code>VOP_NRESOLVE()</code> if not cached</li> <li>Filesystem handler resolves name to vnode</li> <li>vn_stat() called with vnode</li> <li>VOP_GETATTR() retrieves vnode attributes</li> <li>vn_stat() converts <code>struct vattr</code> to <code>struct stat</code></li> <li>Copy stat structure to user space</li> <li>Release vnode reference</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#filesystem-implementation-guide","title":"Filesystem Implementation Guide","text":""},{"location":"sys/kern/vfs/vfs-operations/#implementing-a-new-filesystem","title":"Implementing a New Filesystem","text":"<p>To implement a new filesystem, you need to:</p> <ol> <li>Define a <code>vop_ops</code> structure with handlers for all supported operations</li> <li>Implement new API operations (<code>vop_nresolve</code>, <code>vop_ncreate</code>, etc.)</li> <li>Mark filesystem as MPSAFE if using fine-grained locking</li> <li>Integrate with buffer cache via <code>cluster_read()</code> / <code>cluster_write()</code></li> <li>Handle reference counting properly (vnode lifecycle)</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#minimal-operation-set","title":"Minimal Operation Set","text":"<p>A minimal filesystem must implement:</p> <p>Required: - <code>vop_nresolve()</code> - Name resolution - <code>vop_nlookupdotdot()</code> - Parent directory lookup (..) - <code>vop_open()</code> / <code>vop_close()</code> - File open/close - <code>vop_read()</code> / <code>vop_write()</code> - Data I/O - <code>vop_getattr()</code> / <code>vop_setattr()</code> - Attribute access - <code>vop_reclaim()</code> - Vnode cleanup</p> <p>For writable filesystems: - <code>vop_ncreate()</code> / <code>vop_nmkdir()</code> - File/directory creation - <code>vop_nremove()</code> / <code>vop_nrmdir()</code> - File/directory removal - <code>vop_fsync()</code> - Synchronize data</p> <p>For VM integration: - <code>vop_getpages()</code> / <code>vop_putpages()</code> - Page I/O</p>"},{"location":"sys/kern/vfs/vfs-operations/#example-implementing-vop_nresolve","title":"Example: Implementing vop_nresolve()","text":"<pre><code>static int\nmyfs_vop_nresolve(struct vop_nresolve_args *ap)\n{\n    struct nchandle *nch = ap-&gt;a_nch;\n    struct vnode *dvp = ap-&gt;a_dvp;\n    struct vnode *vp;\n    int error;\n\n    /* Look up name in parent directory */\n    error = myfs_lookup_name(dvp, nch-&gt;ncp-&gt;nc_name,\n                             nch-&gt;ncp-&gt;nc_nlen, &amp;vp);\n    if (error == 0) {\n        /* Found - cache positive result */\n        cache_setvp(nch, vp);\n        vrele(vp);  /* cache holds reference */\n    } else if (error == ENOENT) {\n        /* Not found - cache negative result */\n        cache_setvp(nch, NULL);\n    }\n\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-operations/#mplock-considerations","title":"MPLOCK Considerations","text":"<p>For MPLOCK-based filesystems: - Set <code>MNTK_MPSAFE</code> to 0 during mount - VOP wrappers will automatically acquire/release MPLOCK - Simpler to implement, but less scalable</p> <p>For fine-grained locking: - Set appropriate <code>MNTK_*_MPSAFE</code> flags during mount - Use per-vnode, per-inode, or per-mount locks - More complex, but better SMP scalability - Must carefully order lock acquisition to avoid deadlock</p>"},{"location":"sys/kern/vfs/vfs-operations/#integration-checklist","title":"Integration Checklist","text":"<ul> <li>[ ] Define <code>struct vop_ops</code> with all handlers</li> <li>[ ] Implement name cache integration (nresolve, ncreate, etc.)</li> <li>[ ] Implement data I/O (read, write, strategy)</li> <li>[ ] Implement attribute operations (getattr, setattr)</li> <li>[ ] Implement VM integration (getpages, putpages)</li> <li>[ ] Handle vnode lifecycle (reclaim, inactive)</li> <li>[ ] Integrate with buffer cache (if applicable)</li> <li>[ ] Set appropriate MPLOCK flags</li> <li>[ ] Handle reference counting correctly</li> <li>[ ] Test with VFS test suite</li> </ul>"},{"location":"sys/kern/vfs/vfs-operations/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/kern/vfs/vfs-operations/#struct-vnode","title":"struct vnode","text":"<p>Defined in <code>sys/sys/vnode.h</code>. Represents a file or directory in the VFS layer.</p> <p>Key fields: - <code>v_ops</code> - Pointer to operation vector - <code>v_mount</code> - Mount point this vnode belongs to - <code>v_type</code> - Type (VREG, VDIR, VCHR, VBLK, etc.) - <code>v_flag</code> - Flags (VROOT, VSEQIO, VRECLAIMED, etc.) - <code>v_data</code> - Filesystem-private data (inode pointer) - <code>v_usecount</code> - Reference count</p>"},{"location":"sys/kern/vfs/vfs-operations/#struct-nchandle","title":"struct nchandle","text":"<p>Defined in <code>sys/sys/namecache.h</code>. Represents a namecache entry.</p> <p>Key fields: - <code>ncp</code> - Pointer to namecache entry (<code>struct namecache</code>) - <code>mount</code> - Associated mount point</p>"},{"location":"sys/kern/vfs/vfs-operations/#struct-namecache","title":"struct namecache","text":"<p>Defined in <code>sys/sys/namecache.h</code>. Represents a cached directory entry.</p> <p>Key fields: - <code>nc_vp</code> - Cached vnode (NULL for negative cache) - <code>nc_name</code> - Component name - <code>nc_nlen</code> - Name length - <code>nc_parent</code> - Parent directory nchandle</p>"},{"location":"sys/kern/vfs/vfs-operations/#struct-componentname","title":"struct componentname","text":"<p>Defined in <code>sys/sys/vnode.h</code>. Used by old API for name lookups (legacy).</p> <p>Key fields: - <code>cn_nameiop</code> - Operation (LOOKUP, CREATE, DELETE, RENAME) - <code>cn_flags</code> - Flags (FOLLOW, LOCKPARENT, etc.) - <code>cn_cred</code> - Credentials - <code>cn_nameptr</code> - Pointer to name string - <code>cn_namelen</code> - Length of name</p>"},{"location":"sys/kern/vfs/vfs-operations/#struct-vattr","title":"struct vattr","text":"<p>Defined in <code>sys/sys/vattr.h</code>. Represents file attributes.</p> <p>Key fields: - <code>va_type</code> - File type - <code>va_mode</code> - Permission bits - <code>va_uid</code> / <code>va_gid</code> - Owner/group - <code>va_size</code> - File size - <code>va_blocksize</code> - Preferred I/O block size - <code>va_atime</code> / <code>va_mtime</code> / <code>va_ctime</code> - Timestamps - <code>va_flags</code> - File flags (immutable, etc.)</p>"},{"location":"sys/kern/vfs/vfs-operations/#debugging-and-diagnostics","title":"Debugging and Diagnostics","text":""},{"location":"sys/kern/vfs/vfs-operations/#vop-call-tracing","title":"VOP Call Tracing","text":"<p>Enable VOP tracing with DDB:</p> <pre><code>db&gt; set vfs_debug_vop=1\n</code></pre> <p>This will log all VOP calls to the console.</p>"},{"location":"sys/kern/vfs/vfs-operations/#common-errors","title":"Common Errors","text":"<p>EOPNOTSUPP - Operation not supported - Filesystem doesn't implement this VOP - Check if default handler is being called</p> <p>EROFS - Read-only filesystem - Attempted write to read-only mount - Check mount flags</p> <p>ENOENT - File not found - Lookup failed - Check namecache and on-disk directory</p> <p>EINVAL - Invalid argument - Wrong operation for vnode type - Check vnode type (VREG, VDIR, etc.)</p> <p>Deadlock detection: - Use lock validation in DEBUG kernels - Check lock ordering in filesystem code</p>"},{"location":"sys/kern/vfs/vfs-operations/#useful-sysctls","title":"Useful Sysctls","text":"<pre><code>vfs.timestamp_precision - Timestamp resolution (0=sec, 1=ms, 2=us, 3=ns)\nvfs.read_max - Maximum read size\nvfs.write_max - Maximum write size\nvfs.hirunningspace - High water mark for async writes\n</code></pre>"},{"location":"sys/kern/vfs/vfs-operations/#summary","title":"Summary","text":"<p>The VFS operations framework provides a sophisticated, layered architecture for implementing filesystem operations in DragonFly BSD. Key takeaways:</p> <ol> <li>VOP dispatch provides uniform interface across filesystem types</li> <li>Wrapper layer handles locking (MPLOCK) and journaling transparently</li> <li>Two API generations coexist with automatic compatibility translation</li> <li>High-level operations (<code>vn_*</code>) built on VOP primitives</li> <li>Default implementations simplify filesystem development</li> <li>Fileops integration connects file descriptors to VOP operations</li> </ol> <p>The framework enables: - Clean separation between VFS layer and filesystem implementations - Gradual migration to modern APIs - Flexible locking strategies (MPLOCK vs fine-grained) - Transparent journaling support - Extensibility for new filesystem types</p> <p>Related documentation: - Name Cache - Pathname lookup and caching - Buffer Cache - Block buffer management - Mounting - Filesystem mounting and VFS infrastructure - VFS Overview - VFS subsystem introduction</p>"},{"location":"sys/net/","title":"Networking Subsystem","text":"<p>The DragonFly BSD networking subsystem provides the infrastructure for all network communications, from low-level interface management to high-level protocol processing.</p>"},{"location":"sys/net/#overview","title":"Overview","text":"<p>The networking code in <code>sys/net/</code> implements:</p> <ul> <li>Network interface management - Interface lifecycle, configuration, and I/O</li> <li>Protocol dispatch - Per-CPU network ISR threads for scalability</li> <li>Routing - Route lookup, management, and routing sockets</li> <li>Packet filtering - pfil hooks, pf, and IPFW firewalls</li> <li>Traffic shaping - ALTQ and dummynet QoS</li> <li>Virtual interfaces - tun, tap, VLAN, bridge, lagg</li> <li>Tunneling - GIF, GRE, WireGuard</li> </ul>"},{"location":"sys/net/#architecture","title":"Architecture","text":"<p>DragonFly's networking stack has several distinguishing features:</p>"},{"location":"sys/net/#per-cpu-protocol-threads","title":"Per-CPU Protocol Threads","text":"<p>Unlike traditional BSD networking which processes packets in interrupt context, DragonFly uses dedicated per-CPU threads for protocol processing. The network ISR (<code>netisr</code>) system dispatches packets to the appropriate CPU based on flow hashing.</p>"},{"location":"sys/net/#message-based-design","title":"Message-Based Design","text":"<p>Network operations use LWKT messages for communication between subsystems, enabling lock-free packet processing paths in many cases.</p>"},{"location":"sys/net/#interface-model","title":"Interface Model","text":"<p>Network interfaces are represented by <code>struct ifnet</code>, which contains:</p> <ul> <li>Interface identification (name, unit, index)</li> <li>Hardware capabilities and flags  </li> <li>Input/output queues</li> <li>Statistics counters</li> <li>Method vectors for driver operations</li> </ul>"},{"location":"sys/net/#source-organization","title":"Source Organization","text":""},{"location":"sys/net/#core-files","title":"Core Files","text":"Component Files Description Interface Core <code>if.c</code>, <code>if_var.h</code> Interface management Network ISR <code>netisr.c</code>, <code>netisr.h</code> Protocol dispatch Routing <code>route.c</code>, <code>rtsock.c</code> Route management Ethernet <code>if_ethersubr.c</code> Ethernet support BPF <code>bpf.c</code> Packet capture Packet Filter Hooks <code>pfil.c</code> Filter framework"},{"location":"sys/net/#firewalls","title":"Firewalls","text":"Component Directory Description Packet Filter (pf) <code>pf/</code> OpenBSD firewall IPFW <code>ipfw/</code> FreeBSD firewall IPFW3 <code>ipfw3*/</code> DragonFly modular firewall"},{"location":"sys/net/#traffic-shaping","title":"Traffic Shaping","text":"Component Directory Description ALTQ <code>altq/</code> Alternate queueing Dummynet <code>dummynet/</code>, <code>dummynet3/</code> Traffic shaping"},{"location":"sys/net/#virtual-interfaces","title":"Virtual Interfaces","text":"Component Directory Description TUN/TAP <code>tun/</code>, <code>tap/</code> Virtual interfaces VLAN <code>vlan/</code> 802.1Q VLANs Bridge <code>bridge/</code> Layer 2 bridging Link Aggregation <code>lagg/</code> LACP/failover Loopback <code>if_loop.c</code> Loopback interface"},{"location":"sys/net/#tunneling","title":"Tunneling","text":"Component Directory Description GIF <code>gif/</code> Generic tunnel GRE <code>gre/</code> GRE tunneling WireGuard <code>wg/</code> WireGuard VPN 6to4 <code>stf/</code> IPv6 transition"},{"location":"sys/net/#high-performance-io","title":"High-Performance I/O","text":"Component Directory Description Netmap <code>netmap/</code> Zero-copy packet I/O Polling <code>if_poll.c</code> Device polling"},{"location":"sys/net/#serialppp","title":"Serial/PPP","text":"Component Directory Description SPPP <code>sppp/</code> Synchronous PPP SLIP <code>sl/</code> Serial Line IP"},{"location":"sys/net/#multicast","title":"Multicast","text":"Component Directory Description IP Multicast <code>ip_mroute/</code> Multicast routing"},{"location":"sys/net/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/net/#struct-ifnet","title":"struct ifnet","text":"<p>The central network interface structure:</p> <pre><code>struct ifnet {\n    char    if_xname[IFNAMSIZ];     /* external name */\n    TAILQ_ENTRY(ifnet) if_link;    /* all interfaces */\n\n    /* Identification */\n    u_short if_index;               /* interface index */\n    u_short if_type;                /* IFT_* type */\n\n    /* Capabilities and flags */\n    int     if_flags;               /* IFF_* flags */\n    int     if_capabilities;        /* IFCAP_* */\n    int     if_capenable;           /* enabled capabilities */\n\n    /* I/O */\n    struct  ifqueue if_snd;         /* output queue */\n    int     (*if_output)();         /* output routine */\n    void    (*if_input)();          /* input routine */\n    int     (*if_ioctl)();          /* ioctl routine */\n    void    (*if_start)();          /* initiate output */\n\n    /* Statistics */\n    u_long  if_ipackets;            /* packets received */\n    u_long  if_opackets;            /* packets sent */\n    ...\n};\n</code></pre>"},{"location":"sys/net/#struct-route","title":"struct route","text":"<p>Route lookup structure:</p> <pre><code>struct route {\n    struct rtentry *ro_rt;          /* resolved route */\n    struct sockaddr ro_dst;         /* destination address */\n};\n</code></pre>"},{"location":"sys/net/#struct-rtentry","title":"struct rtentry","text":"<p>Routing table entry:</p> <pre><code>struct rtentry {\n    struct radix_node rt_nodes[2];  /* tree linkage */\n    struct sockaddr *rt_gateway;    /* gateway address */\n    u_long  rt_flags;               /* RTF_* flags */\n    struct  ifnet *rt_ifp;          /* output interface */\n    struct  rtentry *rt_gwroute;    /* gateway route */\n    struct  rt_metrics rt_rmx;      /* route metrics */\n    ...\n};\n</code></pre>"},{"location":"sys/net/#packet-flow","title":"Packet Flow","text":""},{"location":"sys/net/#receive-path","title":"Receive Path","text":"<pre><code>Hardware interrupt\n    |\n    v\nDriver receive handler\n    |\n    v\nether_input() / interface input\n    |\n    v\nnetisr_dispatch() -&gt; per-CPU protocol thread\n    |\n    v\nip_input() / ip6_input() / etc.\n    |\n    v\nProtocol processing (TCP/UDP/etc.)\n    |\n    v\nSocket receive buffer\n</code></pre>"},{"location":"sys/net/#transmit-path","title":"Transmit Path","text":"<pre><code>Socket send\n    |\n    v\nProtocol output (tcp_output, udp_output)\n    |\n    v\nip_output() / ip6_output()\n    |\n    v\nRoute lookup\n    |\n    v\nether_output() / interface output\n    |\n    v\nInterface queue (if_snd)\n    |\n    v\nDriver transmit (if_start)\n    |\n    v\nHardware\n</code></pre>"},{"location":"sys/net/#related-documentation","title":"Related Documentation","text":"<ul> <li>Sockets - Socket layer implementation</li> <li>Mbufs - Network buffer management</li> <li>Protocol Dispatch - Protocol registration</li> </ul>"},{"location":"sys/net/#source-reference","title":"Source Reference","text":"<ul> <li>Source: <code>sys/net/</code></li> <li>Headers: <code>sys/net/*.h</code></li> <li>Related: <code>sys/netinet/</code> (IPv4), <code>sys/netinet6/</code> (IPv6)</li> </ul>"},{"location":"sys/net/interfaces/","title":"Network Interfaces","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation will be added following the reading plan.</p>"},{"location":"sys/net/interfaces/#overview","title":"Overview","text":"<p>Network interface management is the foundation of the DragonFly BSD networking stack. The <code>if.c</code> and related files implement interface lifecycle management, configuration, and the interface abstraction layer.</p>"},{"location":"sys/net/interfaces/#key-components","title":"Key Components","text":""},{"location":"sys/net/interfaces/#struct-ifnet","title":"struct ifnet","text":"<p>The central interface structure containing:</p> <ul> <li>Interface identification (name, index, type)</li> <li>Capability flags and enabled features</li> <li>Input/output method vectors</li> <li>Transmit queue management</li> <li>Statistics counters</li> </ul>"},{"location":"sys/net/interfaces/#interface-operations","title":"Interface Operations","text":"<ul> <li>Attach/Detach - Interface registration and removal</li> <li>Ioctl - Configuration interface (SIOC* commands)</li> <li>Input/Output - Packet handling methods</li> <li>Cloning - Dynamic interface creation (tun, tap, vlan, etc.)</li> </ul>"},{"location":"sys/net/interfaces/#source-files","title":"Source Files","text":"File Lines Description <code>if.c</code> 4,208 Interface management core <code>if.h</code> 391 Public interface definitions <code>if_var.h</code> 1,004 Internal structures <code>if_clone.c</code> 405 Interface cloning <code>if_clone.h</code> 132 Clone definitions <code>ifq_var.h</code> 639 Interface queue definitions <code>if_types.h</code> 253 Interface type constants"},{"location":"sys/net/interfaces/#key-functions","title":"Key Functions","text":"<pre><code>/* Interface lifecycle */\nvoid if_attach(struct ifnet *ifp);\nvoid if_detach(struct ifnet *ifp);\n\n/* Interface lookup */\nstruct ifnet *ifunit(const char *name);\nstruct ifnet *ifindex2ifnet(int idx);\n\n/* Output */\nint if_output(struct ifnet *ifp, struct mbuf *m, ...);\n\n/* Cloning */\nint if_clone_create(const char *name, int unit);\nint if_clone_destroy(const char *name);\n</code></pre>"},{"location":"sys/net/interfaces/#interface-flags","title":"Interface Flags","text":"<p>Common <code>IFF_*</code> flags:</p> Flag Description <code>IFF_UP</code> Interface is up <code>IFF_BROADCAST</code> Supports broadcast <code>IFF_LOOPBACK</code> Loopback interface <code>IFF_RUNNING</code> Resources allocated <code>IFF_PROMISC</code> Promiscuous mode <code>IFF_MULTICAST</code> Supports multicast"},{"location":"sys/net/interfaces/#related-documentation","title":"Related Documentation","text":"<ul> <li>Ethernet - Ethernet-specific support</li> <li>Loopback - Loopback interface</li> <li>VLAN - VLAN interfaces</li> </ul>"},{"location":"sys/net/interfaces/#source-reference","title":"Source Reference","text":"<ul> <li>Source: <code>sys/net/if.c</code></li> <li>Headers: <code>sys/net/if.h</code>, <code>sys/net/if_var.h</code></li> </ul>"},{"location":"sys/net/netisr/","title":"Network ISR (netisr)","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation will be added following the reading plan.</p>"},{"location":"sys/net/netisr/#overview","title":"Overview","text":"<p>The Network ISR (Interrupt Service Routine) subsystem provides DragonFly BSD's scalable packet dispatch mechanism. Unlike traditional BSD systems that process packets in interrupt context, DragonFly uses per-CPU kernel threads for protocol processing.</p>"},{"location":"sys/net/netisr/#architecture","title":"Architecture","text":""},{"location":"sys/net/netisr/#per-cpu-protocol-threads","title":"Per-CPU Protocol Threads","text":"<p>Each CPU has dedicated threads for network protocol processing:</p> <ul> <li>Packets are dispatched to CPUs based on flow hashing</li> <li>Enables parallel processing of independent flows</li> <li>Reduces lock contention in protocol stacks</li> </ul>"},{"location":"sys/net/netisr/#message-based-dispatch","title":"Message-Based Dispatch","text":"<p>Network operations use LWKT messages:</p> <ul> <li><code>netmsg</code> structures carry packet and control information</li> <li>Asynchronous dispatch to target CPU</li> <li>Ordered delivery within a flow</li> </ul>"},{"location":"sys/net/netisr/#source-files","title":"Source Files","text":"File Lines Description <code>netisr.c</code> 846 Network ISR implementation <code>netisr.h</code> 239 Public definitions <code>netisr2.h</code> 239 Extended definitions <code>netmsg.h</code> 341 Message structures <code>netmsg2.h</code> 102 Extended message definitions"},{"location":"sys/net/netisr/#key-functions","title":"Key Functions","text":"<pre><code>/* Protocol registration */\nvoid netisr_register(int proto, netisr_fn_t func, netisr_hashfn_t hashfn);\n\n/* Packet dispatch */\nvoid netisr_dispatch(int proto, struct mbuf *m);\nvoid netisr_queue(int proto, struct mbuf *m);\n\n/* CPU targeting */\nvoid netisr_cpuport(int proto, int cpu);\n</code></pre>"},{"location":"sys/net/netisr/#protocol-numbers","title":"Protocol Numbers","text":"<p>Common <code>NETISR_*</code> protocol identifiers:</p> Protocol Description <code>NETISR_IP</code> IPv4 <code>NETISR_ARP</code> ARP <code>NETISR_IP6</code> IPv6 <code>NETISR_ETHER</code> Ethernet"},{"location":"sys/net/netisr/#flow-hashing","title":"Flow Hashing","text":"<p>Packets are distributed across CPUs using:</p> <ul> <li>Source/destination IP addresses</li> <li>Source/destination ports (TCP/UDP)</li> <li>Toeplitz hash for RSS-capable hardware</li> </ul>"},{"location":"sys/net/netisr/#related-documentation","title":"Related Documentation","text":"<ul> <li>LWKT - Lightweight kernel threads</li> <li>Interfaces - Network interface layer</li> </ul>"},{"location":"sys/net/netisr/#source-reference","title":"Source Reference","text":"<ul> <li>Source: <code>sys/net/netisr.c</code></li> <li>Headers: <code>sys/net/netisr.h</code>, <code>sys/net/netmsg.h</code></li> </ul>"},{"location":"sys/vfs/","title":"Filesystem Implementations (<code>sys/vfs/</code>)","text":"<p>This section documents the filesystem implementations in DragonFly BSD. The <code>sys/vfs/</code> directory contains 23 filesystems totaling approximately 189,000 lines of code.</p>"},{"location":"sys/vfs/#overview","title":"Overview","text":"<p>DragonFly BSD supports a variety of filesystems for different purposes:</p> <ul> <li>Native filesystems \u2014 HAMMER2 (current) and HAMMER (legacy), developed specifically for DragonFly</li> <li>Pseudo-filesystems \u2014 Virtual filesystems like devfs, procfs, and tmpfs</li> <li>Traditional Unix \u2014 UFS/FFS and NFS for compatibility</li> <li>Compatibility \u2014 Support for ext2, FAT, NTFS, ISO9660, and others</li> </ul> <p>All filesystems integrate with the kernel through the VFS (Virtual File System) layer documented in VFS Core.</p>"},{"location":"sys/vfs/#filesystem-summary","title":"Filesystem Summary","text":""},{"location":"sys/vfs/#dragonfly-native-filesystems","title":"DragonFly-Native Filesystems","text":"Filesystem Lines Description HAMMER2 ~40,800 Current native filesystem with clustering, dedup, compression HAMMER ~34,600 Legacy native filesystem"},{"location":"sys/vfs/#pseudovirtual-filesystems","title":"Pseudo/Virtual Filesystems","text":"Filesystem Lines Description devfs ~6,400 Device filesystem \u2014 dynamic <code>/dev</code> management procfs ~3,800 Process filesystem \u2014 <code>/proc</code> interface tmpfs ~5,000 Memory-based temporary filesystem nullfs ~770 Null/loopback filesystem fifofs ~750 FIFO (named pipe) filesystem deadfs ~210 Dead vnode operations mfs ~700 Memory filesystem (legacy)"},{"location":"sys/vfs/#traditional-unix-filesystems","title":"Traditional Unix Filesystems","text":"Filesystem Lines Description UFS ~19,400 BSD Unix File System (FFS) NFS ~24,700 Network File System client/server"},{"location":"sys/vfs/#compatibility-filesystems","title":"Compatibility Filesystems","text":"Filesystem Lines Description ext2fs ~12,400 Linux ext2/ext3 support msdosfs ~8,100 FAT12/FAT16/FAT32 ntfs ~4,700 Windows NTFS (read-only) isofs ~4,600 ISO 9660 CD-ROM udf ~3,000 Universal Disk Format hpfs ~4,500 OS/2 HPFS"},{"location":"sys/vfs/#networkdistributed-filesystems","title":"Network/Distributed Filesystems","text":"Filesystem Lines Description smbfs ~4,700 SMB/CIFS client fuse ~5,500 Filesystem in Userspace"},{"location":"sys/vfs/#specialized-filesystems","title":"Specialized Filesystems","text":"Filesystem Lines Description autofs ~1,800 Automounter filesystem dirfs ~3,100 Directory filesystem (vkernel)"},{"location":"sys/vfs/#source-location","title":"Source Location","text":"<ul> <li>Source path: <code>sys/vfs/</code></li> <li>Each filesystem: <code>sys/vfs/&lt;name&gt;/</code></li> </ul>"},{"location":"sys/vfs/#see-also","title":"See Also","text":"<ul> <li>VFS Core \u2014 VFS layer implementation</li> <li>Buffer Cache \u2014 Block I/O caching</li> <li>Mounting \u2014 Mount operations</li> </ul>"},{"location":"sys/vfs/autofs/","title":"Automounter Filesystem (autofs)","text":"<p>autofs provides automatic, on-demand filesystem mounting.</p>"},{"location":"sys/vfs/autofs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/autofs/</code> (~1,800 lines)</p> <p>autofs enables automatic mounting:</p> <ul> <li>On-demand \u2014 Mount when accessed</li> <li>Timeout unmount \u2014 Unmount after idle period</li> <li>Daemon integration \u2014 Works with automountd</li> </ul>"},{"location":"sys/vfs/autofs/#source-files","title":"Source Files","text":"<ul> <li><code>autofs_vnops.c</code> \u2014 Vnode operations</li> <li><code>autofs_vfsops.c</code> \u2014 VFS operations</li> <li><code>autofs_ioctl.c</code> \u2014 Daemon communication</li> </ul>"},{"location":"sys/vfs/autofs/#see-also","title":"See Also","text":"<ul> <li>Mounting \u2014 Mount operations</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/deadfs/","title":"Dead Filesystem (deadfs)","text":"<p>deadfs provides vnode operations for revoked or dead vnodes.</p>"},{"location":"sys/vfs/deadfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/deadfs/</code> (~210 lines)</p> <p>deadfs is the simplest filesystem \u2014 it handles vnodes that have been revoked or are otherwise invalid:</p> <ul> <li>Error returns \u2014 All operations return appropriate errors</li> <li>Revoked vnodes \u2014 Used after <code>revoke(2)</code> system call</li> <li>Cleanup \u2014 Safe operations on dead file handles</li> </ul>"},{"location":"sys/vfs/deadfs/#source-files","title":"Source Files","text":"<ul> <li><code>dead_vnops.c</code> \u2014 Vnode operations (all fail gracefully)</li> </ul>"},{"location":"sys/vfs/deadfs/#see-also","title":"See Also","text":"<ul> <li>VFS Operations \u2014 Vnode operations</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/devfs/","title":"Device Filesystem (devfs)","text":"<p>devfs provides dynamic device node management for <code>/dev</code>.</p>"},{"location":"sys/vfs/devfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/devfs/</code> (~6,400 lines)</p> <p>devfs dynamically creates and manages device nodes:</p> <ul> <li>Dynamic creation \u2014 Nodes created/removed as devices attach/detach</li> <li>Rules system \u2014 Configurable permissions and ownership</li> <li>Per-jail support \u2014 Isolated device namespaces</li> </ul>"},{"location":"sys/vfs/devfs/#source-files","title":"Source Files","text":"<ul> <li><code>devfs_core.c</code> \u2014 Core device node management</li> <li><code>devfs_vnops.c</code> \u2014 Vnode operations</li> <li><code>devfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>devfs_rules.c</code> \u2014 Permission rules</li> </ul>"},{"location":"sys/vfs/devfs/#see-also","title":"See Also","text":"<ul> <li>Devices \u2014 Device framework</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/dirfs/","title":"Directory Filesystem (dirfs)","text":"<p>dirfs provides pass-through filesystem access for the virtual kernel (vkernel).</p>"},{"location":"sys/vfs/dirfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/dirfs/</code> (~3,100 lines)</p> <p>dirfs enables vkernel filesystem access:</p> <ul> <li>Vkernel support \u2014 Used by DragonFly's virtual kernel</li> <li>Host passthrough \u2014 Access host filesystem from vkernel</li> <li>Development \u2014 Useful for kernel development/testing</li> </ul>"},{"location":"sys/vfs/dirfs/#source-files","title":"Source Files","text":"<ul> <li><code>dirfs_vnops.c</code> \u2014 Vnode operations</li> <li><code>dirfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>dirfs_subr.c</code> \u2014 Support routines</li> </ul>"},{"location":"sys/vfs/dirfs/#see-also","title":"See Also","text":"<ul> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/ext2fs/","title":"Linux ext2/ext3 Filesystem (ext2fs)","text":"<p>ext2fs provides read/write support for Linux ext2 and ext3 filesystems.</p>"},{"location":"sys/vfs/ext2fs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/ext2fs/</code> (~12,400 lines)</p> <p>ext2fs enables Linux filesystem compatibility:</p> <ul> <li>ext2 support \u2014 Full read/write</li> <li>ext3 support \u2014 Read/write (journal ignored)</li> <li>Linux compatibility \u2014 Access Linux partitions</li> </ul>"},{"location":"sys/vfs/ext2fs/#source-files","title":"Source Files","text":"<ul> <li><code>ext2fs_vnops.c</code> \u2014 Vnode operations</li> <li><code>ext2fs_vfsops.c</code> \u2014 VFS operations</li> <li><code>ext2fs_inode.c</code> \u2014 Inode handling</li> <li><code>ext2fs_lookup.c</code> \u2014 Directory operations</li> <li><code>ext2fs_balloc.c</code> \u2014 Block allocation</li> </ul>"},{"location":"sys/vfs/ext2fs/#see-also","title":"See Also","text":"<ul> <li>UFS \u2014 Native BSD filesystem</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/fifofs/","title":"FIFO Filesystem (fifofs)","text":"<p>fifofs implements named pipes (FIFOs) as a filesystem layer.</p>"},{"location":"sys/vfs/fifofs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/fifofs/</code> (~750 lines)</p> <p>fifofs provides:</p> <ul> <li>Named pipes \u2014 FIFO special files</li> <li>VFS integration \u2014 FIFOs as vnode type</li> <li>Blocking I/O \u2014 Reader/writer synchronization</li> </ul>"},{"location":"sys/vfs/fifofs/#source-files","title":"Source Files","text":"<ul> <li><code>fifo_vnops.c</code> \u2014 Vnode operations for FIFOs</li> </ul>"},{"location":"sys/vfs/fifofs/#see-also","title":"See Also","text":"<ul> <li>Pipes \u2014 Pipe implementation</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/fuse/","title":"Filesystem in Userspace (FUSE)","text":"<p>fuse enables userspace programs to implement filesystems.</p>"},{"location":"sys/vfs/fuse/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/fuse/</code> (~5,500 lines)</p> <p>fuse provides a kernel-userspace filesystem interface:</p> <ul> <li>Userspace filesystems \u2014 Implement FS in user programs</li> <li>FUSE protocol \u2014 Standard Linux FUSE compatibility</li> <li>Flexibility \u2014 Any storage backend possible</li> </ul>"},{"location":"sys/vfs/fuse/#architecture","title":"Architecture","text":"<pre><code>Application \u2192 VFS \u2192 FUSE kernel module \u2192 /dev/fuse \u2192 Userspace daemon\n</code></pre>"},{"location":"sys/vfs/fuse/#source-files","title":"Source Files","text":"<ul> <li><code>fuse_vnops.c</code> \u2014 Vnode operations</li> <li><code>fuse_vfsops.c</code> \u2014 VFS operations</li> <li><code>fuse_ipc.c</code> \u2014 Kernel-userspace communication</li> <li><code>fuse_device.c</code> \u2014 <code>/dev/fuse</code> device</li> </ul>"},{"location":"sys/vfs/fuse/#see-also","title":"See Also","text":"<ul> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/hammer/","title":"HAMMER Filesystem","text":"<p>HAMMER is DragonFly BSD's legacy native filesystem, predecessor to HAMMER2.</p>"},{"location":"sys/vfs/hammer/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/hammer/</code> (~34,600 lines)</p> <p>HAMMER was DragonFly's first native filesystem, featuring:</p> <ul> <li>B-tree based \u2014 Efficient on-disk organization</li> <li>History retention \u2014 Built-in historical snapshots</li> <li>Mirroring \u2014 Native filesystem mirroring</li> <li>Large filesystem support \u2014 Multi-terabyte volumes</li> </ul>"},{"location":"sys/vfs/hammer/#key-features","title":"Key Features","text":"Feature Description Design B-tree based History Automatic retention with pruning Mirroring Streaming replication Max size Up to 1 exabyte (theoretical)"},{"location":"sys/vfs/hammer/#source-files","title":"Source Files","text":"<p>Key source files in <code>sys/vfs/hammer/</code>:</p> <ul> <li><code>hammer.h</code> \u2014 Main header</li> <li><code>hammer_vfsops.c</code> \u2014 VFS operations</li> <li><code>hammer_vnops.c</code> \u2014 Vnode operations</li> <li><code>hammer_btree.c</code> \u2014 B-tree implementation</li> <li><code>hammer_ondisk.c</code> \u2014 On-disk format</li> <li><code>hammer_transaction.c</code> \u2014 Transaction handling</li> <li><code>hammer_mirror.c</code> \u2014 Mirroring support</li> <li><code>hammer_prune.c</code> \u2014 History pruning</li> </ul>"},{"location":"sys/vfs/hammer/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 \u2014 Current native filesystem</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/hammer2/","title":"HAMMER2 Filesystem","text":"<p>HAMMER2 is DragonFly BSD's current native filesystem, designed for modern storage with advanced features.</p>"},{"location":"sys/vfs/hammer2/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/hammer2/</code> (~40,800 lines)</p> <p>HAMMER2 is a high-performance, modern filesystem featuring:</p> <ul> <li>Copy-on-write \u2014 All modifications create new blocks</li> <li>Built-in compression \u2014 LZ4 and zlib support</li> <li>Deduplication \u2014 Block-level dedup</li> <li>Snapshots \u2014 Instant, space-efficient snapshots</li> <li>Clustering \u2014 Multi-master clustering support</li> <li>Checksumming \u2014 Data integrity verification</li> </ul>"},{"location":"sys/vfs/hammer2/#key-features","title":"Key Features","text":"Feature Description Block size Variable, up to 64KB Compression LZ4 (fast), zlib (better ratio) Checksums Per-block integrity checks Snapshots Copy-on-write based Clustering Planned multi-node support"},{"location":"sys/vfs/hammer2/#source-files","title":"Source Files","text":"<p>Key source files in <code>sys/vfs/hammer2/</code>:</p> <ul> <li><code>hammer2.h</code> \u2014 Main header and structures</li> <li><code>hammer2_vfsops.c</code> \u2014 VFS operations</li> <li><code>hammer2_vnops.c</code> \u2014 Vnode operations</li> <li><code>hammer2_chain.c</code> \u2014 Chain topology management</li> <li><code>hammer2_inode.c</code> \u2014 Inode operations</li> <li><code>hammer2_freemap.c</code> \u2014 Free space management</li> <li><code>hammer2_io.c</code> \u2014 I/O subsystem</li> <li><code>hammer2_flush.c</code> \u2014 Flush/sync operations</li> <li><code>hammer2_lz4.c</code> \u2014 LZ4 compression</li> <li><code>hammer2_zlib.c</code> \u2014 Zlib compression</li> </ul>"},{"location":"sys/vfs/hammer2/#see-also","title":"See Also","text":"<ul> <li>HAMMER \u2014 Legacy HAMMER filesystem</li> <li>VFS Overview \u2014 Filesystem implementations</li> <li>VFS Core \u2014 VFS layer</li> </ul>"},{"location":"sys/vfs/hpfs/","title":"OS/2 HPFS Filesystem","text":"<p>hpfs provides support for OS/2's High Performance File System.</p>"},{"location":"sys/vfs/hpfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/hpfs/</code> (~4,500 lines)</p> <p>hpfs enables access to OS/2 HPFS volumes:</p> <ul> <li>Legacy \u2014 OS/2 compatibility</li> <li>Read-only \u2014 Limited write support</li> <li>B+ tree \u2014 Directory structure</li> </ul>"},{"location":"sys/vfs/hpfs/#source-files","title":"Source Files","text":"<ul> <li><code>hpfs_vnops.c</code> \u2014 Vnode operations</li> <li><code>hpfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>hpfs_subr.c</code> \u2014 Support routines</li> </ul>"},{"location":"sys/vfs/hpfs/#see-also","title":"See Also","text":"<ul> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/isofs/","title":"ISO 9660 Filesystem (isofs)","text":"<p>isofs provides support for ISO 9660 CD-ROM/DVD filesystems.</p>"},{"location":"sys/vfs/isofs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/isofs/</code> (~4,600 lines)</p> <p>isofs enables CD/DVD access:</p> <ul> <li>ISO 9660 \u2014 Standard CD-ROM format</li> <li>Rock Ridge \u2014 Unix extensions (permissions, symlinks)</li> <li>Joliet \u2014 Unicode filename support</li> </ul>"},{"location":"sys/vfs/isofs/#source-files","title":"Source Files","text":"<p>Files in <code>sys/vfs/isofs/cd9660/</code>:</p> <ul> <li><code>cd9660_vnops.c</code> \u2014 Vnode operations</li> <li><code>cd9660_vfsops.c</code> \u2014 VFS operations</li> <li><code>cd9660_node.c</code> \u2014 ISO node handling</li> <li><code>cd9660_rrip.c</code> \u2014 Rock Ridge extensions</li> </ul>"},{"location":"sys/vfs/isofs/#see-also","title":"See Also","text":"<ul> <li>UDF \u2014 DVD/Blu-ray filesystem</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/mfs/","title":"Memory Filesystem (mfs)","text":"<p>mfs is a legacy memory-based filesystem using a block device interface.</p>"},{"location":"sys/vfs/mfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/mfs/</code> (~700 lines)</p> <p>mfs provides a memory-backed block device:</p> <ul> <li>Legacy \u2014 Older approach, tmpfs preferred for new uses</li> <li>Block-based \u2014 Presents as block device to UFS</li> <li>Fixed size \u2014 Allocated at mount time</li> </ul>"},{"location":"sys/vfs/mfs/#source-files","title":"Source Files","text":"<ul> <li><code>mfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>mfs_vnops.c</code> \u2014 Vnode operations</li> </ul>"},{"location":"sys/vfs/mfs/#see-also","title":"See Also","text":"<ul> <li>tmpfs \u2014 Modern memory filesystem</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/msdosfs/","title":"FAT Filesystem (msdosfs)","text":"<p>msdosfs provides support for FAT12, FAT16, and FAT32 filesystems.</p>"},{"location":"sys/vfs/msdosfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/msdosfs/</code> (~8,100 lines)</p> <p>msdosfs enables DOS/Windows FAT compatibility:</p> <ul> <li>FAT12/16/32 \u2014 All FAT variants supported</li> <li>Long filenames \u2014 VFAT long filename support</li> <li>USB drives \u2014 Common for removable media</li> </ul>"},{"location":"sys/vfs/msdosfs/#source-files","title":"Source Files","text":"<ul> <li><code>msdosfs_vnops.c</code> \u2014 Vnode operations</li> <li><code>msdosfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>msdosfs_fat.c</code> \u2014 FAT table handling</li> <li><code>msdosfs_denode.c</code> \u2014 Directory entry nodes</li> <li><code>msdosfs_lookup.c</code> \u2014 Directory lookup</li> </ul>"},{"location":"sys/vfs/msdosfs/#see-also","title":"See Also","text":"<ul> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/nfs/","title":"Network File System (NFS)","text":"<p>NFS provides network-transparent file access using Sun's NFS protocol.</p>"},{"location":"sys/vfs/nfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/nfs/</code> (~24,700 lines)</p> <p>NFS implements both client and server:</p> <ul> <li>NFSv2/v3 \u2014 Standard NFS protocol versions</li> <li>RPC-based \u2014 Uses Sun RPC for communication</li> <li>Caching \u2014 Client-side caching with cache coherency</li> <li>Locking \u2014 NLM (Network Lock Manager) support</li> </ul>"},{"location":"sys/vfs/nfs/#source-files","title":"Source Files","text":"<p>Key files in <code>sys/vfs/nfs/</code>:</p> <ul> <li><code>nfs_vnops.c</code> \u2014 Client vnode operations</li> <li><code>nfs_vfsops.c</code> \u2014 Client VFS operations</li> <li><code>nfs_socket.c</code> \u2014 RPC/network handling</li> <li><code>nfs_bio.c</code> \u2014 Client buffer I/O</li> <li><code>nfs_serv.c</code> \u2014 Server operations</li> <li><code>nfs_subs.c</code> \u2014 Support routines</li> </ul>"},{"location":"sys/vfs/nfs/#see-also","title":"See Also","text":"<ul> <li>Sockets \u2014 Network communication</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/ntfs/","title":"NTFS Filesystem","text":"<p>ntfs provides read-only support for Windows NTFS filesystems.</p>"},{"location":"sys/vfs/ntfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/ntfs/</code> (~4,700 lines)</p> <p>ntfs enables Windows NTFS access:</p> <ul> <li>Read-only \u2014 No write support</li> <li>MFT-based \u2014 Master File Table structure</li> <li>Compression \u2014 Basic compressed file support</li> </ul>"},{"location":"sys/vfs/ntfs/#source-files","title":"Source Files","text":"<ul> <li><code>ntfs_vnops.c</code> \u2014 Vnode operations</li> <li><code>ntfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>ntfs_subr.c</code> \u2014 Support routines</li> <li><code>ntfs_compr.c</code> \u2014 Compression handling</li> </ul>"},{"location":"sys/vfs/ntfs/#see-also","title":"See Also","text":"<ul> <li>msdosfs \u2014 FAT filesystem (read/write)</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/nullfs/","title":"Null Filesystem (nullfs)","text":"<p>nullfs provides a loopback/stacking layer to mount directories elsewhere.</p>"},{"location":"sys/vfs/nullfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/nullfs/</code> (~770 lines)</p> <p>nullfs allows mounting a directory tree at another location:</p> <ul> <li>Loopback mount \u2014 Expose directory at alternate path</li> <li>Filesystem stacking \u2014 Layers on top of underlying filesystem</li> <li>Jail support \u2014 Commonly used for jail filesystem setup</li> </ul>"},{"location":"sys/vfs/nullfs/#usage","title":"Usage","text":"<pre><code>mount_null /usr/src /jail/usr/src\n</code></pre>"},{"location":"sys/vfs/nullfs/#source-files","title":"Source Files","text":"<ul> <li><code>null_vnops.c</code> \u2014 Vnode operations (pass-through)</li> <li><code>null_vfsops.c</code> \u2014 VFS operations</li> <li><code>null_subr.c</code> \u2014 Support routines</li> </ul>"},{"location":"sys/vfs/nullfs/#see-also","title":"See Also","text":"<ul> <li>Mounting \u2014 Mount operations</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/procfs/","title":"Process Filesystem (procfs)","text":"<p>procfs exposes process information through a filesystem interface at <code>/proc</code>.</p>"},{"location":"sys/vfs/procfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/procfs/</code> (~3,800 lines)</p> <p>procfs provides:</p> <ul> <li>Process information \u2014 Status, memory maps, file descriptors</li> <li>Process control \u2014 Debugging interfaces</li> <li>Kernel information \u2014 System-wide data</li> </ul>"},{"location":"sys/vfs/procfs/#source-files","title":"Source Files","text":"<ul> <li><code>procfs_subr.c</code> \u2014 Support routines</li> <li><code>procfs_vnops.c</code> \u2014 Vnode operations</li> <li><code>procfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>procfs_status.c</code> \u2014 Process status</li> <li><code>procfs_map.c</code> \u2014 Memory maps</li> <li><code>procfs_mem.c</code> \u2014 Process memory access</li> </ul>"},{"location":"sys/vfs/procfs/#see-also","title":"See Also","text":"<ul> <li>Processes \u2014 Process management</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/smbfs/","title":"SMB/CIFS Filesystem (smbfs)","text":"<p>smbfs provides client access to SMB/CIFS network shares.</p>"},{"location":"sys/vfs/smbfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/smbfs/</code> (~4,700 lines)</p> <p>smbfs enables Windows file sharing access:</p> <ul> <li>SMB protocol \u2014 Windows file sharing</li> <li>Authentication \u2014 User/password authentication</li> <li>Network shares \u2014 Mount remote shares locally</li> </ul>"},{"location":"sys/vfs/smbfs/#source-files","title":"Source Files","text":"<ul> <li><code>smbfs_vnops.c</code> \u2014 Vnode operations</li> <li><code>smbfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>smbfs_io.c</code> \u2014 I/O handling</li> <li><code>smbfs_node.c</code> \u2014 Node management</li> </ul>"},{"location":"sys/vfs/smbfs/#see-also","title":"See Also","text":"<ul> <li>NFS \u2014 Network File System</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/tmpfs/","title":"Temporary Filesystem (tmpfs)","text":"<p>tmpfs is a memory-based filesystem for temporary storage.</p>"},{"location":"sys/vfs/tmpfs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/tmpfs/</code> (~5,000 lines)</p> <p>tmpfs provides fast, memory-backed storage:</p> <ul> <li>RAM-based \u2014 All data stored in memory</li> <li>Swap support \u2014 Can page to swap under memory pressure</li> <li>Full POSIX \u2014 Complete filesystem semantics</li> </ul>"},{"location":"sys/vfs/tmpfs/#source-files","title":"Source Files","text":"<ul> <li><code>tmpfs_subr.c</code> \u2014 Node management</li> <li><code>tmpfs_vnops.c</code> \u2014 Vnode operations</li> <li><code>tmpfs_vfsops.c</code> \u2014 VFS operations</li> <li><code>tmpfs_fifoops.c</code> \u2014 FIFO support</li> </ul>"},{"location":"sys/vfs/tmpfs/#see-also","title":"See Also","text":"<ul> <li>Memory \u2014 Memory management</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/udf/","title":"Universal Disk Format (UDF)","text":"<p>udf provides support for UDF filesystems used on DVDs and Blu-ray discs.</p>"},{"location":"sys/vfs/udf/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/udf/</code> (~3,000 lines)</p> <p>udf enables DVD/Blu-ray access:</p> <ul> <li>UDF standard \u2014 OSTA Universal Disk Format</li> <li>DVD/Blu-ray \u2014 Optical media support</li> <li>Read-only \u2014 Primary use case</li> </ul>"},{"location":"sys/vfs/udf/#source-files","title":"Source Files","text":"<ul> <li><code>udf_vnops.c</code> \u2014 Vnode operations</li> <li><code>udf_vfsops.c</code> \u2014 VFS operations</li> <li><code>udf_subr.c</code> \u2014 Support routines</li> </ul>"},{"location":"sys/vfs/udf/#see-also","title":"See Also","text":"<ul> <li>isofs \u2014 ISO 9660 CD-ROM filesystem</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/ufs/","title":"Unix File System (UFS/FFS)","text":"<p>UFS is the traditional BSD Unix File System, also known as FFS (Fast File System).</p>"},{"location":"sys/vfs/ufs/#overview","title":"Overview","text":"<p>Documentation Status</p> <p>This page is a stub. Detailed documentation is planned.</p> <p>Source: <code>sys/vfs/ufs/</code> (~19,400 lines)</p> <p>UFS/FFS is the classic BSD filesystem:</p> <ul> <li>Mature \u2014 Decades of development and stability</li> <li>Soft updates \u2014 Metadata consistency without journaling</li> <li>Quotas \u2014 User/group disk quotas</li> <li>Snapshots \u2014 Filesystem snapshots</li> </ul>"},{"location":"sys/vfs/ufs/#source-files","title":"Source Files","text":"<p>Key files in <code>sys/vfs/ufs/</code>:</p> <ul> <li><code>ufs_vnops.c</code> \u2014 Vnode operations</li> <li><code>ufs_vfsops.c</code> \u2014 VFS operations (in ffs/)</li> <li><code>ufs_inode.c</code> \u2014 Inode management</li> <li><code>ufs_lookup.c</code> \u2014 Directory lookup</li> <li><code>ufs_quota.c</code> \u2014 Quota support</li> <li><code>ffs_*</code> \u2014 Fast File System specific code</li> </ul>"},{"location":"sys/vfs/ufs/#see-also","title":"See Also","text":"<ul> <li>Buffer Cache \u2014 Block caching</li> <li>VFS Overview \u2014 Filesystem implementations</li> </ul>"},{"location":"sys/vfs/hammer2/","title":"HAMMER2 Filesystem","text":"<p>HAMMER2 is DragonFly BSD's modern native filesystem, designed by Matthew Dillon for contemporary storage requirements with advanced features including copy-on-write, clustering, and built-in compression.</p>"},{"location":"sys/vfs/hammer2/#overview","title":"Overview","text":"<p>Source Location: <code>sys/vfs/hammer2/</code> (~33,400 lines across 28 files)</p> <p>HAMMER2 represents a complete redesign from the original HAMMER filesystem, focusing on:</p> <ul> <li>Copy-on-Write Architecture \u2014 All modifications allocate new blocks, never overwriting existing data</li> <li>Radix Tree Topology \u2014 Dynamic radix trees for directories, files, and the freemap</li> <li>Fat Block References \u2014 128-byte blockrefs that can embed small directory entries</li> <li>Per-CPU Threading \u2014 XOP (cross-operation) system for parallel backend I/O</li> <li>Clustering Support \u2014 Designed for multi-master replication</li> <li>Built-in Compression \u2014 LZ4 and ZLIB compression at the block level</li> <li>Live Deduplication \u2014 Detects and eliminates duplicate blocks during writes</li> </ul>"},{"location":"sys/vfs/hammer2/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"User Space\"\n        APP[Applications]\n    end\n\n    subgraph \"VFS Layer\"\n        VFS[VFS Interface]\n        VNODE[Vnode Operations]\n    end\n\n    subgraph \"HAMMER2 Core\"\n        INODE[Inode Layer]\n        CHAIN[Chain Layer]\n        XOP[XOP System]\n    end\n\n    subgraph \"I/O Subsystem\"\n        DIO[DIO Buffer Layer]\n        STRAT[Strategy Layer]\n    end\n\n    subgraph \"Storage Management\"\n        FREEMAP[Freemap]\n        FLUSH[Flush/Sync]\n    end\n\n    subgraph \"Storage\"\n        DISK[(Block Device)]\n    end\n\n    APP --&gt; VFS\n    VFS --&gt; VNODE\n    VNODE --&gt; INODE\n    INODE --&gt; CHAIN\n    CHAIN --&gt; XOP\n    XOP --&gt; DIO\n    DIO --&gt; STRAT\n    STRAT --&gt; DISK\n    CHAIN --&gt; FREEMAP\n    CHAIN --&gt; FLUSH\n    FLUSH --&gt; DIO\n</code></pre>"},{"location":"sys/vfs/hammer2/#key-characteristics","title":"Key Characteristics","text":"Characteristic Value Block Size Variable: 1KB to 64KB Inode Size 1KB (can embed up to 512 bytes) Blockref Size 128 bytes Max File Size 2^64 bytes (theoretical) Max Filesystem Size 1 Exabyte Compression LZ4 (fast), ZLIB (better ratio) Checksums xxHash64, SHA192, Freemap CRC"},{"location":"sys/vfs/hammer2/#documentation-sections","title":"Documentation Sections","text":"Section Description On-Disk Format Volume headers, blockrefs, inodes, freemap layout Chain Layer In-memory block representation and management Inode Layer Inode management and directory operations I/O Subsystem DIO layer and buffer cache abstraction Flush &amp; Sync Transaction flush mechanism Freemap Space allocation and bulk free operations XOP System Cross-operation threading for parallel I/O VFS Integration VFS and vnode operations Compression LZ4 and ZLIB compression Snapshots Snapshot implementation Clustering Multi-master clustering support"},{"location":"sys/vfs/hammer2/#source-file-organization","title":"Source File Organization","text":""},{"location":"sys/vfs/hammer2/#core-data-structures","title":"Core Data Structures","text":"File Lines Purpose <code>hammer2_disk.h</code> ~1,323 On-disk format definitions <code>hammer2.h</code> ~2,007 In-memory structures and constants <code>hammer2_chain.c</code> ~5,938 Chain (block reference) management <code>hammer2_inode.c</code> ~1,831 Inode management <code>hammer2_cluster.c</code> ~736 Cluster management"},{"location":"sys/vfs/hammer2/#vfs-integration","title":"VFS Integration","text":"File Lines Purpose <code>hammer2_vfsops.c</code> ~3,127 VFS operations (mount, unmount, sync) <code>hammer2_vnops.c</code> ~2,548 Vnode operations (open, read, write)"},{"location":"sys/vfs/hammer2/#io-subsystem","title":"I/O Subsystem","text":"File Lines Purpose <code>hammer2_io.c</code> ~963 Buffer cache abstraction (DIO) <code>hammer2_strategy.c</code> ~1,623 Read/write strategy <code>hammer2_flush.c</code> ~1,538 Flush/sync mechanism"},{"location":"sys/vfs/hammer2/#space-management","title":"Space Management","text":"File Lines Purpose <code>hammer2_freemap.c</code> ~1,251 Free space allocation <code>hammer2_bulkfree.c</code> ~1,447 Bulk free scanning"},{"location":"sys/vfs/hammer2/#xop-system","title":"XOP System","text":"File Lines Purpose <code>hammer2_xops.c</code> ~1,686 Cross-operation backends <code>hammer2_admin.c</code> ~1,262 Thread management <code>hammer2_synchro.c</code> ~1,069 Synchronization threads"},{"location":"sys/vfs/hammer2/#other-components","title":"Other Components","text":"File Lines Purpose <code>hammer2_ioctl.c</code> ~1,450 ioctl interface <code>hammer2_ondisk.c</code> ~740 Volume/device management <code>hammer2_subr.c</code> ~473 Utility functions <code>hammer2_lz4.c</code> ~525 LZ4 compression <code>hammer2_ccms.c</code> ~311 Cache coherency (stub) <code>hammer2_iocom.c</code> ~387 Cluster communication <code>hammer2_msgops.c</code> ~87 Message operations"},{"location":"sys/vfs/hammer2/#design-documents","title":"Design Documents","text":"<p>The HAMMER2 source tree includes detailed design documents:</p> <ul> <li><code>sys/vfs/hammer2/DESIGN</code> \u2014 Overall architecture and design rationale</li> <li><code>sys/vfs/hammer2/FREEMAP</code> \u2014 Freemap allocation strategy</li> </ul>"},{"location":"sys/vfs/hammer2/#see-also","title":"See Also","text":"<ul> <li>HAMMER \u2014 Legacy HAMMER filesystem (predecessor)</li> <li>VFS Core \u2014 VFS layer documentation</li> <li>Buffer Cache \u2014 Buffer cache that DIO wraps</li> <li>VFS Operations \u2014 VOP dispatch framework</li> </ul>"},{"location":"sys/vfs/hammer2/chain-layer/","title":"HAMMER2 Chain Layer","text":"<p>Documentation Status</p> <p>This page documents the chain layer based on <code>hammer2_chain.c</code> and <code>hammer2.h</code>.</p>"},{"location":"sys/vfs/hammer2/chain-layer/#overview","title":"Overview","text":"<p>The chain layer is HAMMER2's central abstraction for managing block references in memory. Every on-disk blockref is represented in memory as a chain structure (<code>hammer2_chain_t</code>). Chains form a cached, in-memory representation of the filesystem's block topology and serve as the primary interface between the VFS layer and the underlying storage.</p> <p>Key responsibilities of the chain layer:</p> <ul> <li>Topology management: Chains organize blockrefs into a tree structure using red-black trees</li> <li>Reference counting: Chains track references to prevent premature deallocation</li> <li>Locking: Chains provide mutex-based locking for concurrent access control</li> <li>Data resolution: Chains manage loading and caching of block data from disk</li> <li>Copy-on-write: Chains implement COW semantics for modifications</li> <li>Flush integration: Chains track modification state for the flush subsystem</li> </ul> <p>Source files:</p> <ul> <li><code>sys/vfs/hammer2/hammer2.h</code> \u2014 Structure definitions and constants</li> <li><code>sys/vfs/hammer2/hammer2_chain.c</code> \u2014 Chain operations implementation</li> </ul>"},{"location":"sys/vfs/hammer2/chain-layer/#chain-structure","title":"Chain Structure","text":""},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_t","title":"hammer2_chain_t","text":"<p>The <code>hammer2_chain_t</code> structure is the primary in-memory representation of any media object (volume header, inode, indirect block, data block, freemap node, etc.).</p> <pre><code>struct hammer2_chain {\n    hammer2_mtx_t           lock;           /* exclusive/shared mutex */\n    hammer2_chain_core_t    core;           /* embedded core (rbtree, etc.) */\n    RB_ENTRY(hammer2_chain) rbnode;         /* linkage in parent's rbtree */\n    hammer2_blockref_t      bref;           /* block reference (128 bytes) */\n    struct hammer2_chain    *parent;        /* parent chain */\n    struct hammer2_dev      *hmp;           /* device mount point */\n    struct hammer2_pfs      *pmp;           /* PFS mount or super-root */\n\n    struct lock             diolk;          /* xop focus interlock */\n    hammer2_io_t            *dio;           /* physical data buffer */\n    hammer2_media_data_t    *data;          /* data pointer shortcut */\n    u_int                   bytes;          /* physical data size */\n    u_int                   flags;          /* chain state flags */\n    u_int                   refs;           /* reference count */\n    u_int                   lockcnt;        /* lock nesting count */\n    int                     error;          /* on-lock data error state */\n    int                     cache_index;    /* heuristic for faster lookup */\n};\n</code></pre> <p>Source: <code>hammer2.h:324-342</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#field-descriptions","title":"Field Descriptions","text":"Field Description <code>lock</code> Mutex for exclusive or shared locking of the chain <code>core</code> Embedded structure containing the RB-tree of child chains <code>rbnode</code> Red-black tree linkage for insertion into parent's child tree <code>bref</code> The 128-byte block reference copied from or destined for media <code>parent</code> Pointer to the parent chain (NULL for root chains) <code>hmp</code> Pointer to the device mount structure <code>pmp</code> Pointer to the PFS mount (NULL for super-root chains) <code>diolk</code> Lock for XOP (cross-cluster operation) focus interlock <code>dio</code> Device I/O structure wrapping the buffer cache <code>data</code> Direct pointer to the chain's data (shortcut through dio) <code>bytes</code> Size of the data in bytes (derived from bref radix) <code>flags</code> State flags (MODIFIED, DELETED, ONFLUSH, etc.) <code>refs</code> Reference count preventing deallocation <code>lockcnt</code> Count of active locks (supports lock nesting) <code>error</code> Error code set during lock/data resolution <code>cache_index</code> Heuristic index to speed up repeated lookups"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_core_t","title":"hammer2_chain_core_t","text":"<p>The <code>hammer2_chain_core_t</code> structure is embedded within each chain and manages the chain's children:</p> <pre><code>struct hammer2_chain_core {\n    hammer2_spin_t          spin;           /* spinlock for rbtree access */\n    struct hammer2_reptrack *reptrack;      /* parent replacement tracking */\n    struct hammer2_chain_tree rbtree;       /* RB-tree of child chains */\n    int                     live_zero;      /* blockref array optimization */\n    u_int                   live_count;     /* count of live (non-deleted) children */\n    u_int                   chain_count;    /* total children (live + deleted) */\n    int                     generation;     /* generation number for iteration */\n};\n</code></pre> <p>Source: <code>hammer2.h:238-246</code></p> Field Description <code>spin</code> Spinlock protecting access to the RB-tree <code>reptrack</code> Linked list for tracking parent replacement during deletion <code>rbtree</code> Red-black tree containing all in-memory child chains <code>live_zero</code> Optimization: index beyond which all blockrefs are empty <code>live_count</code> Number of non-deleted chains in the tree <code>chain_count</code> Total number of chains (including deleted) in the tree <code>generation</code> Incremented on insertions; used to detect iteration races"},{"location":"sys/vfs/hammer2/chain-layer/#chain-flags","title":"Chain Flags","text":"<p>Chain flags indicate the current state of a chain. Multiple flags can be set simultaneously.</p>"},{"location":"sys/vfs/hammer2/chain-layer/#core-state-flags","title":"Core State Flags","text":"Flag Value Description <code>HAMMER2_CHAIN_MODIFIED</code> 0x00000001 Chain data has been modified; requires flush <code>HAMMER2_CHAIN_ALLOCATED</code> 0x00000002 Chain was kmalloc'd (vs. static) <code>HAMMER2_CHAIN_DESTROY</code> 0x00000004 Chain should be destroyed when possible <code>HAMMER2_CHAIN_DEDUPABLE</code> 0x00000008 Chain is registered for deduplication <code>HAMMER2_CHAIN_DELETED</code> 0x00000010 Chain has been deleted <code>HAMMER2_CHAIN_INITIAL</code> 0x00000020 Chain is newly created, data is all-zeros <code>HAMMER2_CHAIN_UPDATE</code> 0x00000040 Parent's blockref needs update <p>Source: <code>hammer2.h:375-381</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#flush-and-topology-flags","title":"Flush and Topology Flags","text":"Flag Value Description <code>HAMMER2_CHAIN_NOTTESTED</code> 0x00000080 CRC has not been generated yet <code>HAMMER2_CHAIN_TESTEDGOOD</code> 0x00000100 CRC has been tested and is good <code>HAMMER2_CHAIN_ONFLUSH</code> 0x00000200 Chain is on the flush list <code>HAMMER2_CHAIN_VOLUMESYNC</code> 0x00000800 Needs volume header sync <code>HAMMER2_CHAIN_COUNTEDBREFS</code> 0x00002000 Block table stats have been counted <code>HAMMER2_CHAIN_ONRBTREE</code> 0x00004000 Chain is in parent's RB-tree <code>HAMMER2_CHAIN_RELEASE</code> 0x00020000 Don't keep chain cached after unlock <code>HAMMER2_CHAIN_BLKMAPPED</code> 0x00040000 Chain is present in parent's blockmap <code>HAMMER2_CHAIN_BLKMAPUPD</code> 0x00080000 Blockmap entry needs updating <code>HAMMER2_CHAIN_PFSBOUNDARY</code> 0x00400000 Chain is a PFS root boundary <p>Source: <code>hammer2.h:382-399</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#flush-mask","title":"Flush Mask","text":"<p>The flush mask combines flags that indicate a chain needs attention from the flush subsystem:</p> <pre><code>#define HAMMER2_CHAIN_FLUSH_MASK    (HAMMER2_CHAIN_MODIFIED |\n                                     HAMMER2_CHAIN_UPDATE |\n                                     HAMMER2_CHAIN_ONFLUSH |\n                                     HAMMER2_CHAIN_DESTROY)\n</code></pre> <p>Source: <code>hammer2.h:401-404</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#error-codes","title":"Error Codes","text":"<p>HAMMER2 uses its own error code system that can be ORed together:</p> Error Code Value Description <code>HAMMER2_ERROR_NONE</code> 0x00000000 No error <code>HAMMER2_ERROR_EIO</code> 0x00000001 Device I/O error <code>HAMMER2_ERROR_CHECK</code> 0x00000002 Checksum verification failed <code>HAMMER2_ERROR_INCOMPLETE</code> 0x00000004 Cluster incomplete or parent error <code>HAMMER2_ERROR_DEPTH</code> 0x00000008 Temporary recursion depth limit <code>HAMMER2_ERROR_BADBREF</code> 0x00000010 Illegal block reference <code>HAMMER2_ERROR_ENOSPC</code> 0x00000020 No space for allocation <code>HAMMER2_ERROR_ENOENT</code> 0x00000040 Entry not found <code>HAMMER2_ERROR_ENOTEMPTY</code> 0x00000080 Directory not empty <code>HAMMER2_ERROR_EAGAIN</code> 0x00000100 Retry operation <code>HAMMER2_ERROR_EOF</code> 0x00002000 End of scan <p>Source: <code>hammer2.h:426-445</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#reference-counting","title":"Reference Counting","text":"<p>Chains use reference counting to manage their lifetime. A chain cannot be freed while it has outstanding references.</p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_ref","title":"hammer2_chain_ref()","text":"<p>Adds a reference to a chain:</p> <pre><code>void hammer2_chain_ref(hammer2_chain_t *chain);\n</code></pre> <ul> <li>Atomically increments <code>chain-&gt;refs</code></li> <li>Can be called with spinlocks held</li> <li>Chain must already have at least one reference</li> </ul> <p>Source: <code>hammer2_chain.c:257-263</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_drop","title":"hammer2_chain_drop()","text":"<p>Releases a reference:</p> <pre><code>void hammer2_chain_drop(hammer2_chain_t *chain);\n</code></pre> <ul> <li>Atomically decrements <code>chain-&gt;refs</code></li> <li>On 1\u21920 transition, attempts to disassociate and free the chain</li> <li>Recursively drops the parent if the chain was the last child</li> <li>The chain cannot be freed if:<ul> <li>It has children in its RB-tree</li> <li>It is flagged MODIFIED or UPDATE</li> <li>It is still connected to a parent</li> </ul> </li> </ul> <p>Source: <code>hammer2_chain.c:345-368</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#holdunhold-operations","title":"Hold/Unhold Operations","text":"<p>For holding chain data across unlock operations:</p> Function Description <code>hammer2_chain_ref_hold()</code> Ref and increment lockcnt to hold data <code>hammer2_chain_unhold()</code> Decrement lockcnt, may need to reacquire lock <code>hammer2_chain_drop_unhold()</code> Combined unhold and drop <code>hammer2_chain_rehold()</code> Lock shared, increment lockcnt, unlock <p>Source: <code>hammer2_chain.c:272-426</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#locking-model","title":"Locking Model","text":"<p>Chains support both exclusive and shared locking through the <code>hammer2_chain_lock()</code> and <code>hammer2_chain_unlock()</code> functions.</p>"},{"location":"sys/vfs/hammer2/chain-layer/#data-resolution-modes","title":"Data Resolution Modes","text":"<p>When locking a chain, you specify how data should be resolved:</p> Mode Value Description <code>HAMMER2_RESOLVE_NEVER</code> 1 Do not resolve data (avoid buffer aliasing) <code>HAMMER2_RESOLVE_MAYBE</code> 2 Resolve metadata but not bulk data <code>HAMMER2_RESOLVE_ALWAYS</code> 3 Always resolve and load data <p>Source: <code>hammer2.h:497-500</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#lock-flags","title":"Lock Flags","text":"Flag Value Description <code>HAMMER2_RESOLVE_SHARED</code> 0x10 Request shared (read) lock <code>HAMMER2_RESOLVE_LOCKAGAIN</code> 0x20 Another shared lock (nesting) <code>HAMMER2_RESOLVE_NONBLOCK</code> 0x80 Non-blocking lock attempt <p>Source: <code>hammer2.h:502-505</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_lock","title":"hammer2_chain_lock()","text":"<pre><code>int hammer2_chain_lock(hammer2_chain_t *chain, int how);\n</code></pre> <p>Locks a referenced chain and optionally resolves its data:</p> <ul> <li>RESOLVE_NEVER: Does not load data; <code>chain-&gt;data</code> remains NULL</li> <li>RESOLVE_MAYBE: Loads metadata (inodes, indirect blocks) but not DATA blocks</li> <li>RESOLVE_ALWAYS: Always loads data from disk if not already present</li> <li>Sets <code>chain-&gt;error</code> on I/O or checksum failure</li> <li>Returns 0 on success, EAGAIN if NONBLOCK specified and lock unavailable</li> <li>Lock can recurse; <code>lockcnt</code> tracks nesting depth</li> </ul> <p>Source: <code>hammer2_chain.c:787-859</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_unlock","title":"hammer2_chain_unlock()","text":"<pre><code>void hammer2_chain_unlock(hammer2_chain_t *chain);\n</code></pre> <p>Unlocks a chain:</p> <ul> <li>Decrements <code>lockcnt</code></li> <li>On last unlock (lockcnt 1\u21920), may drop data reference</li> <li>Data is dropped unless chain has MODIFIED flag set</li> <li>Releases the underlying mutex</li> </ul> <p>Source: <code>hammer2_chain.c:860-920</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#chain-lifecycle","title":"Chain Lifecycle","text":""},{"location":"sys/vfs/hammer2/chain-layer/#allocation","title":"Allocation","text":"<pre><code>hammer2_chain_t *hammer2_chain_alloc(hammer2_dev_t *hmp, hammer2_pfs_t *pmp,\n                                      hammer2_blockref_t *bref);\n</code></pre> <p>Allocates a new disconnected chain:</p> <ol> <li>Calculates <code>bytes</code> from the blockref's radix field</li> <li>Allocates memory with <code>kmalloc_obj()</code></li> <li>Initializes the chain structure:<ul> <li>Copies the blockref to <code>chain-&gt;bref</code></li> <li>Sets <code>refs = 1</code></li> <li>Sets <code>flags = HAMMER2_CHAIN_ALLOCATED</code></li> <li>Initializes mutex, spinlock, and RB-tree</li> </ul> </li> <li>Returns a referenced but unlocked chain</li> </ol> <p>Source: <code>hammer2_chain.c:177-237</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#initialization","title":"Initialization","text":"<pre><code>void hammer2_chain_init(hammer2_chain_t *chain);\n</code></pre> <p>Initializes chain synchronization primitives:</p> <ul> <li>Initializes the RB-tree (<code>RB_INIT</code>)</li> <li>Initializes the chain mutex</li> <li>Initializes the core spinlock</li> <li>Initializes the DIO interlock</li> </ul> <p>Source: <code>hammer2_chain.c:242-249</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#insertion","title":"Insertion","text":"<pre><code>static int hammer2_chain_insert(hammer2_chain_t *parent, hammer2_chain_t *chain,\n                                 int flags, int generation);\n</code></pre> <p>Inserts a chain into a parent's RB-tree:</p> <ul> <li>Acquires parent's spinlock if <code>HAMMER2_CHAIN_INSERT_SPIN</code> is set</li> <li>Checks for race conditions if <code>HAMMER2_CHAIN_INSERT_RACE</code> is set</li> <li>Inserts into parent's <code>core.rbtree</code></li> <li>Sets <code>HAMMER2_CHAIN_ONRBTREE</code> flag</li> <li>Updates <code>chain_count</code> and <code>generation</code></li> <li>Increments <code>live_count</code> if <code>HAMMER2_CHAIN_INSERT_LIVE</code> is set</li> </ul> <p>Source: <code>hammer2_chain.c:290-332</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#deallocation","title":"Deallocation","text":"<p>When <code>refs</code> drops to 0, <code>hammer2_chain_lastdrop()</code> handles cleanup:</p> <ol> <li>Acquires chain's spinlock</li> <li>Checks if chain can be freed:<ul> <li>Must not have MODIFIED or UPDATE flags (if parent exists)</li> <li>Must not have children in RB-tree</li> </ul> </li> <li>Removes chain from parent's RB-tree if present</li> <li>Frees the chain memory</li> <li>May recursively drop parent</li> </ol> <p>Source: <code>hammer2_chain.c:450-699</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#tree-traversal","title":"Tree Traversal","text":""},{"location":"sys/vfs/hammer2/chain-layer/#lookup-initialization","title":"Lookup Initialization","text":"<pre><code>hammer2_chain_t *hammer2_chain_lookup_init(hammer2_chain_t *parent, int flags);\nvoid hammer2_chain_lookup_done(hammer2_chain_t *parent);\n</code></pre> <p>Bracket a series of lookups:</p> <ul> <li><code>lookup_init</code>: Refs and locks the parent</li> <li><code>lookup_done</code>: Unlocks and drops the parent</li> </ul> <p>Source: <code>hammer2_chain.c:2113-2133</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_lookup","title":"hammer2_chain_lookup()","text":"<pre><code>hammer2_chain_t *hammer2_chain_lookup(hammer2_chain_t **parentp, hammer2_key_t *key_nextp,\n                                       hammer2_key_t key_beg, hammer2_key_t key_end,\n                                       int *errorp, int flags);\n</code></pre> <p>Looks up the first chain whose key range overlaps <code>[key_beg, key_end]</code>:</p> <ol> <li>Counts blockrefs if not already counted (<code>COUNTEDBREFS</code>)</li> <li>Acquires parent's spinlock</li> <li>Calls <code>hammer2_combined_find()</code> to search both:<ul> <li>The in-memory RB-tree of child chains</li> <li>The on-disk blockref array</li> </ul> </li> <li>If found in blockref but not in memory, creates chain with <code>hammer2_chain_get()</code></li> <li>Handles MATCHIND flag to return indirect blocks</li> <li>Returns locked chain, sets <code>*key_nextp</code> for iteration</li> </ol> <p>Source: <code>hammer2_chain.c:2339-2600</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#lookup-flags","title":"Lookup Flags","text":"Flag Description <code>HAMMER2_LOOKUP_NODATA</code> Don't resolve data (leave NULL) <code>HAMMER2_LOOKUP_NODIRECT</code> Don't return inode for offset 0 in DIRECTDATA mode <code>HAMMER2_LOOKUP_SHARED</code> Use shared lock <code>HAMMER2_LOOKUP_MATCHIND</code> Allow returning indirect blocks <code>HAMMER2_LOOKUP_ALWAYS</code> Always resolve data <p>Source: <code>hammer2.h:473-480</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_next","title":"hammer2_chain_next()","text":"<pre><code>hammer2_chain_t *hammer2_chain_next(hammer2_chain_t **parentp, hammer2_chain_t *chain,\n                                     hammer2_key_t *key_nextp,\n                                     hammer2_key_t key_beg, hammer2_key_t key_end,\n                                     int *errorp, int flags);\n</code></pre> <p>Continues iteration after <code>hammer2_chain_lookup()</code>:</p> <ul> <li>Unlocks and drops the previous chain</li> <li>Uses <code>*key_nextp</code> to find the next element</li> <li>Returns NULL with <code>HAMMER2_ERROR_EOF</code> when done</li> </ul> <p>Source: <code>hammer2_chain.c:2690-2780</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_scan","title":"hammer2_chain_scan()","text":"<pre><code>int hammer2_chain_scan(hammer2_chain_t *parent, hammer2_chain_t **chainp,\n                        hammer2_blockref_t *bref, int *firstp, int flags);\n</code></pre> <p>Raw scan for iterating all children:</p> <ul> <li>Does not seek to a specific key</li> <li>Fills in <code>bref</code> with the current blockref</li> <li>Only instantiates chains for recursive types (INDIRECT, INODE, etc.)</li> <li>Returns <code>HAMMER2_ERROR_EOF</code> when exhausted</li> </ul> <p>Source: <code>hammer2_chain.c:2820-3020</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#modification-operations","title":"Modification Operations","text":""},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_modify","title":"hammer2_chain_modify()","text":"<pre><code>int hammer2_chain_modify(hammer2_chain_t *chain, hammer2_tid_t mtid,\n                          hammer2_off_t dedup_off, int flags);\n</code></pre> <p>Marks a chain as modified, implementing copy-on-write:</p> <ol> <li>Loads existing data if needed (unless OPTDATA flag)</li> <li>If not already MODIFIED:<ul> <li>Sets MODIFIED flag</li> <li>Determines if COW is required (checks overwrite-in-place eligibility)</li> </ul> </li> <li>Sets UPDATE flag for parent blockref update</li> <li>Acquires <code>diolk</code> for XOP interlock</li> <li>If new allocation needed:<ul> <li>Calls <code>hammer2_freemap_alloc()</code> for new block</li> <li>Copies old data to new block</li> <li>Clears DEDUPABLE flag</li> </ul> </li> <li>If <code>dedup_off</code> specified:<ul> <li>Uses dedup block instead of new allocation</li> <li>Clears MODIFIED, sets DEDUPABLE</li> </ul> </li> </ol> <p>Overwrite-in-place: Allowed when:</p> <ul> <li>Chain is DATA or DIRENT type</li> <li>Check mode is NONE</li> <li><code>modify_tid</code> is beyond the last snapshot</li> </ul> <p>Source: <code>hammer2_chain.c:1436-1750</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_resize","title":"hammer2_chain_resize()","text":"<pre><code>int hammer2_chain_resize(hammer2_chain_t *chain, hammer2_tid_t mtid,\n                          hammer2_off_t dedup_off, int nradix, int flags);\n</code></pre> <p>Resizes a chain's physical storage:</p> <ul> <li>Only DATA, INDIRECT, and DIRENT blocks can be resized</li> <li>Calls <code>hammer2_chain_modify()</code> first</li> <li>Allocates new storage with <code>hammer2_freemap_alloc()</code></li> <li>Updates <code>chain-&gt;bytes</code></li> <li>Caller must copy data if needed</li> </ul> <p>Source: <code>hammer2_chain.c:1339-1407</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#chain-creation-and-deletion","title":"Chain Creation and Deletion","text":""},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_create","title":"hammer2_chain_create()","text":"<pre><code>int hammer2_chain_create(hammer2_chain_t **parentp, hammer2_chain_t **chainp,\n                          hammer2_pfs_t *pmp, int methods,\n                          hammer2_key_t key, int keybits,\n                          int type, size_t bytes,\n                          hammer2_tid_t mtid, hammer2_off_t dedup_off, int flags);\n</code></pre> <p>Creates a new chain or reattaches an existing one:</p> <ol> <li>If <code>*chainp</code> is NULL, allocates a new chain with INITIAL flag</li> <li>Ensures parent has room for new child:<ul> <li>If <code>live_count == count</code>, creates indirect block</li> </ul> </li> <li>Inserts chain into parent's RB-tree</li> <li>Calls <code>hammer2_chain_modify()</code> for newly allocated chains</li> <li>Sets UPDATE flag for reconnected chains</li> <li>Calls <code>hammer2_chain_setflush()</code> on parent</li> </ol> <p>Indirect block creation: When a parent's blockref array is full, <code>hammer2_chain_create_indirect()</code> is called to:</p> <ol> <li>Allocate an indirect block</li> <li>Move some children to the indirect block</li> <li>Return the appropriate parent for the new chain</li> </ol> <p>Source: <code>hammer2_chain.c:3070-3388</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_delete","title":"hammer2_chain_delete()","text":"<pre><code>void hammer2_chain_delete(hammer2_chain_t *parent, hammer2_chain_t *chain,\n                           hammer2_tid_t mtid, int flags);\n</code></pre> <p>Deletes a chain from the topology:</p> <ol> <li>Sets DELETED flag</li> <li>Removes chain from parent's RB-tree</li> <li>Removes blockmap entry from parent (if BLKMAPPED)</li> <li>Decrements parent's <code>live_count</code></li> <li>If <code>HAMMER2_DELETE_PERMANENT</code>:<ul> <li>Marks storage for deallocation via freemap</li> </ul> </li> </ol> <p>Source: <code>hammer2_chain.c:3640-3680</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_rename","title":"hammer2_chain_rename()","text":"<pre><code>void hammer2_chain_rename(hammer2_chain_t **parentp, hammer2_chain_t *chain,\n                           hammer2_tid_t mtid, int flags);\n</code></pre> <p>Moves a chain to a new parent:</p> <ul> <li>Chain must already be disconnected (deleted or never attached)</li> <li>Calls <code>hammer2_chain_create()</code> to insert under new parent</li> <li>Sets UPDATE and ONFLUSH flags</li> </ul> <p>Source: <code>hammer2_chain.c:3415-3461</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#flush-integration","title":"Flush Integration","text":""},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_setflush","title":"hammer2_chain_setflush()","text":"<pre><code>void hammer2_chain_setflush(hammer2_chain_t *chain);\n</code></pre> <p>Marks a chain and its ancestors for flushing:</p> <ol> <li>Sets ONFLUSH flag on chain</li> <li>Walks up to parent, setting ONFLUSH on each</li> <li>Stops at inode boundaries (flush inflection points)</li> <li>Inode chains connect different flush domains</li> </ol> <p>The flusher uses ONFLUSH to efficiently locate modified chains via top-down recursion.</p> <p>Source: <code>hammer2_chain.c:146-165</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#flush-related-flags","title":"Flush-Related Flags","text":"Flag Purpose <code>MODIFIED</code> Chain data was modified, needs writing <code>UPDATE</code> Parent blockref needs updating <code>ONFLUSH</code> Chain is on the flush recursion path <code>BLKMAPPED</code> Chain exists in parent's on-disk blockmap <code>BLKMAPUPD</code> Blockmap entry needs update (bref changed)"},{"location":"sys/vfs/hammer2/chain-layer/#parent-access","title":"Parent Access","text":""},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_getparent","title":"hammer2_chain_getparent()","text":"<pre><code>hammer2_chain_t *hammer2_chain_getparent(hammer2_chain_t *chain, int flags);\n</code></pre> <p>Returns a locked parent while keeping chain locked:</p> <ul> <li>Handles lock order reversal (must unlock child before locking parent)</li> <li>Re-locks child after acquiring parent</li> <li>Handles races where parent changes during unlock window</li> </ul> <p>Source: <code>hammer2_chain.c:2149-2185</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#hammer2_chain_repparent","title":"hammer2_chain_repparent()","text":"<pre><code>hammer2_chain_t *hammer2_chain_repparent(hammer2_chain_t **chainp, int flags);\n</code></pre> <p>Returns parent while dropping the chain:</p> <ul> <li>Uses <code>reptrack</code> mechanism to follow parent if it changes</li> <li>Handles complex deletion scenarios during indirect block cleanup</li> <li>More complex than <code>getparent()</code> but handles unstable chains</li> </ul> <p>Source: <code>hammer2_chain.c:2200-2293</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#rb-tree-organization","title":"RB-Tree Organization","text":"<p>Chains are organized in a red-black tree within their parent, keyed by their blockref's key and keybits:</p> <pre><code>int hammer2_chain_cmp(hammer2_chain_t *chain1, hammer2_chain_t *chain2);\n</code></pre> <p>The comparison function:</p> <ol> <li>Calculates key range for each chain: <code>[key, key + (1 &lt;&lt; keybits) - 1]</code></li> <li>Returns -1 if chain1 is fully left of chain2</li> <li>Returns +1 if chain1 is fully right of chain2</li> <li>Returns 0 for overlap (should not happen in normal operation)</li> </ol> <p>This allows efficient range-based lookups and ensures children don't overlap.</p> <p>Source: <code>hammer2_chain.c:96-118</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#combined-find","title":"Combined Find","text":"<p>The <code>hammer2_combined_find()</code> function merges results from:</p> <ol> <li>In-memory chains: Searched via RB-tree scan</li> <li>On-disk blockrefs: Searched via linear scan of blockref array</li> </ol> <p>This is necessary because:</p> <ul> <li>Not all on-disk blockrefs have in-memory chains</li> <li>In-memory chains may be newly created (not yet on disk)</li> <li>In-memory chains may be deleted (still on disk until flushed)</li> </ul> <p>The combined find returns the best match considering both sources.</p> <p>Source: <code>hammer2_chain.c:1900-2030</code></p>"},{"location":"sys/vfs/hammer2/chain-layer/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview</li> <li>On-Disk Format \u2014 Underlying block references</li> <li>Inode Layer \u2014 Higher-level inode abstraction</li> <li>Flush and Sync \u2014 How chains are flushed to disk</li> <li>XOP System \u2014 Cross-cluster operations using chains</li> </ul>"},{"location":"sys/vfs/hammer2/clustering/","title":"HAMMER2 Clustering","text":"<p>Documentation Status</p> <p>This page documents clustering support based on <code>hammer2_cluster.c</code> and related code.</p>"},{"location":"sys/vfs/hammer2/clustering/#overview","title":"Overview","text":"<p>HAMMER2 is designed for multi-master clustering, though this feature is partially implemented.</p>"},{"location":"sys/vfs/hammer2/clustering/#cluster-architecture","title":"Cluster Architecture","text":"<p>Documentation in progress \u2014 will cover cluster design.</p>"},{"location":"sys/vfs/hammer2/clustering/#current-status","title":"Current Status","text":"<p>Documentation in progress \u2014 will cover implementation status.</p>"},{"location":"sys/vfs/hammer2/clustering/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview</li> <li>Distributed Messaging \u2014 dmsg communication layer</li> </ul>"},{"location":"sys/vfs/hammer2/compression/","title":"HAMMER2 Compression","text":"<p>Documentation Status</p> <p>This page documents compression support based on <code>hammer2_lz4.c</code> and related code.</p>"},{"location":"sys/vfs/hammer2/compression/#overview","title":"Overview","text":"<p>HAMMER2 supports transparent block-level compression using LZ4 and ZLIB algorithms.</p>"},{"location":"sys/vfs/hammer2/compression/#compression-algorithms","title":"Compression Algorithms","text":"<p>Documentation in progress \u2014 will cover LZ4 and ZLIB support.</p>"},{"location":"sys/vfs/hammer2/compression/#compression-integration","title":"Compression Integration","text":"<p>Documentation in progress \u2014 will cover how compression integrates with I/O.</p>"},{"location":"sys/vfs/hammer2/compression/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview</li> <li>I/O Subsystem \u2014 I/O path integration</li> </ul>"},{"location":"sys/vfs/hammer2/flush-sync/","title":"HAMMER2 Flush &amp; Sync","text":"<p>Documentation Status</p> <p>This page documents the flush mechanism based on <code>hammer2_flush.c</code>.</p>"},{"location":"sys/vfs/hammer2/flush-sync/#overview","title":"Overview","text":"<p>HAMMER2's flush mechanism handles writing modified chains back to disk while maintaining consistency.</p>"},{"location":"sys/vfs/hammer2/flush-sync/#flush-phases","title":"Flush Phases","text":"<p>Documentation in progress \u2014 will cover multi-phase flush process.</p>"},{"location":"sys/vfs/hammer2/flush-sync/#transaction-management","title":"Transaction Management","text":"<p>Documentation in progress \u2014 will cover transaction handling.</p>"},{"location":"sys/vfs/hammer2/flush-sync/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview</li> <li>Chain Layer \u2014 Chain modification tracking</li> <li>I/O Subsystem \u2014 Actual I/O operations</li> </ul>"},{"location":"sys/vfs/hammer2/freemap/","title":"HAMMER2 Freemap","text":"<p>Documentation Status</p> <p>This page documents freemap management based on <code>hammer2_freemap.c</code> and <code>hammer2_bulkfree.c</code>.</p>"},{"location":"sys/vfs/hammer2/freemap/#overview","title":"Overview","text":"<p>The freemap manages free space allocation in HAMMER2, using a multi-level radix tree structure.</p>"},{"location":"sys/vfs/hammer2/freemap/#freemap-structure","title":"Freemap Structure","text":"<p>Documentation in progress \u2014 will cover freemap organization.</p>"},{"location":"sys/vfs/hammer2/freemap/#allocation","title":"Allocation","text":"<p>Documentation in progress \u2014 will cover block allocation.</p>"},{"location":"sys/vfs/hammer2/freemap/#bulk-free","title":"Bulk Free","text":"<p>Documentation in progress \u2014 will cover bulk free scanning.</p>"},{"location":"sys/vfs/hammer2/freemap/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview</li> <li>On-Disk Format \u2014 Freemap on-disk layout</li> <li>Chain Layer \u2014 Freemap chain management</li> </ul>"},{"location":"sys/vfs/hammer2/inode-layer/","title":"HAMMER2 Inode Layer","text":"<p>Source Reference</p> <p>Primary source: <code>sys/vfs/hammer2/hammer2_inode.c</code> Header definitions: <code>sys/vfs/hammer2/hammer2.h</code> (lines 673-767)</p>"},{"location":"sys/vfs/hammer2/inode-layer/#overview","title":"Overview","text":"<p>The inode layer provides the high-level file and directory abstraction built on top of the chain layer. It manages in-memory inode structures (<code>hammer2_inode_t</code>), their association with vnodes, reference counting, locking semantics, and synchronization with the underlying chain topology.</p> <p>Key responsibilities:</p> <ul> <li>Inode lifecycle management \u2014 creation, lookup, reference counting, and destruction</li> <li>Vnode association \u2014 bridging between VFS vnodes and HAMMER2 inodes</li> <li>Locking with SYNCQ semantics \u2014 coordinating with the filesystem syncer</li> <li>Flush dependency tracking \u2014 ensuring related inodes are flushed together</li> <li>Metadata synchronization \u2014 propagating in-memory changes to on-disk chains</li> </ul>"},{"location":"sys/vfs/hammer2/inode-layer/#core-data-structures","title":"Core Data Structures","text":""},{"location":"sys/vfs/hammer2/inode-layer/#hammer2_inode_t","title":"hammer2_inode_t","text":"<p>The main in-memory inode structure (<code>hammer2.h:690-710</code>):</p> <pre><code>struct hammer2_inode {\n    struct hammer2_inode *next;         /* hash chain linkage */\n    TAILQ_ENTRY(hammer2_inode) entry;   /* SYNCQ/SIDEQ queue entry */\n    hammer2_depend_t    *depend;        /* flush dependency (non-NULL if SIDEQ) */\n    hammer2_depend_t    depend_static;  /* in-place allocation for depend */\n    hammer2_mtx_t       lock;           /* inode lock */\n    hammer2_mtx_t       truncate_lock;  /* prevent truncates during I/O */\n    struct hammer2_pfs  *pmp;           /* PFS mount point */\n    struct vnode        *vp;            /* associated vnode (may be NULL) */\n    hammer2_spin_t      cluster_spin;   /* protects cluster updates */\n    hammer2_cluster_t   cluster;        /* embedded cluster of chains */\n    hammer2_cluster_item_t ccache[HAMMER2_MAXCLUSTER]; /* cached data chains */\n    int                 ccache_nchains; /* number of cached chains */\n    struct lockf        advlock;        /* advisory lock state */\n    u_int               flags;          /* state flags */\n    u_int               refs;           /* reference count */\n    int                 ihash;          /* XOP worker distribution hash */\n    uint8_t             comp_heuristic; /* compression heuristic */\n    hammer2_inode_meta_t meta;          /* copy of on-disk metadata */\n    hammer2_off_t       osize;          /* original size (for RESIZED) */\n};\n</code></pre> <p>Key fields:</p> Field Purpose <code>next</code> Hash chain linkage for inode number lookup <code>entry</code> Queue entry for SYNCQ (sync in progress) or SIDEQ (dirty, needs sync) <code>depend</code> Flush dependency group pointer <code>lock</code> Primary inode lock (shared/exclusive) <code>truncate_lock</code> Prevents truncates during read/write operations <code>pmp</code> Back-pointer to the PFS mount structure <code>vp</code> Associated vnode (NULL if no vnode attached) <code>cluster</code> Embedded cluster linking to underlying chains <code>flags</code> State flags (see below) <code>refs</code> Reference count (includes vnode ref, flush refs) <code>meta</code> In-memory copy of on-disk inode metadata"},{"location":"sys/vfs/hammer2/inode-layer/#inode-flags","title":"Inode Flags","text":"<p>Defined in <code>hammer2.h:747-767</code>:</p> Flag Value Description <code>HAMMER2_INODE_MODIFIED</code> 0x0001 Inode metadata has been modified <code>HAMMER2_INODE_ONHASH</code> 0x0008 Inode is in the inum hash table <code>HAMMER2_INODE_RESIZED</code> 0x0010 File size changed, requires chain sync <code>HAMMER2_INODE_ISUNLINKED</code> 0x0040 Inode has been unlinked (nlinks \u2192 0) <code>HAMMER2_INODE_SIDEQ</code> 0x0100 On side processing queue (dirty, awaiting sync) <code>HAMMER2_INODE_NOSIDEQ</code> 0x0200 Disable SIDEQ operations <code>HAMMER2_INODE_DIRTYDATA</code> 0x0400 Has dirty data, interlocks flush <code>HAMMER2_INODE_SYNCQ</code> 0x0800 Currently being synced (on sync queue) <code>HAMMER2_INODE_DELETING</code> 0x1000 Marked for deletion during sync <code>HAMMER2_INODE_CREATING</code> 0x2000 Chains not yet inserted into media topology <code>HAMMER2_INODE_SYNCQ_WAKEUP</code> 0x4000 Wakeup requested when SYNCQ clears <code>HAMMER2_INODE_SYNCQ_PASS2</code> 0x8000 Force retry delay during sync <p>The <code>HAMMER2_INODE_DIRTY</code> macro combines flags indicating the inode needs flushing: <pre><code>#define HAMMER2_INODE_DIRTY  (HAMMER2_INODE_MODIFIED | \\\n                              HAMMER2_INODE_DIRTYDATA | \\\n                              HAMMER2_INODE_DELETING | \\\n                              HAMMER2_INODE_CREATING)\n</code></pre></p>"},{"location":"sys/vfs/hammer2/inode-layer/#hammer2_depend_t","title":"hammer2_depend_t","text":"<p>Flush dependency structure (<code>hammer2.h:673-679</code>):</p> <pre><code>struct hammer2_depend {\n    TAILQ_ENTRY(hammer2_depend) entry;  /* pmp-&gt;depq linkage */\n    struct inoq_head    sideq;          /* list of dependent inodes */\n    long                count;          /* number of inodes in group */\n    int                 pass2;          /* requires second pass */\n    int                 unused01;\n};\n</code></pre> <p>Dependencies group related inodes (e.g., directory and its entries) to ensure they are flushed together, maintaining filesystem consistency.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#inode-hash-table","title":"Inode Hash Table","text":"<p>Inodes are indexed by inode number in a per-PFS hash table for fast lookup.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#initialization","title":"Initialization","text":"<pre><code>void hammer2_inum_hash_init(hammer2_pfs_t *pmp);\n</code></pre> <p>Source: <code>hammer2_inode.c:50-60</code></p> <p>Initializes the inode hash table spinlocks for a PFS. Called during PFS mount.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#lookup","title":"Lookup","text":"<pre><code>hammer2_inode_t *hammer2_inode_lookup(hammer2_pfs_t *pmp, hammer2_tid_t inum);\n</code></pre> <p>Source: <code>hammer2_inode.c:548-569</code></p> <p>Looks up an inode by inode number. Returns a referenced inode or NULL if not found in memory. The hash function uses the low bits of the inode number:</p> <pre><code>static __inline hammer2_inum_hash_t *\ninumhash(hammer2_pfs_t *pmp, hammer2_tid_t inum)\n{\n    int hv = (int)inum;\n    return (&amp;pmp-&gt;inumhash[hv &amp; HAMMER2_INUMHASH_MASK]);\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/inode-layer/#reference-counting","title":"Reference Counting","text":""},{"location":"sys/vfs/hammer2/inode-layer/#adding-references","title":"Adding References","text":"<pre><code>void hammer2_inode_ref(hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:577-585</code></p> <p>Atomically increments the reference count. The inode must already have at least one reference. Can be called with spinlocks held.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#dropping-references","title":"Dropping References","text":"<pre><code>void hammer2_inode_drop(hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:591-656</code></p> <p>Decrements the reference count. When the count reaches zero:</p> <ol> <li>Acquires the hash bucket spinlock</li> <li>Removes the inode from the hash table (if <code>ONHASH</code> flag set)</li> <li>Cleans out the embedded cluster via <code>hammer2_inode_repoint(ip, NULL)</code></li> <li>Frees the inode structure</li> </ol> <p>The transition to zero is protected by <code>atomic_cmpset_int()</code> to handle races.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#locking","title":"Locking","text":"<p>HAMMER2 provides shared and exclusive locks on inodes with special SYNCQ semantics.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#basic-locking","title":"Basic Locking","text":"<pre><code>void hammer2_inode_lock(hammer2_inode_t *ip, int how);\nvoid hammer2_inode_unlock(hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:226-375</code></p> <p>The <code>how</code> parameter accepts flags:</p> Flag Effect <code>HAMMER2_RESOLVE_SHARED</code> Acquire shared lock (no SYNCQ wait) <code>HAMMER2_RESOLVE_ALWAYS</code> Ensure inode metadata is resolved <code>HAMMER2_RESOLVE_NEVER</code> Don't require metadata resolution <p>SYNCQ Semantics: When acquiring an exclusive lock, if the inode is on the SYNCQ (being synced), the caller must wait for the sync to complete. This prevents metadata dependencies from being split across flushes. To reduce stall time, the inode is moved to the head of the syncq before blocking:</p> <pre><code>while ((ip-&gt;flags &amp; HAMMER2_INODE_SYNCQ) &amp;&amp; pmp) {\n    /* Move to head of syncq and sleep */\n    TAILQ_REMOVE(&amp;pmp-&gt;syncq, ip, entry);\n    TAILQ_INSERT_HEAD(&amp;pmp-&gt;syncq, ip, entry);\n    hammer2_mtx_unlock(&amp;ip-&gt;lock);\n    tsleep(&amp;ip-&gt;flags, PINTERLOCKED, \"h2sync\", 0);\n    hammer2_mtx_ex(&amp;ip-&gt;lock);\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/inode-layer/#multi-inode-locking","title":"Multi-Inode Locking","text":"<pre><code>void hammer2_inode_lock4(hammer2_inode_t *ip1, hammer2_inode_t *ip2,\n                         hammer2_inode_t *ip3, hammer2_inode_t *ip4);\n</code></pre> <p>Source: <code>hammer2_inode.c:281-358</code></p> <p>Exclusively locks up to four inodes simultaneously, establishing a flush dependency between them. Used for operations like rename that span multiple inodes. The function:</p> <ol> <li>References and locks all inodes in order</li> <li>Establishes flush dependencies via <code>hammer2_inode_setdepend_locked()</code></li> <li>If any inode is on SYNCQ, unlocks all and retries after sleeping</li> </ol>"},{"location":"sys/vfs/hammer2/inode-layer/#lock-upgradedowngrade","title":"Lock Upgrade/Downgrade","text":"<pre><code>int hammer2_inode_lock_upgrade(hammer2_inode_t *ip);\nvoid hammer2_inode_lock_downgrade(hammer2_inode_t *ip, int wasexclusive);\n</code></pre> <p>Source: <code>hammer2_inode.c:508-533</code></p> <p>Upgrades a shared lock to exclusive, or downgrades back. The <code>wasexclusive</code> return value tracks whether the lock was already exclusive.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#temporary-releaserestore","title":"Temporary Release/Restore","text":"<pre><code>hammer2_mtx_state_t hammer2_inode_lock_temp_release(hammer2_inode_t *ip);\nvoid hammer2_inode_lock_temp_restore(hammer2_inode_t *ip, hammer2_mtx_state_t ostate);\n</code></pre> <p>Source: <code>hammer2_inode.c:487-497</code></p> <p>Temporarily releases a lock (to avoid deadlocks) and restores it later. Used by <code>hammer2_igetv()</code> when calling into the vnode layer.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#vnode-association","title":"Vnode Association","text":""},{"location":"sys/vfs/hammer2/inode-layer/#getting-a-vnode-for-an-inode","title":"Getting a Vnode for an Inode","text":"<pre><code>struct vnode *hammer2_igetv(hammer2_inode_t *ip, int *errorp);\n</code></pre> <p>Source: <code>hammer2_inode.c:669-816</code></p> <p>Returns an exclusively locked vnode for the given inode, allocating one if necessary. The caller must hold the inode locked.</p> <p>Algorithm:</p> <ol> <li>If <code>ip-&gt;vp</code> is non-NULL, attempt to reuse the existing vnode via <code>vget()</code></li> <li>If vget fails (lost reclaim race), retry</li> <li>If no vnode exists, allocate via <code>getnewvnode()</code></li> <li>Set vnode type based on <code>ip-&gt;meta.type</code>:</li> <li><code>HAMMER2_OBJTYPE_DIRECTORY</code> \u2192 <code>VDIR</code></li> <li><code>HAMMER2_OBJTYPE_REGFILE</code> \u2192 <code>VREG</code> (with buffer cache)</li> <li><code>HAMMER2_OBJTYPE_SOFTLINK</code> \u2192 <code>VLNK</code></li> <li><code>HAMMER2_OBJTYPE_CDEV/BDEV</code> \u2192 <code>VCHR/VBLK</code></li> <li><code>HAMMER2_OBJTYPE_FIFO</code> \u2192 <code>VFIFO</code></li> <li><code>HAMMER2_OBJTYPE_SOCKET</code> \u2192 <code>VSOCK</code></li> <li>Link vnode and inode: <code>vp-&gt;v_data = ip; ip-&gt;vp = vp;</code></li> <li>Add reference for vnode association</li> </ol> <p>Deadlock Avoidance: The inode lock must be temporarily released during <code>vget()</code> to avoid deadlocking against vnode reclaim:</p> <pre><code>vhold(vp);\nostate = hammer2_inode_lock_temp_release(ip);\nif (vget(vp, LK_EXCLUSIVE)) { /* retry */ }\nhammer2_inode_lock_temp_restore(ip, ostate);\nvdrop(vp);\n</code></pre>"},{"location":"sys/vfs/hammer2/inode-layer/#inode-creation","title":"Inode Creation","text":""},{"location":"sys/vfs/hammer2/inode-layer/#creating-normal-inodes","title":"Creating Normal Inodes","text":"<pre><code>hammer2_inode_t *\nhammer2_inode_create_normal(hammer2_inode_t *pip, struct vattr *vap,\n                            struct ucred *cred, hammer2_key_t inum,\n                            int *errorp);\n</code></pre> <p>Source: <code>hammer2_inode.c:1121-1263</code></p> <p>Creates a new inode (file, directory, symlink, device, etc.) as a child of the parent inode <code>pip</code>. The function:</p> <ol> <li>Allocates an in-memory inode via <code>hammer2_inode_get()</code></li> <li>Sets <code>HAMMER2_INODE_CREATING</code> flag (chains not yet in media topology)</li> <li>Initializes metadata from <code>vattr</code> and parent attributes</li> <li>Inherits compression/checksum algorithms from parent</li> <li>Creates detached media chains via <code>hammer2_inode_create_det_desc</code> XOP</li> <li>Associates chains with inode via <code>hammer2_inode_repoint()</code></li> </ol> <p>Note: The chains are created detached (not inserted into the on-disk tree). They will be inserted during the next filesystem sync when <code>hammer2_inode_chain_ins()</code> is called.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#creating-pfs-inodes","title":"Creating PFS Inodes","text":"<pre><code>hammer2_inode_t *\nhammer2_inode_create_pfs(hammer2_pfs_t *spmp, const char *name,\n                         size_t name_len, int *errorp);\n</code></pre> <p>Source: <code>hammer2_inode.c:988-1112</code></p> <p>Creates a PFS (Pseudo File System) root inode under the superroot. Unlike normal inodes, PFS inodes are inserted immediately (requires flush transaction).</p>"},{"location":"sys/vfs/hammer2/inode-layer/#creating-directory-entries","title":"Creating Directory Entries","text":"<pre><code>int hammer2_dirent_create(hammer2_inode_t *dip, const char *name,\n                          size_t name_len, hammer2_key_t inum,\n                          uint8_t type);\n</code></pre> <p>Source: <code>hammer2_inode.c:1273-1352</code></p> <p>Creates a directory entry linking a name to an inode number. Handles hash collisions by scanning for an unused key in the collision space:</p> <pre><code>lhc = hammer2_dirhash(name, name_len);\n/* Scan for unused key if collision */\nwhile (hammer2_xop_collect(&amp;sxop-&gt;head, 0) == 0) {\n    if (lhc != sxop-&gt;head.cluster.focus-&gt;bref.key)\n        break;\n    ++lhc;  /* Try next slot */\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/inode-layer/#inode-lookup-and-caching","title":"Inode Lookup and Caching","text":""},{"location":"sys/vfs/hammer2/inode-layer/#hammer2_inode_get","title":"hammer2_inode_get","text":"<pre><code>hammer2_inode_t *\nhammer2_inode_get(hammer2_pfs_t *pmp, hammer2_xop_head_t *xop,\n                  hammer2_tid_t inum, int idx);\n</code></pre> <p>Source: <code>hammer2_inode.c:845-979</code></p> <p>Gets or creates an inode structure. If <code>xop</code> is provided, synchronizes the inode's cluster with the XOP's cluster. The function:</p> <ol> <li>Looks up existing inode via <code>hammer2_inode_lookup()</code></li> <li>If found, repoints cluster chains and returns</li> <li>If not found, allocates new <code>hammer2_inode_t</code></li> <li>Initializes cluster, metadata, locks</li> <li>Attempts to insert into hash table (handles races)</li> </ol>"},{"location":"sys/vfs/hammer2/inode-layer/#cluster-repointing","title":"Cluster Repointing","text":""},{"location":"sys/vfs/hammer2/inode-layer/#full-cluster-repoint","title":"Full Cluster Repoint","text":"<pre><code>void hammer2_inode_repoint(hammer2_inode_t *ip, hammer2_cluster_t *cluster);\n</code></pre> <p>Source: <code>hammer2_inode.c:1364-1463</code></p> <p>Replaces all chains in the inode's embedded cluster with chains from the provided cluster. Also clears any cached data chains. Pass <code>NULL</code> to clean out all chains.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#single-chain-repoint","title":"Single Chain Repoint","text":"<pre><code>void hammer2_inode_repoint_one(hammer2_inode_t *ip, hammer2_cluster_t *cluster,\n                               int idx);\n</code></pre> <p>Source: <code>hammer2_inode.c:1470-1529</code></p> <p>Repoints a single chain element. Used by synchronization threads for piecemeal updates.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#modification-and-synchronization","title":"Modification and Synchronization","text":""},{"location":"sys/vfs/hammer2/inode-layer/#marking-inodes-modified","title":"Marking Inodes Modified","text":"<pre><code>void hammer2_inode_modify(hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:1674-1682</code></p> <p>Marks an inode as modified:</p> <ol> <li>Sets <code>HAMMER2_INODE_MODIFIED</code> flag</li> <li>Marks associated vnode dirty via <code>vsetisdirty()</code></li> <li>Adds inode to SIDEQ via <code>hammer2_inode_delayed_sideq()</code> (unless NOSIDEQ)</li> </ol> <p>Can be called with shared or exclusive lock.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#syncing-to-chains","title":"Syncing to Chains","text":"<pre><code>int hammer2_inode_chain_sync(hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:1694-1736</code></p> <p>Synchronizes the inode's frontend state with the chain state. Called before explicit flush or strategy write. Uses the <code>hammer2_inode_chain_sync_desc</code> XOP to propagate metadata changes.</p> <p>Handles <code>RESIZED</code> flag specially \u2014 if file grew past embedded bytes, clears <code>DIRECTDATA</code> flag.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#inserting-chains","title":"Inserting Chains","text":"<pre><code>int hammer2_inode_chain_ins(hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:1742-1767</code></p> <p>Inserts the inode's chains into the on-media tree. Called during sync for inodes with <code>INODE_CREATING</code> flag. Clears the flag upon success.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#deleting-chains","title":"Deleting Chains","text":"<pre><code>int hammer2_inode_chain_des(hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:1781-1806</code></p> <p>Removes the inode's chains from the media topology. Called during sync for inodes with <code>INODE_DELETING</code> flag. Clears both <code>DELETING</code> and <code>ISUNLINKED</code> flags.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#flushing-to-media","title":"Flushing to Media","text":"<pre><code>int hammer2_inode_chain_flush(hammer2_inode_t *ip, int flags);\n</code></pre> <p>Source: <code>hammer2_inode.c:1816-1831</code></p> <p>Flushes the inode's chain and sub-topology to media. Clears <code>DIRTYDATA</code> flag before flush. Uses <code>hammer2_inode_flush_desc</code> XOP.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#unlink-handling","title":"Unlink Handling","text":""},{"location":"sys/vfs/hammer2/inode-layer/#finishing-unlink","title":"Finishing Unlink","text":"<pre><code>int hammer2_inode_unlink_finisher(hammer2_inode_t *ip, struct vnode **vprecyclep);\n</code></pre> <p>Source: <code>hammer2_inode.c:1567-1631</code></p> <p>Called after <code>xop_unlink</code> to decrement nlinks and handle final deletion:</p> <ol> <li>If <code>nlinks &gt; 1</code>, just decrement and update ctime</li> <li>If <code>nlinks &lt;= 1</code>:</li> <li>Sets <code>ISUNLINKED</code> flag</li> <li>If no vnode, sets <code>DELETING</code> and adds to SIDEQ</li> <li>If vnode exists, defers vnode recycling to VOP completion</li> <li>Updates metadata via <code>hammer2_inode_modify()</code></li> </ol>"},{"location":"sys/vfs/hammer2/inode-layer/#vnode-recycling","title":"Vnode Recycling","text":"<pre><code>void hammer2_inode_vprecycle(struct vnode *vp);\n</code></pre> <p>Source: <code>hammer2_inode.c:1641-1652</code></p> <p>Aggressively recycles a vnode after file deletion. Called at the end of VOPs that remove files. Without this, deleted files can linger with nlinks==0 until random system activity reclaims the vnode.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#flush-dependency-management","title":"Flush Dependency Management","text":""},{"location":"sys/vfs/hammer2/inode-layer/#sideq-and-syncq","title":"SIDEQ and SYNCQ","text":"<p>Inodes that need flushing are managed through two queues:</p> <ul> <li>SIDEQ (Side Queue): Dirty inodes awaiting the next sync. Inodes without an attached vnode go here to ensure they're not missed by the vnode-based syncer.</li> <li>SYNCQ (Sync Queue): Inodes currently being synced. During sync, inodes are moved from SIDEQ to SYNCQ.</li> </ul>"},{"location":"sys/vfs/hammer2/inode-layer/#dependency-groups","title":"Dependency Groups","text":"<p>Related inodes are grouped via <code>hammer2_depend_t</code> structures to ensure atomic flushing:</p> <pre><code>void hammer2_inode_depend(hammer2_inode_t *ip1, hammer2_inode_t *ip2);\n</code></pre> <p>Source: <code>hammer2_inode.c:389-400</code></p> <p>Merges two inodes into a single dependency group. Used to ensure directory entries and their target inodes are flushed together.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#adding-to-sideq","title":"Adding to SIDEQ","text":"<pre><code>void hammer2_inode_delayed_sideq(hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:176-189</code></p> <p>Adds a dirty inode to the SIDEQ. Called from <code>hammer2_inode_modify()</code> and other places that dirty an inode.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#statistics","title":"Statistics","text":"<pre><code>hammer2_key_t hammer2_inode_data_count(const hammer2_inode_t *ip);\nhammer2_key_t hammer2_inode_inode_count(const hammer2_inode_t *ip);\n</code></pre> <p>Source: <code>hammer2_inode.c:1531-1561</code></p> <p>Returns the maximum data/inode count across all chains in the inode's cluster. Used for filesystem statistics.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#chain-access","title":"Chain Access","text":"<pre><code>hammer2_chain_t *hammer2_inode_chain(hammer2_inode_t *ip, int clindex, int how);\n</code></pre> <p>Source: <code>hammer2_inode.c:407-427</code></p> <p>Selects and locks a chain from the inode's cluster. The inode doesn't need to be locked. Returns a referenced and locked chain.</p> <pre><code>hammer2_chain_t *\nhammer2_inode_chain_and_parent(hammer2_inode_t *ip, int clindex,\n                               hammer2_chain_t **parentp, int how);\n</code></pre> <p>Source: <code>hammer2_inode.c:429-478</code></p> <p>Returns both a chain and its parent, properly locked. Handles lock ordering (parent before chain) and retries if the topology changes during locking.</p>"},{"location":"sys/vfs/hammer2/inode-layer/#lifecycle-summary","title":"Lifecycle Summary","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 hammer2_inode_  \u2502\n                    \u2502 create_normal() \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502 allocate + CREATING flag\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   In-Memory     \u2502\u25c4\u2500\u2500\u2500\u2500 hammer2_inode_get()\n                    \u2502     Inode       \u2502       (lookup/create)\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                \u2502                \u2502\n            \u25bc                \u25bc                \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502hammer2_igetv()\u2502 \u2502 modify()   \u2502 \u2502 unlink_       \u2502\n    \u2502  (get vnode)  \u2502 \u2502 (\u2192SIDEQ)   \u2502 \u2502 finisher()    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502                \u2502\n                            \u25bc                \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  ISUNLINKED\n                    \u2502   Filesystem    \u2502  DELETING\n                    \u2502      Sync       \u2502      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                             \u2502               \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        \u2502                    \u2502               \u2502\n        \u25bc                    \u25bc               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502chain_ins()    \u2502   \u2502chain_sync()   \u2502 \u2502chain_des()   \u2502\n\u2502(insert chains)\u2502   \u2502(sync metadata)\u2502 \u2502(delete chains)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                    \u2502               \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 chain_flush()   \u2502\n                    \u2502 (write to disk) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"sys/vfs/hammer2/inode-layer/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview \u2014 Filesystem architecture</li> <li>Chain Layer \u2014 Underlying block management</li> <li>VFS Integration \u2014 VFS operations implementation</li> <li>XOP System \u2014 Cross-cluster operations</li> <li>Flush and Sync \u2014 Synchronization mechanisms</li> </ul>"},{"location":"sys/vfs/hammer2/io-subsystem/","title":"HAMMER2 I/O Subsystem","text":"<p>Documentation Status</p> <p>This page documents the I/O subsystem based on <code>hammer2_io.c</code> and <code>hammer2_strategy.c</code>.</p>"},{"location":"sys/vfs/hammer2/io-subsystem/#overview","title":"Overview","text":"<p>HAMMER2's I/O subsystem provides a buffer cache abstraction layer (DIO) and strategy routines for read/write operations.</p>"},{"location":"sys/vfs/hammer2/io-subsystem/#dio-layer","title":"DIO Layer","text":"<p>Documentation in progress \u2014 will cover the DIO buffer abstraction.</p>"},{"location":"sys/vfs/hammer2/io-subsystem/#strategy-layer","title":"Strategy Layer","text":"<p>Documentation in progress \u2014 will cover read/write strategy implementation.</p>"},{"location":"sys/vfs/hammer2/io-subsystem/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview</li> <li>Buffer Cache \u2014 Kernel buffer cache</li> <li>Flush &amp; Sync \u2014 Flush mechanism</li> </ul>"},{"location":"sys/vfs/hammer2/on-disk-format/","title":"HAMMER2 On-Disk Format","text":"<p>This document describes the on-disk media structures for HAMMER2, based on <code>sys/vfs/hammer2/hammer2_disk.h</code>.</p>"},{"location":"sys/vfs/hammer2/on-disk-format/#overview","title":"Overview","text":"<p>HAMMER2 is a copy-on-write filesystem where all data references use 64-bit byte offsets. The filesystem revolves around a hierarchical block reference structure, with everything referenced through 128-byte blockrefs. Key characteristics:</p> <ul> <li>Block device buffers: Always 64KB (<code>HAMMER2_PBUFSIZE</code>)</li> <li>Logical file buffers: Typically 16KB (<code>HAMMER2_LBUFSIZE</code>)</li> <li>Minimum allocation: 1KB (<code>HAMMER2_ALLOC_MIN</code>)</li> <li>Maximum allocation: 64KB (<code>HAMMER2_ALLOC_MAX</code>)</li> <li>All fields: Naturally aligned, host byte order</li> </ul>"},{"location":"sys/vfs/hammer2/on-disk-format/#volume-layout","title":"Volume Layout","text":"<p>HAMMER2 media is organized into 2GB zones. Each zone begins with a 4MB reserved segment containing:</p> <ul> <li>Volume header (or backup)</li> <li>Freemap blocks (8 rotations \u00d7 5 levels)</li> <li>Reserved space for future use</li> </ul> <pre><code>Zone Layout (2GB each):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 4MB Reserved Segment (64 \u00d7 64KB blocks)                     \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 Block 0:  Volume Header (first 4 zones only)            \u2502 \u2502\n\u2502 \u2502 Block 1-5:   Freemap Set 0 (levels 1-5)                 \u2502 \u2502\n\u2502 \u2502 Block 6-10:  Freemap Set 1 (levels 1-5)                 \u2502 \u2502\n\u2502 \u2502 Block 11-15: Freemap Set 2 (levels 1-5)                 \u2502 \u2502\n\u2502 \u2502 Block 16-20: Freemap Set 3 (levels 1-5)                 \u2502 \u2502\n\u2502 \u2502 Block 21-25: Freemap Set 4 (levels 1-5)                 \u2502 \u2502\n\u2502 \u2502 Block 26-30: Freemap Set 5 (levels 1-5)                 \u2502 \u2502\n\u2502 \u2502 Block 31-35: Freemap Set 6 (levels 1-5)                 \u2502 \u2502\n\u2502 \u2502 Block 36-40: Freemap Set 7 (levels 1-5)                 \u2502 \u2502\n\u2502 \u2502 Block 41-63: Reserved/Unused                            \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502 ~2GB - 4MB: Allocatable Storage                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"sys/vfs/hammer2/on-disk-format/#volume-headers","title":"Volume Headers","text":"<p>Up to 4 volume headers exist at the start of the first four 2GB zones (offsets 0, 2GB, 4GB, 6GB). The filesystem rotates through these on each flush, providing crash recovery points.</p> <p>Source: <code>hammer2_disk.h:1139-1141</code> <pre><code>#define HAMMER2_VOLUME_ID_HBO   0x48414d3205172011LLU  /* \"HAM2\" + date */\n#define HAMMER2_VOLUME_ID_ABO   0x11201705324d4148LLU  /* byte-swapped */\n</code></pre></p>"},{"location":"sys/vfs/hammer2/on-disk-format/#volume-header-structure","title":"Volume Header Structure","text":"<p>The volume header is a 64KB block containing filesystem metadata:</p> <p>Source: <code>hammer2_disk.h:1152-1276</code> (<code>struct hammer2_volume_data</code>)</p> <pre><code>Volume Header Layout (64KB):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Sector 0 (0x000-0x1FF): Core Metadata - 512 bytes          \u2502\n\u2502   magic, boot/aux areas, volume size, version, flags       \u2502\n\u2502   fsid, fstype, allocator info, mirror_tid, freemap_tid    \u2502\n\u2502   icrc_sects[8] at end                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Sector 1 (0x200-0x3FF): Super-root Blockset - 512 bytes    \u2502\n\u2502   4 \u00d7 128-byte blockrefs pointing to super-root            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Sector 2 (0x400-0x5FF): Reserved - 512 bytes               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Sector 3 (0x600-0x7FF): Reserved - 512 bytes               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Sector 4 (0x800-0x9FF): Freemap Blockset - 512 bytes       \u2502\n\u2502   4 \u00d7 128-byte blockrefs for freemap root                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Sector 5 (0xA00-0xBFF): Reserved - 512 bytes               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Sector 6 (0xC00-0xDFF): Reserved - 512 bytes               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Sector 7 (0xE00-0xFFF): Volume Offsets - 512 bytes         \u2502\n\u2502   volu_loff[64] - offsets for multi-volume support         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Sectors 8-71 (0x1000-0x8FFF): Copy Info - 32KB             \u2502\n\u2502   copyinfo[256] - cluster configuration                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Reserved (0x9000-0xFFFB): Future Use - ~28KB               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Final 4 bytes (0xFFFC-0xFFFF): Volume Header CRC           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"sys/vfs/hammer2/on-disk-format/#key-volume-header-fields","title":"Key Volume Header Fields","text":"Offset Field Description 0x0000 <code>magic</code> Volume signature (<code>HAMMER2_VOLUME_ID_HBO</code>) 0x0008 <code>boot_beg/end</code> Boot area boundaries 0x0018 <code>aux_beg/end</code> Auxiliary area boundaries 0x0028 <code>volu_size</code> Volume size in bytes 0x0030 <code>version</code> Volume format version 0x0034 <code>flags</code> Volume flags 0x0038 <code>copyid</code> Copy ID of this physical volume 0x0039 <code>freemap_version</code> Freemap algorithm version 0x003B <code>volu_id</code> Volume ID (0-63) 0x003C <code>nvolumes</code> Number of volumes 0x0040 <code>fsid</code> Filesystem UUID 0x0050 <code>fstype</code> Filesystem type UUID 0x0060 <code>allocator_size</code> Total allocatable space 0x0068 <code>allocator_free</code> Free space remaining 0x0070 <code>allocator_beg</code> Start of dynamic allocations 0x0078 <code>mirror_tid</code> Highest committed topology TID 0x0090 <code>freemap_tid</code> Highest committed freemap TID 0x0098 <code>bulkfree_tid</code> Bulkfree incremental TID 0x00C0 <code>total_size</code> Total size across all volumes 0x0200 <code>sroot_blockset</code> Super-root directory blockrefs 0x0800 <code>freemap_blockset</code> Freemap root blockrefs"},{"location":"sys/vfs/hammer2/on-disk-format/#block-reference-blockref","title":"Block Reference (blockref)","text":"<p>The blockref is HAMMER2's fundamental building block \u2014 a 128-byte structure that references any block in the filesystem. Blockrefs appear in:</p> <ul> <li>Volume header (super-root and freemap)</li> <li>Inodes (embedded blockset)</li> <li>Indirect blocks (arrays of 512 blockrefs per 64KB)</li> </ul> <p>Source: <code>hammer2_disk.h:619-697</code> (<code>struct hammer2_blockref</code>)</p> <pre><code>struct hammer2_blockref {          /* MUST BE EXACTLY 128 BYTES */\n    uint8_t     type;              /* 00: type of underlying item */\n    uint8_t     methods;           /* 01: check method &amp; compression method */\n    uint8_t     copyid;            /* 02: specify which copy this is */\n    uint8_t     keybits;           /* 03: #of keybits masked off 0=leaf */\n    uint8_t     vradix;            /* 04: virtual data/meta-data size */\n    uint8_t     flags;             /* 05: blockref flags */\n    uint16_t    leaf_count;        /* 06-07: leaf aggregation count */\n    hammer2_key_t   key;           /* 08-0F: key specification */\n    hammer2_tid_t   mirror_tid;    /* 10-17: media flush tid */\n    hammer2_tid_t   modify_tid;    /* 18-1F: clc modify (not propagated) */\n    hammer2_off_t   data_off;      /* 20-27: physical offset + radix */\n    hammer2_tid_t   update_tid;    /* 28-2F: clc update (propagated up) */\n    union {                        /* 30-3F: embedded data (16 bytes) */\n        char buf[16];\n        hammer2_dirent_head_t dirent;  /* directory entry header */\n        struct {\n            hammer2_key_t data_count;\n            hammer2_key_t inode_count;\n        } stats;                   /* statistics for INODE/INDIRECT */\n    } embed;\n    union {                        /* 40-7F: check data (64 bytes) */\n        char buf[64];\n        struct { uint32_t value; } iscsi32;\n        struct { uint64_t value; } xxhash64;\n        struct { char data[24]; } sha192;\n        struct { char data[32]; } sha256;\n        struct { char data[64]; } sha512;\n        struct {                   /* freemap hints */\n            uint32_t icrc32;\n            uint32_t bigmask;      /* available radixes */\n            uint64_t avail;        /* available bytes */\n        } freemap;\n    } check;\n};\n</code></pre>"},{"location":"sys/vfs/hammer2/on-disk-format/#blockref-type-field","title":"Blockref Type Field","text":"<p>Source: <code>hammer2_disk.h:711-720</code></p> Value Name Description 0 <code>EMPTY</code> Empty/unused slot 1 <code>INODE</code> Inode block 2 <code>INDIRECT</code> Indirect block 3 <code>DATA</code> Data block 4 <code>DIRENT</code> Directory entry (embedded in blockref) 5 <code>FREEMAP_NODE</code> Freemap indirect node 6 <code>FREEMAP_LEAF</code> Freemap leaf (bitmap) 254 <code>FREEMAP</code> Pseudo-type for freemap root 255 <code>VOLUME</code> Pseudo-type for volume header"},{"location":"sys/vfs/hammer2/on-disk-format/#data-offset-encoding","title":"Data Offset Encoding","text":"<p>The <code>data_off</code> field encodes both the physical block address and the block size:</p> <p>Source: <code>hammer2_disk.h:458-461</code> <pre><code>#define HAMMER2_OFF_MASK        0xFFFFFFFFFFFFFFC0ULL  /* address bits */\n#define HAMMER2_OFF_MASK_RADIX  0x000000000000003FULL  /* size radix (low 6 bits) */\n</code></pre></p> <ul> <li>Bits 63-6: Physical byte offset (64-byte aligned)</li> <li>Bits 5-0: Size radix (actual size = <code>1 &lt;&lt; radix</code>)</li> </ul> <p>A radix of 0 with <code>data_off = 0</code> indicates no data associated with the blockref.</p>"},{"location":"sys/vfs/hammer2/on-disk-format/#check-methods","title":"Check Methods","text":"<p>Source: <code>hammer2_disk.h:729-736</code></p> Value Name Description 0 <code>NONE</code> No check code 1 <code>DISABLED</code> Check code disabled 2 <code>ISCSI32</code> 32-bit iSCSI CRC 3 <code>XXHASH64</code> 64-bit xxHash (default) 4 <code>SHA192</code> 192-bit SHA hash 5 <code>FREEMAP</code> Freemap-specific check"},{"location":"sys/vfs/hammer2/on-disk-format/#compression-methods","title":"Compression Methods","text":"<p>Source: <code>hammer2_disk.h:741-746</code></p> Value Name Description 0 <code>NONE</code> No compression 1 <code>AUTOZERO</code> Auto-zero detection 2 <code>LZ4</code> LZ4 compression (default) 3 <code>ZLIB</code> ZLIB compression"},{"location":"sys/vfs/hammer2/on-disk-format/#blockset-structure","title":"Blockset Structure","text":"<p>A blockset is an array of 4 blockrefs, used in volume headers and inodes:</p> <p>Source: <code>hammer2_disk.h:782-786</code> <pre><code>struct hammer2_blockset {\n    hammer2_blockref_t  blockref[HAMMER2_SET_COUNT];  /* 4 entries */\n};\n</code></pre></p> <p>This provides 4 \u00d7 128 = 512 bytes of block references.</p>"},{"location":"sys/vfs/hammer2/on-disk-format/#inode-structure","title":"Inode Structure","text":"<p>Inodes are 1KB structures containing metadata and either direct data or blockrefs:</p> <p>Source: <code>hammer2_disk.h:1010-1020</code> (<code>struct hammer2_inode_data</code>)</p> <pre><code>Inode Layout (1024 bytes):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 meta (0x000-0x0FF): Inode Metadata - 256 bytes             \u2502\n\u2502   version, timestamps, uid/gid, type, mode, size           \u2502\n\u2502   inum, nlinks, name_key, compression/check settings       \u2502\n\u2502   PFS-specific fields, quotas                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 filename (0x100-0x1FF): Filename - 256 bytes               \u2502\n\u2502   Up to 256 characters, unterminated                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 u (0x200-0x3FF): Data/Blockset Union - 512 bytes           \u2502\n\u2502   EITHER: blockset (4 \u00d7 128-byte blockrefs)                \u2502\n\u2502   OR:     data[512] (direct embedded data)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"sys/vfs/hammer2/on-disk-format/#inode-metadata-fields","title":"Inode Metadata Fields","text":"<p>Source: <code>hammer2_disk.h:921-1006</code> (<code>struct hammer2_inode_meta</code>)</p> Offset Field Description 0x00 <code>version</code> Inode data version 0x03 <code>pfs_subtype</code> PFS sub-type (snapshot, etc.) 0x04 <code>uflags</code> User flags (chflags) 0x08 <code>rmajor/rminor</code> Device node major/minor 0x10 <code>ctime</code> Inode change time 0x18 <code>mtime</code> Modification time 0x20 <code>atime</code> Access time (unsupported) 0x28 <code>btime</code> Birth time 0x30 <code>uid</code> User ID (UUID) 0x40 <code>gid</code> Group ID (UUID) 0x50 <code>type</code> Object type 0x51 <code>op_flags</code> Operational flags 0x52 <code>cap_flags</code> Capability flags 0x54 <code>mode</code> Unix permissions 0x58 <code>inum</code> Inode number 0x60 <code>size</code> File size 0x68 <code>nlinks</code> Hard link count 0x70 <code>iparent</code> Nominal parent inode 0x78 <code>name_key</code> Filename hash key 0x80 <code>name_len</code> Filename length 0x82 <code>ncopies</code> Number of copies 0x83 <code>comp_algo</code> Compression algorithm 0x85 <code>check_algo</code> Check code algorithm 0x86 <code>pfs_nmasters</code> PFS master count 0x87 <code>pfs_type</code> PFS type 0x88 <code>pfs_inum</code> PFS inode allocator 0x90 <code>pfs_clid</code> PFS cluster UUID 0xA0 <code>pfs_fsid</code> PFS unique UUID 0xB0 <code>data_quota</code> Data quota in bytes 0xC0 <code>inode_quota</code> Inode count quota"},{"location":"sys/vfs/hammer2/on-disk-format/#object-types","title":"Object Types","text":"<p>Source: <code>hammer2_disk.h:1026-1035</code></p> Value Name Description 0 <code>UNKNOWN</code> Unknown type 1 <code>DIRECTORY</code> Directory 2 <code>REGFILE</code> Regular file 4 <code>FIFO</code> FIFO 5 <code>CDEV</code> Character device 6 <code>BDEV</code> Block device 7 <code>SOFTLINK</code> Symbolic link 9 <code>SOCKET</code> Socket 10 <code>WHITEOUT</code> Whiteout entry"},{"location":"sys/vfs/hammer2/on-disk-format/#direct-data-vs-blockrefs","title":"Direct Data vs Blockrefs","text":"<p>Files \u2264 512 bytes store data directly in the inode's <code>u.data[512]</code> area. The <code>OPFLAG_DIRECTDATA</code> flag indicates this mode:</p> <p>Source: <code>hammer2_disk.h:1022</code> <pre><code>#define HAMMER2_OPFLAG_DIRECTDATA   0x01\n</code></pre></p> <p>Larger files use the <code>u.blockset</code> containing 4 blockrefs, which can reference: - Up to 4 data blocks directly (files \u2264 256KB with 64KB blocks) - Indirect blocks for larger files</p>"},{"location":"sys/vfs/hammer2/on-disk-format/#directory-entry-structure","title":"Directory Entry Structure","text":"<p>Small directory entries (filename \u2264 64 bytes) are embedded directly in blockrefs:</p> <p>Source: <code>hammer2_disk.h:549-555</code> (<code>struct hammer2_dirent_head</code>)</p> <pre><code>struct hammer2_dirent_head {\n    hammer2_tid_t   inum;       /* 00-07: inode number */\n    uint16_t        namlen;     /* 08-09: name length */\n    uint8_t         type;       /* 0A: OBJTYPE_* */\n    uint8_t         unused0B;   /* 0B: unused */\n    uint8_t         unused0C[4];/* 0C-0F: unused */\n};\n</code></pre> <p>The 16-byte header fits in <code>blockref.embed.dirent</code>, and the filename (up to 64 bytes) fits in <code>blockref.check.buf[64]</code>. Longer filenames require a separate 1KB data block.</p>"},{"location":"sys/vfs/hammer2/on-disk-format/#freemap-structure","title":"Freemap Structure","text":"<p>The freemap tracks free space using a hierarchical bitmap structure:</p>"},{"location":"sys/vfs/hammer2/on-disk-format/#freemap-hierarchy","title":"Freemap Hierarchy","text":"<p>Source: <code>hammer2_disk.h:335-341</code></p> Level Radix Coverage Structure 0 22 4MB Bitmap entries (256 \u00d7 16KB chunks) 1 30 1GB Leaf block (<code>hammer2_bmap_data[256]</code>) 2 38 256GB Indirect node (256 blockrefs) 3 46 64TB Indirect node (256 blockrefs) 4 54 16PB Indirect node (256 blockrefs) 5 62 4EB Indirect node (256 blockrefs) 6 64 16EB Volume header (4 blockrefs)"},{"location":"sys/vfs/hammer2/on-disk-format/#freemap-leaf-structure-bmap_data","title":"Freemap Leaf Structure (bmap_data)","text":"<p>Source: <code>hammer2_disk.h:871-885</code> (<code>struct hammer2_bmap_data</code>)</p> <pre><code>struct hammer2_bmap_data {\n    int32_t  linear;           /* 00-03: linear sub-granular offset */\n    uint16_t class;            /* 04-05: clustering class */\n    uint8_t  reserved06;       /* 06 */\n    uint8_t  reserved07;       /* 07 */\n    uint32_t reserved08;       /* 08-0B */\n    uint32_t reserved0C;       /* 0C-0F */\n    uint32_t reserved10;       /* 10-13 */\n    uint32_t reserved14;       /* 14-17 */\n    uint32_t reserved18;       /* 18-1B */\n    uint32_t avail;            /* 1C-1F: available bytes */\n    uint32_t reserved20[8];    /* 20-3F */\n    hammer2_bitmap_t bitmapq[8]; /* 40-7F: 512 bits = 256 \u00d7 2-bit entries */\n};\n</code></pre> <p>Each 128-byte <code>bmap_data</code> entry manages 4MB of storage: - 8 \u00d7 64-bit bitmap words = 512 bits - 2 bits per 16KB chunk = 256 chunks - 256 \u00d7 16KB = 4MB</p>"},{"location":"sys/vfs/hammer2/on-disk-format/#bitmap-encoding","title":"Bitmap Encoding","text":"<p>Source: <code>hammer2_disk.h:836-843</code></p> Bits State Description 00 Free Block is unallocated 01 Reserved (reserved for future use) 10 Possibly Free Marked by first bulkfree pass 11 Allocated Block is in use"},{"location":"sys/vfs/hammer2/on-disk-format/#linear-allocator","title":"Linear Allocator","text":"<p>The <code>linear</code> field enables sub-16KB allocations (down to 1KB) within an already-allocated 16KB chunk. This is tracked in-memory and may be lost on unmount, causing the entire 16KB to appear allocated until a bulkfree scan reclaims it.</p>"},{"location":"sys/vfs/hammer2/on-disk-format/#key-constants","title":"Key Constants","text":"<p>Source: <code>hammer2_disk.h:86-121</code></p> Constant Value Description <code>HAMMER2_ALLOC_MIN</code> 1024 Minimum allocation (1KB) <code>HAMMER2_ALLOC_MAX</code> 65536 Maximum allocation (64KB) <code>HAMMER2_PBUFSIZE</code> 65536 Physical buffer size (64KB) <code>HAMMER2_LBUFSIZE</code> 16384 Logical buffer size (16KB) <code>HAMMER2_SEGSIZE</code> 4MB Freemap segment size <code>HAMMER2_BLOCKREF_BYTES</code> 128 Blockref structure size <code>HAMMER2_INODE_BYTES</code> 1024 Inode structure size <code>HAMMER2_INODE_MAXNAME</code> 256 Maximum filename length <code>HAMMER2_EMBEDDED_BYTES</code> 512 Embedded data/blockset size <code>HAMMER2_SET_COUNT</code> 4 Blockrefs per blockset <code>HAMMER2_IND_COUNT_MAX</code> 512 Max blockrefs per indirect block <code>HAMMER2_NUM_VOLHDRS</code> 4 Number of volume headers <code>HAMMER2_MAX_VOLUMES</code> 64 Maximum volumes in filesystem <code>HAMMER2_ZONE_BYTES64</code> 2GB Zone size <code>HAMMER2_ZONE_SEG</code> 4MB Reserved segment per zone"},{"location":"sys/vfs/hammer2/on-disk-format/#pfs-types","title":"PFS Types","text":"<p>Source: <code>hammer2_disk.h:1063-1073</code></p> Value Name Description 0x00 <code>NONE</code> No PFS type 0x01 <code>CACHE</code> Local cache 0x03 <code>SLAVE</code> Cluster slave 0x04 <code>SOFT_SLAVE</code> Soft slave (local reads) 0x05 <code>SOFT_MASTER</code> Soft master (local writes) 0x06 <code>MASTER</code> Cluster master 0x08 <code>SUPROOT</code> Super-root 0x09 <code>DUMMY</code> Dummy/placeholder"},{"location":"sys/vfs/hammer2/on-disk-format/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview \u2014 Architecture overview</li> <li>Chain Layer \u2014 In-memory representation</li> <li>Freemap Management \u2014 Runtime freemap operations</li> <li>VFS Operations \u2014 VFS framework</li> </ul>"},{"location":"sys/vfs/hammer2/snapshots/","title":"HAMMER2 Snapshots","text":"<p>Documentation Status</p> <p>This page documents snapshot implementation.</p>"},{"location":"sys/vfs/hammer2/snapshots/#overview","title":"Overview","text":"<p>HAMMER2's copy-on-write architecture enables instant, space-efficient snapshots.</p>"},{"location":"sys/vfs/hammer2/snapshots/#snapshot-mechanism","title":"Snapshot Mechanism","text":"<p>Documentation in progress \u2014 will cover snapshot creation and management.</p>"},{"location":"sys/vfs/hammer2/snapshots/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview</li> <li>Chain Layer \u2014 Copy-on-write chains</li> </ul>"},{"location":"sys/vfs/hammer2/vfs-integration/","title":"HAMMER2 VFS Integration","text":"<p>This document describes how HAMMER2 integrates with the DragonFly BSD VFS layer, covering mount/unmount operations, vnode operations, and the key data structures that bridge the filesystem with the kernel.</p> <p>Source files:</p> <ul> <li><code>sys/vfs/hammer2/hammer2_vfsops.c</code> - VFS operations (mount, unmount, sync, statfs)</li> <li><code>sys/vfs/hammer2/hammer2_vnops.c</code> - Vnode operations (read, write, lookup, etc.)</li> <li><code>sys/vfs/hammer2/hammer2.h</code> - Structure definitions</li> </ul>"},{"location":"sys/vfs/hammer2/vfs-integration/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/vfs/hammer2/vfs-integration/#hammer2_dev_t-hmp","title":"hammer2_dev_t (hmp)","text":"<p>The <code>hammer2_dev_t</code> structure represents a physical block device mount. A single device can host multiple PFSs (Pseudo-FileSystems), each potentially part of different clusters.</p> <pre><code>struct hammer2_dev {\n    struct vnode    *devvp;         /* device vnode for root volume */\n    int             ronly;          /* read-only mount */\n    int             mount_count;    /* number of actively mounted PFSs */\n\n    hammer2_chain_t vchain;         /* anchor chain (volume topology) */\n    hammer2_chain_t fchain;         /* anchor chain (freemap) */\n    struct hammer2_pfs *spmp;       /* super-root pmp for transactions */\n\n    hammer2_volume_data_t voldata;  /* in-memory volume header */\n    hammer2_volume_data_t volsync;  /* synchronized voldata */\n\n    hammer2_io_hash_t iohash[HAMMER2_IOHASH_SIZE];  /* DIO cache */\n    hammer2_devvp_list_t devvpl;    /* list of device vnodes */\n    hammer2_volume_t volumes[HAMMER2_MAX_VOLUMES];  /* volume array */\n\n    int             volhdrno;       /* last volume header written */\n    hammer2_off_t   total_size;     /* total size of all volumes */\n    int             nvolumes;       /* number of volumes */\n    /* ... additional fields ... */\n};\n</code></pre> <p>Key relationships:</p> <ul> <li><code>vchain</code> - Root of the volume's block topology (type <code>HAMMER2_BREF_TYPE_VOLUME</code>)</li> <li><code>fchain</code> - Root of the freemap topology (type <code>HAMMER2_BREF_TYPE_FREEMAP</code>)</li> <li><code>spmp</code> - Super-root PFS used for device-level transactions</li> <li><code>voldata</code> - Copy of the on-disk volume header</li> </ul> <p>Defined at: <code>hammer2.h:1126</code></p>"},{"location":"sys/vfs/hammer2/vfs-integration/#hammer2_pfs_t-pmp","title":"hammer2_pfs_t (pmp)","text":"<p>The <code>hammer2_pfs_t</code> structure represents a mounted PFS (Pseudo-FileSystem). Each PFS can be independently mounted and may be part of a multi-node cluster.</p> <pre><code>struct hammer2_pfs {\n    struct mount        *mp;            /* system mount point */\n    uuid_t              pfs_clid;       /* PFS cluster ID */\n    hammer2_dev_t       *spmp_hmp;      /* non-NULL if super-root pmp */\n    hammer2_inode_t     *iroot;         /* PFS root inode */\n\n    uint8_t             pfs_types[HAMMER2_MAXCLUSTER];\n    char                *pfs_names[HAMMER2_MAXCLUSTER];\n    hammer2_dev_t       *pfs_hmps[HAMMER2_MAXCLUSTER];\n\n    hammer2_trans_t     trans;          /* transaction state */\n    int                 ronly;          /* read-only mount */\n\n    hammer2_tid_t       modify_tid;     /* modify transaction id */\n    hammer2_tid_t       inode_tid;      /* next inode number */\n    uint8_t             pfs_nmasters;   /* total masters in cluster */\n\n    hammer2_inum_hash_t inumhash[HAMMER2_INUMHASH_SIZE];  /* inode cache */\n    struct inoq_head    syncq;          /* inodes pending sync */\n    struct depq_head    depq;           /* side-queue inodes */\n\n    hammer2_xop_group_t *xop_groups;    /* XOP worker threads */\n    /* ... additional fields ... */\n};\n</code></pre> <p>Key relationships:</p> <ul> <li><code>mp</code> - Pointer to the kernel <code>struct mount</code> (NULL if not mounted)</li> <li><code>iroot</code> - Root inode of this PFS</li> <li><code>pfs_hmps[]</code> - Array of device mounts backing this cluster</li> </ul> <p>Defined at: <code>hammer2.h:1202</code></p>"},{"location":"sys/vfs/hammer2/vfs-integration/#accessor-macros","title":"Accessor Macros","text":"<pre><code>/* Get pmp from mount point */\n#define MPTOPMP(mp)  ((hammer2_pfs_t *)mp-&gt;mnt_data)\n\n/* Get inode from vnode */\n#define VTOI(vp)     ((hammer2_inode_t *)(vp)-&gt;v_data)\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#vfs-operations","title":"VFS Operations","text":""},{"location":"sys/vfs/hammer2/vfs-integration/#vfs-operations-table","title":"VFS Operations Table","text":"<p>The VFS operations table is defined at <code>hammer2_vfsops.c:221</code>:</p> <pre><code>static struct vfsops hammer2_vfsops = {\n    .vfs_init       = hammer2_vfs_init,\n    .vfs_uninit     = hammer2_vfs_uninit,\n    .vfs_sync       = hammer2_vfs_sync,\n    .vfs_mount      = hammer2_vfs_mount,\n    .vfs_unmount    = hammer2_vfs_unmount,\n    .vfs_root       = hammer2_vfs_root,\n    .vfs_statfs     = hammer2_vfs_statfs,\n    .vfs_statvfs    = hammer2_vfs_statvfs,\n    .vfs_vget       = hammer2_vfs_vget,\n    .vfs_vptofh     = hammer2_vfs_vptofh,\n    .vfs_fhtovp     = hammer2_vfs_fhtovp,\n    .vfs_checkexp   = hammer2_vfs_checkexp,\n    .vfs_modifying  = hammer2_vfs_modifying\n};\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#module-initialization","title":"Module Initialization","text":"<p>Function: <code>hammer2_vfs_init()</code> at <code>hammer2_vfsops.c:245</code></p> <p>Called when the HAMMER2 module is loaded. Performs:</p> <ol> <li> <p>XOP thread calculation - Determines worker thread count based on CPU count:    <pre><code>hammer2_xop_nthreads = ncpus * 2;  /* minimum */\n</code></pre></p> </li> <li> <p>Object cache creation - Creates caches for:</p> </li> <li><code>cache_buffer_read</code> - 64KB decompression buffers</li> <li><code>cache_buffer_write</code> - 32KB compression buffers</li> <li> <p><code>cache_xops</code> - XOP operation structures</p> </li> <li> <p>Global list initialization:</p> </li> <li><code>hammer2_mntlist</code> - List of mounted devices</li> <li><code>hammer2_pfslist</code> - List of mounted PFSs</li> <li> <p><code>hammer2_spmplist</code> - List of super-root PMPs</p> </li> <li> <p>Limit calculation:    <pre><code>hammer2_limit_dirty_chains = maxvnodes / 10;\nhammer2_limit_dirty_inodes = maxvnodes / 25;\n</code></pre></p> </li> </ol>"},{"location":"sys/vfs/hammer2/vfs-integration/#mount-operation","title":"Mount Operation","text":"<p>Function: <code>hammer2_vfs_mount()</code> at <code>hammer2_vfsops.c:913</code></p> <p>The mount operation handles both initial mounts and remounts (MNT_UPDATE).</p>"},{"location":"sys/vfs/hammer2/vfs-integration/#mount-flow","title":"Mount Flow","text":"<pre><code>mount(2)\n    |\n    v\nhammer2_vfs_mount()\n    |\n    +-- If MNT_UPDATE: call hammer2_remount()\n    |\n    +-- Parse device@LABEL specification\n    |       - If no label: auto-select BOOT/ROOT/DATA based on slice\n    |\n    +-- Check if device already mounted (reuse existing hmp)\n    |\n    +-- If new device mount:\n    |       |\n    |       +-- hammer2_init_devvp() - Initialize device vnodes\n    |       +-- hammer2_open_devvp() - Open device(s)\n    |       +-- hammer2_init_volumes() - Read volume headers\n    |       +-- Allocate hammer2_dev_t (hmp)\n    |       +-- Initialize vchain and fchain\n    |       +-- Lookup super-root inode\n    |       +-- Create super-root PFS (spmp)\n    |       +-- hammer2_recovery() - Perform recovery if needed\n    |       +-- hammer2_update_pmps() - Scan and create PFS structures\n    |\n    +-- Lookup requested PFS label under super-root\n    |\n    +-- hammer2_pfsalloc() - Get/create PFS structure\n    |\n    +-- hammer2_mount_helper() - Connect mp to pmp\n    |\n    +-- vfs_add_vnodeops() - Register vnode operations:\n            - hammer2_vnode_vops (regular files/directories)\n            - hammer2_spec_vops (special devices)\n            - hammer2_fifo_vops (FIFOs)\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#label-auto-selection","title":"Label Auto-Selection","text":"<p>When no label is specified, HAMMER2 selects based on the partition: - Slice 'a' -&gt; <code>BOOT</code> - Slice 'd' -&gt; <code>ROOT</code> - Other -&gt; <code>DATA</code></p>"},{"location":"sys/vfs/hammer2/vfs-integration/#super-root-initialization","title":"Super-Root Initialization","text":"<p>The super-root (<code>spmp</code>) is a special PFS that represents the device's namespace. All user PFSs are stored as directory entries under the super-root.</p> <pre><code>hmp-&gt;spmp = hammer2_pfsalloc(NULL, NULL, NULL);\nspmp = hmp-&gt;spmp;\nspmp-&gt;pfs_hmps[0] = hmp;\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#unmount-operation","title":"Unmount Operation","text":"<p>Function: <code>hammer2_vfs_unmount()</code> at <code>hammer2_vfsops.c:1629</code></p> <pre><code>static int\nhammer2_vfs_unmount(struct mount *mp, int mntflags)\n{\n    pmp = MPTOPMP(mp);\n\n    /* Flush vnodes */\n    error = vflush(mp, 0, flags);\n\n    /* Three syncs required for complete flush:\n     * 1. Flush data\n     * 2. Flush freemap updates (lag by one)\n     * 3. Safety sync\n     */\n    hammer2_vfs_sync(mp, MNT_WAIT);\n    hammer2_vfs_sync(mp, MNT_WAIT);\n    hammer2_vfs_sync(mp, MNT_WAIT);\n\n    /* Cleanup XOP threads */\n    hammer2_xop_helper_cleanup(pmp);\n\n    /* Disconnect mount */\n    hammer2_unmount_helper(mp, pmp, NULL);\n}\n</code></pre> <p>The unmount helper (<code>hammer2_unmount_helper()</code>) handles:</p> <ol> <li>Disconnecting <code>mp</code> from <code>pmp</code></li> <li>Decrementing <code>mount_count</code> on each backing device</li> <li>If <code>mount_count</code> reaches zero, cleaning up the device:</li> <li>Flushing vchain and fchain</li> <li>Closing device vnodes</li> <li>Freeing the <code>hammer2_dev_t</code></li> </ol>"},{"location":"sys/vfs/hammer2/vfs-integration/#root-vnode","title":"Root Vnode","text":"<p>Function: <code>hammer2_vfs_root()</code> at <code>hammer2_vfsops.c:1949</code></p> <p>Returns the root vnode for a mounted PFS:</p> <pre><code>static int\nhammer2_vfs_root(struct mount *mp, struct vnode **vpp)\n{\n    pmp = MPTOPMP(mp);\n\n    hammer2_inode_lock(pmp-&gt;iroot, HAMMER2_RESOLVE_SHARED);\n\n    /* First access may need to initialize inode_tid */\n    while (pmp-&gt;inode_tid == 0) {\n        /* Execute ipcluster XOP to get root inode data */\n        xop = hammer2_xop_alloc(pmp-&gt;iroot, HAMMER2_XOP_MODIFYING);\n        hammer2_xop_start(&amp;xop-&gt;head, &amp;hammer2_ipcluster_desc);\n        /* ... initialize pmp-&gt;inode_tid and pmp-&gt;modify_tid ... */\n    }\n\n    *vpp = hammer2_igetv(pmp-&gt;iroot, &amp;error);\n    hammer2_inode_unlock(pmp-&gt;iroot);\n\n    return error;\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#vnode-lookup-by-inode-number","title":"Vnode Lookup by Inode Number","text":"<p>Function: <code>hammer2_vfs_vget()</code> at <code>hammer2_vfsops.c:1898</code></p> <p>Used by NFS and other subsystems to obtain a vnode given an inode number:</p> <pre><code>int\nhammer2_vfs_vget(struct mount *mp, struct vnode *dvp,\n                 ino_t ino, struct vnode **vpp)\n{\n    inum = (hammer2_tid_t)ino &amp; HAMMER2_DIRHASH_USERMSK;\n    pmp = MPTOPMP(mp);\n\n    /* Try inode cache first */\n    ip = hammer2_inode_lookup(pmp, inum);\n    if (ip) {\n        *vpp = hammer2_igetv(ip, &amp;error);\n        return error;\n    }\n\n    /* Search via XOP */\n    xop = hammer2_xop_alloc(pmp-&gt;iroot, 0);\n    xop-&gt;lhc = inum;\n    hammer2_xop_start(&amp;xop-&gt;head, &amp;hammer2_lookup_desc);\n    error = hammer2_xop_collect(&amp;xop-&gt;head, 0);\n\n    if (error == 0)\n        ip = hammer2_inode_get(pmp, &amp;xop-&gt;head, -1, -1);\n\n    if (ip)\n        *vpp = hammer2_igetv(ip, &amp;error);\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#vnode-operations","title":"Vnode Operations","text":""},{"location":"sys/vfs/hammer2/vfs-integration/#vnode-operations-table","title":"Vnode Operations Table","text":"<p>The vnode operations table is defined at <code>hammer2_vnops.c:2481</code>:</p> <pre><code>struct vop_ops hammer2_vnode_vops = {\n    .vop_default        = vop_defaultop,\n    .vop_fsync          = hammer2_vop_fsync,\n    .vop_getpages       = vop_stdgetpages,\n    .vop_putpages       = vop_stdputpages,\n    .vop_access         = hammer2_vop_access,\n    .vop_advlock        = hammer2_vop_advlock,\n    .vop_close          = hammer2_vop_close,\n    .vop_nlink          = hammer2_vop_nlink,\n    .vop_ncreate        = hammer2_vop_ncreate,\n    .vop_nsymlink       = hammer2_vop_nsymlink,\n    .vop_nremove        = hammer2_vop_nremove,\n    .vop_nrmdir         = hammer2_vop_nrmdir,\n    .vop_nrename        = hammer2_vop_nrename,\n    .vop_getattr        = hammer2_vop_getattr,\n    .vop_getattr_lite   = hammer2_vop_getattr_lite,\n    .vop_setattr        = hammer2_vop_setattr,\n    .vop_readdir        = hammer2_vop_readdir,\n    .vop_readlink       = hammer2_vop_readlink,\n    .vop_read           = hammer2_vop_read,\n    .vop_write          = hammer2_vop_write,\n    .vop_open           = hammer2_vop_open,\n    .vop_inactive       = hammer2_vop_inactive,\n    .vop_reclaim        = hammer2_vop_reclaim,\n    .vop_nresolve       = hammer2_vop_nresolve,\n    .vop_nlookupdotdot  = hammer2_vop_nlookupdotdot,\n    .vop_nmkdir         = hammer2_vop_nmkdir,\n    .vop_nmknod         = hammer2_vop_nmknod,\n    .vop_ioctl          = hammer2_vop_ioctl,\n    .vop_mountctl       = hammer2_vop_mountctl,\n    .vop_bmap           = hammer2_vop_bmap,\n    .vop_strategy       = hammer2_vop_strategy,\n    .vop_kqfilter       = hammer2_vop_kqfilter\n};\n</code></pre> <p>Additional tables for special files (<code>hammer2_spec_vops</code>) and FIFOs (<code>hammer2_fifo_vops</code>) are also defined.</p>"},{"location":"sys/vfs/hammer2/vfs-integration/#file-read","title":"File Read","text":"<p>Function: <code>hammer2_vop_read()</code> at <code>hammer2_vnops.c:779</code></p> <pre><code>static int\nhammer2_vop_read(struct vop_read_args *ap)\n{\n    ip = VTOI(vp);\n    seqcount = ap-&gt;a_ioflag &gt;&gt; IO_SEQSHIFT;\n    error = hammer2_read_file(ip, uio, seqcount);\n    return error;\n}\n</code></pre> <p>The internal <code>hammer2_read_file()</code> function: 1. Acquires shared locks on inode and truncate_lock 2. Loops over logical blocks using <code>cluster_readx()</code> for read-ahead 3. Copies data to userspace via <code>uiomovebp()</code></p>"},{"location":"sys/vfs/hammer2/vfs-integration/#file-write","title":"File Write","text":"<p>Function: <code>hammer2_vop_write()</code> at <code>hammer2_vnops.c:811</code></p> <pre><code>static int\nhammer2_vop_write(struct vop_write_args *ap)\n{\n    ip = VTOI(vp);\n\n    /* Check read-only and space */\n    if (ip-&gt;pmp-&gt;ronly)\n        return EROFS;\n    if (hammer2_vfs_enospace(ip, uio-&gt;uio_resid, cred) &gt; 1)\n        return ENOSPC;\n\n    /* Start transaction */\n    hammer2_trans_init(ip-&gt;pmp, 0);\n\n    error = hammer2_write_file(ip, uio, ioflag, seqcount);\n\n    hammer2_trans_done(ip-&gt;pmp, HAMMER2_TRANS_SIDEQ);\n    return error;\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#directory-reading","title":"Directory Reading","text":"<p>Function: <code>hammer2_vop_readdir()</code> at <code>hammer2_vnops.c:581</code></p> <p>Uses the XOP system to read directory entries:</p> <pre><code>static int\nhammer2_vop_readdir(struct vop_readdir_args *ap)\n{\n    ip = VTOI(ap-&gt;a_vp);\n\n    hammer2_inode_lock(ip, HAMMER2_RESOLVE_SHARED);\n\n    /* Handle '.' and '..' entries */\n    if (saveoff == 0) { /* '.' */ }\n    if (saveoff == 1) { /* '..' */ }\n\n    /* Scan directory via XOP */\n    xop = hammer2_xop_alloc(ip, 0);\n    xop-&gt;lkey = saveoff | HAMMER2_DIRHASH_VISIBLE;\n    hammer2_xop_start(&amp;xop-&gt;head, &amp;hammer2_readdir_desc);\n\n    for (;;) {\n        error = hammer2_xop_collect(&amp;xop-&gt;head, 0);\n        if (error) break;\n\n        /* Handle INODE or DIRENT block references */\n        /* Write directory entry via vop_write_dirent() */\n    }\n\n    hammer2_xop_retire(&amp;xop-&gt;head, HAMMER2_XOPMASK_VOP);\n    hammer2_inode_unlock(ip);\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#vnode-lifecycle","title":"Vnode Lifecycle","text":""},{"location":"sys/vfs/hammer2/vfs-integration/#inactive","title":"Inactive","text":"<p>Function: <code>hammer2_vop_inactive()</code> at <code>hammer2_vnops.c:73</code></p> <p>Called when the last reference to a vnode is released but it remains cached:</p> <pre><code>static int\nhammer2_vop_inactive(struct vop_inactive_args *ap)\n{\n    ip = VTOI(vp);\n\n    hammer2_inode_lock(ip, 0);\n    if (ip-&gt;flags &amp; HAMMER2_INODE_ISUNLINKED) {\n        /* Truncate file data */\n        nvtruncbuf(vp, 0, nblksize, 0, 0);\n\n        /* Mark for deletion */\n        atomic_set_int(&amp;ip-&gt;flags, HAMMER2_INODE_DELETING);\n        hammer2_inode_delayed_sideq(ip);\n\n        vrecycle(vp);\n    }\n    hammer2_inode_unlock(ip);\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#reclaim","title":"Reclaim","text":"<p>Function: <code>hammer2_vop_reclaim()</code> at <code>hammer2_vnops.c:133</code></p> <p>Called when the vnode is being recycled:</p> <pre><code>static int\nhammer2_vop_reclaim(struct vop_reclaim_args *ap)\n{\n    ip = VTOI(vp);\n\n    vclrisdirty(vp);\n\n    hammer2_inode_lock(ip, 0);\n    vp-&gt;v_data = NULL;\n    ip-&gt;vp = NULL;\n\n    /* Ensure deletion is queued if needed */\n    if ((ip-&gt;flags &amp; HAMMER2_INODE_ISUNLINKED) &amp;&amp;\n        !(ip-&gt;flags &amp; HAMMER2_INODE_DELETING)) {\n        atomic_set_int(&amp;ip-&gt;flags, HAMMER2_INODE_DELETING);\n        hammer2_inode_delayed_sideq(ip);\n    }\n    hammer2_inode_unlock(ip);\n\n    hammer2_inode_drop(ip);  /* release vp reference */\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#file-sync","title":"File Sync","text":"<p>Function: <code>hammer2_vop_fsync()</code> at <code>hammer2_vnops.c:199</code></p> <pre><code>static int\nhammer2_vop_fsync(struct vop_fsync_args *ap)\n{\n    ip = VTOI(vp);\n\n    hammer2_trans_init(ip-&gt;pmp, 0);\n\n    /* Flush buffer cache */\n    vfsync(vp, ap-&gt;a_waitfor, 1, NULL, NULL);\n    bio_track_wait(&amp;vp-&gt;v_track_write, 0, 0);\n\n    /* Sync inode metadata */\n    hammer2_inode_lock(ip, 0);\n    if (ip-&gt;flags &amp; (HAMMER2_INODE_RESIZED|HAMMER2_INODE_MODIFIED))\n        hammer2_inode_chain_sync(ip);\n\n    /* Flush chains */\n    hammer2_inode_chain_flush(ip, HAMMER2_XOP_INODE_STOP);\n\n    hammer2_inode_unlock(ip);\n    hammer2_trans_done(ip-&gt;pmp, 0);\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#sysctl-tunables","title":"Sysctl Tunables","text":"<p>HAMMER2 exposes numerous tunables under <code>vfs.hammer2</code>:</p> Sysctl Default Description <code>cluster_meta_read</code> 1 Metadata read-ahead (blocks) <code>cluster_data_read</code> 4 Data read-ahead (blocks) <code>cluster_write</code> 0 Write clustering <code>dedup_enable</code> 1 Enable deduplication <code>always_compress</code> 0 Always attempt compression <code>flush_pipe</code> 100 Flush pipeline depth <code>bulkfree_tps</code> 5000 Bulkfree transactions per second <code>dio_limit</code> varies DIO cache size limit <code>limit_dirty_chains</code> varies Max dirty chains <code>limit_dirty_inodes</code> varies Max dirty inodes <p>Statistics (read-only):</p> Sysctl Description <code>iod_file_read</code> File data blocks read <code>iod_meta_read</code> Metadata blocks read <code>iod_file_write</code> File data blocks written <code>iod_file_wdedup</code> Deduplicated writes <code>iod_inode_creates</code> Inodes created <code>iod_inode_deletes</code> Inodes deleted <code>chain_allocs</code> Currently allocated chains"},{"location":"sys/vfs/hammer2/vfs-integration/#pfs-and-cluster-concepts","title":"PFS and Cluster Concepts","text":""},{"location":"sys/vfs/hammer2/vfs-integration/#pfs-types","title":"PFS Types","text":"<pre><code>#define HAMMER2_PFSTYPE_NONE        0x00\n#define HAMMER2_PFSTYPE_CACHE       0x01\n#define HAMMER2_PFSTYPE_SLAVE       0x03\n#define HAMMER2_PFSTYPE_SOFT_SLAVE  0x04\n#define HAMMER2_PFSTYPE_SOFT_MASTER 0x05\n#define HAMMER2_PFSTYPE_MASTER      0x06\n#define HAMMER2_PFSTYPE_SUPROOT     0x08\n</code></pre>"},{"location":"sys/vfs/hammer2/vfs-integration/#cluster-configuration","title":"Cluster Configuration","text":"<p>A cluster can span multiple devices and include multiple PFS nodes:</p> <ul> <li><code>HAMMER2_MAXCLUSTER</code> (8) - Maximum nodes per cluster</li> <li>Each node can be MASTER, SLAVE, or other types</li> <li><code>pfs_nmasters</code> tracks the number of master nodes</li> <li>Synchronization threads maintain consistency across nodes</li> </ul>"},{"location":"sys/vfs/hammer2/vfs-integration/#super-root","title":"Super-Root","text":"<p>The super-root is a special PFS that: - Is automatically created for each device mount - Contains all user PFSs as directory entries - Uses <code>inode_tid = 1</code> for PFS creation - Is not directly mountable by users</p>"},{"location":"sys/vfs/hammer2/vfs-integration/#error-handling","title":"Error Handling","text":"<p>HAMMER2 uses internal error codes that are converted to standard errno values:</p> <pre><code>#define HAMMER2_ERROR_EIO       0x00000001  /* -&gt; EIO */\n#define HAMMER2_ERROR_CHECK     0x00000002  /* -&gt; EDOM */\n#define HAMMER2_ERROR_ENOSPC    0x00000020  /* -&gt; ENOSPC */\n#define HAMMER2_ERROR_ENOENT    0x00000040  /* -&gt; ENOENT */\n/* ... etc ... */\n</code></pre> <p>Conversion is done via <code>hammer2_error_to_errno()</code> defined at <code>hammer2.h:1317</code>.</p>"},{"location":"sys/vfs/hammer2/vfs-integration/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview</li> <li>Chain Layer - Chain structures and operations</li> <li>On-Disk Format - Media structures</li> <li>Inode Layer - Inode management</li> <li>XOP System - Extended operations</li> <li>VFS Operations - Generic VFS framework</li> </ul>"},{"location":"sys/vfs/hammer2/xop-system/","title":"HAMMER2 XOP System","text":"<p>Source Reference</p> <p>Primary sources: <code>sys/vfs/hammer2/hammer2_admin.c</code>, <code>sys/vfs/hammer2/hammer2_xops.c</code> Header definitions: <code>sys/vfs/hammer2/hammer2.h</code> (lines 560-1060)</p>"},{"location":"sys/vfs/hammer2/xop-system/#overview","title":"Overview","text":"<p>The XOP (Cross-cluster OPeration) system provides a framework for executing filesystem operations across multiple cluster nodes in parallel. It decouples the VFS frontend from the storage backend, allowing operations to complete as soon as quorum requirements are met without waiting for slow or failed nodes.</p> <p>Key features:</p> <ul> <li>Parallel execution \u2014 Operations run concurrently on all cluster nodes</li> <li>Quorum-based completion \u2014 Frontend returns when sufficient nodes respond</li> <li>FIFO-based communication \u2014 Per-node FIFOs pass results from backends to frontend</li> <li>Thread pool management \u2014 Dedicated worker threads per cluster node</li> <li>Dependency tracking \u2014 Serializes operations on the same inode</li> </ul>"},{"location":"sys/vfs/hammer2/xop-system/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph Frontend[\"Frontend (VFS Layer)\"]\n        VOP[VOP Operation]\n        ALLOC[hammer2_xop_alloc]\n        START[hammer2_xop_start]\n        COLLECT[hammer2_xop_collect]\n        RETIRE[hammer2_xop_retire]\n    end\n\n    subgraph Backend[\"Backend (Worker Threads)\"]\n        THR0[Thread Node 0]\n        THR1[Thread Node 1]\n        THR2[Thread Node N]\n    end\n\n    subgraph FIFO[\"Per-Node FIFOs\"]\n        FIFO0[FIFO 0]\n        FIFO1[FIFO 1]\n        FIFO2[FIFO N]\n    end\n\n    VOP --&gt; ALLOC\n    ALLOC --&gt; START\n    START --&gt; THR0\n    START --&gt; THR1\n    START --&gt; THR2\n    THR0 --&gt;|hammer2_xop_feed| FIFO0\n    THR1 --&gt;|hammer2_xop_feed| FIFO1\n    THR2 --&gt;|hammer2_xop_feed| FIFO2\n    FIFO0 --&gt; COLLECT\n    FIFO1 --&gt; COLLECT\n    FIFO2 --&gt; COLLECT\n    COLLECT --&gt; RETIRE\n</code></pre>"},{"location":"sys/vfs/hammer2/xop-system/#core-data-structures","title":"Core Data Structures","text":""},{"location":"sys/vfs/hammer2/xop-system/#hammer2_xop_head_t","title":"hammer2_xop_head_t","text":"<p>The common header for all XOP requests (<code>hammer2.h:894-914</code>):</p> <pre><code>struct hammer2_xop_head {\n    hammer2_xop_desc_t  *desc;          /* operation descriptor */\n    hammer2_tid_t       mtid;           /* modify transaction id */\n    struct hammer2_inode *ip1;          /* primary inode */\n    struct hammer2_inode *ip2;          /* secondary inode (rename) */\n    struct hammer2_inode *ip3;          /* tertiary inode (rename) */\n    struct hammer2_inode *ip4;          /* quaternary inode (rename) */\n    uint64_t            run_mask;       /* active backends + frontend */\n    uint64_t            chk_mask;       /* backends that were started */\n    int                 flags;          /* XOP flags */\n    int                 state;          /* collection state */\n    int                 error;          /* collected error */\n    hammer2_key_t       collect_key;    /* key synchronization point */\n    char                *name1;         /* primary name (lookup/create) */\n    size_t              name1_len;\n    char                *name2;         /* secondary name (rename) */\n    size_t              name2_len;\n    hammer2_xop_fifo_t  collect[HAMMER2_MAXCLUSTER]; /* per-node FIFOs */\n    hammer2_cluster_t   cluster;        /* collected results */\n    hammer2_io_t        *focus_dio;     /* focused DIO */\n};\n</code></pre>"},{"location":"sys/vfs/hammer2/xop-system/#hammer2_xop_fifo_t","title":"hammer2_xop_fifo_t","text":"<p>Per-node result FIFO (<code>hammer2.h:879-889</code>):</p> <pre><code>struct hammer2_xop_fifo {\n    TAILQ_ENTRY(hammer2_xop_head) entry;  /* queue linkage */\n    hammer2_chain_t     *array[HAMMER2_XOPFIFO];  /* chain results */\n    int                 errors[HAMMER2_XOPFIFO];  /* error codes */\n    int                 ri;             /* read index */\n    int                 wi;             /* write index */\n    int                 flags;          /* FIFO flags */\n    hammer2_thread_t    *thr;           /* owning thread */\n};\n</code></pre> <p>FIFO flags:</p> Flag Value Description <code>HAMMER2_XOP_FIFO_RUN</code> 0x0001 XOP is currently executing <code>HAMMER2_XOP_FIFO_STALL</code> 0x0002 Backend stalled on full FIFO"},{"location":"sys/vfs/hammer2/xop-system/#hammer2_xop_desc_t","title":"hammer2_xop_desc_t","text":"<p>Operation descriptor (<code>hammer2.h:870-877</code>):</p> <pre><code>struct hammer2_xop_desc {\n    hammer2_xop_func_t  storage_func;   /* backend function */\n    hammer2_xop_func_t  dmsg_dispatch;  /* network dispatch (unused) */\n    hammer2_xop_func_t  dmsg_process;   /* network processing (unused) */\n    const char          *id;            /* operation name for debugging */\n};\n</code></pre>"},{"location":"sys/vfs/hammer2/xop-system/#hammer2_thread_t","title":"hammer2_thread_t","text":"<p>Worker thread structure (<code>hammer2.h:815-824</code>):</p> <pre><code>struct hammer2_thread {\n    struct hammer2_pfs  *pmp;           /* PFS mount */\n    struct hammer2_dev  *hmp;           /* device (for bulkfree) */\n    hammer2_xop_list_t  xopq;           /* pending XOP queue */\n    thread_t            td;             /* kernel thread */\n    uint32_t            flags;          /* thread state flags */\n    int                 clindex;        /* cluster element index */\n    int                 repidx;         /* replication index */\n    char                *scratch;       /* MAXPHYS scratch buffer */\n};\n</code></pre> <p>Thread flags:</p> Flag Value Description <code>HAMMER2_THREAD_UNMOUNTING</code> 0x0001 Unmount in progress <code>HAMMER2_THREAD_DEV</code> 0x0002 Device thread, not PFS <code>HAMMER2_THREAD_WAITING</code> 0x0004 Thread sleeping <code>HAMMER2_THREAD_REMASTER</code> 0x0008 Remaster request <code>HAMMER2_THREAD_STOP</code> 0x0010 Stop request <code>HAMMER2_THREAD_FREEZE</code> 0x0020 Freeze request <code>HAMMER2_THREAD_FROZEN</code> 0x0040 Thread is frozen <code>HAMMER2_THREAD_XOPQ</code> 0x0080 Work pending on queue <code>HAMMER2_THREAD_STOPPED</code> 0x0100 Thread has exited <code>HAMMER2_THREAD_UNFREEZE</code> 0x0200 Unfreeze request"},{"location":"sys/vfs/hammer2/xop-system/#run-mask-bits","title":"Run Mask Bits","text":"<p>The <code>run_mask</code> field tracks active participants (<code>hammer2.h:562-568</code>):</p> Mask Description <code>HAMMER2_XOPMASK_CLUSTER</code> Bits 0-7: One bit per cluster node <code>HAMMER2_XOPMASK_VOP</code> Bit 31: Frontend is active <code>HAMMER2_XOPMASK_FIFOW</code> Bit 30: Backend waiting on FIFO space <code>HAMMER2_XOPMASK_WAIT</code> Bit 29: Frontend waiting for results <code>HAMMER2_XOPMASK_FEED</code> Bit 32+: Feed counter <code>HAMMER2_XOPMASK_ALLDONE</code> VOP + CLUSTER combined"},{"location":"sys/vfs/hammer2/xop-system/#xop-lifecycle","title":"XOP Lifecycle","text":""},{"location":"sys/vfs/hammer2/xop-system/#1-allocation","title":"1. Allocation","text":"<pre><code>void *hammer2_xop_alloc(hammer2_inode_t *ip, int flags);\n</code></pre> <p>Source: <code>hammer2_admin.c:328-361</code></p> <p>Allocates an XOP from the object cache and initializes it:</p> <ol> <li>Gets XOP structure from <code>cache_xops</code></li> <li>Sets <code>ip1</code> to the primary inode (adds reference)</li> <li>Initializes <code>run_mask</code> with <code>HAMMER2_XOPMASK_VOP</code> (frontend active)</li> <li>If <code>HAMMER2_XOP_MODIFYING</code>, allocates a transaction ID via <code>hammer2_trans_sub()</code></li> <li>Copies cluster chain count from inode</li> </ol> <p>XOP Flags:</p> Flag Description <code>HAMMER2_XOP_MODIFYING</code> Operation modifies data <code>HAMMER2_XOP_STRATEGY</code> Strategy (read/write) operation"},{"location":"sys/vfs/hammer2/xop-system/#2-setup","title":"2. Setup","text":"<p>Optional setup functions add parameters:</p> <pre><code>void hammer2_xop_setname(hammer2_xop_head_t *xop, const char *name, size_t len);\nvoid hammer2_xop_setname2(hammer2_xop_head_t *xop, const char *name, size_t len);\nvoid hammer2_xop_setip2(hammer2_xop_head_t *xop, hammer2_inode_t *ip2);\nvoid hammer2_xop_setip3(hammer2_xop_head_t *xop, hammer2_inode_t *ip3);\nvoid hammer2_xop_setip4(hammer2_xop_head_t *xop, hammer2_inode_t *ip4);\n</code></pre>"},{"location":"sys/vfs/hammer2/xop-system/#3-start","title":"3. Start","text":"<pre><code>void hammer2_xop_start(hammer2_xop_head_t *xop, hammer2_xop_desc_t *desc);\n</code></pre> <p>Source: <code>hammer2_admin.c:478-590</code></p> <p>Queues the XOP to worker threads:</p> <ol> <li>Creates XOP helper threads if not already running</li> <li>Selects worker thread group based on:</li> <li>Strategy ops: Hash of inode + logical offset, uses dedicated strategy workers</li> <li>Single-node non-clustered: CPU-local workers to reduce IPI overhead</li> <li>Clustered: Hash-based to serialize same-inode operations</li> <li>For each cluster node with a valid chain:</li> <li>Sets bit in <code>run_mask</code> and <code>chk_mask</code></li> <li>Inserts XOP into thread's <code>xopq</code></li> <li>Signals all worker threads via <code>hammer2_thr_signal()</code></li> </ol>"},{"location":"sys/vfs/hammer2/xop-system/#4-backend-execution","title":"4. Backend Execution","text":"<p>Worker threads execute <code>hammer2_primary_xops_thread()</code> (<code>hammer2_admin.c:1147-1262</code>):</p> <pre><code>graph TB\n    START[Thread Start] --&gt; CHECK{Check Flags}\n    CHECK --&gt;|STOP| EXIT[Exit Thread]\n    CHECK --&gt;|FREEZE| FROZEN[Set FROZEN, Wait]\n    CHECK --&gt;|XOPQ| GETXOP[Get Next XOP]\n    CHECK --&gt;|None| SLEEP[Sleep 30s]\n\n    FROZEN --&gt; UNFREEZE{UNFREEZE?}\n    UNFREEZE --&gt;|Yes| CHECK\n    UNFREEZE --&gt;|No| FROZEN\n\n    GETXOP --&gt; ACTIVE{Frontend Active?}\n    ACTIVE --&gt;|Yes| EXEC[Execute storage_func]\n    ACTIVE --&gt;|No| ABORT[Feed NULL + ECONNABORTED]\n\n    EXEC --&gt; DEQUEUE[Dequeue XOP]\n    ABORT --&gt; DEQUEUE\n    DEQUEUE --&gt; RETIRE[Retire XOP]\n    RETIRE --&gt; CHECK\n\n    SLEEP --&gt; CHECK\n    EXIT --&gt; STOPPED[Signal STOPPED]\n</code></pre> <p>The <code>hammer2_xop_next()</code> function (<code>hammer2_admin.c:1071-1115</code>) selects the next runnable XOP:</p> <ul> <li>Skips XOPs with dependencies on already-running XOPs (same inode)</li> <li>Skips XOPs already marked <code>HAMMER2_XOP_FIFO_RUN</code></li> <li>Uses hash-based dependency tracking to allow parallelism</li> </ul>"},{"location":"sys/vfs/hammer2/xop-system/#5-feeding-results","title":"5. Feeding Results","text":"<pre><code>int hammer2_xop_feed(hammer2_xop_head_t *xop, hammer2_chain_t *chain,\n                     int clindex, int error);\n</code></pre> <p>Source: <code>hammer2_admin.c:792-856</code></p> <p>Backend threads feed results to the frontend:</p> <ol> <li>Check if frontend is still active (<code>HAMMER2_XOPMASK_VOP</code>)</li> <li>If FIFO is full, set <code>HAMMER2_XOP_FIFO_STALL</code> and sleep</li> <li>Add chain to FIFO (with ref+hold)</li> <li>Increment write index</li> <li>If frontend is waiting (<code>HAMMER2_XOPMASK_WAIT</code>), wake it up</li> </ol>"},{"location":"sys/vfs/hammer2/xop-system/#6-collection","title":"6. Collection","text":"<pre><code>int hammer2_xop_collect(hammer2_xop_head_t *xop, int flags);\n</code></pre> <p>Source: <code>hammer2_admin.c:888-1022</code></p> <p>Frontend collects results from all backends:</p> <ol> <li>Advance each FIFO that is behind <code>collect_key</code></li> <li>Find lowest key across all FIFOs</li> <li>Call <code>hammer2_cluster_check()</code> to validate quorum</li> <li>Return results:</li> <li><code>0</code> \u2014 Valid result, cluster populated</li> <li><code>HAMMER2_ERROR_ENOENT</code> \u2014 End of iteration</li> <li><code>HAMMER2_ERROR_EINPROGRESS</code> \u2014 Wait for more results</li> </ol> <p>Collection Flags:</p> Flag Description <code>HAMMER2_XOP_COLLECT_NOWAIT</code> Don't block, poll only <code>HAMMER2_XOP_COLLECT_WAITALL</code> Wait for all backends"},{"location":"sys/vfs/hammer2/xop-system/#7-retirement","title":"7. Retirement","text":"<pre><code>void hammer2_xop_retire(hammer2_xop_head_t *xop, uint64_t mask);\n</code></pre> <p>Source: <code>hammer2_admin.c:596-759</code></p> <p>Called by both frontend and backends to retire from an XOP:</p> <ol> <li>Atomically decrement <code>run_mask</code> by <code>mask</code></li> <li>If others remain:</li> <li>Frontend retiring: Wake backends waiting on FIFO</li> <li>Last backend retiring: Wake frontend</li> <li>If last participant:</li> <li>Cache cluster chains in inode's <code>ccache</code> for performance</li> <li>Drop all chains in XOP cluster and FIFOs</li> <li>Drop inode references</li> <li>Free name buffers</li> <li>Return XOP to object cache</li> </ol>"},{"location":"sys/vfs/hammer2/xop-system/#xop-operations","title":"XOP Operations","text":""},{"location":"sys/vfs/hammer2/xop-system/#directory-operations","title":"Directory Operations","text":"Operation Descriptor Description <code>hammer2_xop_readdir</code> <code>hammer2_readdir_desc</code> Read directory entries <code>hammer2_xop_nresolve</code> <code>hammer2_nresolve_desc</code> Lookup name in directory <code>hammer2_xop_unlink</code> <code>hammer2_unlink_desc</code> Remove directory entry <code>hammer2_xop_nrename</code> <code>hammer2_nrename_desc</code> Rename file/directory <code>hammer2_xop_scanlhc</code> <code>hammer2_scanlhc_desc</code> Scan for hash collisions"},{"location":"sys/vfs/hammer2/xop-system/#inode-operations","title":"Inode Operations","text":"Operation Descriptor Description <code>hammer2_xop_ipcluster</code> <code>hammer2_ipcluster_desc</code> Synchronize inode cluster <code>hammer2_xop_inode_mkdirent</code> <code>hammer2_inode_mkdirent_desc</code> Create directory entry <code>hammer2_xop_inode_create</code> <code>hammer2_inode_create_desc</code> Create inode (attached) <code>hammer2_xop_inode_create_det</code> <code>hammer2_inode_create_det_desc</code> Create inode (detached) <code>hammer2_xop_inode_create_ins</code> <code>hammer2_inode_create_ins_desc</code> Insert detached inode <code>hammer2_xop_inode_destroy</code> <code>hammer2_inode_destroy_desc</code> Destroy inode <code>hammer2_xop_inode_chain_sync</code> <code>hammer2_inode_chain_sync_desc</code> Sync inode to chains <code>hammer2_xop_inode_unlinkall</code> <code>hammer2_inode_unlinkall_desc</code> Unlink all entries <code>hammer2_xop_inode_connect</code> <code>hammer2_inode_connect_desc</code> Connect inode <code>hammer2_xop_inode_flush</code> <code>hammer2_inode_flush_desc</code> Flush inode to media"},{"location":"sys/vfs/hammer2/xop-system/#io-operations","title":"I/O Operations","text":"Operation Descriptor Description <code>hammer2_xop_strategy_read</code> <code>hammer2_strategy_read_desc</code> Read file data <code>hammer2_xop_strategy_write</code> <code>hammer2_strategy_write_desc</code> Write file data <code>hammer2_xop_bmap</code> <code>hammer2_bmap_desc</code> Block mapping"},{"location":"sys/vfs/hammer2/xop-system/#utility-operations","title":"Utility Operations","text":"Operation Descriptor Description <code>hammer2_xop_lookup</code> <code>hammer2_lookup_desc</code> General chain lookup <code>hammer2_xop_scanall</code> <code>hammer2_scanall_desc</code> Scan key range <code>hammer2_xop_delete</code> <code>hammer2_delete_desc</code> Delete chain"},{"location":"sys/vfs/hammer2/xop-system/#thread-management","title":"Thread Management","text":""},{"location":"sys/vfs/hammer2/xop-system/#thread-creation","title":"Thread Creation","text":"<pre><code>void hammer2_thr_create(hammer2_thread_t *thr, hammer2_pfs_t *pmp,\n                        hammer2_dev_t *hmp, const char *id,\n                        int clindex, int repidx, void (*func)(void *));\n</code></pre> <p>Source: <code>hammer2_admin.c:218-244</code></p> <p>Creates a worker thread:</p> <ol> <li>Initializes thread structure</li> <li>Allocates MAXPHYS scratch buffer</li> <li>Creates kernel thread via <code>lwkt_create()</code></li> </ol>"},{"location":"sys/vfs/hammer2/xop-system/#thread-pool","title":"Thread Pool","text":"<pre><code>void hammer2_xop_helper_create(hammer2_pfs_t *pmp);\nvoid hammer2_xop_helper_cleanup(hammer2_pfs_t *pmp);\n</code></pre> <p>Source: <code>hammer2_admin.c:425-470</code></p> <p>Each mounted PFS has a pool of XOP worker threads:</p> <ul> <li><code>hammer2_xop_nthreads</code> threads per cluster node</li> <li>Threads are created on-demand at first XOP</li> <li>Separate thread groups for strategy vs. non-strategy operations</li> </ul>"},{"location":"sys/vfs/hammer2/xop-system/#thread-signaling","title":"Thread Signaling","text":"<pre><code>void hammer2_thr_signal(hammer2_thread_t *thr, uint32_t flags);\nvoid hammer2_thr_signal2(hammer2_thread_t *thr, uint32_t posflags, uint32_t negflags);\nvoid hammer2_thr_wait(hammer2_thread_t *thr, uint32_t flags);\nvoid hammer2_thr_wait_neg(hammer2_thread_t *thr, uint32_t flags);\n</code></pre> <p>Atomic flag manipulation with wakeup support.</p>"},{"location":"sys/vfs/hammer2/xop-system/#thread-lifecycle-control","title":"Thread Lifecycle Control","text":"<pre><code>void hammer2_thr_freeze(hammer2_thread_t *thr);    /* Pause thread */\nvoid hammer2_thr_unfreeze(hammer2_thread_t *thr);  /* Resume thread */\nvoid hammer2_thr_delete(hammer2_thread_t *thr);    /* Stop and cleanup */\nvoid hammer2_thr_remaster(hammer2_thread_t *thr);  /* Request remaster */\n</code></pre>"},{"location":"sys/vfs/hammer2/xop-system/#xop-type-definitions","title":"XOP Type Definitions","text":"<p>All XOP types embed <code>hammer2_xop_head_t</code> as first member:</p> <pre><code>struct hammer2_xop_readdir {\n    hammer2_xop_head_t  head;\n    hammer2_key_t       lkey;       /* starting directory key */\n};\n\nstruct hammer2_xop_nresolve {\n    hammer2_xop_head_t  head;\n    hammer2_key_t       lhc;        /* name hash */\n};\n\nstruct hammer2_xop_unlink {\n    hammer2_xop_head_t  head;\n    int                 isdir;      /* expect directory */\n    int                 dopermanent; /* permanent delete flags */\n};\n\nstruct hammer2_xop_create {\n    hammer2_xop_head_t  head;\n    hammer2_inode_meta_t meta;      /* initial metadata */\n    hammer2_key_t       lhc;        /* hash/key */\n    int                 flags;      /* creation flags */\n};\n\nstruct hammer2_xop_strategy {\n    hammer2_xop_head_t  head;\n    hammer2_key_t       lbase;      /* logical offset */\n    int                 finished;   /* completion flag */\n    hammer2_mtx_t       lock;       /* synchronization */\n    struct bio          *bio;       /* I/O request */\n};\n\nstruct hammer2_xop_fsync {\n    hammer2_xop_head_t  head;\n    hammer2_inode_meta_t meta;      /* metadata to sync */\n    hammer2_off_t       osize;      /* original size */\n    u_int               ipflags;    /* inode flags */\n    int                 clear_directdata;\n};\n</code></pre> <p>The <code>hammer2_xop_t</code> union encompasses all types for allocation:</p> <pre><code>union hammer2_xop {\n    hammer2_xop_head_t      head;\n    hammer2_xop_readdir_t   xop_readdir;\n    hammer2_xop_nresolve_t  xop_nresolve;\n    hammer2_xop_unlink_t    xop_unlink;\n    hammer2_xop_create_t    xop_create;\n    hammer2_xop_strategy_t  xop_strategy;\n    /* ... other types ... */\n};\n</code></pre>"},{"location":"sys/vfs/hammer2/xop-system/#data-access-functions","title":"Data Access Functions","text":""},{"location":"sys/vfs/hammer2/xop-system/#safe-data-access","title":"Safe Data Access","text":"<pre><code>const hammer2_media_data_t *hammer2_xop_gdata(hammer2_xop_head_t *xop);\nvoid hammer2_xop_pdata(hammer2_xop_head_t *xop);\n</code></pre> <p>Source: <code>hammer2.h:1967-2000</code></p> <p>These functions provide safe access to chain data from collected results:</p> <ul> <li><code>hammer2_xop_gdata()</code> \u2014 Get data pointer, ensures I/O synchronization</li> <li><code>hammer2_xop_pdata()</code> \u2014 Put data, releases synchronization</li> </ul> <p>Important: Chains in the collected cluster are neither locked nor I/O synchronized. Always use these functions to access chain data safely.</p>"},{"location":"sys/vfs/hammer2/xop-system/#example-directory-lookup-nresolve","title":"Example: Directory Lookup (nresolve)","text":"<pre><code>/* Frontend (VFS layer) */\nhammer2_xop_nresolve_t *xop;\n\nxop = hammer2_xop_alloc(dip, HAMMER2_XOP_MODIFYING);\nhammer2_xop_setname(&amp;xop-&gt;head, name, name_len);\nhammer2_xop_start(&amp;xop-&gt;head, &amp;hammer2_nresolve_desc);\n\nerror = hammer2_xop_collect(&amp;xop-&gt;head, 0);\nif (error == 0) {\n    /* Access result via xop-&gt;head.cluster */\n    chain = xop-&gt;head.cluster.focus;\n    /* ... process result ... */\n}\nhammer2_xop_retire(&amp;xop-&gt;head, HAMMER2_XOPMASK_VOP);\n</code></pre> <pre><code>/* Backend (hammer2_xops.c:248-318) */\nvoid hammer2_xop_nresolve(hammer2_xop_t *arg, void *scratch, int clindex)\n{\n    hammer2_xop_nresolve_t *xop = &amp;arg-&gt;xop_nresolve;\n    hammer2_chain_t *parent, *chain;\n\n    /* Get parent chain for this cluster node */\n    parent = hammer2_inode_chain(xop-&gt;head.ip1, clindex,\n                                 HAMMER2_RESOLVE_ALWAYS |\n                                 HAMMER2_RESOLVE_SHARED);\n\n    /* Lookup directory entry by name hash */\n    lhc = hammer2_dirhash(xop-&gt;head.name1, xop-&gt;head.name1_len);\n    chain = hammer2_chain_lookup(&amp;parent, &amp;key_next,\n                                 lhc, lhc + HAMMER2_DIRHASH_LOMASK,\n                                 &amp;error, HAMMER2_LOOKUP_SHARED);\n\n    /* Find matching entry */\n    while (chain) {\n        if (hammer2_chain_dirent_test(chain, name, name_len))\n            break;\n        chain = hammer2_chain_next(...);\n    }\n\n    /* If directory entry, resolve to inode */\n    if (chain &amp;&amp; chain-&gt;bref.type == HAMMER2_BREF_TYPE_DIRENT) {\n        error = hammer2_chain_inode_find(...);\n    }\n\n    /* Feed result to frontend */\n    hammer2_xop_feed(&amp;xop-&gt;head, chain, clindex, error);\n\n    /* Cleanup */\n    if (chain) hammer2_chain_drop(chain);\n    if (parent) hammer2_chain_drop(parent);\n}\n</code></pre>"},{"location":"sys/vfs/hammer2/xop-system/#performance-considerations","title":"Performance Considerations","text":"<ol> <li> <p>Thread Affinity: Strategy operations use CPU-local threads to reduce inter-processor interrupts (IPIs)</p> </li> <li> <p>Inode Serialization: Non-strategy operations to the same inode are serialized to prevent race conditions in clustered configurations</p> </li> <li> <p>FIFO Flow Control: Backends stall when FIFOs fill, preventing memory exhaustion</p> </li> <li> <p>Chain Caching: Retired XOPs cache their chains in the inode's <code>ccache</code> to prevent expensive chain destruction</p> </li> <li> <p>Early Completion: Frontend can return as soon as quorum is reached, without waiting for slow nodes</p> </li> </ol>"},{"location":"sys/vfs/hammer2/xop-system/#see-also","title":"See Also","text":"<ul> <li>HAMMER2 Overview \u2014 Filesystem architecture</li> <li>Inode Layer \u2014 Inode management</li> <li>Chain Layer \u2014 Block management</li> <li>VFS Integration \u2014 VFS operations</li> <li>Flush and Sync \u2014 Synchronization mechanisms</li> </ul>"},{"location":"sys/vm/","title":"Virtual Memory Subsystem","text":"<p>The DragonFly BSD virtual memory subsystem manages virtual address spaces, physical memory allocation, paging, and swap. It derives from the Mach VM architecture as adopted by BSD but has been extensively modified for better SMP scalability and LWKT integration.</p>"},{"location":"sys/vm/#introduction","title":"Introduction","text":"<p>Virtual memory is a fundamental operating system service that gives each process its own isolated address space and enables the system to use more memory than physically available through demand paging and swapping.</p> <p>New to virtual memory concepts? Start with Virtual Memory Concepts for an introduction to VM theory, terminology, and the Mach VM heritage that DragonFly builds upon.</p>"},{"location":"sys/vm/#dragonflys-vm-design","title":"DragonFly's VM Design","text":"<p>DragonFly's VM evolved from the Mach/BSD VM architecture with significant enhancements for SMP scalability and LWKT integration:</p> <ul> <li>vm_map_backing chains \u2014 Per-entry backing store chains (vs traditional shadow objects)</li> <li>LWKT token locking \u2014 Soft locks allowing blocking operations</li> <li>Page coloring \u2014 1024-way queuing reduces lock contention</li> <li>Per-CPU vmstats \u2014 Cached statistics avoid global contention</li> <li>Fast fault bypass \u2014 Optimization path for resident pages</li> </ul> <p>See Concepts for details on how DragonFly differs from traditional Mach VM.</p>"},{"location":"sys/vm/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TB\n    subgraph Process[\"Per-process address space\"]\n        vmspace[\"vmspace(vm_map + pmap)\"]\n    end\n\n    vmspace --&gt; vm_map[\"vm_map(vm_map_entry)\"]\n    vm_map --&gt; vm_map_backing[\"vm_map_backingBacking store chain\"]\n    vm_map_backing --&gt; vm_object[\"vm_objectContainer for pages\"]\n\n    vm_object --&gt; page1[\"vm_page(resident)\"]\n    vm_object --&gt; page2[\"vm_page(resident)\"]\n    vm_object --&gt; page3[\"vm_page(swapped)\"]\n\n    page3 --&gt; swap_pager[\"swap_pager\"]\n</code></pre>"},{"location":"sys/vm/#reading-guide","title":"Reading Guide","text":"<p>Start here based on what you're trying to do:</p> If you want to... Read this Why Learn VM fundamentals Concepts Theory primer and terminology Understand how <code>mmap()</code> works Memory Mapping Traces syscall to VM layer Debug a page fault or crash Page Faults Explains fault handling and COW Investigate memory pressure/OOM Pageout and Swap Covers reclamation and OOM killer Add a new memory mapping type Address Space Shows map entry creation Work with page allocation Physical Pages Low-level page management Implement a new pager VM Objects Object-pager relationship <p>Recommended reading order for newcomers:</p> <ol> <li>Concepts (understand VM theory and terminology)</li> <li>This overview (understand the DragonFly-specific hierarchy)</li> <li>Memory Mapping (see how userspace enters the VM)</li> <li>Page Faults (see how pages get populated)</li> <li>Physical Pages (understand page lifecycle)</li> </ol>"},{"location":"sys/vm/#how-operations-flow-through-the-vm","title":"How Operations Flow Through the VM","text":"<pre><code>flowchart TB\n    subgraph User[\"USER SPACE\"]\n        mmap_call[\"mmap(file, ...)\"]\n        access[\"*ptr = 42(first access)\"]\n        pressure[\"(memory pressure)\"]\n    end\n\n    subgraph Kernel[\"KERNEL\"]\n        subgraph sys_mmap[\"vm_mmap.c: sys_mmap()\"]\n            validate[\"Validate parameters\"]\n            create_entry[\"Call vm_map_entry_create()to reserve address range\"]\n        end\n\n        subgraph vm_fault[\"vm_fault.c: vm_fault()\"]\n            lookup[\"Lookup vm_map_entryfor faulting address\"]\n            walk[\"Walk vm_map_backing chainto find/create vm_object\"]\n            call_pager[\"Call pager (vnode_pageror swap_pager) to load page\"]\n        end\n\n        subgraph vm_page_alloc[\"vm_page.c: vm_page_alloc()\"]\n            grab[\"Grab page from PQ_FREE queue\"]\n            associate[\"Associate with vm_objectat page index\"]\n            busy[\"Mark busy while I/O in progress\"]\n        end\n\n        subgraph pageout[\"vm_pageout.c: vm_pageout_daemon()\"]\n            scan[\"Scan inactive queue for victims\"]\n            write[\"Write dirty pages to swap/file\"]\n            move[\"Move clean pages to free queue\"]\n        end\n    end\n\n    mmap_call --&gt; sys_mmap\n    sys_mmap --&gt;|\"returns to userno pages yet\"| access\n    access --&gt; vm_fault\n    vm_fault --&gt; vm_page_alloc\n    vm_page_alloc --&gt;|\"page now resident\"| pressure\n    pressure --&gt; pageout\n</code></pre>"},{"location":"sys/vm/#subsystem-documentation","title":"Subsystem Documentation","text":"Document Description Physical Pages Page allocation, queues, coloring, busy states VM Objects Object lifecycle, reference counting, page management Address Space Address space management, entries, backing chains Page Faults Fault handling, COW, fast path optimization Pageout and Swap Memory reclamation, swap pager, OOM killer Memory Mapping mmap syscalls, vnode pager, file-backed I/O"},{"location":"sys/vm/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/vm/#vm_page","title":"vm_page","text":"<p>Physical page descriptor (128 bytes). Each page tracks its owning object, physical address, queue membership, and busy/dirty state.</p> <p>Key fields: <code>object</code>, <code>pindex</code>, <code>phys_addr</code>, <code>queue</code>, <code>busy_count</code>, <code>wire_count</code>, <code>valid</code>, <code>dirty</code></p> <p>See Physical Pages for details.</p>"},{"location":"sys/vm/#vm_object","title":"vm_object","text":"<p>Container for pages with a specific backing store type. Objects can be anonymous (swap-backed), file-backed (vnode), or device-backed.</p> <p>Key fields: <code>rb_memq</code> (page tree), <code>ref_count</code>, <code>type</code>, <code>size</code>, <code>backing_list</code></p> <p>See VM Objects for details.</p>"},{"location":"sys/vm/#vm_map","title":"vm_map","text":"<p>Virtual address space containing a red-black tree of <code>vm_map_entry</code> structures. Each process has its own map within a <code>vmspace</code>.</p> <p>Key fields: <code>rb_root</code>, <code>pmap</code>, <code>min_addr</code>, <code>max_addr</code>, <code>nentries</code></p>"},{"location":"sys/vm/#vm_map_entry","title":"vm_map_entry","text":"<p>Single address range mapping with protection, inheritance, and backing store information. Contains an embedded <code>vm_map_backing</code> structure.</p>"},{"location":"sys/vm/#vm_map_backing-dragonfly-specific","title":"vm_map_backing (DragonFly-specific)","text":"<p>Backing store chain element. Unlike traditional BSD where entries point directly to objects, DragonFly interposes this structure for:</p> <ul> <li>Efficient shadow chains without modifying vm_object</li> <li>Per-entry backing relationships not shared across pmaps</li> <li>Cumulative offset calculation through the chain</li> </ul>"},{"location":"sys/vm/#page-queues","title":"Page Queues","text":"<p>Pages are organized into queues based on state, with 1024 sub-queues per major queue for cache optimization:</p> Queue Description <code>PQ_FREE</code> Available for allocation <code>PQ_ACTIVE</code> Recently referenced <code>PQ_INACTIVE</code> Candidates for reclamation <code>PQ_CACHE</code> Clean, quickly reclaimable <code>PQ_HOLD</code> Temporarily held"},{"location":"sys/vm/#locking-model","title":"Locking Model","text":"<p>The VM subsystem uses several locking strategies:</p> Lock Type Usage LWKT Tokens Soft locks on vm_objects (blocking allowed) lockmgr Hard locks on vm_map for structural changes Spinlocks Per-page and per-queue locks Per-CPU stats Cached vmstats avoid global contention"},{"location":"sys/vm/#source-files","title":"Source Files","text":"File Lines Description <code>vm_page.c</code> ~4,200 Physical page management <code>vm_object.c</code> ~2,000 VM object management <code>vm_map.c</code> ~4,800 Address space management <code>vm_fault.c</code> ~3,200 Page fault handling <code>vm_pageout.c</code> ~2,900 Pageout daemon <code>swap_pager.c</code> ~2,600 Swap I/O <code>vnode_pager.c</code> ~800 File-backed I/O <code>vm_mmap.c</code> ~1,500 mmap() implementation"},{"location":"sys/vm/#see-also","title":"See Also","text":"<ul> <li>Memory Allocation - kmalloc/objcache</li> <li>Buffer Cache - Filesystem buffers</li> <li>Processes - Process and vmspace lifecycle</li> </ul>"},{"location":"sys/vm/concepts/","title":"Virtual Memory Concepts","text":"<p>This page provides foundational background on virtual memory for readers approaching DragonFly's VM subsystem. If you're already familiar with VM concepts and Mach VM heritage, skip to the VM Subsystem Overview.</p>"},{"location":"sys/vm/concepts/#what-is-virtual-memory","title":"What is Virtual Memory?","text":"<p>Virtual memory is an abstraction that gives each process its own private address space, isolated from other processes. When a program accesses memory at address <code>0x1000</code>, that address is virtual\u2014it doesn't correspond directly to a location in physical RAM. Instead, the CPU's Memory Management Unit (MMU) translates virtual addresses to physical addresses using page tables maintained by the kernel.</p> <p>This translation happens transparently on every memory access. The kernel controls what translations exist, enabling it to:</p> <ul> <li>Isolate processes from each other (different page tables, different views of memory)</li> <li>Map the same physical page into multiple address spaces (sharing)</li> <li>Delay allocation until memory is actually accessed (demand paging)</li> <li>Move pages to disk when RAM is scarce (swapping)</li> </ul> <pre><code>flowchart LR\n    subgraph PROCA[\"Process A\"]\n        VA1[\"0x1000\"]\n    end\n\n    subgraph PROCB[\"Process B\"]\n        VA2[\"0x1000\"]\n    end\n\n    MMU[\"MMUPage Tables\"]\n\n    subgraph RAM[\"Physical RAM\"]\n        PF1[\"Frame 42\"]\n        PF2[\"Frame 108\"]\n    end\n\n    VA1 --&gt;|\"A's page table\"| MMU\n    VA2 --&gt;|\"B's page table\"| MMU\n    MMU --&gt; PF1\n    MMU --&gt; PF2\n</code></pre> <p>Both processes use the same virtual address <code>0x1000</code>, but they map to different physical frames. Neither process can access the other's memory.</p>"},{"location":"sys/vm/concepts/#why-virtual-memory","title":"Why Virtual Memory?","text":"Benefit Description Isolation Processes cannot read or write each other's memory; a bug in one program cannot corrupt another Over-commitment The system can allocate more virtual memory than physical RAM exists, paging to disk as needed Sharing Read-only code (libraries, executables) can be shared across processes without duplication Simplicity Each process sees a simple, contiguous address space starting at zero Demand paging Pages are loaded from disk only when first accessed, reducing startup time and memory pressure Memory-mapped files Files can be accessed as memory (<code>mmap</code>), enabling efficient I/O and sharing"},{"location":"sys/vm/concepts/#core-concepts","title":"Core Concepts","text":""},{"location":"sys/vm/concepts/#virtual-address","title":"Virtual Address","text":"<p>An address in a process's address space. The CPU sees these addresses; the MMU translates them to physical addresses. Each process has its own virtual address space (typically 0 to 2^47 on amd64).</p> <p>See: Wikipedia: Virtual address space</p>"},{"location":"sys/vm/concepts/#physical-address","title":"Physical Address","text":"<p>An actual location in RAM hardware. The kernel and MMU deal with physical addresses; user processes never see them directly.</p> <p>See: Wikipedia: Physical address</p>"},{"location":"sys/vm/concepts/#page","title":"Page","text":"<p>The fundamental unit of memory management, typically 4KB on x86-64. Virtual and physical memory are divided into fixed-size pages. The MMU translates at page granularity\u2014all addresses within a page share the same translation.</p> <p>See: Wikipedia: Page (computer memory)</p>"},{"location":"sys/vm/concepts/#page-table","title":"Page Table","text":"<p>A data structure mapping virtual pages to physical frames. Maintained by the kernel, walked by the MMU hardware on every memory access. Modern systems use multi-level page tables (4 levels on x86-64) to handle sparse address spaces efficiently.</p> <p>See: Wikipedia: Page table</p>"},{"location":"sys/vm/concepts/#page-fault","title":"Page Fault","text":"<p>A CPU exception triggered when accessing a virtual address with no valid translation. The kernel's fault handler determines why: the page may need to be loaded from disk, allocated and zero-filled, or the access may be illegal (segfault).</p> <p>See: Wikipedia: Page fault</p>"},{"location":"sys/vm/concepts/#pager","title":"Pager","text":"<p>A kernel component responsible for providing page contents. Different pagers handle different backing stores: the vnode pager reads from files, the swap pager reads from swap space, the device pager maps hardware memory.</p>"},{"location":"sys/vm/concepts/#demand-paging","title":"Demand Paging","text":"<p>Pages are not loaded until accessed. When a process first touches a page, a fault occurs, and the kernel loads the page from its backing store. This reduces memory usage and startup time.</p> <p>See: Wikipedia: Demand paging</p>"},{"location":"sys/vm/concepts/#copy-on-write-cow","title":"Copy-on-Write (COW)","text":"<p>An optimization for <code>fork()</code>. Parent and child initially share the same physical pages (marked read-only). When either writes, a fault occurs, and the kernel copies the page so each has a private copy. Most pages are never written, so sharing persists.</p> <p>See: Wikipedia: Copy-on-write</p>"},{"location":"sys/vm/concepts/#working-set","title":"Working Set","text":"<p>The set of pages a process is actively using. Good VM performance requires keeping the working set resident in RAM; if it exceeds available memory, the system thrashes.</p> <p>See: Wikipedia: Working set</p>"},{"location":"sys/vm/concepts/#thrashing","title":"Thrashing","text":"<p>A pathological state where the system spends more time paging than executing. Occurs when the combined working sets exceed physical memory, causing continuous page faults.</p> <p>See: Wikipedia: Thrashing</p>"},{"location":"sys/vm/concepts/#mach-vm-heritage","title":"Mach VM Heritage","text":"<p>DragonFly's VM subsystem descends from the Mach VM architecture developed at Carnegie Mellon University (1985-1994). Mach introduced an object-oriented approach to virtual memory that was later adopted by 4.4BSD and inherited by FreeBSD, NetBSD, and DragonFly.</p>"},{"location":"sys/vm/concepts/#key-mach-vm-concepts","title":"Key Mach VM Concepts","text":"<p>vm_object: A container for pages representing a contiguous region of backing store. Objects can be backed by files (vnode), anonymous memory (swap), or devices. Multiple address space mappings can reference the same object.</p> <p>Shadow objects: When a mapping needs copy-on-write semantics, a shadow object is interposed. The shadow starts empty; copied pages go into the shadow while unmodified pages are read from the original. Shadow chains can form during successive forks.</p> <pre><code>flowchart TB\n    subgraph PARENT[\"Parent (after fork)\"]\n        PME[\"vm_map_entry\"]\n        PSHADOW[\"shadow object(modified pages)\"]\n    end\n\n    subgraph CHILD[\"Child (after fork)\"]\n        CME[\"vm_map_entry\"]\n        CSHADOW[\"shadow object(modified pages)\"]\n    end\n\n    ORIG[\"original object(shared pages)\"]\n    BACKING[\"backing store(file or swap)\"]\n\n    PME --&gt; PSHADOW --&gt; ORIG\n    CME --&gt; CSHADOW --&gt; ORIG\n    ORIG --&gt; BACKING\n</code></pre> <p>Pager abstraction: Objects delegate I/O to pagers. This separates memory management policy from I/O mechanism\u2014the VM doesn't care whether pages come from a file, swap device, or network.</p> <p>Memory object: In original Mach, external pagers could manage memory objects from user space. BSD simplified this to kernel-only pagers.</p>"},{"location":"sys/vm/concepts/#bsd-adaptations","title":"BSD Adaptations","text":"<p>When BSD adopted Mach VM, it made several simplifications:</p> <ul> <li>Removed external pagers (user-space memory servers)</li> <li>Integrated tightly with the vnode layer for file mapping</li> <li>Added the unified buffer cache (UBC) for file I/O</li> </ul>"},{"location":"sys/vm/concepts/#how-dragonfly-evolved-the-design","title":"How DragonFly Evolved the Design","text":"<p>DragonFly inherited the FreeBSD 4.x VM but made significant changes for SMP scalability and integration with the LWKT threading system.</p>"},{"location":"sys/vm/concepts/#vm_map_backing-chains","title":"vm_map_backing Chains","text":"<p>Traditional BSD attaches shadow objects directly to <code>vm_object</code>, creating chains shared across all mappings. DragonFly introduces <code>vm_map_backing</code>\u2014a per-entry structure that forms its own chain:</p> <pre><code>vm_map_entry \u2192 vm_map_backing \u2192 vm_map_backing \u2192 vm_object\n                (shadow info)    (shadow info)    (pages)\n</code></pre> <p>Benefits:</p> <ul> <li>Shadow relationships are per-entry, not shared across pmaps</li> <li>Easier to collapse and optimize chains</li> <li>Cleaner separation between mapping metadata and page storage</li> </ul>"},{"location":"sys/vm/concepts/#lwkt-token-locking","title":"LWKT Token Locking","text":"<p>Traditional BSD uses sleep locks (<code>lockmgr</code>) on VM objects. DragonFly uses LWKT tokens\u2014soft serialization primitives that allow blocking while held. This enables:</p> <ul> <li>Multiple threads to block on I/O without lock contention</li> <li>Deadlock-free lock ordering (tokens can be acquired in any order)</li> <li>Better integration with the LWKT message-passing model</li> </ul>"},{"location":"sys/vm/concepts/#page-coloring","title":"Page Coloring","text":"<p>DragonFly organizes page queues into 1024 sub-queues based on physical address bits. This:</p> <ul> <li>Distributes lock contention across many queues</li> <li>Improves cache behavior by separating pages by color</li> <li>Enables parallel scanning during pageout</li> </ul>"},{"location":"sys/vm/concepts/#per-cpu-statistics","title":"Per-CPU Statistics","text":"<p>VM statistics (<code>vm_stat</code>) are cached per-CPU and only aggregated when read. This eliminates cache-line bouncing on the global stats structure during allocation/free.</p>"},{"location":"sys/vm/concepts/#fast-fault-bypass","title":"Fast Fault Bypass","text":"<p>For pages already resident and valid, DragonFly's <code>vm_fault_quick()</code> avoids the full fault path, directly wiring the page into the page table.</p>"},{"location":"sys/vm/concepts/#further-reading","title":"Further Reading","text":""},{"location":"sys/vm/concepts/#textbooks","title":"Textbooks","text":"<ul> <li> <p>Operating Systems: Three Easy Pieces \u2014 Free online textbook with excellent VM chapters (13-23). Start here for foundational understanding.</p> </li> <li> <p>The Design and Implementation of the FreeBSD Operating System (McKusick et al.) \u2014 Chapter 5 covers BSD VM in detail. The 4.4BSD and FreeBSD editions are both relevant.</p> </li> </ul>"},{"location":"sys/vm/concepts/#papers","title":"Papers","text":"<ul> <li>Machine-Independent Virtual Memory Management for Paged Uniprocessor and Multiprocessor Architectures (Rashid et al., 1987) \u2014 The original Mach VM paper describing the object-oriented design.</li> </ul>"},{"location":"sys/vm/concepts/#quick-reference","title":"Quick Reference","text":"<ul> <li>Wikipedia: Virtual memory</li> <li>Wikipedia: Mach (kernel)</li> <li>OSDev Wiki: Paging \u2014 x86-specific page table details</li> </ul>"},{"location":"sys/vm/concepts/#see-also","title":"See Also","text":"<ul> <li>VM Subsystem Overview \u2014 DragonFly's architecture and data structures</li> <li>Physical Pages \u2014 Page allocation, queues, and coloring</li> <li>VM Objects \u2014 Object lifecycle and page management</li> <li>Page Faults \u2014 Fault handling and COW implementation</li> <li>Memory Allocation \u2014 Kernel memory (kmalloc, objcache)</li> </ul>"},{"location":"sys/vm/vm_fault/","title":"Page Fault Handling","text":"<p>The page fault handler resolves virtual memory faults by locating or creating physical pages and establishing pmap mappings. DragonFly BSD's implementation emphasizes SMP scalability through shared locking, lockless fast paths, and burst faulting.</p> <p>Source file: <code>sys/vm/vm_fault.c</code> (~3,243 lines)</p>"},{"location":"sys/vm/vm_fault/#when-this-code-runs","title":"When This Code Runs","text":"<p>Page faults are triggered by hardware when a process accesses memory that:</p> Trigger Cause Typical Resolution First access after mmap() No PTE exists Allocate page, zero-fill or load from file Exec touches new code page Demand paging Load from executable via vnode_pager Write to COW page after fork() PTE is read-only Copy page, update PTE to writable Stack growth Access below current stack Expand stack via <code>vm_map_growstack()</code> Swapped-out page access Page not resident Load from swap via swap_pager MADV_DONTNEED region access Page was freed Zero-fill new page"},{"location":"sys/vm/vm_fault/#high-level-flow","title":"High-Level Flow","text":"<pre><code>flowchart TB\n    trap[\"HARDWARE TRAP\"]\n    vm_fault[\"trap() \u2192 vm_fault(map, vaddr, fault_type, fault_flags)\"]\n\n    subgraph lookup[\"1. vm_map_lookup()\"]\n        find[\"Find vm_map_entry for faulting address\"]\n        cow_setup[\"Handle COW setup (shadow object creation)\"]\n        ret_backing[\"Return backing chain and protection\"]\n    end\n\n    subgraph fast[\"2a. vm_fault_bypass()FAST PATH\"]\n        hash[\"Page in hash cache\"]\n        valid[\"Already valid/dirty\"]\n        nolock[\"No locks needed\"]\n        sbusy[\"Soft-busy only\"]\n    end\n\n    subgraph slow[\"2b. vm_fault_object()SLOW PATH\"]\n        walk[\"Walk backing chain\"]\n        pager[\"Call pager if needed\"]\n        cow_copy[\"Handle COW copy\"]\n        zero[\"Zero-fill if new\"]\n    end\n\n    pmap[\"3. pmap_enter()Install PTE for virtual\u2192physical mapping\"]\n\n    trap --&gt; vm_fault\n    vm_fault --&gt; lookup\n    lookup --&gt; fast\n    lookup --&gt; slow\n    fast --&gt; pmap\n    slow --&gt; pmap\n</code></pre>"},{"location":"sys/vm/vm_fault/#overview","title":"Overview","text":"<p>When a process accesses unmapped or protected memory, the hardware generates a page fault. The fault handler must:</p> <ol> <li>Find the vm_map_entry for the faulting address</li> <li>Walk the vm_map_backing chain to locate the page</li> <li>Handle copy-on-write if needed</li> <li>Page in from backing store (file/swap) if needed</li> <li>Enter the page into the pmap</li> </ol> <p>DragonFly optimizes this path with: - Lockless bypass for frequently accessed pages - Shared object tokens for concurrent read faults - Burst faulting to map multiple pages at once - Two-level prefaulting based on lock mode</p>"},{"location":"sys/vm/vm_fault/#data-structures","title":"Data Structures","text":""},{"location":"sys/vm/vm_fault/#struct-faultstate","title":"struct faultstate","text":"<p>Internal state maintained during fault processing:</p> <pre><code>struct faultstate {\n    vm_page_t mary[VM_FAULT_MAX_QUICK];  /* Burst pages (max 16) */\n    vm_map_backing_t ba;            /* Current backing during iteration */\n    vm_prot_t prot;                 /* Final protection for pmap */\n    vm_page_t first_m;              /* Allocated page for COW target */\n    vm_map_backing_t first_ba;      /* Top-level backing */\n    vm_prot_t first_prot;           /* Protection from map lookup */\n    vm_map_t map;                   /* Map being faulted */\n    vm_map_entry_t entry;           /* Entry being faulted */\n    int lookup_still_valid;         /* Map lock state */\n    int hardfault;                  /* I/O was required */\n    int fault_flags;                /* VM_FAULT_* flags */\n    int shared;                     /* Using shared object lock */\n    int msoftonly;                  /* Pages are soft-busied only */\n    int first_shared;               /* First object has shared lock */\n    int wflags;                     /* FW_* flags from lookup */\n    int first_ba_held;              /* Object lock state */\n    struct vnode *vp;               /* Locked vnode (if any) */\n};\n</code></pre>"},{"location":"sys/vm/vm_fault/#fault-flags","title":"Fault Flags","text":"Flag Description <code>VM_FAULT_NORMAL</code> Standard fault <code>VM_FAULT_WIRE_MASK</code> Wiring operation <code>VM_FAULT_USER_WIRE</code> User wiring (mlock) <code>VM_FAULT_CHANGE_WIRING</code> Kernel wiring <code>VM_FAULT_BURST</code> Enable prefaulting <code>VM_FAULT_DIRTY</code> Mark page dirty <code>VM_FAULT_UNSWAP</code> Remove swap backing <code>VM_FAULT_USERMODE</code> User-mode fault"},{"location":"sys/vm/vm_fault/#wiring-flags-wflags","title":"Wiring Flags (wflags)","text":"Flag Description <code>FW_WIRED</code> Entry is wired <code>FW_DIDCOW</code> COW was performed"},{"location":"sys/vm/vm_fault/#main-entry-point","title":"Main Entry Point","text":"<pre><code>int vm_fault(vm_map_t map, vm_offset_t vaddr, \n             vm_prot_t fault_type, int fault_flags);\n</code></pre> <p>Called from the trap handler when a page fault occurs.</p>"},{"location":"sys/vm/vm_fault/#algorithm","title":"Algorithm","text":"<p>1. Initialization: - Set <code>LWP_PAGING</code> flag on current LWP - Initialize faultstate with shared lock preference</p> <p>2. Map Lookup (RetryFault): <pre><code>result = vm_map_lookup(&amp;fs.map, vaddr, fault_type,\n                       &amp;fs.entry, &amp;fs.first_ba,\n                       &amp;first_pindex, &amp;first_count,\n                       &amp;fs.first_prot, &amp;fs.wflags);\n</code></pre></p> <p>The lookup may: - Create a shadow object for COW - Partition large entries for concurrency - Return protection and wiring state</p> <p>3. Handle Lookup Failures: - <code>KERN_INVALID_ADDRESS</code>: Try <code>vm_map_growstack()</code> for stack faults - <code>KERN_PROTECTION_FAILURE</code> with USER_WIRE: Retry with <code>VM_PROT_OVERRIDE_WRITE</code></p> <p>4. Special Entry Types: - <code>MAP_ENTRY_NOFAULT</code>: Panic (should never fault) - <code>MAP_ENTRY_KSTACK</code> guard page: Panic - <code>VM_MAPTYPE_UKSMAP</code>: Call device callback, map directly</p> <p>5. Fast Path (vm_fault_bypass): <pre><code>if (vm_fault_bypass_count &amp;&amp;\n    vm_fault_bypass(&amp;fs, first_pindex, first_count,\n                   &amp;mextcount, fault_type) == KERN_SUCCESS) {\n    goto success;\n}\n</code></pre></p> <p>6. Slow Path: - Acquire object lock (shared or exclusive) - Lock vnode if needed - Call <code>vm_fault_object()</code> to resolve page</p> <p>7. Success: - Set <code>PG_REFERENCED</code> on page - Enter page(s) into pmap - Handle wiring or activate page - Prefault nearby pages if <code>VM_FAULT_BURST</code></p> <p>8. Statistics: - Increment <code>v_vm_faults</code> - Update <code>ru_majflt</code> (hard) or <code>ru_minflt</code> (soft) - Check RSS limits, deactivate pages if exceeded</p>"},{"location":"sys/vm/vm_fault/#lockless-fast-path","title":"Lockless Fast Path","text":"<pre><code>static int vm_fault_bypass(struct faultstate *fs, vm_pindex_t first_pindex,\n                           vm_pindex_t first_count, int *mextcountp,\n                           vm_prot_t fault_type);\n</code></pre> <p>Attempts to resolve the fault without acquiring object locks:</p>"},{"location":"sys/vm/vm_fault/#requirements","title":"Requirements","text":"<ul> <li>No wiring operation in progress</li> <li>Page exists in object's page hash</li> <li>Object is not dead</li> <li>Page is fully valid, on <code>PQ_ACTIVE</code>, not <code>PG_SWAPPED</code></li> <li>For writes: object and page already marked writable/dirty</li> </ul>"},{"location":"sys/vm/vm_fault/#algorithm_1","title":"Algorithm","text":"<ol> <li>Look up page via <code>vm_page_hash_get()</code> (acquires soft-busy)</li> <li>Validate page state</li> <li>For writes: verify <code>OBJ_WRITEABLE | OBJ_MIGHTBEDIRTY</code> and <code>dirty == VM_PAGE_BITS_ALL</code></li> <li>Call <code>vm_page_soft_activate()</code> (passive queue manipulation)</li> <li>Optionally burst: get additional consecutive pages</li> <li>Return with soft-busied pages in <code>fs-&gt;mary[]</code></li> </ol>"},{"location":"sys/vm/vm_fault/#benefits","title":"Benefits","text":"<ul> <li>No object token acquisition</li> <li>No hard page busy</li> <li>Excellent for shared executables and libraries</li> <li>Multiple pages can be mapped in single operation</li> </ul>"},{"location":"sys/vm/vm_fault/#core-fault-logic","title":"Core Fault Logic","text":"<pre><code>static int vm_fault_object(struct faultstate *fs, vm_pindex_t first_pindex,\n                           vm_prot_t fault_type, int allow_nofault);\n</code></pre> <p>Walks the backing chain to find or create the target page.</p>"},{"location":"sys/vm/vm_fault/#protection-upgrade","title":"Protection Upgrade","text":"<p>For read faults, the code attempts to also enable write access if: - The mapping allows writes - The page is not COW - The pmap doesn't require A/M bit emulation (vkernel)</p>"},{"location":"sys/vm/vm_fault/#main-loop-backing-chain-walk","title":"Main Loop (Backing Chain Walk)","text":"<pre><code>for (;;) {\n    1. Check if object is dead\n    2. Look up page in current object\n    3. If found and valid \u2192 break to PAGE FOUND\n    4. If not found \u2192 try pager or allocate\n    5. Move to next backing object\n}\n</code></pre> <p>Page Lookup: <pre><code>fs-&gt;mary[0] = vm_page_lookup_busy_try(fs-&gt;ba-&gt;object, pindex, TRUE, &amp;error);\n</code></pre></p> <p>If the page is busy, sleep and return <code>KERN_TRY_AGAIN</code>.</p> <p>Page Not Resident:</p> <p>For <code>OBJT_SWAP</code> objects, check <code>swap_pager_haspage_locked()</code> before allocating.</p> <p>If the page might be in the pager (<code>TRYPAGER</code>) or this is the first object: 1. Require exclusive lock for allocation 2. Check <code>pindex &lt; object-&gt;size</code> 3. Allocate via <code>vm_page_alloc()</code></p> <p>Pager I/O: <pre><code>rv = vm_pager_get_page(object, pindex, &amp;fs-&gt;mary[0], seqaccess);\n</code></pre></p> Result Action <code>VM_PAGER_OK</code> Increment hardfault, re-lookup page <code>VM_PAGER_FAIL</code> Continue to next backing object <code>VM_PAGER_ERROR</code> Return <code>KERN_FAILURE</code> <code>VM_PAGER_BAD</code> Return <code>KERN_PROTECTION_FAILURE</code> <p>Continue to Next Object: <pre><code>next_ba = fs-&gt;ba-&gt;backing_ba;\nif (next_ba == NULL) {\n    /* Zero-fill the page */\n    vm_page_zero_fill(fs-&gt;mary[0]);\n    break;\n}\n/* Adjust pindex through offset chain */\npindex -= OFF_TO_IDX(fs-&gt;ba-&gt;offset);\npindex += OFF_TO_IDX(next_ba-&gt;offset);\nfs-&gt;ba = next_ba;\n</code></pre></p>"},{"location":"sys/vm/vm_fault/#copy-on-write","title":"Copy-On-Write","text":"<p>When the page is found in a backing object (<code>ba != first_ba</code>) and this is a write fault:</p> <pre><code>/* Copy from backing page to first_m */\nvm_page_copy(fs-&gt;mary[0], fs-&gt;first_m);\n\n/* Release backing page and object */\nrelease_page(fs);\nvm_object_pip_wakeup(fs-&gt;ba-&gt;object);\nvm_object_drop(fs-&gt;ba-&gt;object);\n\n/* Switch to the copy */\nfs-&gt;ba = fs-&gt;first_ba;\nfs-&gt;mary[0] = fs-&gt;first_m;\n</code></pre> <p>For read faults on backing pages, write permission is masked: <pre><code>fs-&gt;prot &amp;= ~VM_PROT_WRITE;\n</code></pre></p>"},{"location":"sys/vm/vm_fault/#finalization","title":"Finalization","text":"<ol> <li>Activate the page</li> <li>For writes:</li> <li><code>vm_object_set_writeable_dirty()</code></li> <li>Handle <code>PG_SWAPPED</code> (requires exclusive lock for <code>swap_pager_unswapped()</code>)</li> <li>Return <code>KERN_SUCCESS</code> with busied page</li> </ol>"},{"location":"sys/vm/vm_fault/#wiring-support","title":"Wiring Support","text":""},{"location":"sys/vm/vm_fault/#vm_fault_wire","title":"vm_fault_wire","text":"<pre><code>int vm_fault_wire(vm_map_t map, vm_map_entry_t entry,\n                  boolean_t user_wire, int kmflags);\n</code></pre> <p>Wires a range by simulating faults:</p> <ol> <li>Entry must be marked <code>IN_TRANSITION</code></li> <li>Unlock map during faults</li> <li>For each page: call <code>vm_fault()</code> with wire flags</li> <li>On failure: unwire already-wired pages</li> </ol>"},{"location":"sys/vm/vm_fault/#vm_fault_unwire","title":"vm_fault_unwire","text":"<pre><code>void vm_fault_unwire(vm_map_t map, vm_map_entry_t entry);\n</code></pre> <p>Unwires a range: - Calls <code>pmap_unwire()</code> to get page - Calls <code>vm_page_unwire()</code> to decrement wire count - Skips guard page for <code>MAP_ENTRY_KSTACK</code></p>"},{"location":"sys/vm/vm_fault/#shadow-collapse","title":"Shadow Collapse","text":"<pre><code>int vm_fault_collapse(vm_map_t map, vm_map_entry_t entry);\n</code></pre> <p>Used during fork when the backing chain exceeds <code>vm.map_backing_limit</code>:</p> <ol> <li>For each pindex in entry range:</li> <li>Skip if page already in head object</li> <li>Call <code>vm_fault_object()</code> with write permission</li> <li>Activates and wakes page</li> <li>If any pages copied: <code>pmap_remove()</code> entire range</li> </ol> <p>This brings all pages into the head object, allowing the backing chain to be freed.</p>"},{"location":"sys/vm/vm_fault/#physical-page-copy","title":"Physical Page Copy","text":"<pre><code>void vm_fault_copy_entry(vm_map_t dst_map, vm_map_t src_map,\n                         vm_map_entry_t dst_entry, vm_map_entry_t src_entry);\n</code></pre> <p>Physically copies pages between entries when COW is not possible (wired pages):</p> <ol> <li>Allocate destination object</li> <li>For each page:</li> <li>Allocate page in destination</li> <li>Look up page in source (must exist)</li> <li><code>vm_page_copy()</code> contents</li> <li><code>pmap_enter()</code> into destination pmap</li> </ol>"},{"location":"sys/vm/vm_fault/#prefaulting","title":"Prefaulting","text":"<p>Prefaulting maps nearby pages after a fault to reduce future faults.</p>"},{"location":"sys/vm/vm_fault/#full-prefault","title":"Full Prefault","text":"<pre><code>static void vm_prefault(pmap_t pmap, vm_offset_t addra,\n                        vm_map_entry_t entry, int prot, int fault_flags);\n</code></pre> <p>Used when holding exclusive object lock:</p> <ol> <li>Scan \u00b1<code>vm_prefault_pages</code> (default 8) around fault address</li> <li>Skip already-mapped pages (<code>pmap_prefault_ok()</code>)</li> <li>Walk backing chain for each address</li> <li>If not found and <code>vm_fast_fault</code>: allocate zero-fill page</li> <li>Enter page into pmap</li> </ol>"},{"location":"sys/vm/vm_fault/#quick-prefault","title":"Quick Prefault","text":"<pre><code>static void vm_prefault_quick(pmap_t pmap, vm_offset_t addra,\n                              vm_map_entry_t entry, int prot, int fault_flags);\n</code></pre> <p>Used when holding shared object lock:</p> <ul> <li>Only works on terminal objects (no backing chain)</li> <li>Uses <code>vm_page_lookup_sbusy_try()</code> for soft-busy</li> <li>Only maps existing valid pages, no allocation</li> <li>Much lower overhead than full prefault</li> </ul>"},{"location":"sys/vm/vm_fault/#selection","title":"Selection","text":"<pre><code>if (fs.first_shared == 0 &amp;&amp; fs.shared == 0) {\n    vm_prefault(pmap, vaddr, entry, prot, fault_flags);\n} else {\n    vm_prefault_quick(pmap, vaddr, entry, prot, fault_flags);\n}\n</code></pre>"},{"location":"sys/vm/vm_fault/#alternative-entry-points","title":"Alternative Entry Points","text":""},{"location":"sys/vm/vm_fault/#vm_fault_page","title":"vm_fault_page","text":"<pre><code>vm_page_t vm_fault_page(vm_map_t map, vm_offset_t vaddr, vm_prot_t fault_type,\n                        int fault_flags, int *errorp, int *busyp);\n</code></pre> <p>Returns a held (and optionally busied) page without pmap update:</p> <ol> <li>First tries <code>pmap_fault_page_quick()</code> for fast lookup</li> <li>Falls back to full <code>vm_fault_object()</code> path</li> <li>Returns held page, optionally busied for writes</li> <li>Used by vkernel, ptrace, and similar</li> </ol>"},{"location":"sys/vm/vm_fault/#vm_fault_page_quick","title":"vm_fault_page_quick","text":"<pre><code>vm_page_t vm_fault_page_quick(vm_offset_t va, vm_prot_t fault_type,\n                              int *errorp, int *busyp);\n</code></pre> <p>Convenience wrapper using current process vmspace.</p>"},{"location":"sys/vm/vm_fault/#vm_fault_object_page","title":"vm_fault_object_page","text":"<pre><code>vm_page_t vm_fault_object_page(vm_object_t object, vm_ooffset_t offset,\n                               vm_prot_t fault_type, int fault_flags,\n                               int *sharedp, int *errorp);\n</code></pre> <p>Faults a page directly from an object (no map involvement): - Creates fake <code>vm_map_entry</code> - Used internally for direct object access</p>"},{"location":"sys/vm/vm_fault/#sysctls","title":"Sysctls","text":"Sysctl Default Description <code>vm.shared_fault</code> 1 Allow shared object token for faults <code>vm.fault_bypass</code> 1 Enable lockless fast path <code>vm.prefault_pages</code> 8 Pages to prefault each direction <code>vm.fast_fault</code> 1 Allow zero-fill allocation during prefault <code>vm.debug_fault</code> 0 Debug output for faults <code>vm.debug_cluster</code> 0 Debug output for I/O clustering"},{"location":"sys/vm/vm_fault/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_fault/#lockless-bypass","title":"Lockless Bypass","text":"<p><code>vm_fault_bypass()</code> uses the page hash table to find pages without acquiring object locks. Pages are soft-busied only, allowing concurrent access. This dramatically improves performance for shared libraries and executables.</p>"},{"location":"sys/vm/vm_fault/#shared-object-tokens","title":"Shared Object Tokens","text":"<p>The <code>vm_shared_fault</code> sysctl (default on) allows read faults to use shared object tokens. This enables concurrent faults on the same object from different processes, important for fork-heavy workloads.</p>"},{"location":"sys/vm/vm_fault/#exclusive-lock-heuristics","title":"Exclusive Lock Heuristics","text":"<p><code>VM_MAP_BACK_EXCL_HEUR</code> tracks when exclusive locks were needed, avoiding unnecessary shared\u2192exclusive upgrades on subsequent faults.</p>"},{"location":"sys/vm/vm_fault/#burst-faulting","title":"Burst Faulting","text":"<p>The <code>mary[]</code> array (max 16 pages) allows multiple pages to be faulted in a single operation. Combined with prefaulting, this reduces per-page overhead.</p>"},{"location":"sys/vm/vm_fault/#rss-enforcement","title":"RSS Enforcement","text":"<p>After user faults, the code checks process RSS against <code>RLIMIT_RSS</code>: <pre><code>if (size &gt; limit) {\n    vm_pageout_map_deactivate_pages(map, limit);\n}\n</code></pre></p>"},{"location":"sys/vm/vm_fault/#mgtdevice-support","title":"MGTDEVICE Support","text":"<p>For managed device objects (GPU/DRM), pages are not indexed in the VM object. The pager returns pages directly for pmap entry without object insertion.</p>"},{"location":"sys/vm/vm_fault/#see-also","title":"See Also","text":"<ul> <li>VM Subsystem Overview - Architecture overview</li> <li>Physical Pages - Page allocation and states</li> <li>VM Objects - Object lifecycle</li> <li>Address Space - Map lookup and entry management</li> </ul>"},{"location":"sys/vm/vm_map/","title":"Address Space Management","text":"<p>The VM map subsystem manages virtual address spaces in DragonFly BSD. It handles address range mappings, copy-on-write (COW), process forking, stack management, and the relationship between virtual addresses and backing objects.</p> <p>Source file: <code>sys/vm/vm_map.c</code> (~4,781 lines)</p>"},{"location":"sys/vm/vm_map/#why-address-space-management-matters","title":"Why Address Space Management Matters","text":"<p>Every process sees its own private, contiguous address space\u2014from address 0 to the maximum virtual address. But this is an illusion. Physical memory is fragmented, shared between processes, and far smaller than the virtual range. The VM map subsystem creates and maintains this illusion.</p> <p>How does the kernel know what's at a virtual address?</p> <p>When a process accesses address <code>0x7fff12340000</code>, the hardware triggers a page fault. The kernel must answer: Is this address valid? What protection does it have? Where does its data come from? The <code>vm_map</code> answers all these questions. It's a sorted tree of <code>vm_map_entry</code> structures, each describing a contiguous range with uniform properties.</p> <p>How does <code>fork()</code> create a child without copying gigabytes of memory?</p> <p>Copy-on-write (COW) is the answer, but implementing it efficiently is complex. The parent and child must share physical pages until one writes. DragonFly's <code>vm_map_backing</code> chains track these sharing relationships without modifying the underlying <code>vm_object</code>\u2014a key architectural difference from traditional BSD.</p> <p>How do you efficiently find free space in a sparse address space?</p> <p>With thousands of mappings and huge address ranges, finding a suitable hole for <code>mmap()</code> could be expensive. DragonFly uses a freehint cache that provides O(1) lookup for common allocation patterns, falling back to RB tree traversal only when necessary.</p> <p>Why can multiple threads fault on different parts of the same mapping concurrently?</p> <p>Large anonymous mappings (like a process's heap) could serialize all faults through a single lock. DragonFly's 32MB entry partitioning allows different threads to fault on different partitions simultaneously, dramatically improving multi-threaded scalability.</p>"},{"location":"sys/vm/vm_map/#the-life-of-a-mapping","title":"The Life of a Mapping","text":"<p>A mapping's journey from creation through active use to destruction:</p> <pre><code>flowchart TB\n    MMAP[\"mmap() syscall\"]\n    FINDSPACE[\"vm_map_findspace()Find suitable hole\"]\n    INSERT[\"vm_map_insert()Create vm_map_entry\"]\n    ACTIVE[\"Active MappingEntry in RB tree\"]\n    FAULT[\"Page fault on access\"]\n    LOOKUP[\"vm_map_lookup()Find entry, check protection\"]\n    COW{\"Write toCOW page?\"}\n    SHADOW[\"vm_map_entry_shadow()Create shadow chain\"]\n    VMFAULT[\"vm_fault()Populate page\"]\n    PROTECT[\"mprotect() / mlock()\"]\n    MODIFY[\"vm_map_protect()vm_map_user_wiring()\"]\n    FORK[\"fork()\"]\n    COPY[\"vm_map_copy_entry()Set up COW sharing\"]\n    MUNMAP[\"munmap() syscall\"]\n    DELETE[\"vm_map_delete()Remove entry, clean up\"]\n\n    MMAP --&gt; FINDSPACE\n    FINDSPACE --&gt; INSERT\n    INSERT --&gt; ACTIVE\n    ACTIVE --&gt; FAULT\n    FAULT --&gt; LOOKUP\n    LOOKUP --&gt; COW\n    COW --&gt;|yes| SHADOW\n    SHADOW --&gt; VMFAULT\n    COW --&gt;|no| VMFAULT\n    VMFAULT --&gt; ACTIVE\n    ACTIVE --&gt; PROTECT\n    PROTECT --&gt; MODIFY\n    MODIFY --&gt; ACTIVE\n    ACTIVE --&gt; FORK\n    FORK --&gt; COPY\n    COPY --&gt; ACTIVE\n    ACTIVE --&gt; MUNMAP\n    MUNMAP --&gt; DELETE\n</code></pre> <p>Key transitions:</p> <ol> <li> <p>Creation: <code>mmap()</code> \u2192 <code>vm_map_findspace()</code> finds a hole \u2192 <code>vm_map_insert()</code> creates the entry. The mapping exists but has no pages yet.</p> </li> <li> <p>First Access: A page fault triggers <code>vm_map_lookup()</code> which finds the entry and checks permissions. Then <code>vm_fault()</code> populates the page from the backing object or zero-fills it.</p> </li> <li> <p>COW Fault: If a process writes to a shared page (after <code>fork()</code>), <code>vm_map_entry_shadow()</code> creates a shadow chain, and the write goes to a private copy.</p> </li> <li> <p>Destruction: <code>munmap()</code> \u2192 <code>vm_map_delete()</code> removes pmap mappings, releases object references, and frees the entry.</p> </li> </ol>"},{"location":"sys/vm/vm_map/#key-design-principles","title":"Key Design Principles","text":"Problem Traditional Approach DragonFly's Solution COW tracking Modify vm_object shadow chains <code>vm_map_backing</code> chains\u2014objects unchanged when shadowed Finding free space O(n) scan or complex data structures Freehint cache for O(1) common case, RB tree fallback Large mapping contention Single entry = single lock 32MB partitioning for concurrent faults Process exit cleanup Synchronous, heavyweight Two-stage termination (stage-1 while runnable) Entry allocation in fault path Zone allocator (may block) Per-CPU entry cache, pre-reserved entries Shadow chain explosion Unbounded depth from repeated forks <code>vm_map_backing_limit</code> (default 5) with auto-collapse Submap overhead Separate map structure Embedded in entry via <code>VM_MAPTYPE_SUBMAP</code>"},{"location":"sys/vm/vm_map/#common-operations","title":"Common Operations","text":"<p>Understanding when this code runs helps navigate the 4,781 lines. Here's what happens for common scenarios:</p> User Action Syscall Key Functions What Happens <code>mmap()</code> <code>sys_mmap</code> <code>vm_map_find()</code> \u2192 <code>vm_map_insert()</code> Creates vm_map_entry, may coalesce with previous <code>munmap()</code> <code>sys_munmap</code> <code>vm_map_remove()</code> \u2192 <code>vm_map_delete()</code> Clips entries, removes pmap mappings, frees objects <code>mprotect()</code> <code>sys_mprotect</code> <code>vm_map_protect()</code> Two-pass: validate then apply to entries and pmap <code>mlock()</code> <code>sys_mlock</code> <code>vm_map_user_wiring()</code> Faults in pages, sets USER_WIRED flag <code>fork()</code> <code>sys_fork</code> <code>vmspace_fork()</code> \u2192 <code>vm_map_copy_entry()</code> Clones entries, sets up COW sharing Stack growth (fault) <code>vm_map_growstack()</code> Expands stack entry into reserved space Page fault (trap) <code>vm_map_lookup()</code> Finds entry, handles COW, returns backing info <pre><code>flowchart TB\n    subgraph User[\"USER\"]\n        mmap[\"mmap(NULL, 4096, ...)\"]\n        access[\"*ptr = 42\"]\n        fork[\"fork()\"]\n        munmap[\"munmap(ptr, 4096)\"]\n    end\n\n    subgraph vmmap[\"vm_map.c\"]\n        findspace[\"vm_map_findspace()\"]\n        insert[\"vm_map_insert()\"]\n        lookup[\"vm_map_lookup()\"]\n        fault[\"(vm_fault handles rest)\"]\n        vmfork[\"vmspace_fork()\"]\n        copyentry[\"vm_map_copy_entry()\"]\n        delete[\"vm_map_delete()\"]\n        pmapremove[\"pmap_remove()\"]\n    end\n\n    subgraph Result[\"RESULT\"]\n        addr[\"0x7f0000000000\"]\n        newentry[\"new vm_map_entry\"]\n        entryinfo[\"entry + backing info\"]\n        childvm[\"child vmspace\"]\n        cowentries[\"COW entries\"]\n        removed[\"entry removed\"]\n        cleared[\"PTEs cleared\"]\n    end\n\n    mmap --&gt; findspace --&gt; addr\n    mmap --&gt; insert --&gt; newentry\n    access --&gt; lookup --&gt; entryinfo\n    access --&gt; fault\n    fork --&gt; vmfork --&gt; childvm\n    fork --&gt; copyentry --&gt; cowentries\n    munmap --&gt; delete --&gt; removed\n    munmap --&gt; pmapremove --&gt; cleared\n</code></pre>"},{"location":"sys/vm/vm_map/#overview","title":"Overview","text":"<p>A virtual address space is represented by two key structures:</p> <ul> <li>vmspace - Per-process address space containing both <code>vm_map</code> and <code>pmap</code></li> <li>vm_map - Collection of address range mappings organized in a red-black tree</li> </ul> <p>Each mapping (<code>vm_map_entry</code>) describes a contiguous virtual address range with its protection, inheritance, and backing store. DragonFly uses <code>vm_map_backing</code> chains to link entries to objects, enabling efficient shadow object handling without modifying the objects themselves.</p>"},{"location":"sys/vm/vm_map/#data-structures","title":"Data Structures","text":""},{"location":"sys/vm/vm_map/#struct-vmspace","title":"struct vmspace","text":"<p>Per-process address space descriptor:</p> <pre><code>struct vmspace {\n    struct vm_map vm_map;       /* Embedded address map */\n    struct pmap vm_pmap;        /* Embedded page table */\n\n    /* Copied on fork (vm_startcopy to vm_endcopy) */\n    segsz_t vm_swrss;          /* Swap reserved for process */\n    segsz_t vm_tsize;          /* Text size (pages) */\n    segsz_t vm_dsize;          /* Data size (pages) */\n    segsz_t vm_ssize;          /* Stack size (pages) */\n    caddr_t vm_taddr;          /* Text address */\n    caddr_t vm_daddr;          /* Data address */\n    caddr_t vm_maxsaddr;       /* Max stack address */\n\n    int vm_flags;              /* VMSPACE_EXIT1/EXIT2 */\n    int vm_exitingcnt;         /* Threads currently exiting */\n    int vm_refcnt;             /* Reference count */\n    int vm_holdcnt;            /* Hold count (prevents stage-2 exit) */\n};\n</code></pre>"},{"location":"sys/vm/vm_map/#struct-vm_map","title":"struct vm_map","text":"<p>Virtual address space:</p> <pre><code>struct vm_map {\n    struct vm_map_rb_tree rb_root;  /* RB tree of entries */\n    struct pmap *pmap;              /* Physical address map */\n\n    vm_offset_t min_addr;           /* Lowest valid address */\n    vm_offset_t max_addr;           /* Highest valid address */\n    vm_size_t size;                 /* Total mapped size */\n    int nentries;                   /* Number of entries */\n\n    unsigned int timestamp;         /* Modification counter */\n    struct lock lock;               /* Hard lock (100ms timeout) */\n    struct lwkt_token token;        /* Soft serializer */\n\n    vm_flags_t flags;               /* MAP_WIREFUTURE, etc. */\n    struct vm_map_freehint freehint[VM_MAP_FFCOUNT];  /* Findspace hints */\n    struct spinlock ilock_spin;     /* For range interlocks */\n};\n</code></pre>"},{"location":"sys/vm/vm_map/#struct-vm_map_entry","title":"struct vm_map_entry","text":"<p>Single address range mapping:</p> <pre><code>struct vm_map_entry {\n    /* RB tree linkage */\n    RB_ENTRY(vm_map_entry) rb_entry;\n\n    /* Embedded backing store */\n    struct vm_map_backing ba;       /* pmap, start, end, offset, object */\n\n    /* Properties */\n    vm_prot_t protection;           /* Current protection */\n    vm_prot_t max_protection;       /* Maximum allowed */\n    vm_inherit_t inheritance;       /* Fork behavior */\n    u_int wired_count;              /* Wiring reference count */\n    vm_eflags_t eflags;             /* Entry flags */\n    vm_maptype_t maptype;           /* NORMAL, SUBMAP, UKSMAP */\n    vm_subsys_t id;                 /* Subsystem identifier */\n\n    union {\n        vm_offset_t avail_ssize;    /* Stack: available growth */\n        int dev_prot;               /* Device protection */\n    } aux;\n};\n</code></pre>"},{"location":"sys/vm/vm_map/#struct-vm_map_backing","title":"struct vm_map_backing","text":"<p>Backing store chain element (DragonFly-specific):</p> <pre><code>struct vm_map_backing {\n    /* Chain linkage */\n    struct vm_map_backing *backing_ba;   /* Next in shadow chain */\n    int backing_count;                   /* Depth counter */\n\n    /* Address range (mirrored from entry but can differ in chain) */\n    vm_offset_t start;\n    vm_offset_t end;\n    vm_ooffset_t offset;\n\n    /* Backing store */\n    struct pmap *pmap;                   /* Page table reference */\n    union {\n        struct vm_object *object;        /* NORMAL: backing object */\n        struct vm_map *sub_map;          /* SUBMAP: nested map */\n        int (*uksmap)(struct vm_map_backing *ba, int op,\n                      cdev_t dev, vm_page_t page);  /* UKSMAP: callback */\n    };\n    void *aux_info;                      /* UKSMAP auxiliary */\n\n    /* Object linkage */\n    TAILQ_ENTRY(vm_map_backing) entry;   /* On object's backing_list */\n};\n</code></pre>"},{"location":"sys/vm/vm_map/#entry-flags-vm_eflags_t","title":"Entry Flags (vm_eflags_t)","text":"Flag Description <code>MAP_ENTRY_COW</code> Copy-on-write enabled <code>MAP_ENTRY_NEEDS_COPY</code> COW not yet performed <code>MAP_ENTRY_NOFAULT</code> No faults allowed <code>MAP_ENTRY_USER_WIRED</code> User-level wiring (mlock) <code>MAP_ENTRY_IN_TRANSITION</code> Entry being modified <code>MAP_ENTRY_NEEDS_WAKEUP</code> Wake waiters when done <code>MAP_ENTRY_NOSYNC</code> Don't sync to filesystem <code>MAP_ENTRY_NOCOREDUMP</code> Exclude from core dumps <code>MAP_ENTRY_STACK</code> User stack mapping <code>MAP_ENTRY_KSTACK</code> Kernel stack mapping"},{"location":"sys/vm/vm_map/#map-types-vm_maptype_t","title":"Map Types (vm_maptype_t)","text":"Type Description <code>VM_MAPTYPE_NORMAL</code> Standard mapping with backing object <code>VM_MAPTYPE_SUBMAP</code> Nested map (kernel use) <code>VM_MAPTYPE_UKSMAP</code> User-kernel shared memory with device callback"},{"location":"sys/vm/vm_map/#vmspace-lifecycle","title":"vmspace Lifecycle","text":""},{"location":"sys/vm/vm_map/#two-stage-termination","title":"Two-Stage Termination","text":"<p>DragonFly uses a two-stage termination for vmspace cleanup:</p> <p>Stage 1 (ref_count reaches 0): - Triggered by <code>vmspace_rel()</code> or <code>vmspace_relexit()</code> - Sets <code>VMSPACE_EXIT1</code> flag - Detaches SysV shared memory - Removes pmap mappings and frees most VM resources - Process can still run (using cached TLB entries)</p> <p>Stage 2 (hold_count reaches 0): - Triggered by <code>vmspace_drop()</code> or <code>vmspace_exitfree()</code> - Sets <code>VMSPACE_EXIT2</code> flag - Final cleanup via <code>vm_map_delete()</code> - Releases pmap resources - Returns vmspace to objcache</p> <p>This separation allows efficient process exit - the heavyweight cleanup happens while threads are still runnable.</p>"},{"location":"sys/vm/vm_map/#allocation-and-initialization","title":"Allocation and Initialization","text":"<pre><code>/* Allocate new vmspace */\nstruct vmspace *vmspace_alloc(vm_offset_t min, vm_offset_t max);\n</code></pre> <p>Initialization sequence: 1. Get vmspace from objcache 2. Zero the copyable region (<code>vm_startcopy</code> to <code>vm_endcopy</code>) 3. Initialize embedded <code>vm_map</code> with RB tree, locks, freehints 4. Set refs=1, holds=1 5. Initialize pmap via <code>pmap_pinit()</code> 6. Architecture-specific setup via <code>cpu_vmspace_alloc()</code></p>"},{"location":"sys/vm/vm_map/#reference-counting","title":"Reference Counting","text":"Function Description <code>vmspace_ref(vm)</code> Add reference (increments both ref and hold) <code>vmspace_rel(vm)</code> Release reference (triggers stage-1 on 1\u21920) <code>vmspace_hold(vm)</code> Add hold only (acquires map token) <code>vmspace_drop(vm)</code> Release hold (triggers stage-2 on 1\u21920) <code>vmspace_relexit(p)</code> Exit path: add hold, release ref <code>vmspace_exitfree(p)</code> Reap path: clear p_vmspace, drop hold"},{"location":"sys/vm/vm_map/#map-operations","title":"Map Operations","text":""},{"location":"sys/vm/vm_map/#entry-lookup","title":"Entry Lookup","text":"<pre><code>boolean_t vm_map_lookup_entry(vm_map_t map, vm_offset_t address, \n                               vm_map_entry_t *entry);\n</code></pre> <p>Performs RB tree lookup: - Returns TRUE if address is within an entry - Sets <code>*entry</code> to containing entry or closest predecessor - <code>*entry = NULL</code> if address is before all entries</p>"},{"location":"sys/vm/vm_map/#finding-free-space","title":"Finding Free Space","text":"<pre><code>int vm_map_findspace(vm_map_t map, vm_offset_t start, vm_size_t length,\n                     vm_size_t align, int flags, vm_offset_t *addr);\n</code></pre> <p>Finds a hole of <code>length</code> bytes starting at or after <code>start</code>: - Respects alignment requirements - <code>MAP_32BIT</code> flag restricts to low 4GB - Uses freehint cache for O(1) common case - Handles stack entry reserved space (<code>avail_ssize</code>) - For kernel_map: calls <code>pmap_growkernel()</code> if needed</p> <p>Freehint Optimization:</p> <p>The map maintains <code>VM_MAP_FFCOUNT</code> hints for recently successful allocations:</p> <pre><code>struct vm_map_freehint {\n    vm_offset_t start;\n    vm_offset_t length;\n    vm_offset_t align;\n};\n</code></pre> <p>When findspace succeeds, it updates the hint cache. Subsequent requests with matching (length, align) get O(1) lookup.</p>"},{"location":"sys/vm/vm_map/#inserting-mappings","title":"Inserting Mappings","text":"<pre><code>int vm_map_insert(vm_map_t map, int *countp, vm_object_t object,\n                  void *aux, vm_ooffset_t offset,\n                  vm_offset_t start, vm_offset_t end,\n                  vm_maptype_t maptype, vm_subsys_t id,\n                  vm_prot_t prot, vm_prot_t max, int cow);\n</code></pre> <p>COWF_* flags:</p> Flag Effect <code>COWF_COPY_ON_WRITE</code> Enable COW (sets COW + NEEDS_COPY) <code>COWF_NOFAULT</code> No faults allowed (object must be NULL) <code>COWF_PREFAULT</code> Prepopulate page tables <code>COWF_32BIT</code> Restrict to 32-bit addresses <code>COWF_DISABLE_SYNCER</code> Set NOSYNC flag <code>COWF_DISABLE_COREDUMP</code> Set NOCOREDUMP flag <code>COWF_IS_STACK</code> Mark as stack <p>Coalescing Optimization:</p> <p>When no object is specified and the previous entry is compatible (same flags, id, maptype, no backing chain), insert attempts to extend the previous entry's object via <code>vm_object_coalesce()</code> instead of creating a new entry.</p>"},{"location":"sys/vm/vm_map/#high-level-find-and-insert","title":"High-Level Find and Insert","text":"<pre><code>int vm_map_find(vm_map_t map, vm_object_t object, void *aux,\n                vm_ooffset_t offset, vm_offset_t *addr,\n                vm_size_t length, vm_size_t align,\n                boolean_t fitit, vm_maptype_t maptype,\n                vm_subsys_t id, vm_prot_t prot, vm_prot_t max, int cow);\n</code></pre> <p>Combines findspace + insert: - If <code>fitit</code> is TRUE: finds space starting at <code>*addr</code> - If <code>fitit</code> is FALSE: uses exact address <code>*addr</code> - Handles UKSMAP aux_info setup for upmap/kpmap/lpmap</p>"},{"location":"sys/vm/vm_map/#entry-clipping","title":"Entry Clipping","text":"<pre><code>void vm_map_clip_start(vm_map_t map, vm_map_entry_t entry,\n                       vm_offset_t startaddr, int *countp);\nvoid vm_map_clip_end(vm_map_t map, vm_map_entry_t entry,\n                     vm_offset_t endaddr, int *countp);\n</code></pre> <p>Clips split an entry at a given address: - <code>clip_start</code>: Creates new entry for front portion - <code>clip_end</code>: Creates new entry for tail portion - Both replicate the backing chain via <code>vm_map_backing_replicated()</code></p> <p>Partition Optimization:</p> <p>For large anonymous mappings, clip may allocate an object to enable 32MB partitioning for concurrent faults:</p> <pre><code>#define MAP_ENTRY_PARTITION_SIZE  (32 * 1024 * 1024)\n</code></pre>"},{"location":"sys/vm/vm_map/#range-operations","title":"Range Operations","text":"<p>Many operations work on address ranges using clip_range/unclip_range:</p> <pre><code>vm_map_entry_t vm_map_clip_range(vm_map_t map, vm_offset_t start,\n                                  vm_offset_t end, int *countp, int flags);\nvoid vm_map_unclip_range(vm_map_t map, vm_map_entry_t entry,\n                         vm_offset_t start, vm_offset_t end,\n                         int *countp, int flags);\n</code></pre> <p><code>clip_range</code>: 1. Clips entries to exact range boundaries 2. Sets <code>IN_TRANSITION</code> on all covered entries 3. Returns first entry (or NULL) 4. <code>MAP_CLIP_NO_HOLES</code> fails if gaps exist</p> <p><code>unclip_range</code>: 1. Clears <code>IN_TRANSITION</code> flags 2. Wakes any waiters (<code>NEEDS_WAKEUP</code>) 3. Simplifies entries (merges compatible neighbors)</p>"},{"location":"sys/vm/vm_map/#removing-mappings","title":"Removing Mappings","text":"<pre><code>int vm_map_remove(vm_map_t map, vm_offset_t start, vm_offset_t end);\n</code></pre> <p>Internal workhorse <code>vm_map_delete()</code>: 1. Clips entries to range 2. For each entry:    - Unwires if wired    - Removes pmap mappings    - For anonymous with ONEMAPPING: removes pages + swap    - Deletes entry 3. Updates freehint with new hole</p>"},{"location":"sys/vm/vm_map/#protection-changes","title":"Protection Changes","text":"<pre><code>int vm_map_protect(vm_map_t map, vm_offset_t start, vm_offset_t end,\n                   vm_prot_t new_prot, boolean_t set_max);\n</code></pre> <p>Two-pass algorithm: 1. Validation: Check submaps, verify new protection fits max 2. Apply: Clip entries, update protection, call <code>pmap_protect()</code></p> <p>For COW entries, write protection is masked from pmap even if logically allowed.</p>"},{"location":"sys/vm/vm_map/#wiring","title":"Wiring","text":"<p>User wiring (mlock/munlock):</p> <pre><code>int vm_map_user_wiring(vm_map_t map, vm_offset_t start, vm_offset_t end,\n                       boolean_t new_pageable);\n</code></pre> <ul> <li>Sets <code>USER_WIRED</code> flag</li> <li>Increments <code>wired_count</code></li> <li>Calls <code>vm_fault_wire()</code> to fault in pages</li> <li>On failure: backs out changes</li> </ul> <p>Kernel wiring:</p> <pre><code>int vm_map_kernel_wiring(vm_map_t map, vm_offset_t start, vm_offset_t end,\n                         int kmflags);\n</code></pre> <ul> <li><code>KM_KRESERVE</code> avoids zone allocation recursion</li> <li>Two-pass: prepare entries, then wire</li> </ul>"},{"location":"sys/vm/vm_map/#msync-implementation","title":"msync Implementation","text":"<pre><code>int vm_map_clean(vm_map_t map, vm_offset_t start, vm_offset_t end,\n                 boolean_t syncio, boolean_t invalidate);\n</code></pre> <p>For each entry: - SUBMAP: recurse - NORMAL: follow backing chain to vnode object, call <code>vm_object_page_clean()</code> - If invalidate: remove pmap mappings</p>"},{"location":"sys/vm/vm_map/#copy-on-write","title":"Copy-On-Write","text":""},{"location":"sys/vm/vm_map/#shadow-object-creation","title":"Shadow Object Creation","text":"<p>When a COW fault occurs on a <code>NEEDS_COPY</code> entry:</p> <pre><code>void vm_map_entry_shadow(vm_map_entry_t entry);\n</code></pre> <p>Optimization case - If source object is: - ref_count == 1 - No vnode - OBJT_DEFAULT or OBJT_SWAP - No handle</p> <p>Then: Just clear NEEDS_COPY, no shadow needed.</p> <p>General case: 1. Allocate new result object (OBJT_DEFAULT) 2. Create new <code>vm_map_backing</code> for old object 3. Chain: <code>entry-&gt;ba.object = new</code>, <code>entry-&gt;ba.backing_ba-&gt;object = old</code> 4. Increment <code>backing_count</code> 5. Clear <code>OBJ_ONEMAPPING</code> on source</p>"},{"location":"sys/vm/vm_map/#shadow-chain-depth-limiting","title":"Shadow Chain Depth Limiting","text":"<pre><code>sysctl vm.map_backing_limit=5  /* Default max depth */\n</code></pre> <p>During fork, if <code>backing_count &gt;= limit</code>: - Collapse via <code>vm_fault_collapse()</code> to reduce depth - Prevents unbounded chain growth</p>"},{"location":"sys/vm/vm_map/#process-fork","title":"Process Fork","text":"<pre><code>struct vmspace *vmspace_fork(struct vmspace *vm1, struct proc *p2,\n                             struct lwp *lp2);\n</code></pre> <p>Creates child address space:</p> <ol> <li>Allocate new vmspace</li> <li>Copy size/address fields (<code>vm_startcopy</code> to <code>vm_endcopy</code>)</li> <li>For each parent entry, based on inheritance:</li> </ol> Inheritance Action <code>VM_INHERIT_NONE</code> Skip entry <code>VM_INHERIT_SHARE</code> Clone entry, share backing (allocate object if needed, shadow if NEEDS_COPY) <code>VM_INHERIT_COPY</code> Clone entry, set up COW via <code>vm_map_copy_entry()</code> <p><code>vm_map_copy_entry()</code> for COW: - If wired: physically copy pages via <code>vm_fault_copy_entry()</code> - If not wired:   - Write-protect parent PTEs   - Set COW + NEEDS_COPY on both   - Copy PTEs via <code>pmap_copy()</code></p>"},{"location":"sys/vm/vm_map/#uksmap-fork-handling","title":"UKSMAP Fork Handling","text":"<p>For user-kernel shared mappings: - upmap/kpmap: Fork normally with updated aux_info - lpmap: Only fork if thread ID matches new LWP</p>"},{"location":"sys/vm/vm_map/#stack-management","title":"Stack Management","text":""},{"location":"sys/vm/vm_map/#stack-creation","title":"Stack Creation","text":"<pre><code>int vm_map_stack(vm_map_t map, vm_offset_t addrbos, vm_size_t max_ssize,\n                 int flags, vm_prot_t prot, vm_prot_t max, int cow);\n</code></pre> <p>Creates auto-grow stack: 1. Initial size = min(max_ssize, sgrowsiz) 2. Find space for full <code>max_ssize</code> 3. Insert mapping at top (grows down) 4. Set <code>entry-&gt;aux.avail_ssize = max_ssize - init_ssize</code></p>"},{"location":"sys/vm/vm_map/#stack-growth","title":"Stack Growth","text":"<pre><code>int vm_map_growstack(vm_map_t map, vm_offset_t addr);\n</code></pre> <p>Called on stack fault: 1. Verify faulting address is in reserved stack space 2. Calculate growth amount (page-aligned) 3. Check against:    - Available space (<code>avail_ssize</code>)    - Previous entry gap    - <code>RLIMIT_STACK</code>    - <code>RLIMIT_VMEM</code> 4. Insert new mapping below stack entry 5. Update <code>avail_ssize</code> and <code>vm_ssize</code> 6. If <code>MAP_WIREFUTURE</code>: wire new region</p>"},{"location":"sys/vm/vm_map/#fault-path-lookup","title":"Fault Path Lookup","text":"<pre><code>int vm_map_lookup(vm_map_t *var_map, vm_offset_t vaddr, vm_prot_t fault_type,\n                  vm_map_entry_t *out_entry, struct vm_map_backing **bap,\n                  vm_pindex_t *pindex, vm_pindex_t *pcount,\n                  vm_prot_t *out_prot, int *wflags);\n</code></pre> <p>Core function called from page fault handler:</p> <ol> <li>Reserve entries (with recursion protection)</li> <li>Lock map (read or write)</li> <li>Lookup entry for address</li> <li>Handle submaps: switch to submap, retry</li> <li>Check protection:</li> <li><code>OVERRIDE_WRITE</code> uses max_protection</li> <li>Normal uses current protection</li> <li>Handle <code>NEEDS_COPY</code>:</li> <li>Write fault: upgrade lock, shadow entry, set <code>FW_DIDCOW</code></li> <li>Read fault: mask out write permission</li> <li>Partition large entries for concurrent faults</li> <li>Allocate object if needed</li> <li>Return backing, page index, protection, flags</li> </ol> <p>Entry Partitioning:</p> <p>For entries larger than 32MB, <code>vm_map_entry_partition()</code> clips to the 32MB partition containing the fault address. This allows concurrent faults on different partitions.</p>"},{"location":"sys/vm/vm_map/#range-interlocks","title":"Range Interlocks","text":"<p>For <code>MADV_INVAL</code> coordination with faults:</p> <pre><code>void vm_map_interlock(vm_map_t map, struct vm_map_ilock *ilock,\n                      vm_offset_t ran_beg, vm_offset_t ran_end);\nvoid vm_map_deinterlock(vm_map_t map, struct vm_map_ilock *ilock);\n</code></pre> <p>Interlocks wait for overlapping operations to complete, preventing races between pmap manipulation and fault handling.</p>"},{"location":"sys/vm/vm_map/#per-cpu-entry-cache","title":"Per-CPU Entry Cache","text":"<p>To avoid zone allocation in hot paths:</p> <pre><code>/* Per-CPU freelist */\ngd-&gt;gd_vme_base  /* Entry freelist head */\ngd-&gt;gd_vme_avail /* Available count */\n</code></pre> <p>Boot initialization: - BSP gets <code>MAPENTRYBSP_CACHE</code> (MAXCPU+1) entries - Each AP gets <code>MAPENTRYAP_CACHE</code> (8) entries</p> <p>Reserve/release: - <code>vm_map_entry_reserve(count)</code>: Ensure count available - <code>vm_map_entry_release(count)</code>: Return entries, trim if excessive - <code>kreserve/krelease</code>: Special versions for kernel_map (zalloc recursion avoidance)</p>"},{"location":"sys/vm/vm_map/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_map/#vm_map_backing-chains","title":"vm_map_backing Chains","text":"<p>Unlike traditional BSD where entries point directly to objects, DragonFly interposes <code>vm_map_backing</code>:</p> <ul> <li>Shadow chains via <code>backing_ba</code> linkage</li> <li>Objects unchanged when shadowed</li> <li>Per-entry backing not shared across pmaps</li> <li>Efficient fork without object manipulation</li> </ul>"},{"location":"sys/vm/vm_map/#two-stage-vmspace-termination","title":"Two-Stage vmspace Termination","text":"<p>Separates heavyweight cleanup (stage-1) from final release (stage-2): - Stage-1 on ref\u21920: bulk pmap/object cleanup - Stage-2 on hold\u21920: final map delete, return to cache</p>"},{"location":"sys/vm/vm_map/#freehint-cache","title":"Freehint Cache","text":"<p>O(1) findspace for repeated similar allocations (common in mmap patterns).</p>"},{"location":"sys/vm/vm_map/#entry-partitioning","title":"Entry Partitioning","text":"<p>32MB partitions enable concurrent faults on large anonymous mappings without serializing on the entire entry.</p>"},{"location":"sys/vm/vm_map/#uksmap","title":"UKSMAP","text":"<p>User-kernel shared memory with device callbacks: - <code>/dev/upmap</code> (minor 5): Per-process shared page - <code>/dev/kpmap</code> (minor 6): Kernel-wide shared page - <code>/dev/lpmap</code> (minor 7): Per-LWP shared page</p> <p>Device provides callback for mapping operations.</p>"},{"location":"sys/vm/vm_map/#coalescing-on-insert","title":"Coalescing on Insert","text":"<p>Automatically extends previous entry's object when possible, reducing entry count.</p>"},{"location":"sys/vm/vm_map/#sysctls","title":"Sysctls","text":"Sysctl Default Description <code>vm.randomize_mmap</code> 0 Enable mmap ASLR <code>vm.map_relock_enable</code> 1 Relock optimization for prefault <code>vm.map_partition_enable</code> 1 Enable 32MB entry partitioning <code>vm.map_backing_limit</code> 5 Max shadow chain depth <code>vm.map_backing_shadow_test</code> 1 Test backing object shadows"},{"location":"sys/vm/vm_map/#debugging","title":"Debugging","text":"<p>DDB commands:</p> Command Description <code>show map &lt;addr&gt;</code> Print map entries with protection and backing <code>show procvm</code> Print current process vmspace"},{"location":"sys/vm/vm_map/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"sys/vm/vm_map/#pattern-1-creating-a-mapping-in-kernel-code","title":"Pattern 1: Creating a Mapping in Kernel Code","text":"<p>From <code>sys/kern/init_main.c</code> - allocating the initial stack for init:</p> <pre><code>vm_offset_t addr;\nint error;\n\n/* Allocate one page at a specific address (fitit=FALSE) */\naddr = trunc_page(USRSTACK - PAGE_SIZE);\nerror = vm_map_find(&amp;p-&gt;p_vmspace-&gt;vm_map,\n                    NULL, NULL,         /* object, aux_info (anonymous) */\n                    0,                  /* offset */\n                    &amp;addr,              /* address (in/out) */\n                    PAGE_SIZE,          /* length */\n                    PAGE_SIZE,          /* alignment */\n                    FALSE,              /* fitit=FALSE: use exact address */\n                    VM_MAPTYPE_NORMAL,\n                    VM_SUBSYS_INIT,\n                    VM_PROT_ALL,\n                    VM_PROT_ALL,\n                    0);                 /* cow flags */\n</code></pre>"},{"location":"sys/vm/vm_map/#pattern-2-looking-up-an-entry-for-ptrace","title":"Pattern 2: Looking Up an Entry for ptrace","text":"<p>From <code>sys/kern/sys_process.c</code> - reading process memory:</p> <pre><code>vm_map_t tmap;\nvm_map_entry_t out_entry;\nvm_map_backing_t ba;\nvm_object_t object;\nvm_pindex_t pindex, pcount;\nvm_prot_t out_prot;\nint wflags;\nint rv;\n\ntmap = map;  /* vm_map_lookup may change the map pointer */\nrv = vm_map_lookup(&amp;tmap, pageno, VM_PROT_READ, &amp;out_entry,\n                   &amp;ba, &amp;pindex, &amp;pcount, &amp;out_prot, &amp;wflags);\n\nif (ba)\n    object = ba-&gt;object;\nelse\n    object = NULL;\n\nif (rv != KERN_SUCCESS)\n    return EINVAL;\n\nvm_map_lookup_done(tmap, out_entry, 0);\n</code></pre>"},{"location":"sys/vm/vm_map/#pattern-3-temporarily-changing-protection-for-ptrace-write","title":"Pattern 3: Temporarily Changing Protection for ptrace Write","text":"<p>From <code>sys/kern/sys_process.c</code> - writing to read-only memory:</p> <pre><code>vm_map_t map = &amp;procp-&gt;p_vmspace-&gt;vm_map;\nboolean_t fix_prot = 0;\n\n/* Check if page is writable */\nif (vm_map_check_protection(map, pageno, pageno + PAGE_SIZE,\n                            VM_PROT_WRITE, FALSE) == FALSE) {\n    fix_prot = 1;\n    /* Temporarily make it writable */\n    rv = vm_map_protect(map, pageno, pageno + PAGE_SIZE,\n                        VM_PROT_ALL, 0);\n    if (rv != KERN_SUCCESS)\n        return EFAULT;\n}\n\n/* ... do the write ... */\n\n/* Restore original protection */\nif (fix_prot)\n    vm_map_protect(map, pageno, pageno + PAGE_SIZE,\n                   VM_PROT_READ | VM_PROT_EXECUTE, 0);\n</code></pre>"},{"location":"sys/vm/vm_map/#pattern-4-wiring-pages-for-mlock","title":"Pattern 4: Wiring Pages for mlock","text":"<p>From <code>sys/vm/vm_mmap.c</code> - implementing mlock/munlock:</p> <pre><code>/* mlock: wire the pages (new_pageable=FALSE) */\nerror = vm_map_user_wiring(&amp;p-&gt;p_vmspace-&gt;vm_map,\n                           addr, addr + size, FALSE);\n\n/* munlock: unwire the pages (new_pageable=TRUE) */\nerror = vm_map_user_wiring(&amp;p-&gt;p_vmspace-&gt;vm_map,\n                           addr, addr + size, TRUE);\n</code></pre>"},{"location":"sys/vm/vm_map/#pattern-5-removing-a-mapping","title":"Pattern 5: Removing a Mapping","text":"<p>From <code>sys/kern/kern_exec.c</code> - clearing address space before exec:</p> <pre><code>/* Remove all user mappings */\nvm_map_remove(map, 0, VM_MAX_USER_ADDRESS);\n</code></pre> <p>From <code>sys/kern/sys_process.c</code> - cleaning up temporary kernel mapping:</p> <pre><code>/* Remove temporary kernel mapping after use */\nvm_map_remove(kernel_map, kva, kva + PAGE_SIZE);\n</code></pre>"},{"location":"sys/vm/vm_map/#further-reading","title":"Further Reading","text":"<ul> <li>\"The Design and Implementation of the 4.4BSD Operating System\" (McKusick et al.) - Chapter 5 covers VM system design including maps and entries</li> <li>\"Design elements of the FreeBSD VM system\" - FreeBSD Architecture Handbook - DragonFly's heritage, though backing chains differ significantly</li> <li>DragonFly commit history - Search for \"vm_map_backing\" to see the evolution from shadow objects to backing chains</li> <li>vm_fault.md - How faults use <code>vm_map_lookup()</code> results</li> <li>vm_object.md - Objects that back map entries</li> </ul>"},{"location":"sys/vm/vm_map/#see-also","title":"See Also","text":"<ul> <li>VM Subsystem Overview - Architecture overview</li> <li>Physical Pages - Page allocation and queues</li> <li>VM Objects - Object lifecycle and management</li> </ul>"},{"location":"sys/vm/vm_mmap/","title":"Memory Mapping and Pagers","text":"<p>The memory mapping subsystem provides the mmap() interface for applications to map files and anonymous memory into their address space. The vnode pager handles file-backed I/O for memory-mapped files.</p> <p>Source files: <code>sys/vm/vm_mmap.c</code> (~1,530 lines), <code>sys/vm/vnode_pager.c</code> (~832 lines)</p>"},{"location":"sys/vm/vm_mmap/#common-use-cases","title":"Common Use Cases","text":"What You Want Syscall Flags What Happens Allocate heap memory <code>mmap(NULL, size, RW, MAP_ANON\\|MAP_PRIVATE, -1, 0)</code> Anonymous, private Creates vm_map_entry, pages zero-filled on demand Map a file read-only <code>mmap(NULL, size, R, MAP_SHARED, fd, 0)</code> File-backed, shared Creates entry pointing to vnode's vm_object Shared memory (IPC) <code>shm_open()</code> + <code>mmap(MAP_SHARED)</code> Anonymous, shared Multiple processes share same vm_object Memory-mapped I/O <code>mmap(/dev/mem or device)</code> Device-backed Uses dev_pager, may be uncached Copy-on-write fork (internal) Private after fork Parent/child share until write triggers COW"},{"location":"sys/vm/vm_mmap/#syscall-to-vm-layer-flow","title":"Syscall to VM Layer Flow","text":"<pre><code>flowchart TB\n    subgraph APP[\"APPLICATION\"]\n        MMAP[\"mmap(addr, len, prot, flags, fd, offset)\"]\n    end\n\n    subgraph KERNEL[\"KERNEL\"]\n        subgraph SYSMMAP[\"sys_mmap() \u2192 kern_mmap()\"]\n            K1[\"1. Validate parameters (alignment, flags combinations)\"]\n            K2[\"2. If file: get vnode, check permissions\"]\n            K3[\"3. If anonymous: object = NULL (deferred) or new OBJT_DEFAULT\"]\n        end\n\n        subgraph VMMMAP[\"vm_mmap() \u2192 vm_map_find()\"]\n            V1[\"1. Find free address range (or use MAP_FIXED address)\"]\n            V2[\"2. Create vm_map_entry with backing info\"]\n            V3[\"3. For files: entry points to vnode's shared vm_object\"]\n            V4[\"4. For anon: entry has NULL object (allocated on first fault)\"]\n        end\n\n        RET[\"Returns address to user\"]\n\n        subgraph FAULT[\"vm_fault()\"]\n            F1[\"For files: vnode_pager_getpage() \u2192 VOP_READ()\"]\n            F2[\"For anon: zero-fill page, create OBJT_DEFAULT object if needed\"]\n        end\n    end\n\n    MMAP --&gt; SYSMMAP\n    SYSMMAP --&gt; VMMMAP\n    VMMMAP --&gt; RET\n    RET --&gt;|\"later: first access\"| FAULT\n</code></pre>"},{"location":"sys/vm/vm_mmap/#overview","title":"Overview","text":"<p>Memory mapping connects user address space to backing store:</p> <pre><code>flowchart LR\n    subgraph USER[\"User Address Space\"]\n        MA[\"mmap()MAP_ANON\"]\n        MF[\"mmap()file fd\"]\n        MD[\"mmap()device\"]\n    end\n\n    subgraph VM[\"VM Layer\"]\n        VA[\"vm_map_findvm_object\"]\n        VF[\"vm_map_findvm_object\"]\n        VD[\"vm_map_findvm_object\"]\n    end\n\n    subgraph BACKING[\"Backing Store\"]\n        BA[\"Anonymous(swap)\"]\n        BF[\"vnode_pager(file)\"]\n        BD[\"dev_pager(device)\"]\n    end\n\n    MA --&gt; VA --&gt; BA\n    MF --&gt; VF --&gt; BF\n    MD --&gt; VD --&gt; BD\n</code></pre>"},{"location":"sys/vm/vm_mmap/#vnode-pager-vnode_pagerc","title":"Vnode Pager (vnode_pager.c)","text":"<p>The vnode pager provides VM object backing for regular files, allowing memory-mapped file I/O.</p>"},{"location":"sys/vm/vm_mmap/#pager-operations","title":"Pager Operations","text":"<pre><code>struct pagerops vnodepagerops = {\n    .pgo_dealloc  = vnode_pager_dealloc,\n    .pgo_getpage  = vnode_pager_getpage,\n    .pgo_putpages = vnode_pager_putpages,\n    .pgo_haspage  = vnode_pager_haspage\n};\n</code></pre> Operation Description <code>dealloc</code> Clean up when object destroyed <code>getpage</code> Page in from file <code>putpages</code> Page out to file <code>haspage</code> Check if file has backing for page"},{"location":"sys/vm/vm_mmap/#object-allocation","title":"Object Allocation","text":"<p><code>vnode_pager_alloc(handle, length, prot, offset, blksize, boff)</code></p> <p>Creates or references a VM object for a vnode:</p> <ol> <li>Acquire vnode token for serialization</li> <li>If object exists: reference it, validate size</li> <li>If no object: create new <code>OBJT_VNODE</code> object</li> <li>Set <code>vp-&gt;v_object</code>, <code>vp-&gt;v_filesize</code></li> <li>If mount has <code>MNTK_NOMSYNC</code>: set <code>OBJ_NOMSYNC</code></li> <li>Take vnode reference</li> </ol> <p>Object sizing: <pre><code>/* Round up to next block, then to page boundary */\nif (boff &lt; 0)\n    boff = (int)(length % blksize);\nif (boff)\n    loffset = length + (blksize - boff);\nelse\n    loffset = length;\nlsize = OFF_TO_IDX(round_page64(loffset));\n</code></pre></p> <p>The object size includes any partial buffer cache block straddling EOF.</p> <p><code>vnode_pager_reference(vp)</code></p> <p>Adds a reference to an existing vnode's VM object without creating a new one. Returns NULL if no object exists.</p>"},{"location":"sys/vm/vm_mmap/#page-in-vnode_pager_getpage","title":"Page-In (vnode_pager_getpage)","text":"<p><code>vnode_pager_getpage(object, pindex, mpp, seqaccess)</code></p> <p>Wrapper that calls <code>VOP_GETPAGES()</code> on the vnode.</p> <p><code>vnode_pager_generic_getpages(vp, mpp, bytecount, reqpage, seqaccess)</code></p> <p>Generic implementation for filesystems that don't implement <code>VOP_GETPAGES</code>:</p> <ol> <li>Validate vnode mount state</li> <li>Discard pages past file EOF</li> <li>For block/char devices: round up to sector size</li> <li>Release page busy state temporarily (deadlock avoidance)</li> <li>Issue <code>VOP_READ()</code> with <code>IO_VMIO</code> flag</li> <li>Re-acquire page busy state</li> <li>Handle results per page:</li> <li>Non-requested pages: activate if referenced, else deactivate</li> <li>Requested page: validate, zero-fill partial pages</li> </ol> <p>I/O flags: <pre><code>ioflags = IO_VMIO;\nif (seqaccess)\n    ioflags |= IO_SEQMAX &lt;&lt; IO_SEQSHIFT;\n</code></pre></p>"},{"location":"sys/vm/vm_mmap/#page-out-vnode_pager_putpages","title":"Page-Out (vnode_pager_putpages)","text":"<p><code>vnode_pager_putpages(object, m, count, flags, rtvals)</code></p> <p>Wrapper that calls <code>VOP_PUTPAGES()</code> on the vnode.</p> <p>Low memory handling: <pre><code>if ((vmstats.v_free_count + vmstats.v_cache_count) &lt;\n    vmstats.v_pageout_free_min) {\n    flags |= OBJPC_SYNC;  /* Force synchronous */\n}\n</code></pre></p> <p><code>vnode_pager_generic_putpages(vp, m, bytecount, flags, rtvals)</code></p> <p>Generic implementation:</p> <ol> <li>Truncate write to file EOF</li> <li>Set I/O flags based on <code>OBJPC_*</code> flags</li> <li>Issue <code>VOP_WRITE()</code> with <code>IO_VMIO</code></li> <li>Mark pages clean on success</li> </ol> <p>I/O clustering: <pre><code>ioflags = IO_VMIO;\nif (flags &amp; (OBJPC_SYNC | OBJPC_INVAL))\n    ioflags |= IO_SYNC;\nelse if ((flags &amp; OBJPC_CLUSTER_OK) == 0)\n    ioflags |= IO_ASYNC;\nioflags |= IO_SEQMAX &lt;&lt; IO_SEQSHIFT;\n</code></pre></p>"},{"location":"sys/vm/vm_mmap/#file-size-changes","title":"File Size Changes","text":"<p><code>vnode_pager_setsize(vp, nsize)</code></p> <p>Called when file size changes (truncate, extend):</p> <ol> <li>Acquire object hold</li> <li>If shrinking:</li> <li>Update <code>object-&gt;size</code> and <code>vp-&gt;v_filesize</code></li> <li>Remove pages beyond new EOF via <code>vm_object_page_remove()</code></li> <li>Zero partial page at new EOF</li> <li>Clear dirty bits for truncated portion</li> <li>If extending:</li> <li>Update <code>vp-&gt;v_filesize</code></li> </ol> <p>Partial page handling on truncate: <pre><code>if (nsize &amp; PAGE_MASK) {\n    m = vm_page_lookup_busy_wait(object, OFF_TO_IDX(nsize), TRUE, \"vsetsz\");\n    if (m &amp;&amp; m-&gt;valid) {\n        /* Zero trailing bytes */\n        bzero((caddr_t)kva + base, PAGE_SIZE - base);\n        /* Unmap to sync all CPUs */\n        vm_page_protect(m, VM_PROT_NONE);\n        /* Clear partial dirty bits */\n        vm_page_clear_dirty_beg_nonincl(m, base, size);\n    }\n}\n</code></pre></p>"},{"location":"sys/vm/vm_mmap/#vnode-locking-helper","title":"Vnode Locking Helper","text":"<p><code>vnode_pager_lock(ba)</code></p> <p>Walks backing chain and locks the bottom-most vnode:</p> <ol> <li>Find deepest backing_ba in chain</li> <li>Get object from that backing</li> <li>If object is <code>OBJT_VNODE</code> and not dead:</li> <li>Call <code>vget(vp, LK_SHARED | LK_RETRY | LK_CANRECURSE)</code></li> <li>Retry on failure with 1-second sleep</li> </ol> <p>Returns locked vnode or NULL.</p>"},{"location":"sys/vm/vm_mmap/#memory-mapping-vm_mmapc","title":"Memory Mapping (vm_mmap.c)","text":""},{"location":"sys/vm/vm_mmap/#system-calls-overview","title":"System Calls Overview","text":"Syscall Function Description <code>mmap</code> <code>sys_mmap()</code> Create memory mapping <code>munmap</code> <code>sys_munmap()</code> Remove mapping <code>mprotect</code> <code>sys_mprotect()</code> Change protection <code>msync</code> <code>sys_msync()</code> Synchronize to backing store <code>madvise</code> <code>sys_madvise()</code> Advise kernel about usage <code>mlock</code> <code>sys_mlock()</code> Wire pages in memory <code>munlock</code> <code>sys_munlock()</code> Unwire pages <code>mlockall</code> <code>sys_mlockall()</code> Wire entire address space <code>munlockall</code> <code>sys_munlockall()</code> Unwire entire address space <code>mincore</code> <code>sys_mincore()</code> Query page residency <code>minherit</code> <code>sys_minherit()</code> Set inheritance"},{"location":"sys/vm/vm_mmap/#mmap-implementation","title":"mmap Implementation","text":"<p><code>sys_mmap(sysmsg, uap)</code></p> <p>Entry point for mmap() system call:</p> <ol> <li>Handle <code>MAP_STACK</code> \u2192 convert to <code>MAP_ANON</code> (stack auto-grow disabled for userland)</li> <li>Call <code>kern_mmap()</code></li> </ol> <p><code>kern_mmap(vms, uaddr, ulen, uprot, uflags, fd, upos, res)</code></p> <p>Main mmap implementation:</p> <p>Validation: <pre><code>if ((flags &amp; MAP_ANON) &amp;&amp; (fd != -1 || pos != 0))\n    return (EINVAL);\nif (size == 0)\n    return (EINVAL);\nif (flags &amp; MAP_STACK) {\n    if (fd != -1)\n        return (EINVAL);\n    if ((prot &amp; (PROT_READ|PROT_WRITE)) != (PROT_READ|PROT_WRITE))\n        return (EINVAL);\n    flags |= MAP_ANON;\n}\n</code></pre></p> <p>Address alignment: <pre><code>pageoff = (pos &amp; PAGE_MASK);\npos -= pageoff;\nsize += pageoff;\nsize = round_page(size);\n</code></pre></p> <p>File mapping setup:</p> <p>For <code>fd != -1</code>: 1. Get file pointer via <code>holdfp()</code> 2. Validate file type is <code>DTYPE_VNODE</code> 3. Handle <code>FPOSIXSHM</code> \u2192 add <code>MAP_NOSYNC</code> 4. Check vnode type (VREG, VCHR allowed) 5. Validate protections against file open mode 6. Handle <code>/dev/zero</code> as anonymous</p> <p>Protection calculation: <pre><code>maxprot = VM_PROT_EXECUTE;\nif (fp-&gt;f_flag &amp; FREAD)\n    maxprot |= VM_PROT_READ;\nif ((flags &amp; MAP_SHARED) &amp;&amp; (fp-&gt;f_flag &amp; FWRITE))\n    maxprot |= VM_PROT_WRITE;  /* Check IMMUTABLE/APPEND */\n</code></pre></p> <p>Entry limit: <pre><code>if (max_proc_mmap &amp;&amp; vms-&gt;vm_map.nentries &gt;= max_proc_mmap)\n    return (ENOMEM);\n</code></pre></p> <p><code>vm_mmap(map, addr, size, prot, maxprot, flags, handle, foff, fp)</code></p> <p>Internal mmap implementation:</p> <ol> <li>Check RLIMIT_VMEM</li> <li>Validate page-aligned file offset</li> <li>Calculate alignment:</li> <li><code>MAP_SIZEALIGN</code>: align to size (must be power of 2)</li> <li>Large mappings (\u2265 SEG_SIZE or &gt; 16\u00d7SEG_SIZE): SEG_SIZE align</li> <li>Otherwise: PAGE_SIZE align</li> </ol> <p>Object lookup: <pre><code>if (flags &amp; MAP_ANON) {\n    if (handle)\n        object = default_pager_alloc(handle, objsize, prot, foff);\n    else\n        object = NULL;  /* Deferred allocation */\n} else {\n    vp = (struct vnode *)handle;\n    if (vp-&gt;v_type == VCHR &amp;&amp; vp-&gt;v_rdev-&gt;si_ops-&gt;d_uksmap) {\n        /* UKSMAP device mapping */\n        uksmap = vp-&gt;v_rdev-&gt;si_ops-&gt;d_uksmap;\n        object = NULL;\n    } else if (vp-&gt;v_type == VCHR) {\n        /* Device mapping */\n        object = dev_pager_alloc(...);\n    } else {\n        /* Regular file */\n        object = vnode_pager_reference(vp);\n    }\n}\n</code></pre></p> <p>Map entry creation: <pre><code>if (uksmap) {\n    rv = vm_map_find(map, uksmap, vp-&gt;v_rdev, foff, addr, size,\n                     align, fitit, VM_MAPTYPE_UKSMAP, ...);\n} else if (flags &amp; MAP_STACK) {\n    rv = vm_map_stack(map, addr, size, flags, prot, maxprot, docow);\n} else {\n    rv = vm_map_find(map, object, NULL, foff, addr, size,\n                     align, fitit, VM_MAPTYPE_NORMAL, ...);\n}\n</code></pre></p> <p>Post-processing: - Set <code>VM_INHERIT_SHARE</code> for <code>MAP_SHARED</code>/<code>MAP_INHERIT</code> - Wire if <code>MAP_WIREFUTURE</code> is set - Update vnode access time</p>"},{"location":"sys/vm/vm_mmap/#munmap-implementation","title":"munmap Implementation","text":"<p><code>sys_munmap(sysmsg, uap)</code></p> <ol> <li>Page-align address and size</li> <li>Validate address range within user space</li> <li>Check entire range is allocated via <code>vm_map_check_protection()</code></li> <li>Call <code>vm_map_remove()</code></li> </ol>"},{"location":"sys/vm/vm_mmap/#mprotect-implementation","title":"mprotect Implementation","text":"<p><code>sys_mprotect(sysmsg, uap)</code></p> <ol> <li>Page-align address and size</li> <li>Call <code>vm_map_protect()</code> with new protection</li> <li>Return appropriate errno for kernel result</li> </ol>"},{"location":"sys/vm/vm_mmap/#msync-implementation","title":"msync Implementation","text":"<p><code>sys_msync(sysmsg, uap)</code></p> <ol> <li>Page-align address and size</li> <li>Validate flags (MS_ASYNC|MS_INVALIDATE mutually exclusive)</li> <li>If size == 0: find containing map entry, use its range</li> <li>Call <code>vm_map_clean()</code> with sync/invalidate flags</li> </ol>"},{"location":"sys/vm/vm_mmap/#madvisemcontrol-implementation","title":"madvise/mcontrol Implementation","text":"<p><code>sys_madvise(sysmsg, uap)</code></p> <p>Calls <code>vm_map_madvise()</code> with behavior:</p> Behavior Action <code>MADV_NORMAL</code> Reset to default <code>MADV_SEQUENTIAL</code> Expect sequential access <code>MADV_RANDOM</code> Expect random access <code>MADV_WILLNEED</code> Prefault pages <code>MADV_DONTNEED</code> May discard pages <code>MADV_FREE</code> May free pages <code>MADV_NOSYNC</code> Don't sync to disk <code>MADV_AUTOSYNC</code> Resume sync <code>MADV_NOCORE</code> Exclude from core dump <code>MADV_CORE</code> Include in core dump <code>MADV_INVAL</code> Invalidate pages <p><code>sys_mcontrol(sysmsg, uap)</code></p> <p>Extended madvise with value parameter.</p>"},{"location":"sys/vm/vm_mmap/#mincore-implementation","title":"mincore Implementation","text":"<p><code>sys_mincore(sysmsg, uap)</code></p> <p>Reports page residency:</p> <ol> <li>Lock map for reading</li> <li>For each page in range:</li> <li>Check pmap first (<code>pmap_mincore()</code>)</li> <li>If not in pmap, check VM object for resident page</li> <li>Build result flags: <code>MINCORE_INCORE</code>, <code>MINCORE_MODIFIED_OTHER</code>, <code>MINCORE_REFERENCED_OTHER</code></li> <li>Write results to user byte vector</li> <li>Restart if map changed during scan</li> </ol>"},{"location":"sys/vm/vm_mmap/#mlockmunlock-implementation","title":"mlock/munlock Implementation","text":"<p><code>sys_mlock(sysmsg, uap)</code></p> <ol> <li>Check against <code>vm_page_max_wired</code> limit</li> <li>Check privilege or <code>RLIMIT_MEMLOCK</code></li> <li>Call <code>vm_map_user_wiring()</code> with <code>FALSE</code> (wire)</li> </ol> <p><code>sys_munlock(sysmsg, uap)</code></p> <ol> <li>Check privilege</li> <li>Call <code>vm_map_user_wiring()</code> with <code>TRUE</code> (unwire)</li> </ol>"},{"location":"sys/vm/vm_mmap/#mlockallmunlockall-implementation","title":"mlockall/munlockall Implementation","text":"<p><code>sys_mlockall(sysmsg, uap)</code></p> <ol> <li>Check privilege</li> <li>If <code>MCL_CURRENT</code>: wire all existing entries</li> <li>If <code>MCL_FUTURE</code>: set <code>MAP_WIREFUTURE</code> flag</li> </ol> <p><code>sys_munlockall(sysmsg, uap)</code></p> <ol> <li>Clear <code>MAP_WIREFUTURE</code></li> <li>Unwire all user-wired entries</li> <li>Handle in-transition entries with retry</li> </ol>"},{"location":"sys/vm/vm_mmap/#minherit-implementation","title":"minherit Implementation","text":"<p><code>sys_minherit(sysmsg, uap)</code></p> <p>Sets fork inheritance via <code>vm_map_inherit()</code>:</p> Inheritance Behavior <code>VM_INHERIT_NONE</code> Not inherited <code>VM_INHERIT_COPY</code> COW copy (default) <code>VM_INHERIT_SHARE</code> Share with child"},{"location":"sys/vm/vm_mmap/#key-sysctls","title":"Key Sysctls","text":"Sysctl Default Description <code>vm.max_proc_mmap</code> 1000000 Max map entries per process <code>vm.vkernel_enable</code> 0 Enable vkernel features"},{"location":"sys/vm/vm_mmap/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_mmap/#uksmap-device-mappings","title":"UKSMAP Device Mappings","text":"<p>Devices can provide direct user-kernel shared memory via <code>d_uksmap</code>:</p> <pre><code>if (vp-&gt;v_type == VCHR &amp;&amp; vp-&gt;v_rdev-&gt;si_ops-&gt;d_uksmap) {\n    uksmap = vp-&gt;v_rdev-&gt;si_ops-&gt;d_uksmap;\n    object = NULL;  /* No VM object */\n    flags |= MAP_SHARED;\n}\n</code></pre> <p>Used for <code>/dev/upmap</code>, <code>/dev/kpmap</code>, <code>/dev/lpmap</code>.</p>"},{"location":"sys/vm/vm_mmap/#map_stack-handling","title":"MAP_STACK Handling","text":"<p>User MAP_STACK is converted to MAP_ANON:</p> <pre><code>if (flags &amp; MAP_STACK) {\n    flags &amp;= ~MAP_STACK;\n    flags |= MAP_ANON;\n}\n</code></pre> <p>Only the exec-created user stack uses true MAP_STACK internally.</p>"},{"location":"sys/vm/vm_mmap/#alignment-optimization","title":"Alignment Optimization","text":"<p>Large mappings are aligned to SEG_SIZE for MMU optimization:</p> <pre><code>if ((flags &amp; MAP_FIXED) == 0 &amp;&amp;\n    ((size &amp; SEG_MASK) == 0 || size &gt; SEG_SIZE * 16)) {\n    align = SEG_SIZE;\n}\n</code></pre>"},{"location":"sys/vm/vm_mmap/#address-hint","title":"Address Hint","text":"<p>Non-fixed mappings get ASLR-randomized hint:</p> <pre><code>addr = vm_map_hint(p, addr, prot, flags);\n</code></pre>"},{"location":"sys/vm/vm_mmap/#posix-shared-memory","title":"POSIX Shared Memory","text":"<p>Files opened with <code>shm_open()</code> set <code>FPOSIXSHM</code>:</p> <pre><code>if (fp-&gt;f_flag &amp; FPOSIXSHM)\n    flags |= MAP_NOSYNC;\n</code></pre>"},{"location":"sys/vm/vm_mmap/#function-reference","title":"Function Reference","text":""},{"location":"sys/vm/vm_mmap/#vnode-pager","title":"Vnode Pager","text":"Function Description <code>vnode_pager_alloc()</code> Create/reference vnode object <code>vnode_pager_reference()</code> Reference existing object <code>vnode_pager_dealloc()</code> Destroy vnode object <code>vnode_pager_haspage()</code> Check file backing <code>vnode_pager_getpage()</code> Page in from file <code>vnode_pager_putpages()</code> Page out to file <code>vnode_pager_generic_getpages()</code> Generic page-in implementation <code>vnode_pager_generic_putpages()</code> Generic page-out implementation <code>vnode_pager_setsize()</code> Handle file size change <code>vnode_pager_freepage()</code> Release page from getpages <code>vnode_pager_lock()</code> Lock backing vnode"},{"location":"sys/vm/vm_mmap/#memory-mapping","title":"Memory Mapping","text":"Function Description <code>kern_mmap()</code> Internal mmap implementation <code>vm_mmap()</code> Core mapping function <code>sys_mmap()</code> mmap syscall handler <code>sys_munmap()</code> munmap syscall handler <code>sys_mprotect()</code> mprotect syscall handler <code>sys_msync()</code> msync syscall handler <code>sys_madvise()</code> madvise syscall handler <code>sys_mcontrol()</code> mcontrol syscall handler <code>sys_mincore()</code> mincore syscall handler <code>sys_mlock()</code> mlock syscall handler <code>sys_munlock()</code> munlock syscall handler <code>sys_mlockall()</code> mlockall syscall handler <code>sys_munlockall()</code> munlockall syscall handler <code>sys_minherit()</code> minherit syscall handler <code>vm_mmap_to_errno()</code> Convert VM return to errno"},{"location":"sys/vm/vm_mmap/#see-also","title":"See Also","text":"<ul> <li>Address Space - Map entry management</li> <li>VM Objects - Object lifecycle</li> <li>Page Faults - Fault handling for mapped pages</li> <li>Pageout and Swap - Anonymous page backing</li> </ul>"},{"location":"sys/vm/vm_object/","title":"VM Objects","text":"<p>VM objects are the fundamental abstraction for managing virtual memory contents in DragonFly BSD. Each object represents a contiguous range of virtual memory that can be backed by files, swap space, devices, or anonymous memory.</p> <p>Source file: <code>sys/vm/vm_object.c</code> (~2,034 lines)</p>"},{"location":"sys/vm/vm_object/#why-vm-objects-matter","title":"Why VM Objects Matter","text":"<p>Every byte of virtual memory in the system\u2014whether it's a mapped file, heap allocation, or shared library\u2014is ultimately backed by a VM object. Objects are the kernel's answer to several fundamental questions:</p> <p>What happens when two processes map the same file?</p> <p>Without a unifying abstraction, each process would have its own copy of the file's pages, wasting memory and creating consistency nightmares. VM objects solve this: a single <code>OBJT_VNODE</code> object holds all pages for a file, and multiple processes simply reference the same object. Modify page 5 in one process, and every other mapping sees it immediately.</p> <p>How does the kernel know where to get a page's contents?</p> <p>When a page fault occurs, the kernel needs to know: is this page in memory? On disk? In a swap file? Part of a device? The VM object knows. Its <code>type</code> field determines the pager\u2014the mechanism for populating missing pages. Each object type has different rules:</p> Object Type Where Pages Come From When Used <code>OBJT_VNODE</code> Read from file via vnode <code>mmap(file)</code>, executables, shared libraries <code>OBJT_SWAP</code> Read from swap partition Heap memory under pressure, <code>shm_open()</code> <code>OBJT_DEFAULT</code> Zero-filled on demand Fresh <code>malloc()</code>, new stack pages <code>OBJT_DEVICE</code> Device provides pages GPU memory, framebuffers <code>OBJT_MGTDEVICE</code> Device manages page lifecycle Modern GPU drivers <p>How does copy-on-write work efficiently?</p> <p>When <code>fork()</code> creates a child process, copying all memory would be prohibitively slow. Instead, the child's <code>vm_map_backing</code> structures point to the same objects as the parent. Pages are marked read-only; only when either process writes does the kernel copy the page. The object is the shared foundation that makes COW possible.</p> <p>How does the kernel track dirty pages for writeback?</p> <p>The filesystem syncer needs to know which files have modified pages. Rather than scanning all pages in the system, it checks <code>OBJ_MIGHTBEDIRTY</code> on vnode objects. Objects aggregate page state, making system-wide operations tractable.</p>"},{"location":"sys/vm/vm_object/#the-life-of-an-object","title":"The Life of an Object","text":"<p>Objects transition through different types and states during their lifetime:</p> <pre><code>flowchart TB\n    ALLOC[\"vm_object_allocate()Create new object\"]\n    TYPE{\"Object Type?\"}\n    DEFAULT[\"OBJT_DEFAULTAnonymous memory\"]\n    VNODE[\"OBJT_VNODEFile-backed\"]\n    DEVICE[\"OBJT_DEVICEDevice memory\"]\n    ACTIVE[\"Active Useref_count &gt; 0, pages in rb_memq\"]\n    PRESSURE[\"Memory PressurePageout daemon active\"]\n    SWAP[\"OBJT_DEFAULT \u2192 OBJT_SWAPPages pushed to swap\"]\n    CLEAN[\"vm_object_page_clean()Dirty pages written\"]\n    TERMINATE[\"vm_object_terminate()OBJ_DEAD set\"]\n    WAIT[\"Wait for paging_in_progress = 0\"]\n    FREE[\"Object freedhold_count = 0\"]\n\n    ALLOC --&gt; TYPE\n    TYPE --&gt; DEFAULT\n    TYPE --&gt; VNODE\n    TYPE --&gt; DEVICE\n    DEFAULT --&gt; ACTIVE\n    VNODE --&gt; ACTIVE\n    DEVICE --&gt; ACTIVE\n    ACTIVE --&gt; PRESSURE\n    PRESSURE --&gt; SWAP\n    PRESSURE --&gt; CLEAN\n    SWAP --&gt; ACTIVE\n    CLEAN --&gt; ACTIVE\n    ACTIVE --&gt;|\"ref_count drops to 0\"| TERMINATE\n    TERMINATE --&gt; WAIT\n    WAIT --&gt; FREE\n</code></pre> <p>Key transitions:</p> <ol> <li> <p>DEFAULT \u2192 SWAP: When memory pressure forces an anonymous page to swap, the object silently converts from <code>OBJT_DEFAULT</code> to <code>OBJT_SWAP</code>. The first <code>swp_pager_meta_build()</code> call triggers this.</p> </li> <li> <p>Active \u2192 Dead: When <code>ref_count</code> drops to zero, <code>vm_object_terminate()</code> sets <code>OBJ_DEAD</code> and begins teardown. The object can't accept new references.</p> </li> <li> <p>Dead \u2192 Freed: The object waits for <code>paging_in_progress</code> to reach zero (no pending I/O), frees all pages, notifies the pager, and finally frees itself when <code>hold_count</code> reaches zero.</p> </li> </ol>"},{"location":"sys/vm/vm_object/#key-design-principles","title":"Key Design Principles","text":"Problem Without Objects DragonFly's Solution Shared file mappings Each process has separate pages; no coherence Single object per vnode; all mappings share pages Page fault handling Fault handler needs to know backing store type Object's <code>type</code> field selects appropriate pager Copy-on-write Full copy on fork (slow, wasteful) Child references same objects; COW on write Dirty page tracking Scan all pages to find dirty ones <code>OBJ_MIGHTBEDIRTY</code> flag aggregates state Lock contention Single lock for all pages Per-object LWKT token; operations on different objects don't contend Reference counting races Complex locking for ref count changes Atomic ops + hold_count pattern for safe access Page lookup Linear scan or hash table Per-object RB tree; O(log n) by page index"},{"location":"sys/vm/vm_object/#where-objects-fit-in-the-vm-hierarchy","title":"Where Objects Fit in the VM Hierarchy","text":"<pre><code>flowchart TB\n    subgraph User[\"USER PROCESS\"]\n        mmap[\"ptr = mmap(file)\"]\n    end\n\n    subgraph VM[\"VM SUBSYSTEM\"]\n        entry[\"vm_map_entry\"]\n        backing[\"vm_map_backing\"]\n        object[\"vm_object\"]\n        pager[\"vnode_pager\"]\n\n        entry --&gt; backing\n        backing --&gt; object\n        pager --&gt; object\n\n        subgraph pages[\"RB tree by page index\"]\n            page1[\"vm_page\"]\n            page2[\"vm_page\"]\n            page3[\"vm_page\"]\n        end\n\n        object --&gt; pages\n    end\n\n    mmap --&gt; entry\n</code></pre> <p>Key insight: Objects are the container for pages. They don't manage address spaces (that's <code>vm_map</code>) or physical allocation (that's <code>vm_page</code>). They manage:</p> <ul> <li>Which pages belong together logically</li> <li>How to populate missing pages (via pager)</li> <li>Sharing between processes (file-backed)</li> </ul>"},{"location":"sys/vm/vm_object/#common-scenarios","title":"Common Scenarios","text":"Scenario Object Type What Happens <code>malloc(large)</code> OBJT_DEFAULT\u2192OBJT_SWAP Anonymous object created, pages added on fault, swapped under pressure <code>mmap(file)</code> OBJT_VNODE Object tied to vnode, pages loaded from file on fault <code>fork()</code> Parent's objects Child gets new vm_map_backing pointing to same objects (COW) GPU memory OBJT_MGTDEVICE Device manages pages, object tracks mappings <code>shm_open()</code> OBJT_SWAP Swap-backed object shared between processes"},{"location":"sys/vm/vm_object/#overview","title":"Overview","text":"<p>A VM object maintains:</p> <ul> <li>A red-black tree of resident physical pages (<code>rb_memq</code>)</li> <li>Reference count for lifetime management</li> <li>Type-specific backing store (file, swap, device)</li> <li>A list of <code>vm_map_backing</code> structures that reference this object</li> </ul> <p>Objects are the bridge between address space mappings (<code>vm_map_entry</code>) and physical pages (<code>vm_page</code>). A single page exists within exactly one object at any given time.</p>"},{"location":"sys/vm/vm_object/#object-types","title":"Object Types","text":"<pre><code>enum obj_type {\n    OBJT_DEFAULT,    /* Anonymous memory, initially no backing */\n    OBJT_SWAP,       /* Backed by swap blocks */\n    OBJT_VNODE,      /* Backed by file (vnode) */\n    OBJT_DEVICE,     /* Device-backed pages */\n    OBJT_MGTDEVICE,  /* Managed device pager */\n    OBJT_PHYS,       /* Physical pages (no paging) */\n    OBJT_DEAD,       /* Being destroyed */\n    OBJT_MARKER,     /* List iteration marker */\n};\n</code></pre>"},{"location":"sys/vm/vm_object/#type-characteristics","title":"Type Characteristics","text":"Type Backing Store Pages in rb_memq Swappable OBJT_DEFAULT None initially Yes Converts to SWAP OBJT_SWAP Swap blocks Yes Yes OBJT_VNODE File Yes Via file I/O OBJT_DEVICE Device memory No (typically) No OBJT_MGTDEVICE Managed device Via backing_list No"},{"location":"sys/vm/vm_object/#data-structures","title":"Data Structures","text":""},{"location":"sys/vm/vm_object/#struct-vm_object","title":"struct vm_object","text":"<pre><code>struct vm_object {\n    struct lwkt_token token;           /* Soft-lock for object */\n    struct lock backing_lk;            /* Lock for backing_list only */\n    struct vm_page_rb_tree rb_memq;    /* Resident pages (RB tree) */\n    TAILQ_HEAD(,vm_map_backing) backing_list;  /* Who references us */\n\n    vm_pindex_t size;                  /* Size in pages */\n    int ref_count;                     /* Reference count */\n    int hold_count;                    /* Destruction prevention */\n    u_int paging_in_progress;          /* Active I/O operations */\n\n    objtype_t type;                    /* OBJT_* type */\n    u_short flags;                     /* OBJ_* flags */\n    vm_memattr_t memattr;              /* Memory attributes (PAT) */\n    u_short pg_color;                  /* Base page color */\n\n    void *handle;                      /* Type-specific (vnode, dev) */\n    long resident_page_count;          /* Cached page count */\n    int generation;                    /* Modification counter */\n\n    /* Swap support */\n    struct swblock_rb_tree swblock_root;\n    long swblock_count;\n};\n</code></pre>"},{"location":"sys/vm/vm_object/#object-flags","title":"Object Flags","text":"Flag Description <code>OBJ_ACTIVE</code> Object is active <code>OBJ_DEAD</code> Being destroyed <code>OBJ_NOSPLIT</code> Don't split this object <code>OBJ_ONEMAPPING</code> Each page maps to at most one vm_map_entry <code>OBJ_WRITEABLE</code> Has been made writeable <code>OBJ_MIGHTBEDIRTY</code> May have dirty pages <code>OBJ_CLEANING</code> Page cleaning in progress <code>OBJ_DEADWNT</code> Waiter for object death"},{"location":"sys/vm/vm_object/#global-hash-table","title":"Global Hash Table","text":"<p>Objects are tracked in a 256-bucket hash table for global enumeration:</p> <pre><code>struct vm_object_hash vm_object_hash[VMOBJ_HSIZE];  /* VMOBJ_HSIZE = 256 */\n</code></pre> <p>Each bucket contains a TAILQ list protected by an LWKT token. The hash function uses two large primes for distribution.</p>"},{"location":"sys/vm/vm_object/#locking-model","title":"Locking Model","text":"<p>DragonFly uses LWKT tokens (soft-locks) for VM objects, allowing blocking while held:</p> Function Description <code>vm_object_hold(obj)</code> Acquire hold + exclusive token <code>vm_object_hold_shared(obj)</code> Acquire hold + shared token <code>vm_object_hold_try(obj)</code> Non-blocking hold attempt <code>vm_object_drop(obj)</code> Release hold + token"},{"location":"sys/vm/vm_object/#hold-count-vs-reference-count","title":"Hold Count vs Reference Count","text":"<ul> <li>ref_count: Logical references (from mappings, etc.)</li> <li>hold_count: Prevents object from being freed while working with it</li> </ul> <p>The hold/drop pattern is critical:</p> <pre><code>/* Must increment hold_count BEFORE blocking on token */\nrefcount_acquire(&amp;obj-&gt;hold_count);  /* Makes object stable */\nvm_object_lock(obj);                 /* May block */\n/* ... work with object ... */\nvm_object_unlock(obj);\nif (refcount_release(&amp;obj-&gt;hold_count)) {\n    if (obj-&gt;ref_count == 0 &amp;&amp; (obj-&gt;flags &amp; OBJ_DEAD))\n        kfree_obj(obj, M_VM_OBJECT);  /* Final free */\n}\n</code></pre>"},{"location":"sys/vm/vm_object/#object-lifecycle","title":"Object Lifecycle","text":""},{"location":"sys/vm/vm_object/#allocation","title":"Allocation","text":"<pre><code>/* Returns unheld object */\nvm_object_t vm_object_allocate(objtype_t type, vm_pindex_t size);\n\n/* Returns held object for atomic initialization */\nvm_object_t vm_object_allocate_hold(objtype_t type, vm_pindex_t size);\n</code></pre> <p>Initialization (<code>_vm_object_allocate()</code>) performs:</p> <ol> <li>Initialize page RB tree and token</li> <li>Initialize <code>backing_list</code> and <code>backing_lk</code></li> <li>Set type, size, ref_count=1</li> <li>For DEFAULT/SWAP: set <code>OBJ_ONEMAPPING</code></li> <li>Assign random page color via <code>vm_quickcolor()</code></li> <li>Initialize swap block tree</li> <li>Insert into global hash table</li> </ol>"},{"location":"sys/vm/vm_object/#reference-counting","title":"Reference Counting","text":"<p>Adding references:</p> <pre><code>/* Must hold object token */\nvoid vm_object_reference_locked(vm_object_t object);\n\n/* Safe without token when object is deterministically referenced */\nvoid vm_object_reference_quick(vm_object_t object);\n</code></pre> <p>For <code>OBJT_VNODE</code> objects, these also call <code>vref()</code> on the vnode.</p> <p>Releasing references:</p> <pre><code>void vm_object_deallocate(vm_object_t object);\n</code></pre> <p>The deallocation path optimizes for the common case:</p> <ul> <li>Fast path (ref_count &gt; 3): Atomic decrement without locking</li> <li>Slow path (ref_count &lt;= 3): Hold object, handle termination</li> </ul> <p>This avoids exclusive lock contention on highly-shared binaries during exec/exit.</p>"},{"location":"sys/vm/vm_object/#termination","title":"Termination","text":"<p>When ref_count reaches zero, <code>vm_object_terminate()</code> is called:</p> <ol> <li>Set <code>OBJ_DEAD</code> flag</li> <li>Wait for <code>paging_in_progress</code> to reach 0</li> <li>For <code>OBJT_VNODE</code>:</li> <li><code>vinvalbuf()</code> - flush buffers</li> <li><code>vm_object_page_clean()</code> - write dirty pages</li> <li><code>vinvalbuf()</code> again (TMPFS special case)</li> <li>Free all resident pages via callback</li> <li><code>vm_pager_deallocate()</code> - notify pager</li> <li>Remove from hash table</li> <li>Object freed when hold_count reaches 0</li> </ol>"},{"location":"sys/vm/vm_object/#page-management","title":"Page Management","text":""},{"location":"sys/vm/vm_object/#page-cleaning","title":"Page Cleaning","text":"<p><code>vm_object_page_clean()</code> writes dirty pages to backing store:</p> <pre><code>void vm_object_page_clean(vm_object_t object, \n                          vm_pindex_t start, \n                          vm_pindex_t end,\n                          int flags);\n</code></pre> <p>Flags:</p> Flag Description <code>OBJPC_SYNC</code> Synchronous I/O <code>OBJPC_INVAL</code> Invalidate after cleaning <code>OBJPC_NOSYNC</code> Skip PG_NOSYNC pages <code>OBJPC_CLUSTER_OK</code> Allow I/O clustering <p>Two-pass algorithm:</p> <ol> <li>Pass 1: Mark all pages read-only (<code>vm_page_protect(VM_PROT_READ)</code>)</li> <li>Sets <code>PG_CLEANCHK</code> flag on each page</li> <li> <p>If entire object cleaned: clears <code>OBJ_WRITEABLE|OBJ_MIGHTBEDIRTY</code></p> </li> <li> <p>Pass 2: Write dirty pages</p> </li> <li>Skips pages without <code>PG_CLEANCHK</code> (inserted after pass 1)</li> <li>Clusters adjacent dirty pages for efficient I/O</li> <li>Repeats if object's generation changes</li> </ol>"},{"location":"sys/vm/vm_object/#page-removal","title":"Page Removal","text":"<pre><code>void vm_object_page_remove(vm_object_t object,\n                           vm_pindex_t start,\n                           vm_pindex_t end,\n                           boolean_t clean_only);\n</code></pre> <p>This function:</p> <ol> <li>Scans <code>backing_list</code> to remove pmap mappings (important for MGTDEVICE)</li> <li>Scans <code>rb_memq</code> to free pages</li> <li>Frees related swap blocks</li> </ol> <p>The <code>clean_only</code> flag preserves dirty pages.</p>"},{"location":"sys/vm/vm_object/#madvise-support","title":"madvise Support","text":"<pre><code>void vm_object_madvise(vm_object_t object,\n                       vm_pindex_t pindex,\n                       vm_pindex_t count,\n                       int advise);\n</code></pre> Advise Action <code>MADV_WILLNEED</code> Activate pages (move to active queue) <code>MADV_DONTNEED</code> Deactivate pages (candidate for reclaim) <code>MADV_FREE</code> Mark clean + deactivate + free swap <p><code>MADV_FREE</code> is restricted to <code>OBJT_DEFAULT</code>/<code>OBJT_SWAP</code> objects with <code>OBJ_ONEMAPPING</code>.</p>"},{"location":"sys/vm/vm_object/#object-coalescing","title":"Object Coalescing","text":"<pre><code>boolean_t vm_object_coalesce(vm_object_t prev_object,\n                             vm_pindex_t prev_pindex,\n                             vm_size_t prev_size,\n                             vm_size_t next_size);\n</code></pre> <p>Extends an object into adjacent virtual memory:</p> <ul> <li>Only for <code>OBJT_DEFAULT</code>/<code>OBJT_SWAP</code></li> <li>Requires single reference (or extending into new space)</li> <li>Removes any existing pages in the new region</li> <li>Updates <code>object-&gt;size</code></li> </ul>"},{"location":"sys/vm/vm_object/#vnode-object-handling","title":"Vnode Object Handling","text":"<p><code>OBJT_VNODE</code> objects have special handling:</p> <ul> <li>Reference counting: <code>vref()</code>/<code>vrele()</code> called alongside object refs</li> <li>VTEXT flag: Cleared on last reference (executable text)</li> <li>Dirty tracking: <code>VOBJDIRTY</code> flag on vnode for syncer</li> <li>Page cleaning: Double <code>vinvalbuf()</code> for TMPFS compatibility</li> </ul> <p>The <code>vm_object_vndeallocate()</code> function handles the complex 1-&gt;0 transition:</p> <pre><code>/* Atomically handle ref_count with retry loop */\nif (count == 1) {\n    vm_object_upgrade(object);      /* Need exclusive for VTEXT */\n    if (atomic_fcmpset_int(&amp;object-&gt;ref_count, &amp;count, 0)) {\n        vclrflags(vp, VTEXT);\n        break;\n    }\n}\n</code></pre>"},{"location":"sys/vm/vm_object/#dirty-flag-management","title":"Dirty Flag Management","text":"<pre><code>void vm_object_set_writeable_dirty(vm_object_t object);\n</code></pre> <p>Called from the fault path when a page becomes writeable:</p> <ol> <li>Sets <code>OBJ_WRITEABLE | OBJ_MIGHTBEDIRTY</code> on object</li> <li>For <code>OBJT_VNODE</code>: sets <code>VOBJDIRTY</code> on vnode</li> <li>Uses <code>vsetobjdirty()</code> for <code>MNTK_THR_SYNC</code> mounts</li> <li>Uses <code>vsetflags()</code> for traditional mounts</li> </ol> <p>The flags check before atomic operation avoids contention in the fault path.</p>"},{"location":"sys/vm/vm_object/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_object/#lwkt-token-locking","title":"LWKT Token Locking","text":"<p>Unlike traditional BSD mutexes, LWKT tokens allow:</p> <ul> <li>Blocking while held</li> <li>Other threads to \"squeeze in\" work</li> <li>Shared/exclusive modes</li> <li>Token swapping for lock ordering</li> </ul>"},{"location":"sys/vm/vm_object/#backing_list","title":"backing_list","text":"<p>Each object maintains a list of <code>vm_map_backing</code> structures:</p> <pre><code>TAILQ_HEAD(, vm_map_backing) backing_list;\nstruct lock backing_lk;  /* Separate lock for this list */\n</code></pre> <p>This enables:</p> <ul> <li>Efficient pmap removal during page removal</li> <li>Support for <code>OBJT_MGTDEVICE</code> (pages not in rb_memq)</li> <li>Tracking all mappings of an object</li> </ul>"},{"location":"sys/vm/vm_object/#page-coloring","title":"Page Coloring","text":"<p><code>vm_quickcolor()</code> provides semi-random initial page colors:</p> <pre><code>int vm_quickcolor(void) {\n    globaldata_t gd = mycpu;\n    int pg_color = (int)(intptr_t)gd-&gt;gd_curthread &gt;&gt; 10;\n    pg_color += gd-&gt;gd_quick_color;\n    gd-&gt;gd_quick_color += PQ_PRIME2;\n    return pg_color;\n}\n</code></pre> <p>This spreads page allocations across queues for SMP scalability.</p>"},{"location":"sys/vm/vm_object/#debugging","title":"Debugging","text":"<p>DDB commands for object inspection:</p> Command Description <code>show vmochk</code> Verify internal objects are mapped <code>show object &lt;addr&gt;</code> Print object details and pages <code>show vmopag</code> Print page runs for all objects"},{"location":"sys/vm/vm_object/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"sys/vm/vm_object/#pattern-1-allocating-an-anonymous-object","title":"Pattern 1: Allocating an Anonymous Object","text":"<p>When kernel code needs a private, anonymous memory region (e.g., for a new process's stack):</p> <pre><code>vm_object_t object;\n\n/* Allocate a 16-page anonymous object */\nobject = vm_object_allocate(OBJT_DEFAULT, 16);\n\n/* Object starts with ref_count=1, no pages yet */\n/* Pages will be zero-filled on first fault */\n\n/* Later, when done: */\nvm_object_deallocate(object);\n</code></pre>"},{"location":"sys/vm/vm_object/#pattern-2-working-with-an-object-safely","title":"Pattern 2: Working with an Object Safely","text":"<p>The hold/drop pattern ensures an object won't disappear while you're using it:</p> <pre><code>vm_object_t object = /* obtained from somewhere */;\n\n/* Acquire hold + exclusive token (may block) */\nvm_object_hold(object);\n\n/* Safe to access object fields and pages */\nif (object-&gt;resident_page_count &gt; 0) {\n    vm_page_t p;\n    /* Iterate pages in RB tree */\n    RB_FOREACH(p, vm_page_rb_tree, &amp;object-&gt;rb_memq) {\n        /* Process page */\n    }\n}\n\n/* Release hold + token */\nvm_object_drop(object);\n</code></pre>"},{"location":"sys/vm/vm_object/#pattern-3-cleaning-dirty-pages-before-unmount","title":"Pattern 3: Cleaning Dirty Pages Before Unmount","text":"<p>Filesystems call this to flush modified pages back to disk:</p> <pre><code>vm_object_t object = vp-&gt;v_object;\n\nif (object != NULL) {\n    vm_object_hold(object);\n\n    /* Synchronously clean all pages, invalidate after */\n    vm_object_page_clean(object, \n                         0,                  /* start page index */\n                         object-&gt;size,       /* end page index */\n                         OBJPC_SYNC | OBJPC_INVAL);\n\n    vm_object_drop(object);\n}\n</code></pre>"},{"location":"sys/vm/vm_object/#pattern-4-looking-up-a-page-in-an-object","title":"Pattern 4: Looking Up a Page in an Object","text":"<p>Finding a specific page by its index within an object:</p> <pre><code>vm_object_hold(object);\n\nvm_page_t page = vm_page_lookup(object, pindex);\nif (page != NULL) {\n    /* Page exists in object's rb_memq */\n    if (page-&gt;valid == VM_PAGE_BITS_ALL) {\n        /* Page is fully valid, can be used */\n    }\n} else {\n    /* Page not resident - would need fault or explicit alloc */\n}\n\nvm_object_drop(object);\n</code></pre>"},{"location":"sys/vm/vm_object/#pattern-5-handling-object-type-transitions","title":"Pattern 5: Handling Object Type Transitions","text":"<p>From <code>sys/vm/swap_pager.c</code> - anonymous objects convert to swap-backed when pages are pushed out:</p> <pre><code>/* \n * swp_pager_meta_convert() - called when first swap block is allocated\n * The conversion is simple - just change the type field.\n */\nstatic void\nswp_pager_meta_convert(vm_object_t object)\n{\n    if (object-&gt;type == OBJT_DEFAULT) {\n        object-&gt;type = OBJT_SWAP;\n        KKASSERT(object-&gt;swblock_count == 0);\n    }\n}\n\n/* Code checking for anonymous memory should handle both types */\nif (object-&gt;type == OBJT_DEFAULT || object-&gt;type == OBJT_SWAP) {\n    /* Anonymous memory - may or may not have swap blocks */\n}\n</code></pre>"},{"location":"sys/vm/vm_object/#further-reading","title":"Further Reading","text":"<ul> <li>\"The Design and Implementation of the 4.4BSD Operating System\" (McKusick et al.) - Chapter 5 covers the VM system design that DragonFly inherited and evolved</li> <li>\"Design elements of the FreeBSD VM system\" - FreeBSD Architecture Handbook - DragonFly's VM has diverged but shares heritage</li> <li>DragonFly commit history for vm_object.c - Shows evolution of LWKT tokens, backing_list, and other DragonFly-specific changes</li> <li>vm_page.md - Physical page management, closely related to object page operations</li> <li>vm_fault.md - How page faults interact with objects to populate pages</li> </ul>"},{"location":"sys/vm/vm_object/#see-also","title":"See Also","text":"<ul> <li>VM Subsystem Overview - Architecture overview</li> <li>Physical Page Management - Page allocation and queues</li> </ul>"},{"location":"sys/vm/vm_page/","title":"Physical Page Management","text":"<p>This document describes DragonFly BSD's physical page management subsystem, implemented in <code>sys/vm/vm_page.c</code>. The subsystem manages all physical memory pages in the system, including allocation, freeing, queue management, and state transitions.</p> <p>Source file: <code>sys/vm/vm_page.c</code> (~4,200 lines)</p>"},{"location":"sys/vm/vm_page/#why-physical-page-management-matters","title":"Why Physical Page Management Matters","text":"<p>Physical memory is a finite resource that every process, the kernel, and the filesystem cache compete for. The page management subsystem is the arbiter of this competition\u2014it decides which processes get memory, which pages stay resident, and which get evicted to disk.</p> <p>Without sophisticated page management:</p> <ul> <li>Lock contention would serialize all memory operations on multi-core systems</li> <li>Memory fragmentation would prevent allocation of contiguous regions for DMA</li> <li>Unfair allocation would let memory-hungry processes starve others</li> <li>Poor locality would cause cache misses as CPUs access remote NUMA memory</li> </ul> <p>DragonFly's page management addresses these with 1024-way queue coloring (reducing lock contention), NUMA-aware allocation (improving locality), and nice-aware paging thresholds (ensuring fairness).</p>"},{"location":"sys/vm/vm_page/#the-life-of-a-page","title":"The Life of a Page","text":"<p>Before diving into data structures, it helps to understand the typical journey of a physical page:</p> <pre><code>flowchart TB\n    subgraph BIRTH[\"Birth\"]\n        BOOT[\"System boot\"]\n        FREE1[\"Page added to PQ_FREE queue\"]\n    end\n\n    subgraph LIFE[\"Active Life\"]\n        ALLOC[\"vm_page_alloc() - Page assigned to object\"]\n        ACTIVE[\"PQ_ACTIVE - Recently used\"]\n        INACTIVE[\"PQ_INACTIVE - Aging, candidate for reclaim\"]\n    end\n\n    subgraph DEATH[\"Reclamation\"]\n        CACHE[\"PQ_CACHE - Clean, unmapped, quickly reusable\"]\n        FREE2[\"PQ_FREE - Available again\"]\n    end\n\n    BOOT --&gt; FREE1\n    FREE1 --&gt;|\"fault or explicit alloc\"| ALLOC\n    ALLOC --&gt; ACTIVE\n    ACTIVE --&gt;|\"not referenced recently\"| INACTIVE\n    INACTIVE --&gt;|\"cleaned by pageout\"| CACHE\n    INACTIVE --&gt;|\"referenced again\"| ACTIVE\n    CACHE --&gt;|\"stolen for new alloc\"| FREE2\n    FREE2 --&gt;|\"new fault\"| ALLOC\n</code></pre> <p>1. Birth (Boot): During system startup, <code>vm_page_startup()</code> creates a <code>struct vm_page</code> for every physical page and places them on <code>PQ_FREE</code> queues.</p> <p>2. Allocation: When a page fault occurs or the kernel needs memory, <code>vm_page_alloc()</code> pulls a page from <code>PQ_FREE</code>, associates it with a <code>vm_object</code>, and marks it busy while I/O loads its contents.</p> <p>3. Active Use: The page lives on <code>PQ_ACTIVE</code> while processes access it. Each access refreshes its \"activity count,\" keeping it resident.</p> <p>4. Aging: If a page isn't accessed for a while, the pageout daemon moves it to <code>PQ_INACTIVE</code>. This is a \"second chance\"\u2014if accessed again, it returns to <code>PQ_ACTIVE</code>.</p> <p>5. Cleaning: If the page remains inactive and is dirty, the pageout daemon writes it to its backing store (swap or file). Now clean, it moves to <code>PQ_CACHE</code>.</p> <p>6. Reclamation: <code>PQ_CACHE</code> pages are still valid but unmapped. If memory pressure rises, they're the first victims\u2014instantly reusable without I/O. Otherwise, they may be reactivated if faulted again.</p> <p>7. Rebirth: Eventually the page returns to <code>PQ_FREE</code>, ready for a new allocation.</p>"},{"location":"sys/vm/vm_page/#when-you-need-this","title":"When You Need This","text":"Scenario Key Functions Section Allocating a page during fault handling <code>vm_page_alloc()</code>, <code>vm_page_grab()</code> Page Allocation Understanding why a page can't be freed Wire count, hold count, busy state Wire/Unwire, Hold, Busy State Implementing a new pager <code>vm_page_set_valid()</code>, <code>vm_page_dirty()</code> Valid/Dirty Bits Debugging memory pressure issues Page queues, vmstats Page Queues, Memory Pressure Writing DMA-capable driver code <code>vm_page_alloc_contig()</code> Contiguous Allocation Understanding pageout victim selection Queue transitions Page State Transitions"},{"location":"sys/vm/vm_page/#overview","title":"Overview","text":"<p>Every physical page in the system is represented by a <code>struct vm_page</code> (128 bytes). These structures are stored in a global array (<code>vm_page_array</code>) and indexed by physical page number. The VM system organizes pages into multiple queues based on their state and uses sophisticated coloring and NUMA-aware algorithms to optimize memory locality.</p>"},{"location":"sys/vm/vm_page/#key-design-principles","title":"Key Design Principles","text":"Principle Problem Solved DragonFly's Solution Page coloring Lock contention on page queues 1024 sub-queues per queue type; each CPU typically hits different queues NUMA awareness Cross-socket memory latency Color calculation incorporates socket/core topology Atomic busy state Lock overhead for page access Busy counts instead of locks; soft-busy for shared access Per-CPU statistics Cache-line bouncing on global counters Each CPU caches vmstats locally; periodic rollup to global"},{"location":"sys/vm/vm_page/#why-five-queues","title":"Why Five Queues?","text":"<p>The five page queues implement a multi-stage replacement policy that balances responsiveness with efficiency:</p> Queue Purpose Why It Exists <code>PQ_FREE</code> Immediately allocatable Fast allocation without any cleanup needed <code>PQ_ACTIVE</code> Recently used pages Protects working set from premature eviction <code>PQ_INACTIVE</code> Second-chance candidates Gives pages time to be re-referenced before eviction <code>PQ_CACHE</code> Clean, valid, unmapped Zero-cost reuse if same data needed; instant free otherwise <code>PQ_HOLD</code> Temporarily pinned Prevents race conditions during sensitive operations <p>This design avoids the pathological behavior of simpler schemes: - Pure LRU would evict pages that happen to be accessed in large sequential scans - Pure FIFO would evict frequently-used pages just because they're old - The active/inactive split approximates LRU while the cache queue enables instant reclamation of clean pages</p>"},{"location":"sys/vm/vm_page/#dragonfly-specific-design-choices","title":"DragonFly-Specific Design Choices","text":"<p>This section explains why DragonFly's page management differs from traditional BSD.</p>"},{"location":"sys/vm/vm_page/#1024-color-page-queues","title":"1024-Color Page Queues","text":"<p>Traditional BSD systems use a single lock per queue type. On a 64-core system, this becomes a severe bottleneck\u2014every page allocation/free contends on the same lock.</p> <p>DragonFly divides each queue type into 1024 sub-queues (\"colors\"). The page color is derived from its physical address and CPU topology, so:</p> <ul> <li>CPUs on different sockets naturally hit different queue subsets</li> <li>Even CPUs on the same socket distribute across colors</li> <li>Lock contention drops by ~1000x compared to single-queue designs</li> </ul> <p>The 1024 value balances granularity (more = less contention) against memory overhead (each queue has its own spinlock and list head).</p>"},{"location":"sys/vm/vm_page/#atomic-busy-counts-vs-locks","title":"Atomic Busy Counts vs. Locks","text":"<p>Traditional BSD uses <code>lockmgr</code> locks on pages. DragonFly replaces this with atomic busy counts:</p> <pre><code>busy_count field:\n  [31]    PBUSY_LOCKED   - Hard busy (exclusive)\n  [30]    PBUSY_WANTED   - Someone waiting\n  [29]    PBUSY_SWAPINPROG - Swap I/O active\n  [28:0]  Soft-busy count (shared)\n</code></pre> <p>Benefits: - No lock structure overhead per page - Soft-busy allows concurrency: Multiple readers can soft-busy a page simultaneously - Atomic operations are faster than lock acquire/release cycles - Integrates with LWKT: Waiters use <code>tsleep()</code>/<code>wakeup()</code> patterns</p>"},{"location":"sys/vm/vm_page/#per-cpu-statistics","title":"Per-CPU Statistics","text":"<p>Global <code>vmstats</code> would cause cache-line bouncing on every alloc/free. DragonFly caches adjustments per-CPU:</p> <pre><code>mycpu-&gt;gd_vmstats_adj.v_free_count += delta;  // Fast, local\n// Periodically or when threshold exceeded:\natomic_add(&amp;vmstats.v_free_count, accumulated_delta);  // Slow, global\n</code></pre> <p>This reduces cross-CPU cache invalidations from O(allocations) to O(sync_intervals).</p>"},{"location":"sys/vm/vm_page/#numa-aware-coloring","title":"NUMA-Aware Coloring","text":"<p>On NUMA systems, accessing memory on a remote socket incurs 2-3x latency. DragonFly's <code>vm_get_pg_color()</code> encodes CPU topology into the color:</p> <pre><code>Color bits: [socket_id][core_id][ht_id][set_associativity]\n</code></pre> <p>Result: A CPU naturally allocates pages from queues that contain pages physically close to it.</p>"},{"location":"sys/vm/vm_page/#nice-aware-paging-thresholds","title":"Nice-Aware Paging Thresholds","text":"<p>When memory is low, all processes compete to allocate. Traditional systems give equal priority to all. DragonFly adjusts paging thresholds based on process nice value:</p> <ul> <li>Nice 0 process: blocks at <code>v_free_min</code></li> <li>Nice +10 process: blocks earlier (at higher free count)</li> <li>Nice -10 process: blocks later (can dig deeper into reserves)</li> </ul> <p>This prevents a <code>nice +19</code> background job from consuming memory needed by interactive processes.</p>"},{"location":"sys/vm/vm_page/#data-structures","title":"Data Structures","text":""},{"location":"sys/vm/vm_page/#page-array","title":"Page Array","text":"<pre><code>vm_page_t vm_page_array;           // Global array of all vm_page structures\nvm_pindex_t vm_page_array_size;    // Number of entries\nvm_pindex_t first_page;            // First physical page index\n</code></pre> <p>The macro <code>PHYS_TO_VM_PAGE(pa)</code> converts a physical address to its corresponding <code>vm_page</code> pointer.</p>"},{"location":"sys/vm/vm_page/#page-queues","title":"Page Queues","text":"<pre><code>struct vpgqueues vm_page_queues[PQ_COUNT];\n</code></pre> <p>Five queue types, each with 1024 color variants (<code>PQ_L2_SIZE</code>):</p> Queue Type Purpose <code>PQ_FREE</code> Available for allocation <code>PQ_CACHE</code> Clean pages, immediately reusable <code>PQ_INACTIVE</code> Low activity, candidates for reclamation <code>PQ_ACTIVE</code> Recently referenced pages <code>PQ_HOLD</code> Temporarily held (prevents freeing) <p>Each <code>struct vpgqueues</code> contains: - <code>spin</code> - Per-queue spinlock - <code>pl</code> - Page list (TAILQ) - <code>lcnt</code> - Local count - <code>lastq</code> - Heuristic for skipping empty queues</p>"},{"location":"sys/vm/vm_page/#page-hash-table","title":"Page Hash Table","text":"<p>A lockless heuristic cache for fast page lookups:</p> <pre><code>struct vm_page_hash_elm {\n    vm_page_t   m;\n    vm_object_t object;   // Cached for fast comparison\n    vm_pindex_t pindex;   // Cached for fast comparison\n    int         ticks;    // LRU timestamp\n};\n</code></pre> <ul> <li>4-way set associative (<code>VM_PAGE_HASH_SET</code>)</li> <li>Maximum 8 million entries</li> <li>Only caches pages with <code>PG_MAPPEDMULTI</code> flag</li> </ul>"},{"location":"sys/vm/vm_page/#boot-time-initialization","title":"Boot-Time Initialization","text":""},{"location":"sys/vm/vm_page/#vm_page_startup","title":"<code>vm_page_startup()</code>","text":"<p>Called early in boot to initialize the page management subsystem:</p> <ol> <li>Aligns physical memory ranges - Rounds <code>phys_avail[]</code> to page boundaries</li> <li>Initializes page queues - Creates 5120 queue structures (5 types \u00d7 1024 colors)</li> <li>Allocates minidump bitmap - For crash dump support</li> <li>Allocates vm_page_array - One <code>struct vm_page</code> per physical page</li> <li>Initializes page structures - Sets up spinlocks and physical addresses</li> <li>Populates free queues - Adds pages in ascending physical order</li> </ol>"},{"location":"sys/vm/vm_page/#page-color-calculation","title":"Page Color Calculation","text":"<p>During boot, page colors are calculated with CPU locality twisting:</p> <pre><code>m-&gt;pc = (pa &gt;&gt; PAGE_SHIFT);\nm-&gt;pc ^= ((pa &gt;&gt; PAGE_SHIFT) / PQ_L2_SIZE);\nm-&gt;pc ^= ((pa &gt;&gt; PAGE_SHIFT) / (PQ_L2_SIZE * PQ_L2_SIZE));\nm-&gt;pc &amp;= PQ_L2_MASK;\n</code></pre> <p>This distributes pages across queues while maintaining locality.</p>"},{"location":"sys/vm/vm_page/#numa-organization","title":"NUMA Organization","text":"<p><code>vm_numa_organize()</code> reorganizes page colors based on physical socket ID:</p> <pre><code>socket_mod = PQ_L2_SIZE / cpu_topology_phys_ids;\nsocket_value = (physid % cpu_topology_phys_ids) * socket_mod;\n</code></pre> <p><code>vm_numa_organize_finalize()</code> then balances queues to prevent empty queues that would force cross-socket borrowing.</p>"},{"location":"sys/vm/vm_page/#dma-reserve","title":"DMA Reserve","text":"<p>Low physical memory is reserved for DMA operations:</p> <ul> <li><code>vm_low_phys_reserved</code>: Threshold for DMA reserve (default 65536 pages)</li> <li><code>vm_dma_reserved</code>: Tunable amount to keep reserved (default 128MB on 2G+ systems)</li> <li>Pages in this range are marked <code>PG_FICTITIOUS | PG_UNQUEUED</code> and managed by <code>vm_contig_alist</code></li> </ul>"},{"location":"sys/vm/vm_page/#page-allocation","title":"Page Allocation","text":""},{"location":"sys/vm/vm_page/#vm_page_alloc","title":"<code>vm_page_alloc()</code>","text":"<p>The primary page allocation function.</p> <pre><code>vm_page_t vm_page_alloc(vm_object_t object, vm_pindex_t pindex, int page_req);\n</code></pre> <p>Allocation flags:</p> Flag Description <code>VM_ALLOC_NORMAL</code> Can use cache pages <code>VM_ALLOC_QUICK</code> Free queue only, skip cache <code>VM_ALLOC_SYSTEM</code> Can exhaust most of free list <code>VM_ALLOC_INTERRUPT</code> Can exhaust entire free list <code>VM_ALLOC_CPU(n)</code> CPU localization hint <code>VM_ALLOC_ZERO</code> Zero page if allocated <code>VM_ALLOC_NULL_OK</code> Return NULL on collision <p>Algorithm:</p> <ol> <li>Calculate page color via <code>vm_get_pg_color(cpuid, object, pindex)</code></li> <li>Check free count against thresholds</li> <li>Search free queue (and optionally cache queue)</li> <li>If using cache page, free it first then retry</li> <li>Insert into object if provided</li> <li>Return BUSY page</li> </ol>"},{"location":"sys/vm/vm_page/#cpu-localized-color-selection","title":"CPU-Localized Color Selection","text":"<p><code>vm_get_pg_color()</code> calculates colors considering CPU topology:</p> <pre><code>// General format: [phys_id][core_id][cpuid][set-associativity]\nphyscale = PQ_L2_SIZE / cpu_topology_phys_ids;\ngrpscale = physcale / cpu_topology_core_ids;\ncpuscale = grpscale / cpu_topology_ht_ids;\n\npg_color = phys_id * physcale;\npg_color += core_id * grpscale;\npg_color += ht_id * cpuscale;\npg_color += (pindex + object_pg_color) % cpuscale;\n</code></pre>"},{"location":"sys/vm/vm_page/#queue-search-algorithm","title":"Queue Search Algorithm","text":"<p><code>_vm_page_list_find()</code> searches for pages with widening locality:</p> <ol> <li>Try exact color queue first</li> <li>Widen search: 16 \u2192 32 \u2192 64 \u2192 128 \u2192 ... \u2192 1024 queues</li> <li>Track <code>lastq</code> to skip known-empty queues</li> <li>Return spinlocked page removed from queue</li> </ol>"},{"location":"sys/vm/vm_page/#contiguous-allocation","title":"Contiguous Allocation","text":"<p>For DMA and device drivers requiring physically contiguous memory:</p> <pre><code>vm_page_t vm_page_alloc_contig(vm_paddr_t low, vm_paddr_t high,\n                               unsigned long alignment,\n                               unsigned long boundary,\n                               unsigned long size,\n                               vm_memattr_t memattr);\n</code></pre> <p>Uses the <code>vm_contig_alist</code> allocator for low-memory DMA pages.</p>"},{"location":"sys/vm/vm_page/#other-allocation-functions","title":"Other Allocation Functions","text":"Function Description <code>vm_page_alloczwq()</code> Allocate without object, returns wired page <code>vm_page_grab()</code> Lookup-or-allocate with object"},{"location":"sys/vm/vm_page/#page-freeing","title":"Page Freeing","text":""},{"location":"sys/vm/vm_page/#vm_page_free_toq","title":"<code>vm_page_free_toq()</code>","text":"<p>The main page freeing function:</p> <ol> <li>Assert page not mapped (calls <code>pmap_mapped_sync()</code> if needed)</li> <li>Remove from object via <code>vm_page_remove()</code></li> <li>For fictitious pages: just wakeup and return</li> <li>Remove from current queue</li> <li>Clear valid/dirty bits</li> <li>Place on appropriate queue:</li> <li><code>PQ_HOLD</code> if <code>hold_count != 0</code></li> <li><code>PQ_FREE</code> otherwise (at head for cache-hot)</li> <li>Wake up page waiters</li> <li>Wake memory-waiting threads via <code>vm_page_free_wakeup()</code></li> </ol>"},{"location":"sys/vm/vm_page/#free-wakeup-logic","title":"Free Wakeup Logic","text":"<p><code>vm_page_free_wakeup()</code> signals: - Pageout daemon: If it needs pages and threshold met - Memory-waiting processes: If above hysteresis threshold</p>"},{"location":"sys/vm/vm_page/#page-state-transitions","title":"Page State Transitions","text":"<pre><code>flowchart TB\n    PQ_FREE[\"PQ_FREE\"]\n    PQ_ACTIVE[\"PQ_ACTIVE\"]\n    PQ_INACTIVE[\"PQ_INACTIVE\"]\n    PQ_CACHE[\"PQ_CACHEclean, not mapped\"]\n    PQ_FREE_OR_HOLD[\"PQ_FREE or PQ_HOLD\"]\n\n    PQ_FREE --&gt;|alloc| PQ_ACTIVE\n    PQ_ACTIVE --&gt;|deactivate| PQ_INACTIVE\n    PQ_INACTIVE --&gt;|clean| PQ_CACHE\n    PQ_CACHE --&gt;|free| PQ_FREE_OR_HOLD\n</code></pre>"},{"location":"sys/vm/vm_page/#activation","title":"Activation","text":"<p><code>vm_page_activate()</code> moves a page to <code>PQ_ACTIVE</code>: - Sets <code>act_count</code> to at least <code>ACT_INIT</code> - Wakes pagedaemon if page was on cache/free queue</p>"},{"location":"sys/vm/vm_page/#deactivation","title":"Deactivation","text":"<p><code>vm_page_deactivate()</code> moves a page to <code>PQ_INACTIVE</code>: - Clears <code>PG_WINATCFLS</code> flag - Optional <code>athead</code> for pseudo-cache behavior (MADV_DONTNEED)</p>"},{"location":"sys/vm/vm_page/#caching","title":"Caching","text":"<p><code>vm_page_cache()</code> moves a clean page to <code>PQ_CACHE</code>: - Removes all pmap mappings first - Dirty pages are deactivated instead</p>"},{"location":"sys/vm/vm_page/#busy-state-management","title":"Busy State Management","text":"<p>DragonFly uses atomic busy counts instead of traditional locks.</p>"},{"location":"sys/vm/vm_page/#hard-busy-pbusy_locked","title":"Hard Busy (<code>PBUSY_LOCKED</code>)","text":"<p>Exclusive access to the page.</p> <pre><code>void vm_page_busy_wait(vm_page_t m, int also_m_busy, const char *msg);\nint  vm_page_busy_try(vm_page_t m, int also_m_busy);\nvoid vm_page_wakeup(vm_page_t m);\n</code></pre> <ul> <li><code>vm_page_busy_wait()</code>: Blocks until page not busy</li> <li><code>vm_page_busy_try()</code>: Non-blocking attempt, returns TRUE on failure</li> <li><code>vm_page_wakeup()</code>: Clears busy and wakes waiters</li> </ul>"},{"location":"sys/vm/vm_page/#soft-busy-pbusy_mask","title":"Soft Busy (<code>PBUSY_MASK</code>)","text":"<p>Shared access for compatible operations (e.g., read-only mapping during write).</p> <pre><code>void vm_page_io_start(vm_page_t m);   // Increment soft-busy (requires hard-busy)\nvoid vm_page_io_finish(vm_page_t m);  // Decrement soft-busy\nint  vm_page_sbusy_try(vm_page_t m);  // Non-blocking soft-busy acquire\n</code></pre>"},{"location":"sys/vm/vm_page/#waiting","title":"Waiting","text":"<p><code>vm_page_sleep_busy()</code> sleeps until page not busy without acquiring it.</p>"},{"location":"sys/vm/vm_page/#wireunwire","title":"Wire/Unwire","text":"<p>Wiring prevents a page from being paged out.</p> <pre><code>void vm_page_wire(vm_page_t m);\nvoid vm_page_unwire(vm_page_t m, int activate);\n</code></pre> <ul> <li><code>vm_page_wire()</code>: Increments <code>wire_count</code>, adjusts vmstats on 0\u21921</li> <li><code>vm_page_unwire()</code>: Decrements <code>wire_count</code>, activates or deactivates on 1\u21920</li> <li>Fictitious pages ignore wire operations</li> </ul>"},{"location":"sys/vm/vm_page/#holdunhold","title":"Hold/Unhold","text":"<p>Holding prevents page reuse but not disassociation from object.</p> <pre><code>void vm_page_hold(vm_page_t m);\nvoid vm_page_unhold(vm_page_t m);\n</code></pre> <p>On last unhold, if page is on <code>PQ_HOLD</code>, it moves to <code>PQ_FREE</code>.</p>"},{"location":"sys/vm/vm_page/#page-lookup","title":"Page Lookup","text":""},{"location":"sys/vm/vm_page/#standard-lookup","title":"Standard Lookup","text":"<pre><code>vm_page_t vm_page_lookup(vm_object_t object, vm_pindex_t pindex);\n</code></pre> <p>Requires object token held. Does RB-tree lookup and populates hash cache.</p>"},{"location":"sys/vm/vm_page/#lookup-busy","title":"Lookup + Busy","text":"<pre><code>// Blocking\nvm_page_t vm_page_lookup_busy_wait(vm_object_t object, vm_pindex_t pindex,\n                                   int also_m_busy, const char *msg);\n\n// Non-blocking\nvm_page_t vm_page_lookup_busy_try(vm_object_t object, vm_pindex_t pindex,\n                                  int also_m_busy, int *errorp);\n</code></pre>"},{"location":"sys/vm/vm_page/#fast-heuristic-lookup","title":"Fast Heuristic Lookup","text":"<pre><code>vm_page_t vm_page_hash_get(vm_object_t object, vm_pindex_t pindex);\n</code></pre> <p>Lockless lookup returning soft-busied page on hit.</p>"},{"location":"sys/vm/vm_page/#validdirty-bit-management","title":"Valid/Dirty Bit Management","text":"<p>Each page has 8 valid and 8 dirty bits (one per DEV_BSIZE chunk, typically 512 bytes).</p>"},{"location":"sys/vm/vm_page/#functions","title":"Functions","text":"Function Description <code>vm_page_bits(base, size)</code> Convert range to bit mask <code>vm_page_set_valid()</code> Set valid bits, zero invalid portions <code>vm_page_set_validclean()</code> Set valid, clear dirty <code>vm_page_set_validdirty()</code> Set both valid and dirty <code>vm_page_clear_dirty()</code> Clear dirty bits <code>vm_page_dirty()</code> Set all dirty bits <code>vm_page_test_dirty()</code> Sync dirty from pmap <code>vm_page_zero_invalid()</code> Zero invalid portions before mapping <code>vm_page_is_valid()</code> Check if range is valid"},{"location":"sys/vm/vm_page/#memory-pressure-handling","title":"Memory Pressure Handling","text":""},{"location":"sys/vm/vm_page/#waiting-functions","title":"Waiting Functions","text":"Function Description <code>vm_wait()</code> Block until memory available (I/O path) <code>vm_wait_pfault()</code> Block in page fault path (nice-aware) <code>vm_wait_nominal()</code> Block for kernel heavy operations <code>vm_test_nominal()</code> Test if vm_wait_nominal would block"},{"location":"sys/vm/vm_page/#nice-aware-paging","title":"Nice-Aware Paging","text":"<p>Process nice value affects paging thresholds: - Higher nice = earlier blocking - Prevents nice'd memory hogs from impacting normal processes</p>"},{"location":"sys/vm/vm_page/#low-memory-kill","title":"Low Memory Kill","text":"<p>Processes with <code>P_LOWMEMKILL</code> flag can break out of wait loops.</p>"},{"location":"sys/vm/vm_page/#madvise-support","title":"madvise Support","text":""},{"location":"sys/vm/vm_page/#vm_page_dontneed","title":"<code>vm_page_dontneed()</code>","text":"<p>Implements <code>MADV_DONTNEED</code>: - 3/32 chance: deactivate page - 28/32 chance: deactivate at head (pseudo-cache) - Clears <code>PG_REFERENCED</code></p>"},{"location":"sys/vm/vm_page/#special-page-types","title":"Special Page Types","text":""},{"location":"sys/vm/vm_page/#fictitious-pages","title":"Fictitious Pages","text":"<p>Pages with <code>PG_FICTITIOUS</code> flag: - Not in normal page array - Created via <code>vm_page_initfake()</code> - Wire/unwire operations ignored - Used for device mappings (GPU, etc.)</p>"},{"location":"sys/vm/vm_page/#pages-requiring-commit","title":"Pages Requiring Commit","text":"<p>Pages with <code>PG_NEED_COMMIT</code> flag: - Cannot be reclaimed even if clean - Used by tmpfs, NFS - Set via <code>vm_page_need_commit()</code></p>"},{"location":"sys/vm/vm_page/#locking-rules","title":"Locking Rules","text":""},{"location":"sys/vm/vm_page/#queue-operations","title":"Queue Operations","text":"<p>Locking order: Page spinlock first, then queue spinlock</p> <pre><code>vm_page_spin_lock(m);           // Lock page\n_vm_page_queue_spin_lock(m);    // Then lock its queue\n// ... manipulate queue ...\n_vm_page_queue_spin_unlock(m);  // Unlock queue first\nvm_page_spin_unlock(m);         // Then unlock page\n</code></pre>"},{"location":"sys/vm/vm_page/#per-cpu-statistics_1","title":"Per-CPU Statistics","text":"<p>Queue adjustments update per-CPU vmstats: - <code>mycpu-&gt;gd_vmstats_adj</code> - Accumulated adjustments - <code>mycpu-&gt;gd_vmstats</code> - Current view - Synchronized to global <code>vmstats</code> periodically or when threshold exceeded</p>"},{"location":"sys/vm/vm_page/#common-usage-patterns","title":"Common Usage Patterns","text":"<p>This section shows how page management functions work together in typical scenarios.</p>"},{"location":"sys/vm/vm_page/#pattern-1-page-fault-allocation","title":"Pattern 1: Page Fault Allocation","text":"<p>From <code>sys/vm/vm_fault.c</code> - when a process accesses unmapped memory:</p> <pre><code>/* Try to find existing page (non-blocking busy try) */\nint error;\nfs-&gt;mary[0] = vm_page_lookup_busy_try(fs-&gt;ba-&gt;object, pindex,\n                                      TRUE, &amp;error);\nif (error) {\n    /* Page exists but is busy - sleep and retry */\n    vm_page_sleep_busy(fs-&gt;mary[0], TRUE, \"vmpfw\");\n    return (KERN_TRY_AGAIN);\n}\n\nif (fs-&gt;mary[0] == NULL) {\n    /* Page doesn't exist - allocate new one (returns BUSY) */\n    fs-&gt;mary[0] = vm_page_alloc(fs-&gt;ba-&gt;object, pindex,\n                                VM_ALLOC_NULL_OK | VM_ALLOC_NORMAL);\n    if (fs-&gt;mary[0] == NULL) {\n        vm_wait_pfault();  /* Block if low memory */\n        return (KERN_TRY_AGAIN);\n    }\n    /* Page is busy, safe to do I/O to populate it */\n}\n\n/* ... fault handling continues ... */\n\n/* When done, release busy state */\nvm_page_wakeup(fs-&gt;mary[0]);\n</code></pre>"},{"location":"sys/vm/vm_page/#pattern-2-pageout-daemon-scanning","title":"Pattern 2: Pageout Daemon Scanning","text":"<p>From <code>sys/vm/vm_pageout.c</code> - reclaiming inactive pages:</p> <pre><code>/* Simplified from vm_pageout_scan_inactive() */\nTAILQ_FOREACH(m, &amp;vm_page_queues[PQ_INACTIVE + q].pl, pageq) {\n    /* Non-blocking busy attempt - skip if can't acquire */\n    if (vm_page_busy_try(m, TRUE))\n        continue;\n\n    /*\n     * Page is now busy - we own it.\n     * Remaining operations run with page busy.\n     */\n    if (m-&gt;dirty) {\n        /* Must clean before freeing */\n        count = vm_pageout_clean_helper(m, vmflush_flags);\n    } else {\n        /* Clean page - move to cache for instant reuse */\n        vm_page_cache(m);\n    }\n\n    vm_page_wakeup(m);\n}\n</code></pre>"},{"location":"sys/vm/vm_page/#pattern-3-driver-dma-buffer","title":"Pattern 3: Driver DMA Buffer","text":"<p>Device drivers needing physically contiguous memory:</p> <pre><code>/* Allocate 64KB contiguous, 16-byte aligned, below 4GB */\nm = vm_page_alloc_contig(\n    0,                      /* low address */\n    0xFFFFFFFFULL,          /* high address (4GB) */\n    16,                     /* alignment */\n    0,                      /* boundary (0 = none) */\n    64 * 1024,              /* size */\n    VM_MEMATTR_UNCACHEABLE  /* memory attribute */\n);\n\nif (m) {\n    phys_addr = VM_PAGE_TO_PHYS(m);\n    /* Program DMA controller with phys_addr */\n\n    /* Later, when done: */\n    vm_page_free_contig(m, 64 * 1024);\n}\n</code></pre>"},{"location":"sys/vm/vm_page/#pattern-4-wiring-pages-for-io","title":"Pattern 4: Wiring Pages for I/O","text":"<p>Ensuring pages stay resident during I/O operations:</p> <pre><code>/* Wire range to prevent pageout during I/O */\nfor (each page m in range) {\n    vm_page_wire(m);\n}\n\n/* Pages now guaranteed resident */\nstart_io(buffer, length);\nwait_for_io_completion();\n\n/* Unwire when done */\nfor (each page m in range) {\n    vm_page_unwire(m, 0);  /* 0 = deactivate, 1 = keep active */\n}\n</code></pre>"},{"location":"sys/vm/vm_page/#debugging","title":"Debugging","text":""},{"location":"sys/vm/vm_page/#ddb-commands","title":"DDB Commands","text":"Command Description <code>show page</code> Display vmstats counters <code>show pageq</code> Display queue lengths per color"},{"location":"sys/vm/vm_page/#sysctls","title":"Sysctls","text":"Sysctl Description <code>vm.dma_reserved</code> Memory reserved for DMA <code>vm.dma_free_pages</code> Available DMA pages <code>vm.page_hash_vnode_only</code> Only hash vnode pages"},{"location":"sys/vm/vm_page/#further-reading","title":"Further Reading","text":""},{"location":"sys/vm/vm_page/#dragonfly-resources","title":"DragonFly Resources","text":"<ul> <li>VM Concepts \u2014 Foundational VM theory and Mach heritage</li> <li>VM Objects \u2014 How pages are grouped into objects</li> <li>Page Faults \u2014 How faults trigger page allocation</li> <li>Pageout Daemon \u2014 Memory reclamation in detail</li> </ul>"},{"location":"sys/vm/vm_page/#external-resources","title":"External Resources","text":"<ul> <li>The Design and Implementation of the FreeBSD Operating System (McKusick et al.) \u2014 Chapter 5 covers BSD VM; DragonFly inherits this design</li> <li>Operating Systems: Three Easy Pieces \u2014 Free online textbook; Chapters 18-22 cover paging and replacement</li> <li>Wikipedia: Page replacement algorithm \u2014 Overview of LRU, Clock, and related algorithms</li> <li>Wikipedia: NUMA \u2014 Background on NUMA architecture and why locality matters</li> <li>LWN: Memory management \u2014 Linux perspective, but concepts transfer</li> </ul>"},{"location":"sys/vm/vm_page/#source-files","title":"Source Files","text":"File Description <code>sys/vm/vm_page.c</code> Implementation (~4,200 lines) <code>sys/vm/vm_page.h</code> <code>struct vm_page</code> definition, flags <code>sys/vm/vm_page2.h</code> Inline functions, paging thresholds <code>sys/vm/vm_pageout.c</code> Pageout daemon (consumer of this API)"},{"location":"sys/vm/vm_page/#see-also","title":"See Also","text":"<ul> <li>VM Architecture Overview</li> <li>Memory Allocation \u2014 Kernel memory (kmalloc, objcache)</li> <li>Buffer Cache \u2014 Filesystem buffer management</li> </ul>"},{"location":"sys/vm/vm_pageout/","title":"Pageout and Swap","text":"<p>The pageout daemon manages memory reclamation when physical memory becomes scarce, while the swap pager provides backing store for anonymous memory. DragonFly BSD features dual pageout threads for deadlock recovery, a radix bitmap swap allocator, and aggressive RSS enforcement.</p> <p>Source files: <code>sys/vm/vm_pageout.c</code> (~2,895 lines), <code>sys/vm/swap_pager.c</code> (~2,600 lines)</p>"},{"location":"sys/vm/vm_pageout/#two-subsystems-one-goal","title":"Two Subsystems, One Goal","text":"<p>This document covers two related but distinct subsystems:</p> <pre><code>flowchart TB\n    subgraph PAGEOUT[\"PAGEOUT DAEMON (vm_pageout.c)When do we reclaim memory?\"]\n        P1[\"Monitors free memory against thresholds\"]\n        P2[\"Scans page queues to find victims\"]\n        P3[\"Decides: activate, deactivate, cache, or launder\"]\n        P4[\"Calls pagers to write dirty pages\"]\n        P5[\"Triggers OOM killer when all else fails\"]\n    end\n\n    subgraph SWAP[\"SWAP PAGER (swap_pager.c)Where do we store anonymous pages?\"]\n        S1[\"Allocates blocks on swap device\"]\n        S2[\"Tracks which page \u2192 which block (RB-tree metadata)\"]\n        S3[\"Handles I/O for pageout (write) and page fault (read)\"]\n        S4[\"Also used by: swapcache (file page caching on swap)\"]\n    end\n\n    PAGEOUT --&gt;|\"Write this dirty page\"| SWAP\n</code></pre> <p>When you need to understand: - Why the system is low on memory \u2192 Pageout daemon (thresholds, scanning) - Why a process was OOM killed \u2192 Pageout daemon (OOM killer) - Why swap I/O is slow \u2192 Swap pager (async limits, clustering) - Why swap space is exhausted \u2192 Swap pager (allocation, hysteresis)</p>"},{"location":"sys/vm/vm_pageout/#overview","title":"Overview","text":"<p>When free memory falls below critical thresholds, the pageout daemon scans page queues to:</p> <ol> <li>Deactivate referenced pages from active to inactive queue</li> <li>Launder dirty pages by writing to backing store</li> <li>Free clean pages to replenish the free pool</li> <li>Kill processes when swap is exhausted (OOM)</li> </ol> <p>The swap pager provides block storage for anonymous pages:</p> <ol> <li>Allocate swap space using a radix bitmap allocator</li> <li>Track swap assignments per-object in RB-tree metadata</li> <li>Page out dirty pages with clustering and async I/O</li> <li>Page in with burst reading for sequential access</li> </ol>"},{"location":"sys/vm/vm_pageout/#pageout-daemon-vm_pageoutc","title":"Pageout Daemon (vm_pageout.c)","text":""},{"location":"sys/vm/vm_pageout/#dual-pageout-threads","title":"Dual Pageout Threads","text":"<p>DragonFly runs two pageout threads for deadlock recovery:</p> Thread Purpose <code>pagedaemon</code> Primary pageout daemon for all page types <code>emergpager</code> Emergency pager for swap-only pages when primary deadlocks <p>The emergency pager activates when the primary daemon blocks on a vnode lock for more than 2 seconds. It only processes anonymous pages (<code>OBJT_DEFAULT</code>, <code>OBJT_SWAP</code>) to avoid the vnode deadlock.</p>"},{"location":"sys/vm/vm_pageout/#memory-thresholds","title":"Memory Thresholds","text":"<p>The pageout daemon maintains several free memory thresholds, calculated from <code>v_free_min</code> in <code>vm_pageout_free_page_calc()</code>:</p> <pre><code>v_interrupt_free_min   Low-level allocation reserve (swap structures)\n         \u2193\nv_pageout_free_min     Pageout daemon allocation reserve\n         \u2193\nv_free_reserved        System allocation reserve\n         \u2193\nv_free_min             Normal allocation minimum\n         \u2193\nv_free_target          Target free pages (2x v_free_min)\n         \u2193\nv_paging_wait          Start blocking allocators (3x v_free_min)\n         \u2193\nv_paging_start         Begin paging to target1 (3.5x v_free_min)\n         \u2193\nv_paging_target1       Aggressive paging target (4x v_free_min)\n         \u2193\nv_paging_target2       Lazy paging target (5x v_free_min)\n</code></pre>"},{"location":"sys/vm/vm_pageout/#paging-states","title":"Paging States","text":"<p>The daemon operates in three states:</p> State Behavior <code>PAGING_IDLE</code> No paging needed, daemon sleeps <code>PAGING_TARGET1</code> Aggressive paging toward <code>v_paging_target1</code> <code>PAGING_TARGET2</code> Lazy paging toward <code>v_paging_target2</code> <p>State transitions prevent thrashing while ensuring responsiveness:</p> <pre><code>flowchart LR\n    IDLE --&gt;|\"shortage detected\"| TARGET1\n    TARGET1 --&gt;|\"target1 reached\"| TARGET2\n    TARGET2 --&gt;|\"target2 reached\"| IDLE\n</code></pre>"},{"location":"sys/vm/vm_pageout/#key-sysctls","title":"Key Sysctls","text":"Sysctl Default Description <code>vm.anonmem_decline</code> ACT_DECLINE Active\u2192inactive rate for anon pages <code>vm.filemem_decline</code> ACT_DECLINE*2 Active\u2192inactive rate for file pages <code>vm.max_launder</code> physmem/256+16 Max dirty pages to flush per pass <code>vm.emerg_launder</code> 100 Emergency pager minimum launder <code>vm.pageout_memuse_mode</code> 2 RSS enforcement (0=off, 1=passive, 2=active) <code>vm.pageout_allow_active</code> 1 Allow scanning active queue <code>vm.swap_enabled</code> 1 Enable swap pageouts <code>vm.defer_swapspace_pageouts</code> 0 Prefer keeping dirty pages in memory <code>vm.disable_swapspace_pageouts</code> 0 Completely disable swap writes"},{"location":"sys/vm/vm_pageout/#queue-scanning","title":"Queue Scanning","text":"<p>The daemon scans page queues in order: inactive \u2192 active \u2192 cache.</p>"},{"location":"sys/vm/vm_pageout/#inactive-queue-scan","title":"Inactive Queue Scan","text":"<p><code>vm_pageout_scan_inactive()</code> processes candidates for reclamation:</p> <ol> <li>Scan ~1/10 of queue per pass using per-queue markers</li> <li>Calculate <code>max_launder</code> budget from <code>vm_max_launder</code></li> <li>For each page, call <code>vm_pageout_page()</code> for decision</li> </ol> <p>Per-page decision in <code>vm_pageout_page()</code>:</p> <pre><code>Page state              Action\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nWired                   Remove from queue\nHeld                    Requeue at tail\nReferenced (pmap/flag)  Activate with boosted act_count\nInvalid (valid==0)      Free directly\nClean (dirty==0)        Cache via vm_page_cache()\nDirty, first pass       Set PG_WINATCFLS, requeue (double-LRU)\nDirty, second pass      Attempt pageout via vm_pageout_clean_helper()\n</code></pre> <p>Double-LRU for Dirty Pages:</p> <p>Dirty pages cycle through the inactive queue twice before laundering. The <code>PG_WINATCFLS</code> (\"win at cache flush\") flag tracks the first pass. This reduces unnecessary I/O for pages that may be freed or become clean naturally.</p> <p>When <code>vm_pageout_memuse_mode &gt;= 3</code>, single-LRU mode is used instead.</p>"},{"location":"sys/vm/vm_pageout/#active-queue-scan","title":"Active Queue Scan","text":"<p><code>vm_pageout_scan_active()</code> moves pages to the inactive queue:</p> <ol> <li>Scan ~1/10 of queue per iteration</li> <li>Check page activity via <code>pmap_ts_referenced()</code> and <code>PG_REFERENCED</code></li> <li>Active pages: bump <code>act_count</code>, leave in active queue</li> <li>Inactive pages: decrement <code>act_count</code>, deactivate when threshold reached</li> </ol> <p>Activity Decline Rates:</p> <ul> <li>Anonymous pages decline at <code>vm_anonmem_decline</code> per pass</li> <li>File-backed pages decline at <code>vm_filemem_decline</code> (2x anon by default)</li> </ul> <p>Faster file page decay implements the \"scan resistance\" principle\u2014file scans shouldn't evict working set.</p>"},{"location":"sys/vm/vm_pageout/#cache-queue-scan","title":"Cache Queue Scan","text":"<p><code>vm_pageout_scan_cache()</code> frees clean cached pages:</p> <ul> <li>Uses two rovers (primary and emergency) to avoid contention</li> <li>Frees pages until <code>v_free_target</code> reached</li> <li>Stops early if target satisfied</li> </ul>"},{"location":"sys/vm/vm_pageout/#page-clustering","title":"Page Clustering","text":"<p><code>vm_pageout_clean_helper()</code> clusters dirty pages for efficient I/O:</p> <ol> <li>Find clusterable neighbors (dirty, not wired/held, inactive or allowed-active)</li> <li>Align cluster to <code>BLIST_MAX_ALLOC</code> for swap optimization</li> <li>Set <code>PG_WINATCFLS</code> on cluster pages to match primary</li> <li>Call <code>vm_pageout_flush()</code> for actual I/O</li> </ol> <p><code>vm_pageout_flush()</code> handles the write:</p> <ol> <li>Mark pages read-only via <code>vm_page_protect()</code></li> <li>Clear pmap modified bits</li> <li>Call <code>vm_pager_put_pages()</code> for I/O</li> <li>Handle results and update page states</li> </ol>"},{"location":"sys/vm/vm_pageout/#rss-enforcement","title":"RSS Enforcement","text":"<p>When <code>vm_pageout_memuse_mode &gt;= 1</code>, the daemon enforces process RSS limits:</p> <p><code>vm_pageout_map_deactivate_pages()</code>:</p> <ol> <li>Called for processes exceeding <code>RLIMIT_RSS</code></li> <li>Walks address space from <code>map-&gt;pgout_offset</code></li> <li>Scans each mapped page via <code>pmap_pgscan()</code></li> <li>Removes unreferenced pages from pmap</li> <li>Deactivates unmapped pages, optionally launders dirty ones</li> <li>Continues until RSS below limit</li> </ol> <p>RSS enforcement modes:</p> Mode Behavior 0 Disabled 1 Passive\u2014enforce during memory pressure 2 Active\u2014enforce proactively (default) 3 Active + single-LRU for dirty pages"},{"location":"sys/vm/vm_pageout/#oom-killer","title":"OOM Killer","text":"<p>When swap exhausts and pages cannot be reclaimed:</p> <ol> <li>Rate-limited to once per second</li> <li>Scan all processes via <code>allproc_scan()</code></li> <li>Select victim with largest memory footprint</li> <li>Skip: system processes, init (pid 1), low-pid processes with swap</li> <li>Set <code>P_LOWMEMKILL</code> flag and call <code>killproc()</code></li> </ol> <p>Victim size calculation: <pre><code>size = vmspace_anonymous_count(vm) + vmspace_swap_count(vm)\n</code></pre></p>"},{"location":"sys/vm/vm_pageout/#main-loop","title":"Main Loop","text":"<p><code>vm_pageout_thread()</code> main loop:</p> <pre><code>Initialize markers, thresholds, swap pager\n\nwhile (TRUE) {\n    1. Sleep until vm_pages_needed or timeout\n    2. Calculate avail_shortage from targets\n    3. vm_pageout_scan_inactive() - launder/free pages\n    4. Calculate inactive_shortage\n    5. vm_pageout_scan_active() - feed inactive queue\n    6. vm_pageout_scan_cache() - free cached pages\n    7. Determine next state (IDLE/TARGET1/TARGET2)\n    8. Wakeup memory waiters if appropriate\n}\n</code></pre> <p>Emergency pager differences:</p> <ul> <li>Sleeps on <code>&amp;vm_pagedaemon_uptime</code> instead of <code>&amp;vm_pages_needed</code></li> <li>Activates if primary hasn't updated uptime for 2+ seconds</li> <li>Only processes anonymous/swap pages</li> <li>Iterates queues in reverse direction</li> </ul>"},{"location":"sys/vm/vm_pageout/#swap-pager-swap_pagerc","title":"Swap Pager (swap_pager.c)","text":""},{"location":"sys/vm/vm_pageout/#architecture","title":"Architecture","text":"<p>The swap pager provides block storage for anonymous memory:</p> <pre><code>flowchart TB\n    OBJ[\"vm_object\"]\n\n    subgraph RBTREE[\"swblock_root (RB-tree)RB-tree of swap metadata\"]\n        SWB1[\"swblock(16 pgs)\"]\n        SWB2[\"swblock(16 pgs)\"]\n        SWB3[\"swblock(16 pgs)\"]\n    end\n\n    BLIST[\"swapblist (blist)Radix bitmap allocatorSwap block allocation\"]\n\n    DISK[\"Swap devices (disks)\"]\n\n    OBJ --&gt; RBTREE\n    SWB1 --&gt; BLIST\n    SWB2 --&gt; BLIST\n    SWB3 --&gt; BLIST\n    BLIST --&gt; DISK\n</code></pre>"},{"location":"sys/vm/vm_pageout/#key-data-structures","title":"Key Data Structures","text":"<p>struct swblock - Swap metadata per 16-page range:</p> <pre><code>struct swblock {\n    RB_ENTRY(swblock) swb_entry;         /* RB-tree linkage */\n    vm_pindex_t       swb_index;         /* Base page index (aligned) */\n    int               swb_count;         /* Valid entries */\n    swblk_t           swb_pages[SWAP_META_PAGES];  /* Block numbers (16) */\n};\n</code></pre> <p>Global state:</p> <pre><code>struct blist *swapblist;      /* Radix bitmap allocator */\nint swap_pager_full;          /* Swap exhausted flag (triggers OOM) */\nint swap_pager_almost_full;   /* Near exhaustion (with hysteresis) */\nswblk_t vm_swap_anon_use;     /* Swap used for anonymous pages */\nswblk_t vm_swap_cache_use;    /* Swap used for swapcache */\n</code></pre>"},{"location":"sys/vm/vm_pageout/#swap-space-allocation","title":"Swap Space Allocation","text":"<p>Radix bitmap (blist) allocator:</p> <p>The blist provides O(log n) allocation and deallocation with efficient fragmentation handling. Swap is allocated in contiguous runs when possible.</p> <p><code>swp_pager_getswapspace(npages, object_type)</code>:</p> <ol> <li>Try <code>blist_allocat()</code> with iterator hint</li> <li>Fall back to start of swap if hint fails</li> <li>Update <code>vm_swap_anon_use</code> or <code>vm_swap_cache_use</code></li> <li>Set <code>swap_pager_full=2</code> on allocation failure</li> </ol> <p>Hysteresis thresholds:</p> <ul> <li><code>nswap_lowat</code> = 4% of total swap (minimum 128 pages)</li> <li><code>nswap_hiwat</code> = 6% of total swap (minimum 512 pages)</li> <li><code>swap_pager_almost_full</code> set when below lowat</li> <li>Cleared when above hiwat</li> </ul>"},{"location":"sys/vm/vm_pageout/#swap-io-limits","title":"Swap I/O Limits","text":"<pre><code>nsw_rcount         = nswbuf_kva / 2   /* Read buffer limit */\nnsw_wcount_sync    = nswbuf_kva / 4   /* Sync write limit */\nnsw_wcount_async   = swap_async_max   /* Async write limit (default 4) */\nnsw_cluster_max    = MAXPHYS / PAGE_SIZE  /* Max cluster size */\n</code></pre>"},{"location":"sys/vm/vm_pageout/#page-in-swap_pager_getpage","title":"Page-In (swap_pager_getpage)","text":"<p>Burst reading for sequential access:</p> <ol> <li>If page already valid, switch to read-ahead only mode</li> <li>Scan for contiguous swap blocks on same stripe</li> <li>Allocate pages for burst (up to <code>swap_burst_read</code>)</li> <li>Set <code>PG_RAM</code> on last page for pipeline continuation</li> <li>Issue I/O and wait for <code>PBUSY_SWAPINPROG</code> to clear</li> </ol> <p>Read process:</p> <pre><code>1. Verify object match\n2. Look up swap block for requested page\n3. Scan for contiguous swap blocks (same stripe)\n4. Allocate pages for burst read\n5. Map pages to KVA, issue I/O\n6. Wait for completion\n7. Return VM_PAGER_OK or VM_PAGER_ERROR\n</code></pre>"},{"location":"sys/vm/vm_pageout/#page-out-swap_pager_putpages","title":"Page-Out (swap_pager_putpages)","text":"<p>Object conversion:</p> <p>First pageout converts <code>OBJT_DEFAULT</code> to <code>OBJT_SWAP</code>.</p> <p>Write process:</p> <ol> <li>Allocate swap blocks (fall back to smaller chunks on failure)</li> <li>Validate stripe boundary, adjust if crossing</li> <li>Build swap metadata entries</li> <li>Issue I/O (async or sync based on flags)</li> <li>For sync: wait and call completion handler directly</li> </ol> <p>Async throttling:</p> <ul> <li><code>nsw_wcount_async_max</code> controlled by <code>vm.swap_async_max</code></li> <li>Prevents swap I/O from starving other disk I/O</li> <li>Non-pageout threads forced sync unless <code>swap_user_async=1</code></li> </ul>"},{"location":"sys/vm/vm_pageout/#io-completion-swp_pager_async_iodone","title":"I/O Completion (swp_pager_async_iodone)","text":"<p>Read success:</p> <ol> <li>Set <code>m-&gt;valid = VM_PAGE_BITS_ALL</code></li> <li>Clear dirty bits</li> <li>Set <code>PG_SWAPPED</code> flag</li> <li>Deactivate non-requested pages</li> </ol> <p>Read error:</p> <ol> <li>Set <code>m-&gt;valid = 0</code></li> <li>Deactivate non-requested pages</li> <li>Leave requested page busy for caller</li> </ol> <p>Write success:</p> <ol> <li>Clear dirty bits (OBJT_SWAP only)</li> <li>Set <code>PG_SWAPPED</code> flag</li> <li>Deactivate if <code>vm_paging_severe()</code></li> <li>Try to cache if <code>SWBIO_TTC</code> flag</li> </ol> <p>Write error:</p> <ol> <li>Remove swap assignment</li> <li>Re-dirty OBJT_SWAP pages (no other backing)</li> <li>Activate page to prevent data loss</li> <li>Don't dirty non-OBJT_SWAP (has vnode backing)</li> </ol>"},{"location":"sys/vm/vm_pageout/#swap-metadata-management","title":"Swap Metadata Management","text":"<p>RB-tree operations:</p> Function Purpose <code>swp_pager_lookup()</code> Find swblock by page index <code>swp_pager_meta_build()</code> Add/update swap entry <code>swp_pager_meta_free()</code> Free range of entries <code>swp_pager_meta_free_all()</code> Destroy all metadata <code>swp_pager_meta_ctl()</code> Control (free/pop) single entry <p>Metadata control flags:</p> Flag Action <code>SWM_FREE</code> Remove and free swap block <code>SWM_POP</code> Remove but keep block (for transfer)"},{"location":"sys/vm/vm_pageout/#swapoff-support","title":"Swapoff Support","text":"<p><code>swap_pager_swapoff()</code> removes a device from use:</p> <ol> <li>Scan all <code>OBJT_SWAP</code> and <code>OBJT_VNODE</code> objects</li> <li>Skip <code>OBJ_NOPAGEIN</code> objects</li> <li>Page in all blocks on target device</li> <li>Return 1 if blocks remain (partial success)</li> </ol>"},{"location":"sys/vm/vm_pageout/#key-sysctls_1","title":"Key Sysctls","text":"Sysctl Default Description <code>vm.swap_async_max</code> 4 Max concurrent async writes <code>vm.swap_burst_read</code> 16 Pages to read-ahead on swap-in <code>vm.swap_user_async</code> 0 Allow user async swap writes"},{"location":"sys/vm/vm_pageout/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_pageout/#dual-pageout-threads_1","title":"Dual Pageout Threads","text":"<p>The emergency pager (<code>emergpager</code>) provides deadlock recovery:</p> <ul> <li>Activates when primary daemon blocks on vnode for 2+ seconds</li> <li>Only processes anonymous pages to avoid the deadlock source</li> <li>Enables memory reclamation even during filesystem stalls</li> </ul>"},{"location":"sys/vm/vm_pageout/#queue-distribution","title":"Queue Distribution","text":"<p>Pageout work is distributed across 1024 sub-queues per major queue type:</p> <ul> <li>Each sub-queue has its own marker for incremental scanning</li> <li><code>PQAVERAGE()</code> distributes work across queues</li> <li>Reduces lock contention on SMP systems</li> </ul>"},{"location":"sys/vm/vm_pageout/#three-state-paging","title":"Three-State Paging","text":"<p>The IDLE \u2192 TARGET1 \u2192 TARGET2 state machine:</p> <ul> <li>Prevents thrashing between paging and normal operation</li> <li>TARGET1 aggressive paging ensures quick recovery</li> <li>TARGET2 lazy paging maintains headroom without wasting I/O</li> </ul>"},{"location":"sys/vm/vm_pageout/#radix-bitmap-allocator","title":"Radix Bitmap Allocator","text":"<p>The blist provides:</p> <ul> <li>O(log n) allocation and deallocation</li> <li>Efficient handling of fragmentation</li> <li>Scalability to arbitrary swap sizes</li> <li>Per-device stripe awareness</li> </ul>"},{"location":"sys/vm/vm_pageout/#rb-tree-swap-metadata","title":"RB-Tree Swap Metadata","text":"<p>Per-object swap tracking via RB-tree:</p> <ul> <li>16 pages per swblock entry reduces overhead</li> <li>Efficient range operations for large frees</li> <li>Separate anon vs cache accounting</li> </ul>"},{"location":"sys/vm/vm_pageout/#kvabio-support","title":"KVABIO Support","text":"<p>Swap I/O uses KVABIO to avoid pmap synchronization:</p> <ul> <li><code>pmap_qenter_noinval()</code> for mapping</li> <li>Reduces IPI overhead on SMP</li> </ul>"},{"location":"sys/vm/vm_pageout/#stripe-aware-clustering","title":"Stripe-Aware Clustering","text":"<p>I/O operations respect device stripe boundaries:</p> <ul> <li><code>SWB_DMMASK</code> for boundary detection</li> <li>Prevents I/O from crossing stripes</li> <li>Optimizes disk access patterns</li> </ul>"},{"location":"sys/vm/vm_pageout/#function-reference","title":"Function Reference","text":""},{"location":"sys/vm/vm_pageout/#pageout-daemon","title":"Pageout Daemon","text":"Function Description <code>vm_pageout_thread()</code> Main daemon loop <code>vm_pageout_scan_inactive()</code> Scan inactive queue <code>vm_pageout_scan_active()</code> Scan active queue <code>vm_pageout_scan_cache()</code> Scan cache queue <code>vm_pageout_page()</code> Per-page decision <code>vm_pageout_clean_helper()</code> Cluster and flush dirty pages <code>vm_pageout_flush()</code> Write pages to backing store <code>vm_pageout_map_deactivate_pages()</code> RSS enforcement <code>pagedaemon_wakeup()</code> Wake pageout daemon"},{"location":"sys/vm/vm_pageout/#swap-pager","title":"Swap Pager","text":"Function Description <code>swap_pager_getpage()</code> Page in from swap <code>swap_pager_putpages()</code> Page out to swap <code>swap_pager_haspage()</code> Check for swap backing <code>swap_pager_unswapped()</code> Remove swap when dirtied <code>swap_pager_freespace()</code> Free swap range <code>swap_pager_reserve()</code> Pre-allocate swap <code>swap_pager_copy()</code> Transfer swap metadata <code>swap_pager_swapoff()</code> Remove device from use <code>swp_pager_async_iodone()</code> I/O completion handler"},{"location":"sys/vm/vm_pageout/#see-also","title":"See Also","text":"<ul> <li>Physical Pages - Page allocation and queue management</li> <li>VM Objects - Object lifecycle and page containers</li> <li>Page Faults - Fault handling and COW</li> <li>Memory Management - kmalloc/objcache</li> </ul>"}]}