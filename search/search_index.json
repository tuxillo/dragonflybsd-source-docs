{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DragonFly BSD Kernel Documentation","text":"<p>Welcome to the comprehensive documentation for the DragonFly BSD kernel source code.</p>"},{"location":"#about-this-documentation","title":"About This Documentation","text":"<p>This documentation project aims to provide clear, accessible explanations of the DragonFly BSD kernel's internals, making it easier for developers, researchers, and enthusiasts to understand how this sophisticated operating system works.</p>"},{"location":"#what-is-dragonfly-bsd","title":"What is DragonFly BSD?","text":"<p>DragonFly BSD is a fork of FreeBSD designed with a focus on:</p> <ul> <li>Multiprocessor scalability \u2014 Efficient performance on multi-core systems</li> <li>Message-passing architecture \u2014 LWKT (Lightweight Kernel Threading) for lock-free concurrency</li> <li>Advanced filesystems \u2014 HAMMER and HAMMER2 with clustering capabilities</li> <li>Innovation \u2014 Modern kernel design patterns while maintaining UNIX heritage</li> </ul>"},{"location":"#documentation-organization","title":"Documentation Organization","text":"<p>The documentation is organized to mirror the kernel source tree structure, making it easy to find information about specific subsystems:</p>"},{"location":"#kernel-core-syskern","title":"Kernel Core (<code>sys/kern/</code>)","text":"<p>The heart of the kernel, containing:</p> <ul> <li>LWKT Threading \u2014 DragonFly's unique message-passing concurrency model</li> <li>Process &amp; Thread Management \u2014 How processes and threads are created, scheduled, and managed</li> <li>Virtual Filesystem (VFS) \u2014 The abstraction layer for filesystems</li> <li>IPC &amp; Sockets \u2014 Inter-process communication and networking foundations</li> <li>Memory Management \u2014 Kernel memory allocation and management</li> <li>Device Framework \u2014 How devices and drivers integrate with the kernel</li> </ul>"},{"location":"#virtual-memory-sysvm","title":"Virtual Memory (<code>sys/vm/</code>)","text":"<p>The virtual memory subsystem managing:</p> <ul> <li>VM objects and pages</li> <li>Paging and swap</li> <li>Memory mapping</li> <li>Page cache</li> </ul>"},{"location":"#cpu-architecture-syscpux86_64","title":"CPU Architecture (<code>sys/cpu/x86_64/</code>)","text":"<p>Machine-dependent code for x86-64:</p> <ul> <li>Low-level CPU interfaces</li> <li>MMU management</li> <li>Trap handling</li> <li>Assembly routines</li> </ul>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<ol> <li>Start with the basics \u2014 If you're new to DragonFly, begin with Getting Started</li> <li>Explore by subsystem \u2014 Navigate through the Kernel Subsystems section</li> <li>Follow the architecture \u2014 Documentation mirrors the source tree for easy cross-referencing</li> <li>Deep dive \u2014 Each subsystem page provides overviews, key concepts, and code flows</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>This documentation is a living project. If you find areas that need clarification or expansion, contributions are welcome.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Check out the Getting Started guide to learn how to navigate this documentation and understand DragonFly's unique architecture.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you navigate the DragonFly BSD kernel documentation and understand the unique aspects of DragonFly's architecture.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>To get the most out of this documentation, you should have:</p> <ul> <li>C programming experience \u2014 The kernel is written in C</li> <li>Basic OS concepts \u2014 Understanding of processes, memory management, and I/O</li> <li>UNIX/BSD familiarity \u2014 Knowledge of UNIX system architecture is helpful</li> </ul>"},{"location":"getting-started/#understanding-dragonflys-unique-architecture","title":"Understanding DragonFly's Unique Architecture","text":"<p>DragonFly BSD differs from traditional BSD systems in several key ways:</p>"},{"location":"getting-started/#lwkt-lightweight-kernel-threading","title":"LWKT: Lightweight Kernel Threading","text":"<p>The most distinctive feature of DragonFly is its message-passing concurrency model:</p> <ul> <li>Traditional kernels use locks to protect shared data</li> <li>DragonFly uses message passing and tokens for most synchronization</li> <li>Reduces lock contention and improves multiprocessor scalability</li> <li>Start with the LWKT Threading documentation to understand this foundational concept</li> </ul>"},{"location":"getting-started/#token-based-synchronization","title":"Token-Based Synchronization","text":"<p>Instead of traditional mutexes and read-write locks for most operations, DragonFly uses:</p> <ul> <li>Tokens \u2014 Serializing tokens that can be held across blocking operations</li> <li>Message ports \u2014 Each thread has message ports for asynchronous communication</li> <li>IPIQs \u2014 Inter-Processor Interrupt Queues for cross-CPU messaging</li> </ul> <p>See Synchronization for details.</p>"},{"location":"getting-started/#documentation-structure","title":"Documentation Structure","text":""},{"location":"getting-started/#mirror-of-source-tree","title":"Mirror of Source Tree","text":"<p>The documentation mirrors the kernel source tree at <code>~/s/dragonfly/sys/</code>:</p> <pre><code>Source: ~/s/dragonfly/sys/kern/kern_proc.c\n  Docs: docs/sys/kern/processes.md\n\nSource: ~/s/dragonfly/sys/vm/vm_page.c\n  Docs: docs/sys/vm/index.md\n</code></pre> <p>This makes it easy to:</p> <ul> <li>Find documentation for specific source directories</li> <li>Cross-reference between code and docs</li> <li>Navigate familiar territory if you know the source layout</li> </ul>"},{"location":"getting-started/#documentation-pages","title":"Documentation Pages","text":"<p>Each subsystem documentation page follows a consistent structure:</p> <ol> <li>Overview \u2014 What the subsystem does and why</li> <li>Key Concepts \u2014 Important ideas and terminology</li> <li>Data Structures \u2014 Core structures and their roles</li> <li>Key Functions \u2014 Important entry points and operations</li> <li>Subsystem Interactions \u2014 How it connects to other parts</li> <li>Code Flow Examples \u2014 Walkthrough of typical operations</li> <li>Files \u2014 Relevant source files</li> <li>References \u2014 Links to related topics</li> </ol>"},{"location":"getting-started/#recommended-reading-order","title":"Recommended Reading Order","text":""},{"location":"getting-started/#for-first-time-readers","title":"For First-Time Readers","text":"<ol> <li>LWKT Threading \u2014 Understand DragonFly's concurrency model first</li> <li>Synchronization \u2014 Learn about tokens, locks, and message passing</li> <li>Processes &amp; Threads \u2014 How processes and threads work</li> <li>Virtual Filesystem \u2014 VFS layer and file operations</li> <li>Memory Management \u2014 Kernel memory allocation</li> </ol>"},{"location":"getting-started/#for-specific-interests","title":"For Specific Interests","text":"<ul> <li>Filesystem developers \u2192 Start with VFS</li> <li>Network programmers \u2192 Begin with IPC &amp; Sockets</li> <li>Driver developers \u2192 Check out Devices &amp; Drivers</li> <li>Scheduler hackers \u2192 Head to Scheduling</li> <li>Architecture enthusiasts \u2192 Explore CPU/x86_64</li> </ul>"},{"location":"getting-started/#code-references","title":"Code References","text":"<p>Throughout the documentation, you'll see references like:</p> <ul> <li><code>kern_proc.c:142</code> \u2014 File and line number</li> <li><code>fork1()</code> \u2014 Function name</li> <li><code>struct proc</code> \u2014 Data structure</li> </ul> <p>These help you locate the relevant source code in <code>~/s/dragonfly/sys/</code>.</p>"},{"location":"getting-started/#understanding-the-planning-documents","title":"Understanding the Planning Documents","text":"<p>The repository also contains planning documents (in <code>planning/</code> directory) that outline:</p> <ul> <li>Reading order for source code</li> <li>Phases for documentation development</li> <li>Subsystem categorization</li> </ul> <p>These are primarily for documentation maintainers but can be useful if you want to understand how the source tree is organized.</p>"},{"location":"getting-started/#viewing-the-documentation","title":"Viewing the Documentation","text":"<p>This documentation is built with MkDocs and can be:</p> <ul> <li>Viewed locally: Run <code>make serve</code> in the repository root</li> <li>Built as static HTML: Run <code>make build</code> to generate <code>site/</code> directory</li> <li>Read as Markdown: All <code>.md</code> files in <code>docs/</code> are readable as plain text</li> </ul>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<p>Now that you understand how the documentation is organized, you're ready to explore:</p> <ul> <li>Kernel Subsystems Overview \u2014 High-level view of all subsystems</li> <li>kern/ Overview \u2014 Start with the kernel core</li> <li>LWKT Threading \u2014 Dive into DragonFly's unique concurrency model</li> </ul> <p>Happy exploring!</p>"},{"location":"sys/","title":"Kernel Subsystems Overview","text":"<p>The DragonFly BSD kernel (<code>sys/</code>) is organized into subsystems that handle different aspects of the operating system. This page provides a high-level overview of the major components.</p>"},{"location":"sys/#core-kernel-kern","title":"Core Kernel (<code>kern/</code>)","text":"<p>The kernel core is the heart of DragonFly, containing fundamental subsystems:</p> <ul> <li>LWKT Threading \u2014 Message-passing based threading model unique to DragonFly</li> <li>Synchronization \u2014 Tokens, locks, and synchronization primitives</li> <li>Memory Management \u2014 Kernel memory allocation (malloc, slab allocator, object caches)</li> <li>Processes &amp; Threads \u2014 Process lifecycle (fork, exec, exit) and thread management</li> <li>Scheduling \u2014 CPU scheduling framework and policies</li> <li>Virtual Filesystem (VFS) \u2014 Filesystem abstraction layer</li> <li>IPC &amp; Sockets \u2014 Inter-process communication and socket layer</li> <li>Devices &amp; Drivers \u2014 Device framework and driver infrastructure</li> <li>System Calls \u2014 System call infrastructure and implementation</li> </ul> <p>Explore kern/ in detail \u2192</p>"},{"location":"sys/#virtual-memory-vm","title":"Virtual Memory (<code>vm/</code>)","text":"<p>The virtual memory subsystem manages memory at the page level:</p> <ul> <li>VM objects and shadow objects</li> <li>Physical page management</li> <li>Paging and page replacement</li> <li>Swap management</li> <li>Memory mapping (<code>mmap</code>)</li> <li>Buffer cache integration</li> </ul> <p>Explore vm/ in detail \u2192</p>"},{"location":"sys/#cpu-architecture-cpux86_64","title":"CPU Architecture (<code>cpu/x86_64/</code>)","text":"<p>Machine-dependent code for the x86-64 architecture:</p> <ul> <li>CPU initialization and control</li> <li>MMU and page table management</li> <li>Trap and interrupt handling</li> <li>Context switching</li> <li>Assembly routines</li> <li>Low-level primitives</li> </ul> <p>Explore cpu/x86_64/ in detail \u2192</p>"},{"location":"sys/#networking","title":"Networking","text":""},{"location":"sys/#core-networking-net","title":"Core Networking (<code>net/</code>)","text":"<p>Generic networking infrastructure:</p> <ul> <li>Network interfaces (<code>struct ifnet</code>)</li> <li>Routing tables</li> <li>Network message passing (<code>netisr</code>)</li> <li>BPF (Berkeley Packet Filter)</li> </ul>"},{"location":"sys/#ipv6-netinet6","title":"IPv6 (<code>netinet6/</code>)","text":"<p>IPv6 protocol stack:</p> <ul> <li>IPv6 packet processing</li> <li>Neighbor discovery</li> <li>Routing and forwarding</li> <li>ICMPv6</li> <li>Multicast support</li> </ul>"},{"location":"sys/#bluetooth-netbt","title":"Bluetooth (<code>netbt/</code>)","text":"<p>Bluetooth protocol stack:</p> <ul> <li>HCI (Host Controller Interface)</li> <li>L2CAP (Logical Link Control and Adaptation Protocol)</li> <li>RFCOMM</li> <li>SCO (Synchronous Connection-Oriented)</li> </ul>"},{"location":"sys/#storage-and-filesystems","title":"Storage and Filesystems","text":""},{"location":"sys/#device-io","title":"Device I/O","text":"<ul> <li>Disk layer and partitioning</li> <li>Device statistics</li> <li>I/O scheduling</li> <li>DMA support</li> </ul>"},{"location":"sys/#vfs-layer","title":"VFS Layer","text":"<p>See kern/vfs/ for the filesystem abstraction layer.</p>"},{"location":"sys/#security","title":"Security","text":"<ul> <li>Capabilities \u2014 Capability-based security model</li> <li>ACLs \u2014 Access control lists</li> <li>Jails \u2014 Container-like isolation</li> </ul>"},{"location":"sys/#debugging-and-monitoring","title":"Debugging and Monitoring","text":"<ul> <li>DDB \u2014 In-kernel debugger</li> <li>KTR \u2014 Kernel trace buffer</li> <li>ktrace \u2014 System call tracing</li> <li>sysctl \u2014 Runtime configuration and monitoring</li> </ul>"},{"location":"sys/#libraries","title":"Libraries","text":""},{"location":"sys/#kernel-libraries","title":"Kernel Libraries","text":"<ul> <li>libkern \u2014 C library functions for kernel use</li> <li>libiconv \u2014 Character set conversion</li> <li>libprop \u2014 Property lists (kernel/userland shared)</li> </ul>"},{"location":"sys/#cryptography","title":"Cryptography","text":"<ul> <li>opencrypto \u2014 Cryptographic framework</li> <li>Software crypto implementations</li> <li>Hardware crypto driver support</li> </ul>"},{"location":"sys/#kernel-configuration","title":"Kernel Configuration","text":"<ul> <li>config/ \u2014 Kernel configuration files</li> <li>compile/ \u2014 Build output directories</li> </ul>"},{"location":"sys/#key-architectural-principles","title":"Key Architectural Principles","text":""},{"location":"sys/#message-passing-over-locking","title":"Message Passing Over Locking","text":"<p>DragonFly minimizes traditional locking by using:</p> <ul> <li>Message-based IPC between threads</li> <li>Tokens for serialization when needed</li> <li>Per-CPU data structures to avoid contention</li> </ul>"},{"location":"sys/#scalability-focus","title":"Scalability Focus","text":"<p>Design choices emphasize multiprocessor performance:</p> <ul> <li>Lock-free algorithms where possible</li> <li>Cache-friendly data structures</li> <li>Minimal serialization points</li> </ul>"},{"location":"sys/#cache-coherency","title":"Cache Coherency","text":"<p>DragonFly's HAMMER filesystem and buffer cache leverage:</p> <ul> <li>Distributed caching</li> <li>Cluster-aware design</li> <li>Cache coherency protocols</li> </ul>"},{"location":"sys/#next-steps","title":"Next Steps","text":"<ul> <li>Start with kern/ to explore the kernel core</li> <li>Learn about LWKT Threading to understand DragonFly's unique approach</li> <li>Dive into specific subsystems based on your interests</li> </ul>"},{"location":"sys/#subsystem-dependencies","title":"Subsystem Dependencies","text":"<p>Understanding dependencies helps navigate the kernel:</p> <pre><code>LWKT Threading (foundational)\n    \u2193\nSynchronization Primitives\n    \u2193\nMemory Management\n    \u2193\nProcess Management \u2192 VFS \u2192 Filesystems\n    \u2193              \u2193\nScheduling    Device I/O\n</code></pre> <p>Start with LWKT to understand the foundation, then explore other subsystems.</p>"},{"location":"sys/cpu/x86_64/","title":"CPU Architecture: x86_64","text":"<p>The <code>sys/cpu/x86_64/</code> directory contains machine-dependent headers and utility code for the x86-64 (AMD64/Intel 64) architecture.  Platform-specific implementations (PC hardware) live in <code>sys/platform/pc64/</code>.</p>"},{"location":"sys/cpu/x86_64/#directory-structure","title":"Directory Structure","text":"<pre><code>sys/cpu/x86_64/\n    include/            Machine-dependent headers\n        cpu.h           CPU definitions and macros\n        cpufunc.h       Inline assembly for CPU instructions\n        pmap.h          Page table entry definitions\n        frame.h         Trap/interrupt frame layout\n        segments.h      GDT/IDT segment descriptors\n        specialreg.h    Control registers and CPUID features\n        atomic.h        Atomic operations\n        ...\n    misc/               Assembly and utility implementations\n        bzeront.s       Non-temporal zero fill\n        cputimer_tsc.c  TSC-based timers\n        db_disasm.c     DDB disassembler\n        elf_machdep.c   ELF relocation handling\n        in_cksum2.s     Optimized IP checksum\n        lwbuf.c         Lightweight buffer mapping\n        ...\n\nsys/platform/pc64/x86_64/\n    machdep.c           Platform initialization\n    pmap.c              Page table management\n    trap.c              Trap/exception handling\n    mp_machdep.c        SMP support\n    exception.S         Low-level exception entry\n    ...\n</code></pre>"},{"location":"sys/cpu/x86_64/#control-registers","title":"Control Registers","text":"<p>x86-64 provides several control registers that govern CPU behavior:</p>"},{"location":"sys/cpu/x86_64/#cr0-specialregh","title":"CR0 (specialreg.h)","text":"Bit Name Description 0 <code>CR0_PE</code> Protected Mode Enable 1 <code>CR0_MP</code> Math Present (FPU) 2 <code>CR0_EM</code> FPU Emulation 3 <code>CR0_TS</code> Task Switched 16 <code>CR0_WP</code> Write Protect (honor page protection in supervisor mode) 29 <code>CR0_NW</code> Not Write-through 30 <code>CR0_CD</code> Cache Disable 31 <code>CR0_PG</code> Paging Enable"},{"location":"sys/cpu/x86_64/#cr4-specialregh","title":"CR4 (specialreg.h)","text":"Bit Name Description 4 <code>CR4_PSE</code> Page Size Extensions (2MB pages) 5 <code>CR4_PAE</code> Physical Address Extension 7 <code>CR4_PGE</code> Page Global Enable 9 <code>CR4_OSFXSR</code> OS supports FXSAVE/FXRSTOR 10 <code>CR4_OSXMMEXCPT</code> OS handles SIMD exceptions 16 <code>CR4_FSGSBASE</code> Enable RDFSBASE/WRFSBASE 17 <code>CR4_PCIDE</code> Process Context Identifiers 18 <code>CR4_OSXSAVE</code> OS supports XSAVE 20 <code>CR4_SMEP</code> Supervisor-Mode Execution Prevention 21 <code>CR4_SMAP</code> Supervisor-Mode Access Prevention"},{"location":"sys/cpu/x86_64/#cr2-and-cr3","title":"CR2 and CR3","text":"<ul> <li>CR2 - Holds the faulting linear address on a page fault</li> <li>CR3 - Holds the physical address of the PML4 page table root</li> </ul>"},{"location":"sys/cpu/x86_64/#cpu-feature-detection-cpufunch-specialregh","title":"CPU Feature Detection (cpufunc.h, specialreg.h)","text":"<p>CPUID instruction returns feature flags in %edx and %ecx:</p> <p>CPUID Fn0000_0001 %edx: - <code>CPUID_FPU</code> - x87 FPU present - <code>CPUID_VME</code> - Virtual 8086 extensions - <code>CPUID_TSC</code> - Time Stamp Counter - <code>CPUID_MSR</code> - Model Specific Registers - <code>CPUID_PAE</code> - Physical Address Extension - <code>CPUID_APIC</code> - On-chip APIC - <code>CPUID_MTRR</code> - Memory Type Range Registers - <code>CPUID_PGE</code> - Page Global Enable - <code>CPUID_SSE</code>, <code>CPUID_SSE2</code> - SIMD extensions</p> <p>CPUID Fn0000_0001 %ecx: - <code>CPUID2_SSE3</code>, <code>CPUID2_SSSE3</code>, <code>CPUID2_SSE41</code>, <code>CPUID2_SSE42</code> - <code>CPUID2_VMX</code> - Intel VMX (virtualization) - <code>CPUID2_AESNI</code> - AES instruction set - <code>CPUID2_AVX</code> - Advanced Vector Extensions - <code>CPUID2_XSAVE</code> - XSAVE/XRSTOR support</p> <pre><code>/* cpufunc.h:134 - CPUID wrapper */\nstatic __inline void\ndo_cpuid(u_int ax, u_int *p)\n{\n    __asm __volatile(\"cpuid\"\n        : \"=a\" (p[0]), \"=b\" (p[1]), \"=c\" (p[2]), \"=d\" (p[3])\n        :  \"0\" (ax));\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#cpu-inline-functions-cpufunch","title":"CPU Inline Functions (cpufunc.h)","text":"<p>The <code>cpufunc.h</code> header provides inline assembly wrappers for privileged instructions:</p>"},{"location":"sys/cpu/x86_64/#interrupt-control","title":"Interrupt Control","text":"<pre><code>static __inline void cpu_disable_intr(void)\n{\n    __asm __volatile(\"cli\" : : : \"memory\");\n}\n\nstatic __inline void cpu_enable_intr(void)\n{\n    __asm __volatile(\"sti\");\n}\n\nstatic __inline register_t intr_disable(void)\n{\n    register_t rflags = read_rflags();\n    cpu_disable_intr();\n    return rflags;\n}\n\nstatic __inline void intr_restore(register_t rflags)\n{\n    write_rflags(rflags);\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#memory-barriers-cpufunch177","title":"Memory Barriers (cpufunc.h:177)","text":"<pre><code>/* Full memory fence */\nstatic __inline void cpu_mfence(void)\n{\n    __asm __volatile(\"mfence\" : : : \"memory\");\n}\n\n/* Load fence - orders reads */\nstatic __inline void cpu_lfence(void)\n{\n    __asm __volatile(\"lfence\" : : : \"memory\");\n}\n\n/* Store fence - orders writes (mostly compiler barrier on Intel) */\nstatic __inline void cpu_sfence(void)\n{\n    __asm __volatile(\"\" : : : \"memory\");\n}\n\n/* Compiler-only fence */\nstatic __inline void cpu_ccfence(void)\n{\n    __asm __volatile(\"\" : : : \"memory\");\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#tlb-management","title":"TLB Management","text":"<pre><code>/* Invalidate single TLB entry */\nstatic __inline void cpu_invlpg(void *addr)\n{\n    __asm __volatile(\"invlpg %0\" : : \"m\" (*(char *)addr) : \"memory\");\n}\n\n/* Flush entire TLB (reload CR3) */\nstatic __inline void cpu_invltlb(void)\n{\n    load_cr3(rcr3());\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#msr-access-cpufunch524","title":"MSR Access (cpufunc.h:524)","text":"<pre><code>static __inline u_int64_t rdmsr(u_int msr)\n{\n    u_int32_t low, high;\n    __asm __volatile(\"rdmsr\" : \"=a\" (low), \"=d\" (high) : \"c\" (msr));\n    return (low | ((u_int64_t)high &lt;&lt; 32));\n}\n\nstatic __inline void wrmsr(u_int msr, u_int64_t newval)\n{\n    u_int32_t low = newval, high = newval &gt;&gt; 32;\n    __asm __volatile(\"wrmsr\" : : \"a\" (low), \"d\" (high), \"c\" (msr) : \"memory\");\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#tsc-time-stamp-counter","title":"TSC (Time Stamp Counter)","text":"<pre><code>static __inline tsc_uclock_t rdtsc(void)\n{\n    u_int32_t low, high;\n    __asm __volatile(\"rdtsc\" : \"=a\" (low), \"=d\" (high));\n    return (low | ((tsc_uclock_t)high &lt;&lt; 32));\n}\n\n/* Ordered TSC read with appropriate fence */\nstatic __inline tsc_uclock_t rdtsc_ordered(void)\n{\n    if (cpu_vendor_id == CPU_VENDOR_INTEL)\n        cpu_lfence();\n    else\n        cpu_mfence();\n    return rdtsc();\n}\n</code></pre>"},{"location":"sys/cpu/x86_64/#page-table-entries-pmaph","title":"Page Table Entries (pmap.h)","text":"<p>x86-64 uses 4-level paging with 48-bit virtual addresses:</p> <pre><code>PML4 (level 4) -&gt; PDP (level 3) -&gt; PD (level 2) -&gt; PT (level 1) -&gt; Page\n   9 bits           9 bits          9 bits          9 bits        12 bits\n</code></pre>"},{"location":"sys/cpu/x86_64/#page-table-entry-bits-pmaph67","title":"Page Table Entry Bits (pmap.h:67)","text":"Bit Name Description 0 <code>X86_PG_V</code> Valid/Present 1 <code>X86_PG_RW</code> Read/Write 2 <code>X86_PG_U</code> User/Supervisor 3 <code>X86_PG_NC_PWT</code> Write-Through 4 <code>X86_PG_NC_PCD</code> Cache Disable 5 <code>X86_PG_A</code> Accessed 6 <code>X86_PG_M</code> Dirty (Modified) 7 <code>X86_PG_PS</code> Page Size (2MB if set at PD level) 8 <code>X86_PG_G</code> Global (not flushed on CR3 reload) 63 <code>X86_PG_NX</code> No Execute"},{"location":"sys/cpu/x86_64/#page-protection-exceptions-pmaph126","title":"Page Protection Exceptions (pmap.h:126)","text":"<pre><code>#define PGEX_P      0x01    /* Protection violation (vs not present) */\n#define PGEX_W      0x02    /* Write access */\n#define PGEX_U      0x04    /* User mode access */\n#define PGEX_RSV    0x08    /* Reserved bit set in PTE */\n#define PGEX_I      0x10    /* Instruction fetch */\n</code></pre>"},{"location":"sys/cpu/x86_64/#trap-frame-frameh55","title":"Trap Frame (frame.h:55)","text":"<p>The <code>trapframe</code> structure captures CPU state on exception/interrupt entry:</p> <pre><code>struct trapframe {\n    /* Syscall arguments (rdi, rsi, rdx, rcx, r8, r9) come first */\n    register_t  tf_rdi;\n    register_t  tf_rsi;\n    register_t  tf_rdx;\n    register_t  tf_rcx;\n    register_t  tf_r8;\n    register_t  tf_r9;\n    register_t  tf_rax;\n    register_t  tf_rbx;\n    register_t  tf_rbp;\n    register_t  tf_r10;\n    register_t  tf_r11;\n    register_t  tf_r12;\n    register_t  tf_r13;\n    register_t  tf_r14;\n    register_t  tf_r15;\n    register_t  tf_xflags;      /* Software flags */\n    register_t  tf_trapno;\n    register_t  tf_addr;\n    register_t  tf_flags;\n    /* Hardware-pushed portion */\n    register_t  tf_err;         /* Error code */\n    register_t  tf_rip;\n    register_t  tf_cs;\n    register_t  tf_rflags;\n    register_t  tf_rsp;\n    register_t  tf_ss;\n} __packed;\n</code></pre> <p>The first six registers match the System V AMD64 ABI calling convention, allowing direct syscall argument extraction.</p>"},{"location":"sys/cpu/x86_64/#segment-descriptors-segmentsh","title":"Segment Descriptors (segments.h)","text":""},{"location":"sys/cpu/x86_64/#global-descriptor-table-layout-segmentsh236","title":"Global Descriptor Table Layout (segments.h:236)","text":"Selector Name Description 0 <code>GNULL_SEL</code> Null descriptor 1 <code>GCODE_SEL</code> Kernel 64-bit code 2 <code>GDATA_SEL</code> Kernel data 3 <code>GUCODE32_SEL</code> User 32-bit code (compat) 4 <code>GUDATA_SEL</code> User data (32/64) 5 <code>GUCODE_SEL</code> User 64-bit code 6-7 <code>GPROC0_SEL</code> TSS (128-bit system descriptor) 8 <code>GUGS32_SEL</code> User 32-bit GS"},{"location":"sys/cpu/x86_64/#idt-entries-segmentsh206","title":"IDT Entries (segments.h:206)","text":"<pre><code>#define IDT_DE      0       /* Divide Error */\n#define IDT_DB      1       /* Debug */\n#define IDT_NMI     2       /* Non-Maskable Interrupt */\n#define IDT_BP      3       /* Breakpoint */\n#define IDT_OF      4       /* Overflow */\n#define IDT_BR      5       /* Bound Range Exceeded */\n#define IDT_UD      6       /* Invalid Opcode */\n#define IDT_NM      7       /* Device Not Available */\n#define IDT_DF      8       /* Double Fault */\n#define IDT_TS      10      /* Invalid TSS */\n#define IDT_NP      11      /* Segment Not Present */\n#define IDT_SS      12      /* Stack Segment Fault */\n#define IDT_GP      13      /* General Protection */\n#define IDT_PF      14      /* Page Fault */\n#define IDT_MF      16      /* x87 FPU Error */\n#define IDT_AC      17      /* Alignment Check */\n#define IDT_MC      18      /* Machine Check */\n#define IDT_XF      19      /* SIMD Exception */\n#define IDT_SYSCALL 0x80    /* System call vector */\n</code></pre>"},{"location":"sys/cpu/x86_64/#atomic-operations-atomich","title":"Atomic Operations (atomic.h)","text":"<p>x86-64 atomic operations use the <code>LOCK</code> prefix for SMP safety:</p> <pre><code>#define MPLOCKED    \"lock ; \"\n\n/* Example: atomic_add_int */\nstatic __inline void\natomic_add_int(volatile u_int *p, u_int v)\n{\n    __asm __volatile(MPLOCKED \"addl %1,%0\"\n        : \"+m\" (*p)\n        : \"iq\" (v));\n}\n</code></pre> <p>Supported operations (char/short/int/long variants): - <code>atomic_set_*</code> - OR value - <code>atomic_clear_*</code> - AND with complement - <code>atomic_add_*</code> - Add value - <code>atomic_subtract_*</code> - Subtract value - <code>atomic_cmpset_*</code> - Compare and swap - <code>atomic_fetchadd_*</code> - Fetch and add (returns old value) - <code>atomic_readandclear_*</code> - Read and zero</p> <p>Lock elision variants (<code>_xacquire</code>, <code>_xrelease</code>) use Intel TSX hints.</p>"},{"location":"sys/cpu/x86_64/#trap-handling-platformpc64x86_64trapc","title":"Trap Handling (platform/pc64/x86_64/trap.c)","text":"<p>The <code>trap()</code> function dispatches exceptions based on <code>tf_trapno</code>:</p> Trap Name Action T_PAGEFLT (12) Page Fault Call <code>trap_pfault()</code> -&gt; <code>vm_fault()</code> T_PROTFLT (9) General Protection Signal or panic T_DIVIDE (18) Divide Error SIGFPE T_BPTFLT (3) Breakpoint SIGTRAP or DDB T_TRCTRAP (10) Trace Trap SIGTRAP T_NMI (19) NMI DDB or panic T_DNA (22) Device Not Available FPU state restore T_DOUBLEFLT (23) Double Fault Panic <p>Page faults (T_PAGEFLT) are the most common, handled by: 1. Reading faulting address from CR2 2. Calling <code>vm_fault()</code> to resolve the fault 3. Returning to retry the instruction, or sending SIGSEGV</p>"},{"location":"sys/cpu/x86_64/#cpu-scheduling-interface-cpuh","title":"CPU Scheduling Interface (cpu.h)","text":"<p>Macros for requesting reschedule via <code>gd_reqflags</code>:</p> <pre><code>#define need_lwkt_resched()  \\\n    atomic_set_int(&amp;mycpu-&gt;gd_reqflags, RQF_AST_LWKT_RESCHED)\n#define need_user_resched()  \\\n    atomic_set_int(&amp;mycpu-&gt;gd_reqflags, RQF_AST_USER_RESCHED)\n#define signotify()          \\\n    atomic_set_int(&amp;mycpu-&gt;gd_reqflags, RQF_AST_SIGNAL)\n</code></pre> <p>These set bits that are checked on return from trap/syscall to trigger context switches or signal delivery.</p>"},{"location":"sys/cpu/x86_64/#smp-and-inter-processor-communication","title":"SMP and Inter-Processor Communication","text":""},{"location":"sys/cpu/x86_64/#tlb-shootdown","title":"TLB Shootdown","text":"<p>When page mappings change, other CPUs must invalidate their TLB:</p> <pre><code>void smp_invltlb(void);     /* Broadcast TLB invalidation */\n</code></pre> <p>Uses IPI (Inter-Processor Interrupt) via the LAPIC.</p>"},{"location":"sys/cpu/x86_64/#ipiq-inter-processor-interrupt-queue","title":"IPIQ (Inter-Processor Interrupt Queue)","text":"<p>The <code>need_ipiq()</code> macro signals pending cross-CPU work.  IPIQs are used for TLB shootdowns, scheduler requests, and other SMP coordination.</p>"},{"location":"sys/cpu/x86_64/#meltdownspectre-mitigations","title":"Meltdown/Spectre Mitigations","text":"<p>The <code>trampframe</code> structure (frame.h:126) supports isolated page tables for mitigating Meltdown (CVE-2017-5754):</p> <pre><code>struct trampframe {\n    register_t  tr_cr2;\n    register_t  tr_rax, tr_rcx, tr_rdx;\n    register_t  tr_err, tr_rip, tr_cs, tr_rflags, tr_rsp, tr_ss;\n    register_t  tr_pcb_rsp;         /* Trampoline stack */\n    register_t  tr_pcb_flags;\n    register_t  tr_pcb_cr3_iso;     /* Isolated PML4 */\n    register_t  tr_pcb_cr3;         /* Full kernel PML4 */\n    uint32_t    tr_pcb_spec_ctrl[2]; /* SPEC_CTRL MSR values */\n    ...\n};\n</code></pre> <p>On syscall/interrupt entry, the trampoline switches from the isolated user page table to the full kernel page table.</p>"},{"location":"sys/cpu/x86_64/#see-also","title":"See Also","text":"<ul> <li>Virtual Memory Subsystem - Page fault handling</li> <li>Processes and Threads - Context switching</li> <li>LWKT Scheduler - Thread scheduling</li> <li>Synchronization - Locking primitives</li> </ul>"},{"location":"sys/kern/","title":"Kernel Core (<code>kern/</code>) Overview","text":"<p>The <code>sys/kern/</code> directory contains the core kernel subsystems \u2014 the fundamental building blocks of the DragonFly BSD operating system. With approximately 180 source files, <code>kern/</code> implements everything from threading and process management to filesystems and device drivers.</p>"},{"location":"sys/kern/#what-makes-dragonfly-unique","title":"What Makes DragonFly Unique","text":"<p>Before diving into specifics, understand that DragonFly's kernel core differs significantly from traditional BSD kernels:</p>"},{"location":"sys/kern/#lwkt-the-foundation","title":"LWKT: The Foundation","text":"<p>Lightweight Kernel Threading (LWKT) is DragonFly's message-passing based concurrency model. Instead of pervasive locking, DragonFly uses:</p> <ul> <li>Message ports attached to threads</li> <li>Tokens for serialization (can be held across blocking operations)</li> <li>Cross-CPU message passing via IPIQs (Inter-Processor Interrupt Queues)</li> </ul> <p>This architecture reduces lock contention and enables better scalability on multi-core systems.</p> <p>Start here: LWKT Threading is essential to understand before exploring other subsystems.</p>"},{"location":"sys/kern/#major-subsystems","title":"Major Subsystems","text":""},{"location":"sys/kern/#core-infrastructure","title":"Core Infrastructure","text":""},{"location":"sys/kern/#lwkt-threading","title":"LWKT Threading","text":"<p>DragonFly's message-passing threading model \u2014 the architectural foundation.</p> <ul> <li><code>lwkt_thread.c</code> \u2014 Thread management</li> <li><code>lwkt_msgport.c</code> \u2014 Message ports and message passing</li> <li><code>lwkt_token.c</code> \u2014 Serializing tokens</li> <li><code>lwkt_ipiq.c</code> \u2014 Inter-processor interrupt queues</li> <li><code>lwkt_serialize.c</code> \u2014 Serialization helpers</li> </ul>"},{"location":"sys/kern/#synchronization","title":"Synchronization","text":"<p>Locks, tokens, and synchronization primitives.</p> <ul> <li>Spinlocks, mutexes, condition variables</li> <li>Token-based serialization</li> <li>Sleep queues</li> <li>Reference counting</li> </ul>"},{"location":"sys/kern/#memory-management","title":"Memory Management","text":"<p>Kernel memory allocation and management.</p> <ul> <li><code>kern_kmalloc.c</code> \u2014 Kernel malloc wrapper</li> <li><code>kern_slaballoc.c</code> \u2014 Slab allocator</li> <li><code>kern_objcache.c</code> \u2014 Per-CPU object caches</li> <li><code>kern_mpipe.c</code> \u2014 Lock-free message pipe allocator</li> </ul>"},{"location":"sys/kern/#process-and-thread-management","title":"Process and Thread Management","text":""},{"location":"sys/kern/#processes-threads","title":"Processes &amp; Threads","text":"<p>Process lifecycle and thread management.</p> <ul> <li><code>kern_fork.c</code> \u2014 Process creation (fork, rfork)</li> <li><code>kern_exec.c</code> \u2014 Program execution (execve)</li> <li><code>kern_exit.c</code> \u2014 Process termination and wait</li> <li><code>kern_proc.c</code> \u2014 Process structure management</li> <li><code>kern_threads.c</code> \u2014 Thread management</li> <li><code>kern_sig.c</code> \u2014 Signal handling</li> </ul>"},{"location":"sys/kern/#scheduling","title":"Scheduling","text":"<p>CPU scheduling framework and policies.</p> <ul> <li><code>kern_sched.c</code> \u2014 Generic scheduler framework</li> <li><code>usched_dfly.c</code> \u2014 DragonFly's message-based scheduler</li> <li><code>usched_bsd4.c</code> \u2014 Traditional BSD4 scheduler</li> <li><code>kern_synch.c</code> \u2014 Sleep/wakeup primitives</li> </ul>"},{"location":"sys/kern/#storage-and-filesystems","title":"Storage and Filesystems","text":""},{"location":"sys/kern/#virtual-filesystem-vfs","title":"Virtual Filesystem (VFS)","text":"<p>Filesystem abstraction layer.</p> <ul> <li>VFS core (<code>vfs_*.c</code>, 23 files)</li> <li>Name lookup and caching</li> <li>Buffer cache and I/O clustering</li> <li>Mount/unmount operations</li> <li>Journaling support</li> <li>VFS/VM integration</li> </ul>"},{"location":"sys/kern/#communication","title":"Communication","text":""},{"location":"sys/kern/#ipc-sockets","title":"IPC &amp; Sockets","text":"<p>Inter-process communication and networking foundation.</p> <ul> <li>Unix domain sockets (<code>uipc_*.c</code>, 11 files)</li> <li>Mbuf management</li> <li>Socket layer</li> <li>System V IPC (messages, semaphores, shared memory)</li> <li>Pipes and POSIX message queues</li> </ul>"},{"location":"sys/kern/#hardware-interface","title":"Hardware Interface","text":""},{"location":"sys/kern/#devices-drivers","title":"Devices &amp; Drivers","text":"<p>Device framework and driver infrastructure.</p> <ul> <li>Device registration and management</li> <li>Bus/driver model (newbus)</li> <li>Disk layer and partitioning</li> <li>DMA framework</li> <li>I/O scheduling</li> </ul>"},{"location":"sys/kern/#system-calls","title":"System Calls","text":"<p>System call infrastructure and kernel module loading.</p> <ul> <li>System call dispatch</li> <li>Dynamic kernel linking</li> <li>Module management</li> </ul>"},{"location":"sys/kern/#subsystem-organization","title":"Subsystem Organization","text":"<p>The ~180 files in <code>kern/</code> cluster by prefix:</p> <ul> <li><code>kern_*</code> (70 files) \u2014 Core kernel services</li> <li><code>subr_*</code> (36 files) \u2014 Kernel subroutines and utilities</li> <li><code>vfs_*</code> (23 files) \u2014 Virtual filesystem</li> <li><code>uipc_*</code> (11 files) \u2014 Unix IPC</li> <li><code>lwkt_*</code> (5 files) \u2014 Lightweight kernel threading</li> <li><code>sys_*</code> (5 files) \u2014 System call implementations</li> <li><code>tty_*</code> (5 files) \u2014 Terminal I/O</li> <li><code>usched_*</code> (3 files) \u2014 User schedulers</li> <li>Plus image activators, dynamic linkers, and utilities</li> </ul>"},{"location":"sys/kern/#reading-order","title":"Reading Order","text":"<p>If you're new to DragonFly's kernel core:</p> <ol> <li>LWKT Threading \u2190 Start here! Essential foundation</li> <li>Synchronization \u2014 Tokens, locks, message passing</li> <li>Memory Management \u2014 Kernel allocation</li> <li>Processes &amp; Threads \u2014 Process lifecycle</li> <li>Scheduling \u2014 CPU scheduling</li> <li>VFS \u2014 Filesystem layer</li> <li>IPC &amp; Sockets \u2014 Communication</li> <li>Devices \u2014 Device framework</li> <li>System Calls \u2014 Syscall infrastructure</li> </ol>"},{"location":"sys/kern/#key-concepts","title":"Key Concepts","text":""},{"location":"sys/kern/#message-passing","title":"Message Passing","text":"<p>Instead of calling functions directly across threads, DragonFly often uses asynchronous messages:</p> <pre><code>Thread A                    Thread B\n   |                           |\n   |-- send message ----------&gt;|\n   |                           | process message\n   |&lt;-- reply message ---------|\n   |                           |\n</code></pre> <p>This reduces lock contention and cache coherency traffic.</p>"},{"location":"sys/kern/#tokens-vs-locks","title":"Tokens vs. Locks","text":"<p>Tokens can be held across blocking operations:</p> <pre><code>lwkt_gettoken(&amp;mp-&gt;mnt_token);\n/* Can sleep here - token is retained */\nlwkt_reltoken(&amp;mp-&gt;mnt_token);\n</code></pre> <p>Traditional locks typically cannot be held across sleeps.</p>"},{"location":"sys/kern/#per-cpu-data","title":"Per-CPU Data","text":"<p>Many data structures are per-CPU to avoid cache-line bouncing:</p> <pre><code>struct globaldata *gd = mycpu;\n/* Access CPU-local data without locks */\n</code></pre>"},{"location":"sys/kern/#dependencies","title":"Dependencies","text":"<p>Understanding dependencies helps navigate <code>kern/</code>:</p> <pre><code>LWKT (foundation)\n  \u2193\nSynchronization\n  \u2193\nMemory allocation\n  \u2193\nProcesses/Threads \u2192 VFS \u2192 Filesystems\n  \u2193                 \u2193\nScheduling      Device I/O\n</code></pre>"},{"location":"sys/kern/#source-location","title":"Source Location","text":"<p>All source discussed here lives in:</p> <pre><code>~/s/dragonfly/sys/kern/\n</code></pre> <p>With documentation mirrored at:</p> <pre><code>~/s/dragonfly-docs/docs/sys/kern/\n</code></pre>"},{"location":"sys/kern/#whats-next","title":"What's Next?","text":"<p>Ready to dive deeper? Start with LWKT Threading to understand DragonFly's unique concurrency model, then explore other subsystems based on your interests.</p> <p>For a detailed reading plan covering all ~180 files, see the planning document at <code>planning/sys/kern/PLAN.md</code>.</p>"},{"location":"sys/kern/bus-resources/","title":"Bus Resource Management and DMA","text":"<p>This document covers the resource manager (rman) subsystem for managing hardware resources and the bus DMA subsystem for DMA memory management.</p> <p>Source files: - <code>sys/kern/subr_rman.c</code> - Resource manager implementation (~735 lines) - <code>sys/kern/subr_busdma.c</code> - Bus DMA helper functions (~144 lines) - <code>sys/platform/pc64/x86_64/busdma_machdep.c</code> - x86_64 DMA implementation (~1470 lines) - <code>sys/sys/rman.h</code> - Resource manager data structures - <code>sys/sys/bus_dma.h</code> - Bus DMA interface - <code>sys/cpu/x86_64/include/bus_dma.h</code> - Architecture-specific types</p>"},{"location":"sys/kern/bus-resources/#resource-manager-rman","title":"Resource Manager (rman)","text":"<p>The resource manager provides generic infrastructure for tracking and allocating hardware resources like IRQs, I/O ports, and memory regions. It is used by NewBus to manage bus resources.</p>"},{"location":"sys/kern/bus-resources/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/bus-resources/#struct-resource","title":"struct resource","text":"<p>Represents an allocated resource:</p> <pre><code>struct resource {\n    TAILQ_ENTRY(resource)   r_link;         /* list linkage */\n    LIST_ENTRY(resource)    r_sharelink;    /* sharing list link */\n    LIST_HEAD(, resource)   *r_sharehead;   /* head of sharing list */\n    u_long  r_start;        /* first index in resource */\n    u_long  r_end;          /* last index (inclusive) */\n    u_int   r_flags;        /* RF_* flags */\n    void    *r_virtual;     /* virtual address */\n    bus_space_tag_t r_bustag;       /* bus_space tag */\n    bus_space_handle_t r_bushandle; /* bus_space handle */\n    device_t r_dev;         /* owning device */\n    struct  rman *r_rm;     /* owning resource manager */\n    int     r_rid;          /* resource identifier */\n};\n</code></pre> <p>Defined in <code>sys/sys/rman.h:98-111</code>.</p>"},{"location":"sys/kern/bus-resources/#struct-rman","title":"struct rman","text":"<p>The resource manager:</p> <pre><code>struct rman {\n    struct  resource_head   rm_list;    /* list of resources */\n    struct  lwkt_token      *rm_slock;  /* mutex for rm_list */\n    TAILQ_ENTRY(rman)       rm_link;    /* link in global list */\n    u_long  rm_start;       /* globally first entry */\n    u_long  rm_end;         /* globally last entry */\n    enum    rman_type rm_type;  /* RMAN_ARRAY or RMAN_GAUGE */\n    const   char *rm_descr; /* text description */\n    int     rm_cpuid;       /* owner CPU ID */\n    int     rm_hold;        /* destruction interlock */\n};\n</code></pre> <p>Defined in <code>sys/sys/rman.h:115-125</code>.</p>"},{"location":"sys/kern/bus-resources/#resource-types","title":"Resource Types","text":"<pre><code>enum rman_type { RMAN_UNINIT = 0, RMAN_GAUGE, RMAN_ARRAY };\n</code></pre> <ul> <li><code>RMAN_ARRAY</code> - Sequential, individually-allocatable resources (common case)</li> <li><code>RMAN_GAUGE</code> - Fungible resources like power budgets (not currently used)</li> </ul> <p>Defined in <code>sys/sys/rman.h:60</code>.</p>"},{"location":"sys/kern/bus-resources/#resource-flags","title":"Resource Flags","text":"Flag Value Description <code>RF_ALLOCATED</code> 0x0001 Resource has been reserved <code>RF_ACTIVE</code> 0x0002 Resource allocation activated <code>RF_SHAREABLE</code> 0x0004 Permits contemporaneous sharing <code>RF_TIMESHARE</code> 0x0008 Permits time-division sharing <code>RF_WANTED</code> 0x0010 Someone is waiting for resource <code>RF_FIRSTSHARE</code> 0x0020 First in sharing list <code>RF_PREFETCHABLE</code> 0x0040 Memory is prefetchable <code>RF_OPTIONAL</code> 0x0080 For bus_alloc_resources() <p>Alignment can be encoded in flags using: <pre><code>#define RF_ALIGNMENT_SHIFT  10\n#define RF_ALIGNMENT_LOG2(x) ((x) &lt;&lt; RF_ALIGNMENT_SHIFT)\n#define RF_ALIGNMENT(x)     (((x) &amp; RF_ALIGNMENT_MASK) &gt;&gt; RF_ALIGNMENT_SHIFT)\n</code></pre></p> <p>Defined in <code>sys/sys/rman.h:45-58</code>.</p>"},{"location":"sys/kern/bus-resources/#api-functions","title":"API Functions","text":""},{"location":"sys/kern/bus-resources/#rman_init","title":"rman_init","text":"<p>Initializes a resource manager:</p> <pre><code>int rman_init(struct rman *rm, int cpuid);\n</code></pre> <ul> <li>Creates a per-rman lwkt_token for locking</li> <li>Adds the rman to global <code>rman_head</code> list</li> <li>Sets <code>rm_type</code> to <code>RMAN_ARRAY</code></li> </ul> <p>See <code>sys/kern/subr_rman.c:86-116</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_manage_region","title":"rman_manage_region","text":"<p>Adds a range of resources to the manager:</p> <pre><code>int rman_manage_region(struct rman *rm, u_long start, u_long end);\n</code></pre> <ul> <li>Maintains sorted order by start address</li> <li>Does NOT check for overlapping regions</li> <li>Caller must ensure regions don't overlap</li> </ul> <p>See <code>sys/kern/subr_rman.c:122-152</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_reserve_resource","title":"rman_reserve_resource","text":"<p>Reserves resources from the manager:</p> <pre><code>struct resource *rman_reserve_resource(struct rman *rm, u_long start,\n                                       u_long end, u_long count,\n                                       u_int flags, device_t dev);\n</code></pre> <p>Allocation algorithm: 1. Search for unshared region that fits the request 2. Split region into 1-3 parts as needed:    - Allocating from beginning: split into 2 parts    - Allocating from end: split into 2 parts    - Allocating from middle: split into 3 parts 3. If <code>RF_SHAREABLE</code> or <code>RF_TIMESHARE</code>, search for exact match 4. If <code>RF_ACTIVE</code> specified, atomically activate</p> <p>See <code>sys/kern/subr_rman.c:204-404</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_activate_resource-rman_deactivate_resource","title":"rman_activate_resource / rman_deactivate_resource","text":"<p>Activate or deactivate a resource:</p> <pre><code>int rman_activate_resource(struct resource *r);\nint rman_deactivate_resource(struct resource *r);\n</code></pre> <ul> <li>Activation marks resource as <code>RF_ACTIVE</code></li> <li>Time-shared resources: only one active at a time</li> <li>Deactivation wakes waiters via <code>wakeup(r-&gt;r_sharehead)</code></li> </ul> <p>See <code>sys/kern/subr_rman.c:406-515</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_release_resource","title":"rman_release_resource","text":"<p>Releases a resource back to the pool:</p> <pre><code>int rman_release_resource(struct resource *r);\n</code></pre> <p>Merging algorithm: 1. If sharing list exists, update the list 2. Try to merge with previous adjacent segment 3. Try to merge with next adjacent segment 4. If both neighbors are free, merge all three 5. If neither neighbor is free, just mark as unallocated</p> <p>See <code>sys/kern/subr_rman.c:517-613</code>.</p>"},{"location":"sys/kern/bus-resources/#rman_fini","title":"rman_fini","text":"<p>Destroys a resource manager:</p> <pre><code>int rman_fini(struct rman *rm);\n</code></pre> <ul> <li>Fails if any resources are still allocated</li> <li>Waits for <code>rm_hold</code> to drop to zero before destroying</li> </ul> <p>See <code>sys/kern/subr_rman.c:154-202</code>.</p>"},{"location":"sys/kern/bus-resources/#helper-macros","title":"Helper Macros","text":"<p>Accessor macros for resource fields:</p> <pre><code>#define rman_get_start(r)       ((r)-&gt;r_start)\n#define rman_get_end(r)         ((r)-&gt;r_end)\n#define rman_get_size(r)        ((r)-&gt;r_end - (r)-&gt;r_start + 1)\n#define rman_get_device(r)      ((r)-&gt;r_dev)\n#define rman_get_flags(r)       ((r)-&gt;r_flags)\n#define rman_get_virtual(r)     ((r)-&gt;r_virtual)\n#define rman_get_bustag(r)      ((r)-&gt;r_bustag)\n#define rman_get_bushandle(r)   ((r)-&gt;r_bushandle)\n#define rman_get_rid(r)         ((r)-&gt;r_rid)\n#define rman_get_cpuid(r)       ((r)-&gt;r_rm-&gt;rm_cpuid)\n</code></pre> <p>Defined in <code>sys/sys/rman.h:141-157</code>.</p>"},{"location":"sys/kern/bus-resources/#sysctl-interface","title":"Sysctl Interface","text":"<p>Resource manager information is exported via <code>hw.bus.rman</code> sysctl for userspace introspection. The exported structures are:</p> <pre><code>struct u_rman {\n    uintptr_t      rm_handle;\n    char           rm_descr[RM_TEXTLEN];\n    u_long         rm_start;\n    u_long         rm_size;\n    enum rman_type rm_type;\n};\n\nstruct u_resource {\n    uintptr_t r_handle;\n    uintptr_t r_parent;\n    uintptr_t r_device;\n    char      r_devname[RM_TEXTLEN];\n    u_long    r_start;\n    u_long    r_size;\n    u_int     r_flags;\n};\n</code></pre> <p>Defined in <code>sys/sys/rman.h:70-88</code>.</p>"},{"location":"sys/kern/bus-resources/#bus-dma-subsystem","title":"Bus DMA Subsystem","text":"<p>The bus DMA subsystem provides portable DMA memory management with support for devices that have address limitations requiring bounce buffers.</p>"},{"location":"sys/kern/bus-resources/#data-structures_1","title":"Data Structures","text":""},{"location":"sys/kern/bus-resources/#bus_dma_tag_t-struct-bus_dma_tag","title":"bus_dma_tag_t (struct bus_dma_tag)","text":"<p>Defines constraints for DMA operations:</p> <pre><code>struct bus_dma_tag {\n    bus_size_t      alignment;      /* required alignment */\n    bus_size_t      boundary;       /* boundary that segments can't cross */\n    bus_addr_t      lowaddr;        /* low address constraint */\n    bus_addr_t      highaddr;       /* high address constraint */\n    bus_size_t      maxsize;        /* maximum mapping size */\n    u_int           nsegments;      /* max number of segments */\n    bus_size_t      maxsegsz;       /* max size per segment */\n    int             flags;          /* BUS_DMA_* flags */\n    int             map_count;      /* number of active maps */\n    bus_dma_segment_t *segments;    /* segment array */\n    struct bounce_zone *bounce_zone;/* bounce buffer zone */\n    struct spinlock spin;           /* lock for segment array */\n};\n</code></pre> <p>Defined in <code>sys/platform/pc64/x86_64/busdma_machdep.c:64-77</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_t-struct-bus_dmamap","title":"bus_dmamap_t (struct bus_dmamap)","text":"<p>Tracks a DMA mapping:</p> <pre><code>struct bus_dmamap {\n    struct bp_list  bpages;         /* list of bounce pages */\n    int             pagesneeded;    /* pages needed for transfer */\n    int             pagesreserved;  /* pages currently reserved */\n    bus_dma_tag_t   dmat;           /* associated tag */\n    void            *buf;           /* original buffer pointer */\n    bus_size_t      buflen;         /* original buffer length */\n    bus_dmamap_callback_t *callback;/* completion callback */\n    void            *callback_arg;  /* callback argument */\n    STAILQ_ENTRY(bus_dmamap) links; /* waitlist linkage */\n};\n</code></pre> <p>Defined in <code>sys/platform/pc64/x86_64/busdma_machdep.c:140-150</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dma_segment_t","title":"bus_dma_segment_t","text":"<p>Describes a DMA segment:</p> <pre><code>typedef struct bus_dma_segment {\n    bus_addr_t  ds_addr;    /* DMA address */\n    bus_size_t  ds_len;     /* length of transfer */\n} bus_dma_segment_t;\n</code></pre> <p>Defined in <code>sys/sys/bus_dma.h:146-149</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamem_t","title":"bus_dmamem_t","text":"<p>Convenience structure for coherent memory:</p> <pre><code>typedef struct bus_dmamem {\n    bus_dma_tag_t   dmem_tag;       /* tag used */\n    bus_dmamap_t    dmem_map;       /* map created */\n    void            *dmem_addr;     /* virtual address */\n    bus_addr_t      dmem_busaddr;   /* bus address */\n} bus_dmamem_t;\n</code></pre> <p>Defined in <code>sys/sys/bus_dma.h:151-156</code>.</p>"},{"location":"sys/kern/bus-resources/#dma-flags","title":"DMA Flags","text":"Flag Value Description <code>BUS_DMA_WAITOK</code> 0x0000 Safe to sleep (pseudo-flag) <code>BUS_DMA_NOWAIT</code> 0x0001 Not safe to sleep <code>BUS_DMA_ALLOCNOW</code> 0x0002 Perform resource allocation now <code>BUS_DMA_COHERENT</code> 0x0004 Map memory to not require sync <code>BUS_DMA_ZERO</code> 0x0008 Allocate zero'd memory <code>BUS_DMA_ONEBPAGE</code> 0x0100 Allocate one bounce page per map <code>BUS_DMA_ALIGNED</code> 0x0200 Memory is already properly aligned <code>BUS_DMA_PRIVBZONE</code> 0x0400 Need private bounce zone <code>BUS_DMA_ALLOCALL</code> 0x0800 Allocate all needed resources <code>BUS_DMA_PROTECTED</code> 0x1000 Functions are already protected <code>BUS_DMA_KEEP_PG_OFFSET</code> 0x2000 Preserve page offset in first segment <code>BUS_DMA_NOCACHE</code> 0x4000 Map memory uncached <p>Defined in <code>sys/sys/bus_dma.h:83-104</code>.</p>"},{"location":"sys/kern/bus-resources/#sync-operations","title":"Sync Operations","text":"<pre><code>typedef int bus_dmasync_op_t;\n#define BUS_DMASYNC_PREREAD     0x01  /* before device reads from memory */\n#define BUS_DMASYNC_POSTREAD    0x02  /* after device reads from memory */\n#define BUS_DMASYNC_PREWRITE    0x04  /* before device writes to memory */\n#define BUS_DMASYNC_POSTWRITE   0x08  /* after device writes to memory */\n</code></pre> <p>On x86, these operations primarily handle bounce buffer data copying: - <code>PREWRITE</code>: Copy data from client buffer to bounce buffer - <code>POSTREAD</code>: Copy data from bounce buffer to client buffer - <code>PREREAD</code>/<code>POSTWRITE</code>: No-ops on cache-coherent x86</p> <p>Defined in <code>sys/sys/bus_dma.h:116-121</code>.</p>"},{"location":"sys/kern/bus-resources/#api-functions_1","title":"API Functions","text":""},{"location":"sys/kern/bus-resources/#bus_dma_tag_create","title":"bus_dma_tag_create","text":"<p>Creates a DMA tag with specified constraints:</p> <pre><code>int bus_dma_tag_create(bus_dma_tag_t parent, bus_size_t alignment,\n                       bus_size_t boundary, bus_addr_t lowaddr,\n                       bus_addr_t highaddr, bus_size_t maxsize,\n                       int nsegments, bus_size_t maxsegsz,\n                       int flags, bus_dma_tag_t *dmat);\n</code></pre> <p>Key behavior: - Validates alignment/boundary are powers of 2 - Inherits constraints from parent tag - Sets bounce flags if <code>lowaddr &lt; Maxmem</code> or <code>alignment &gt; 1</code> - Pre-allocates bounce pages if <code>BUS_DMA_ALLOCNOW</code></p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:222-331</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dma_tag_destroy","title":"bus_dma_tag_destroy","text":"<p>Destroys a DMA tag:</p> <pre><code>int bus_dma_tag_destroy(bus_dma_tag_t dmat);\n</code></pre> <ul> <li>Fails with <code>EBUSY</code> if maps still exist</li> <li>Frees bounce zone (for private zones)</li> <li>Frees segment array and tag</li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:333-346</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_create","title":"bus_dmamap_create","text":"<p>Creates a DMA map:</p> <pre><code>int bus_dmamap_create(bus_dma_tag_t dmat, int flags, bus_dmamap_t *mapp);\n</code></pre> <ul> <li>Returns NULL map if no bouncing needed</li> <li>Allocates map structure and initializes bounce page list</li> <li>Allocates bounce pages incrementally up to <code>max_bounce_pages</code></li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:358-433</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_destroy","title":"bus_dmamap_destroy","text":"<p>Destroys a DMA map:</p> <pre><code>int bus_dmamap_destroy(bus_dma_tag_t dmat, bus_dmamap_t map);\n</code></pre> <ul> <li>Fails with <code>EBUSY</code> if bounce pages still attached</li> <li>Decrements <code>map_count</code> on tag</li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:439-449</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamem_alloc","title":"bus_dmamem_alloc","text":"<p>Allocates DMA-safe memory:</p> <pre><code>int bus_dmamem_alloc(bus_dma_tag_t dmat, void **vaddr, int flags,\n                     bus_dmamap_t *mapp);\n</code></pre> <p>Allocation method selection: - Small allocations (<code>maxsize &lt;= PAGE_SIZE</code>, <code>lowaddr &gt;= Maxmem</code>): <code>kmalloc</code> - Large/constrained allocations: <code>contigmalloc</code></p> <p>The map pointer encodes which allocator was used for later freeing.</p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:480-546</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamem_free","title":"bus_dmamem_free","text":"<p>Frees DMA-safe memory:</p> <pre><code>void bus_dmamem_free(bus_dma_tag_t dmat, void *vaddr, bus_dmamap_t map);\n</code></pre> <ul> <li><code>map == NULL</code>: uses <code>kfree</code></li> <li><code>map == (void *)-1</code>: uses <code>contigfree</code></li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:552-565</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_load","title":"bus_dmamap_load","text":"<p>Loads a buffer for DMA:</p> <pre><code>int bus_dmamap_load(bus_dma_tag_t dmat, bus_dmamap_t map, void *buf,\n                    bus_size_t buflen, bus_dmamap_callback_t *callback,\n                    void *callback_arg, int flags);\n</code></pre> <p>Algorithm: 1. Count bounce pages needed 2. Reserve bounce pages with zone lock held 3. Per-page loop:    - Extract physical address    - If needs bouncing, substitute bounce page address    - Coalesce contiguous segments    - Handle boundary and <code>maxsegsz</code> constraints 4. If insufficient bounce pages, return <code>EINPROGRESS</code> (deferred) 5. Otherwise call callback immediately with segments</p> <p>Bounce page need determination: <pre><code>static __inline int addr_needs_bounce(bus_dma_tag_t dmat, bus_addr_t paddr)\n{\n    if ((paddr &gt; dmat-&gt;lowaddr &amp;&amp; paddr &lt;= dmat-&gt;highaddr) ||\n         (bounce_alignment &amp;&amp; (paddr &amp; (dmat-&gt;alignment - 1)) != 0))\n        return (1);\n    return (0);\n}\n</code></pre></p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:766-807</code>.</p>"},{"location":"sys/kern/bus-resources/#specialized-load-functions","title":"Specialized Load Functions","text":"<p>bus_dmamap_load_mbuf() - Loads an mbuf chain: <pre><code>int bus_dmamap_load_mbuf(bus_dma_tag_t dmat, bus_dmamap_t map,\n                         struct mbuf *m0,\n                         bus_dmamap_callback2_t *callback,\n                         void *callback_arg, int flags);\n</code></pre> See <code>sys/platform/pc64/x86_64/busdma_machdep.c:836-867</code>.</p> <p>bus_dmamap_load_mbuf_segment() - Loads mbuf with direct segment return: <pre><code>int bus_dmamap_load_mbuf_segment(bus_dma_tag_t dmat, bus_dmamap_t map,\n                                 struct mbuf *m0,\n                                 bus_dma_segment_t *segs, int maxsegs,\n                                 int *nsegs, int flags);\n</code></pre> See <code>sys/platform/pc64/x86_64/busdma_machdep.c:869-923</code>.</p> <p>bus_dmamap_load_uio() - Loads user I/O vector: <pre><code>int bus_dmamap_load_uio(bus_dma_tag_t dmat, bus_dmamap_t map,\n                        struct uio *uio,\n                        bus_dmamap_callback2_t *callback,\n                        void *callback_arg, int flags);\n</code></pre> See <code>sys/platform/pc64/x86_64/busdma_machdep.c:928-1017</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_unload","title":"bus_dmamap_unload","text":"<p>Unloads a DMA mapping:</p> <pre><code>void bus_dmamap_unload(bus_dma_tag_t dmat, bus_dmamap_t map);\n</code></pre> <ul> <li>Frees all bounce pages associated with map</li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1022-1031</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_sync","title":"bus_dmamap_sync","text":"<p>Synchronizes DMA memory:</p> <pre><code>void bus_dmamap_sync(bus_dma_tag_t dmat, bus_dmamap_t map,\n                     bus_dmasync_op_t op);\n</code></pre> <p>Bounce buffer handling on x86: - <code>PREWRITE</code>: Copy data from client to bounce buffer, then <code>cpu_sfence()</code> - <code>POSTREAD</code>: <code>cpu_lfence()</code>, then copy data from bounce to client buffer - <code>PREREAD</code>/<code>POSTWRITE</code>: No operation (cache coherent)</p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1033-1067</code>.</p>"},{"location":"sys/kern/bus-resources/#bounce-buffer-infrastructure","title":"Bounce Buffer Infrastructure","text":""},{"location":"sys/kern/bus-resources/#struct-bounce_page","title":"struct bounce_page","text":"<p>Individual bounce page:</p> <pre><code>struct bounce_page {\n    vm_offset_t vaddr;          /* kva of bounce buffer */\n    bus_addr_t  busaddr;        /* physical address */\n    vm_offset_t datavaddr;      /* kva of client data */\n    bus_size_t  datacount;      /* client data count */\n    STAILQ_ENTRY(bounce_page) links;\n};\n</code></pre> <p>Defined in <code>sys/platform/pc64/x86_64/busdma_machdep.c:93-99</code>.</p>"},{"location":"sys/kern/bus-resources/#struct-bounce_zone","title":"struct bounce_zone","text":"<p>Zone managing bounce pages:</p> <pre><code>struct bounce_zone {\n    STAILQ_ENTRY(bounce_zone) links;\n    STAILQ_HEAD(bp_list, bounce_page) bounce_page_list;\n    STAILQ_HEAD(, bus_dmamap) bounce_map_waitinglist;\n    struct spinlock spin;\n    int             total_bpages;       /* total bounce pages */\n    int             free_bpages;        /* free bounce pages */\n    int             reserved_bpages;    /* reserved bounce pages */\n    int             active_bpages;      /* in-use bounce pages */\n    int             total_bounced;      /* total transfers bounced */\n    int             total_deferred;     /* total deferred operations */\n    int             reserve_failed;     /* failed reservations */\n    bus_size_t      alignment;          /* zone alignment */\n    bus_addr_t      lowaddr;            /* zone low address limit */\n    char            zoneid[8];          /* zone identifier */\n    char            lowaddrid[20];      /* low address string */\n    struct sysctl_ctx_list sysctl_ctx;\n    struct sysctl_oid *sysctl_tree;\n};\n</code></pre> <p>Defined in <code>sys/platform/pc64/x86_64/busdma_machdep.c:101-119</code>.</p>"},{"location":"sys/kern/bus-resources/#zone-management","title":"Zone Management","text":"<p>Bounce zones are shared by default. Multiple tags with compatible constraints share a zone. Private zones can be requested with <code>BUS_DMA_PRIVBZONE</code>.</p> <p>alloc_bounce_zone() - Creates or finds compatible zone: See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1069-1167</code>.</p> <p>alloc_bounce_pages() - Allocates pages using <code>contigmalloc</code>: See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1169-1206</code>.</p> <p>reserve_bounce_pages() - Reserves pages from free pool: See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1262-1283</code>.</p> <p>return_bounce_pages() - Returns pages to free pool, wakes waiters: See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1285-1312</code>.</p>"},{"location":"sys/kern/bus-resources/#deferred-operations","title":"Deferred Operations","text":"<p>When bounce pages are unavailable, <code>bus_dmamap_load()</code> returns <code>EINPROGRESS</code> and the map is added to a waiting list. The <code>busdma_swi()</code> software interrupt handler processes waiting maps when bounce pages become available.</p> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1436-1450</code>.</p>"},{"location":"sys/kern/bus-resources/#helper-functions","title":"Helper Functions","text":""},{"location":"sys/kern/bus-resources/#bus_dmamem_coherent","title":"bus_dmamem_coherent","text":"<p>Allocates coherent DMA memory in one call:</p> <pre><code>int bus_dmamem_coherent(bus_dma_tag_t parent,\n                        bus_size_t alignment, bus_size_t boundary,\n                        bus_addr_t lowaddr, bus_addr_t highaddr,\n                        bus_size_t maxsize, int flags,\n                        bus_dmamem_t *dmem);\n</code></pre> <p>Creates tag, allocates memory, and loads mapping.</p> <p>See <code>sys/kern/subr_busdma.c:53-95</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamem_coherent_any","title":"bus_dmamem_coherent_any","text":"<p>Simplified coherent allocation with no boundary:</p> <pre><code>void *bus_dmamem_coherent_any(bus_dma_tag_t parent,\n                              bus_size_t alignment, bus_size_t size,\n                              int flags,\n                              bus_dma_tag_t *dtag, bus_dmamap_t *dmap,\n                              bus_addr_t *busaddr);\n</code></pre> <p>See <code>sys/kern/subr_busdma.c:97-117</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_dmamap_load_mbuf_defrag","title":"bus_dmamap_load_mbuf_defrag","text":"<p>Loads mbuf with automatic defragmentation:</p> <pre><code>int bus_dmamap_load_mbuf_defrag(bus_dma_tag_t dmat, bus_dmamap_t map,\n                                struct mbuf **m_head,\n                                bus_dma_segment_t *segs, int maxsegs,\n                                int *nsegs, int flags);\n</code></pre> <p>Tries normal load first; if <code>EFBIG</code> (too many segments), defragments the mbuf and retries.</p> <p>See <code>sys/kern/subr_busdma.c:119-143</code>.</p>"},{"location":"sys/kern/bus-resources/#bus-space-operations","title":"Bus Space Operations","text":""},{"location":"sys/kern/bus-resources/#types-x86_64","title":"Types (x86_64)","text":"<pre><code>typedef uint64_t bus_addr_t;\ntypedef uint64_t bus_size_t;\ntypedef uint64_t bus_space_tag_t;\ntypedef uint64_t bus_space_handle_t;\n</code></pre> <p>Bus space tags: - <code>X86_64_BUS_SPACE_IO</code> (0) - I/O port space - <code>X86_64_BUS_SPACE_MEM</code> (1) - Memory-mapped space</p> <p>Address limits: <pre><code>#define BUS_SPACE_MAXADDR_24BIT 0xFFFFFFUL\n#define BUS_SPACE_MAXADDR_32BIT 0xFFFFFFFFUL\n#define BUS_SPACE_MAXADDR       0xFFFFFFFFFFFFFFFFUL\n</code></pre></p> <p>Defined in <code>sys/cpu/x86_64/include/bus_dma.h:36-55</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_space_map-bus_space_unmap","title":"bus_space_map / bus_space_unmap","text":"<p>Maps bus space to kernel virtual address:</p> <pre><code>int bus_space_map(bus_space_tag_t t, bus_addr_t addr, bus_size_t size,\n                  int flags, bus_space_handle_t *bshp);\nvoid bus_space_unmap(bus_space_tag_t t, bus_space_handle_t bsh,\n                     bus_size_t size);\n</code></pre> <ul> <li>Memory space: uses <code>pmap_mapdev()</code>/<code>pmap_unmapdev()</code></li> <li>I/O space: returns address directly (no mapping needed)</li> </ul> <p>See <code>sys/platform/pc64/x86_64/busdma_machdep.c:1452-1469</code>.</p>"},{"location":"sys/kern/bus-resources/#bus_space_barrier","title":"bus_space_barrier","text":"<p>Memory barrier for bus operations:</p> <pre><code>static __inline void\nbus_space_barrier(bus_space_tag_t tag, bus_space_handle_t bsh,\n                  bus_size_t offset, bus_size_t len, int flags)\n{\n    if (flags &amp; BUS_SPACE_BARRIER_READ)\n        __asm __volatile(\"lock; addl $0,0(%%rsp)\" : : : \"memory\");\n    else\n        __asm __volatile(\"\" : : : \"memory\");\n}\n</code></pre> <ul> <li>Read barrier: uses locked instruction for MFENCE semantics</li> <li>Write barrier: compiler barrier only (x86 has strong ordering)</li> </ul> <p>Defined in <code>sys/cpu/x86_64/include/bus_dma.h:893-901</code>.</p>"},{"location":"sys/kern/bus-resources/#sysctl-interface_1","title":"Sysctl Interface","text":"<p>Global tunables: <pre><code>hw.busdma.max_bpages     - Maximum bounce pages (default 1024)\nhw.busdma.bounce_alignment - Enable alignment bouncing (default 1)\n</code></pre></p> <p>Per-zone statistics via <code>hw.busdma.zoneN.*</code>: - <code>total_bpages</code> - Total bounce pages in zone - <code>free_bpages</code> - Free bounce pages - <code>reserved_bpages</code> - Reserved bounce pages - <code>active_bpages</code> - Active (in-use) bounce pages - <code>total_bounced</code> - Total bounce operations - <code>total_deferred</code> - Total deferred operations - <code>reserve_failed</code> - Failed reservations - <code>lowaddr</code> - Zone low address constraint - <code>alignment</code> - Zone alignment constraint</p>"},{"location":"sys/kern/bus-resources/#example-dma-buffer-allocation","title":"Example: DMA Buffer Allocation","text":"<pre><code>bus_dma_tag_t   tag;\nbus_dmamap_t    map;\nvoid            *vaddr;\nbus_addr_t      paddr;\nbus_dma_segment_t seg;\nint             nseg;\n\n/* Create a tag for 4K-aligned, 32-bit addressable memory */\nerror = bus_dma_tag_create(NULL,            /* parent */\n                           4096,            /* alignment */\n                           0,               /* boundary */\n                           BUS_SPACE_MAXADDR_32BIT,  /* lowaddr */\n                           BUS_SPACE_MAXADDR,        /* highaddr */\n                           4096,            /* maxsize */\n                           1,               /* nsegments */\n                           4096,            /* maxsegsz */\n                           0,               /* flags */\n                           &amp;tag);\n\n/* Allocate DMA-safe memory */\nerror = bus_dmamem_alloc(tag, &amp;vaddr, BUS_DMA_WAITOK | BUS_DMA_ZERO, &amp;map);\n\n/* Load the buffer to get physical address */\nerror = bus_dmamap_load(tag, map, vaddr, 4096, callback, &amp;paddr, BUS_DMA_NOWAIT);\n\n/* Use the buffer... */\n\n/* Before device reads: */\nbus_dmamap_sync(tag, map, BUS_DMASYNC_PREREAD);\n\n/* After device reads: */\nbus_dmamap_sync(tag, map, BUS_DMASYNC_POSTREAD);\n\n/* Cleanup */\nbus_dmamap_unload(tag, map);\nbus_dmamem_free(tag, vaddr, map);\nbus_dmamap_destroy(tag, map);\nbus_dma_tag_destroy(tag);\n</code></pre>"},{"location":"sys/kern/bus-resources/#cross-references","title":"Cross-References","text":"<ul> <li>NewBus Framework - Device/driver infrastructure using rman</li> <li>Device Framework - Character device layer</li> <li>Memory Management - Kernel memory allocation</li> <li>Buffer Cache - BIO and buffer management</li> </ul>"},{"location":"sys/kern/devices/","title":"Device Framework","text":"<p>The device framework provides the infrastructure for creating, managing, and operating on character and block devices in DragonFly BSD. It consists of two main components: device number management (<code>kern_conf.c</code>) and device operations dispatch (<code>kern_device.c</code>).</p> <p>Source files: - <code>sys/kern/kern_conf.c</code> - Device creation, destruction, aliases - <code>sys/kern/kern_device.c</code> - Device operations dispatch layer - <code>sys/sys/device.h</code> - Data structures and prototypes - <code>sys/sys/conf.h</code> - Device type definitions</p>"},{"location":"sys/kern/devices/#overview","title":"Overview","text":"<p>DragonFly's device framework evolved from traditional BSD but incorporates several DragonFly-specific enhancements:</p> <ol> <li>DEVFS Integration - All device creation goes through devfs</li> <li>MPSAFE Support - Per-device MPSAFE flags control locking</li> <li>KVABIO Support - Efficient buffer handling for capable devices</li> <li>Reference Counting - Sysref-based lifecycle management</li> <li>Operation Interception - Console and layered device support</li> </ol>"},{"location":"sys/kern/devices/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/devices/#struct-dev_ops","title":"struct dev_ops","text":"<p>The device operations switch table. Each device driver provides one of these to define how the device responds to operations.</p> <pre><code>struct dev_ops {\n    struct {\n        const char  *name;   /* base name, e.g. 'da' */\n        int          maj;    /* major device number */\n        u_int        flags;  /* D_XXX flags */\n        void        *data;   /* custom driver data */\n        int          refs;   /* ref count */\n        int          id;\n    } head;\n\n    d_default_t     *d_default;\n    d_open_t        *d_open;\n    d_close_t       *d_close;\n    d_read_t        *d_read;\n    d_write_t       *d_write;\n    d_ioctl_t       *d_ioctl;\n    d_mmap_t        *d_mmap;\n    d_mmap_single_t *d_mmap_single;\n    d_strategy_t    *d_strategy;\n    d_dump_t        *d_dump;\n    d_psize_t       *d_psize;\n    d_kqfilter_t    *d_kqfilter;\n    d_clone_t       *d_clone;\n    d_revoke_t      *d_revoke;\n    int (*d_uksmap)(...);\n};\n</code></pre> <p>Defined in <code>sys/sys/device.h:229-257</code>.</p>"},{"location":"sys/kern/devices/#device-flags-headflags","title":"Device Flags (head.flags)","text":"<p>Type flags (mutually exclusive):</p> Flag Value Description <code>D_TAPE</code> 0x0001 Tape device <code>D_DISK</code> 0x0002 Disk device <code>D_TTY</code> 0x0004 Terminal device <code>D_MEM</code> 0x0008 Memory device <p>Behavior flags:</p> Flag Value Description <code>D_MEMDISK</code> 0x00010000 Memory-type disk <code>D_CANFREE</code> 0x00040000 Supports TRIM/free blocks <code>D_TRACKCLOSE</code> 0x00080000 Track all close calls <code>D_MASTER</code> 0x00100000 Used by pty/tty code <code>D_NOEMERGPGR</code> 0x00200000 Skip in emergency pager <code>D_MPSAFE</code> 0x00400000 All operations are MPSAFE <code>D_KVABIO</code> 0x00800000 Supports KVABIO API <code>D_QUICK</code> 0x01000000 No fancy open/close needed <p>Defined in <code>sys/sys/device.h:263-287</code>.</p>"},{"location":"sys/kern/devices/#cdev_t","title":"cdev_t","text":"<p>Opaque pointer to a device structure. The actual structure (<code>struct cdev</code>) is managed by devfs and contains:</p> <ul> <li><code>si_ops</code> - Pointer to <code>struct dev_ops</code></li> <li><code>si_umajor</code>, <code>si_uminor</code> - Major/minor numbers</li> <li><code>si_name</code> - Device name</li> <li><code>si_uid</code>, <code>si_gid</code>, <code>si_perms</code> - Ownership and permissions</li> <li><code>si_sysref</code> - Reference count</li> <li><code>si_track_read</code>, <code>si_track_write</code> - BIO tracking</li> <li><code>si_lastread</code>, <code>si_lastwrite</code> - Access timestamps</li> </ul>"},{"location":"sys/kern/devices/#operation-argument-structures","title":"Operation Argument Structures","text":"<p>Each device operation receives a typed argument structure derived from <code>struct dev_generic_args</code>:</p> <pre><code>struct dev_generic_args {\n    struct syslink_desc *a_desc;  /* operation descriptor */\n    struct cdev *a_dev;           /* device pointer */\n};\n</code></pre> <p>Common argument structures:</p> Structure Operation Key Fields <code>dev_open_args</code> <code>d_open</code> <code>a_oflags</code>, <code>a_devtype</code>, <code>a_cred</code>, <code>a_fpp</code> <code>dev_close_args</code> <code>d_close</code> <code>a_fflag</code>, <code>a_devtype</code>, <code>a_fp</code> <code>dev_read_args</code> <code>d_read</code> <code>a_uio</code>, <code>a_ioflag</code>, <code>a_fp</code> <code>dev_write_args</code> <code>d_write</code> <code>a_uio</code>, <code>a_ioflag</code>, <code>a_fp</code> <code>dev_ioctl_args</code> <code>d_ioctl</code> <code>a_cmd</code>, <code>a_data</code>, <code>a_fflag</code>, <code>a_cred</code> <code>dev_strategy_args</code> <code>d_strategy</code> <code>a_bio</code> <code>dev_mmap_args</code> <code>d_mmap</code> <code>a_offset</code>, <code>a_nprot</code>, <code>a_result</code> <code>dev_dump_args</code> <code>d_dump</code> <code>a_virtual</code>, <code>a_physical</code>, <code>a_offset</code>, <code>a_length</code> <code>dev_psize_args</code> <code>d_psize</code> <code>a_result</code> <p>Defined in <code>sys/sys/device.h:59-204</code>.</p>"},{"location":"sys/kern/devices/#device-creation","title":"Device Creation","text":""},{"location":"sys/kern/devices/#make_dev","title":"make_dev","text":"<p>Creates a device node visible in <code>/dev</code>:</p> <pre><code>cdev_t make_dev(struct dev_ops *ops, int minor,\n                uid_t uid, gid_t gid, int perms,\n                const char *fmt, ...);\n</code></pre> <p>Operation: 1. Call <code>compile_dev_ops()</code> to fill in NULL handlers 2. Create device via <code>devfs_new_cdev()</code> 3. Format device name from <code>fmt</code> and varargs 4. Create devfs entry via <code>devfs_create_dev()</code> 5. Return unreferenced device pointer</p> <p>The returned <code>cdev_t</code> is an ad-hoc reference. Callers who store it long-term must call <code>reference_dev()</code>.</p> <p>See <code>sys/kern/kern_conf.c:188-211</code>.</p>"},{"location":"sys/kern/devices/#make_dev_covering","title":"make_dev_covering","text":"<p>Creates a device that layers over another device:</p> <pre><code>cdev_t make_dev_covering(struct dev_ops *ops, struct dev_ops *bops,\n                         int minor, uid_t uid, gid_t gid, int perms,\n                         const char *fmt, ...);\n</code></pre> <p>Used by disk label code to create partition devices that cover the base disk device.</p> <p>See <code>sys/kern/kern_conf.c:218-241</code>.</p>"},{"location":"sys/kern/devices/#make_only_dev","title":"make_only_dev","text":"<p>Creates a device without a devfs entry (internal use):</p> <pre><code>cdev_t make_only_dev(struct dev_ops *ops, int minor,\n                     uid_t uid, gid_t gid, int perms,\n                     const char *fmt, ...);\n</code></pre> <p>Unlike <code>make_dev()</code>, this returns a referenced device.</p> <p>See <code>sys/kern/kern_conf.c:270-296</code>.</p>"},{"location":"sys/kern/devices/#make_autoclone_dev","title":"make_autoclone_dev","text":"<p>Creates an auto-cloning device:</p> <pre><code>cdev_t make_autoclone_dev(struct dev_ops *ops, struct devfs_bitmap *bitmap,\n                          d_clone_t *nhandler, uid_t uid, gid_t gid,\n                          int perms, const char *fmt, ...);\n</code></pre> <p>Operation: 1. Initialize clone bitmap (if provided) 2. Register clone handler with devfs 3. Create base device covering <code>default_dev_ops</code> 4. Clone handler called on-demand for new instances</p> <p>Used for devices like <code>/dev/pty*</code> that create instances dynamically.</p> <p>See <code>sys/kern/kern_conf.c:407-427</code>.</p>"},{"location":"sys/kern/devices/#device-destruction","title":"Device Destruction","text":""},{"location":"sys/kern/devices/#destroy_dev","title":"destroy_dev","text":"<p>Destroys a device and revectors its ops to <code>dead_dev_ops</code>:</p> <pre><code>void destroy_dev(cdev_t dev);\n</code></pre> <p>Important: The caller must hold a reference to the device. The ad-hoc reference from <code>make_dev()</code> is not sufficient:</p> <pre><code>/* Wrong: */\ndestroy_dev(make_dev(...));\n\n/* Correct: */\ncdev_t dev = make_dev(...);\nreference_dev(dev);\n/* ... use device ... */\ndestroy_dev(dev);  /* releases caller's reference + ad-hoc reference */\n</code></pre> <p>See <code>sys/kern/kern_conf.c:345-354</code>.</p>"},{"location":"sys/kern/devices/#sync_devs","title":"sync_devs","text":"<p>Synchronizes asynchronous disk and devfs operations:</p> <pre><code>void sync_devs(void);\n</code></pre> <p>Called before mountroot and on module unload to ensure all devices are fully probed and ops structures dereferenced.</p> <p>See <code>sys/kern/kern_conf.c:364-371</code>.</p>"},{"location":"sys/kern/devices/#device-aliases","title":"Device Aliases","text":""},{"location":"sys/kern/devices/#make_dev_alias","title":"make_dev_alias","text":"<p>Creates a symbolic alias for an existing device:</p> <pre><code>int make_dev_alias(cdev_t target, const char *fmt, ...);\n</code></pre> <p>See <code>sys/kern/kern_conf.c:373-387</code>.</p>"},{"location":"sys/kern/devices/#destroy_dev_alias","title":"destroy_dev_alias","text":"<p>Removes a device alias:</p> <pre><code>int destroy_dev_alias(cdev_t target, const char *fmt, ...);\n</code></pre> <p>See <code>sys/kern/kern_conf.c:389-403</code>.</p>"},{"location":"sys/kern/devices/#reference-counting","title":"Reference Counting","text":"<p>Devices use sysref-based reference counting:</p>"},{"location":"sys/kern/devices/#reference_dev","title":"reference_dev","text":"<p>Adds a reference to a device:</p> <pre><code>cdev_t reference_dev(cdev_t dev);\n</code></pre> <p>Returns the device pointer for convenience. Callers storing device pointers long-term should call this to prevent premature destruction.</p> <p>See <code>sys/kern/kern_conf.c:453-467</code>.</p>"},{"location":"sys/kern/devices/#release_dev","title":"release_dev","text":"<p>Releases a device reference:</p> <pre><code>void release_dev(cdev_t dev);\n</code></pre> <p>The device is freed when the last reference is released.</p> <p>See <code>sys/kern/kern_conf.c:476-484</code>.</p>"},{"location":"sys/kern/devices/#device-operations-dispatch","title":"Device Operations Dispatch","text":"<p>The <code>dev_d*()</code> functions in <code>kern_device.c</code> dispatch operations to drivers while handling MPSAFE and KVABIO requirements.</p>"},{"location":"sys/kern/devices/#mpsafe-handling","title":"MPSAFE Handling","text":"<p>Each dispatch function checks the <code>D_MPSAFE</code> flag:</p> <pre><code>static __inline int\ndev_needmplock(cdev_t dev)\n{\n    return ((dev-&gt;si_ops-&gt;head.flags &amp; D_MPSAFE) == 0);\n}\n</code></pre> <p>If the device is not MPSAFE, the dispatch function acquires/releases the big kernel lock:</p> <pre><code>if (needmplock)\n    get_mplock();\nerror = dev-&gt;si_ops-&gt;d_open(&amp;ap);\nif (needmplock)\n    rel_mplock();\n</code></pre> <p>See <code>sys/kern/kern_device.c:117-122</code>.</p>"},{"location":"sys/kern/devices/#kvabio-handling","title":"KVABIO Handling","text":"<p>For strategy operations, if the device doesn't support KVABIO but the buffer uses it, data is synchronized to all CPUs:</p> <pre><code>if (dev_nokvabio(dev) &amp;&amp; (bp-&gt;b_flags &amp; B_KVABIO))\n    bkvasync_all(bp);\n</code></pre> <p>See <code>sys/kern/kern_device.c:366-367</code>.</p>"},{"location":"sys/kern/devices/#dev_dopen","title":"dev_dopen","text":"<p>Opens a device:</p> <pre><code>int dev_dopen(cdev_t dev, int oflags, int devtype,\n              struct ucred *cred, struct file **fpp, struct vnode *vp);\n</code></pre> <p>The <code>fpp</code> parameter allows the driver to replace the file pointer during open (used by some devices for per-open state).</p> <p>See <code>sys/kern/kern_device.c:137-168</code>.</p>"},{"location":"sys/kern/devices/#dev_dstrategy","title":"dev_dstrategy","text":"<p>Issues I/O to a device:</p> <pre><code>void dev_dstrategy(cdev_t dev, struct bio *bio);\n</code></pre> <p>Operation: 1. Handle KVABIO synchronization if needed 2. Select read or write tracking based on <code>bio-&gt;bio_buf-&gt;b_cmd</code> 3. Reference the appropriate <code>bio_track</code> 4. Call <code>dsched_buf_enter()</code> for disk scheduling 5. Dispatch to driver's <code>d_strategy</code></p> <p>The BIO tracking allows <code>sync_devs()</code> to wait for outstanding I/O.</p> <p>See <code>sys/kern/kern_device.c:354-389</code>.</p>"},{"location":"sys/kern/devices/#dev_dstrategy_chain","title":"dev_dstrategy_chain","text":"<p>Chained strategy call (reuses existing BIO setup):</p> <pre><code>void dev_dstrategy_chain(cdev_t dev, struct bio *bio);\n</code></pre> <p>Used when forwarding I/O through device layers. Unlike <code>dev_dstrategy()</code>, it doesn't add new tracking.</p> <p>See <code>sys/kern/kern_device.c:391-416</code>.</p>"},{"location":"sys/kern/devices/#dev_dpsize","title":"dev_dpsize","text":"<p>Gets device/partition size:</p> <pre><code>int64_t dev_dpsize(cdev_t dev);\n</code></pre> <p>Returns the size in device blocks, or -1 on error.</p> <p>See <code>sys/kern/kern_device.c:448-467</code>.</p>"},{"location":"sys/kern/devices/#operation-compilation","title":"Operation Compilation","text":""},{"location":"sys/kern/devices/#compile_dev_ops","title":"compile_dev_ops","text":"<p>Fills in NULL operation pointers with defaults:</p> <pre><code>void compile_dev_ops(struct dev_ops *ops);\n</code></pre> <p>For each NULL function pointer: - If <code>d_default</code> is set, use that - Otherwise, use the corresponding function from <code>default_dev_ops</code></p> <p>Called automatically by <code>make_dev()</code> and related functions.</p> <p>See <code>sys/kern/kern_device.c:588-606</code>.</p>"},{"location":"sys/kern/devices/#default_dev_ops","title":"default_dev_ops","text":"<p>Default operations that return <code>ENODEV</code> for most calls:</p> <pre><code>struct dev_ops default_dev_ops = {\n    { \"null\" },\n    .d_default = NULL,\n    .d_open = noopen,      /* returns ENODEV */\n    .d_close = noclose,    /* returns ENODEV */\n    .d_read = noread,      /* returns ENODEV */\n    ...\n};\n</code></pre> <p>See <code>sys/kern/kern_device.c:99-115</code>.</p>"},{"location":"sys/kern/devices/#dead_dev_ops","title":"dead_dev_ops","text":"<p>Operations for destroyed devices. When a device is destroyed, its <code>si_ops</code> is revectored to point here.</p> <p>See <code>sys/kern/kern_device.c:83</code>.</p>"},{"location":"sys/kern/devices/#operation-interception","title":"Operation Interception","text":""},{"location":"sys/kern/devices/#dev_ops_intercept","title":"dev_ops_intercept","text":"<p>Intercepts device operations (used by console code):</p> <pre><code>struct dev_ops *dev_ops_intercept(cdev_t dev, struct dev_ops *iops);\n</code></pre> <p>Operation: 1. Save original ops 2. Copy major, data, and flags to interceptor ops 3. Replace device's ops with interceptor 4. Set <code>SI_INTERCEPTED</code> flag 5. Return original ops</p> <p>See <code>sys/kern/kern_device.c:654-667</code>.</p>"},{"location":"sys/kern/devices/#dev_ops_restore","title":"dev_ops_restore","text":"<p>Restores original operations after interception:</p> <pre><code>void dev_ops_restore(cdev_t dev, struct dev_ops *oops);\n</code></pre> <p>See <code>sys/kern/kern_device.c:669-679</code>.</p>"},{"location":"sys/kern/devices/#major-number-management","title":"Major Number Management","text":"<p>Major numbers are tracked in a red-black tree for efficient lookup:</p> <pre><code>struct dev_ops_rb_tree dev_ops_rbhead;\n</code></pre> <p>The tree maps major numbers to <code>struct dev_ops_maj</code> entries, which link to the associated <code>dev_ops</code> structures.</p> <p>See <code>sys/kern/kern_device.c:638-640</code>.</p>"},{"location":"sys/kern/devices/#device-number-primitives","title":"Device Number Primitives","text":""},{"location":"sys/kern/devices/#major-minor","title":"major / minor","text":"<p>Extract major/minor numbers from a device:</p> <pre><code>int major(cdev_t dev);\nint minor(cdev_t dev);\n</code></pre> <p>Note: Major number comes from <code>si_umajor</code>, not <code>si_ops</code>, because <code>si_ops</code> may be replaced when a device is destroyed.</p> <p>See <code>sys/kern/kern_conf.c:61-75</code>.</p>"},{"location":"sys/kern/devices/#devid_from_dev-dev_from_devid","title":"devid_from_dev / dev_from_devid","text":"<p>Convert between <code>cdev_t</code> and old-style <code>dev_t</code>:</p> <pre><code>dev_t devid_from_dev(cdev_t dev);\ncdev_t dev_from_devid(dev_t x, int b);\n</code></pre> <p>The <code>dev_from_devid()</code> function looks up the device through devfs.</p> <p>See <code>sys/kern/kern_conf.c:99-124</code>.</p>"},{"location":"sys/kern/devices/#dev_is_good","title":"dev_is_good","text":"<p>Check if a device is valid (not dead):</p> <pre><code>int dev_is_good(cdev_t dev);\n</code></pre> <p>Returns 1 if the device exists and its ops is not <code>dead_dev_ops</code>.</p> <p>See <code>sys/kern/kern_conf.c:126-132</code>.</p>"},{"location":"sys/kern/devices/#helper-functions","title":"Helper Functions","text":""},{"location":"sys/kern/devices/#devtoname","title":"devtoname","text":"<p>Gets the device name:</p> <pre><code>const char *devtoname(cdev_t dev);\n</code></pre> <p>Returns the device name, or constructs one from major/minor if no name is set.</p> <p>See <code>sys/kern/kern_conf.c:486-512</code>.</p>"},{"location":"sys/kern/devices/#dev_dname-dev_dflags-dev_dmaj","title":"dev_dname / dev_dflags / dev_dmaj","text":"<p>Quick accessors for device properties:</p> <pre><code>const char *dev_dname(cdev_t dev);   /* ops-&gt;head.name */\nint dev_dflags(cdev_t dev);          /* ops-&gt;head.flags */\nint dev_dmaj(cdev_t dev);            /* ops-&gt;head.maj */\n</code></pre> <p>See <code>sys/kern/kern_device.c:515-537</code>.</p>"},{"location":"sys/kern/devices/#dev_drefs","title":"dev_drefs","text":"<p>Gets the reference count:</p> <pre><code>int dev_drefs(cdev_t dev);\n</code></pre> <p>See <code>sys/kern/kern_device.c:506-510</code>.</p>"},{"location":"sys/kern/devices/#debugging","title":"Debugging","text":""},{"location":"sys/kern/devices/#debugdev_refs","title":"debug.dev_refs","text":"<p>Sysctl to enable device reference debugging:</p> <pre><code>sysctl debug.dev_refs=2\n</code></pre> <p>When set to 2, prints reference/release messages with device name and reference count.</p> <p>See <code>sys/kern/kern_conf.c:52-54</code>.</p>"},{"location":"sys/kern/devices/#example-simple-character-device","title":"Example: Simple Character Device","text":"<pre><code>static d_open_t     mydev_open;\nstatic d_close_t    mydev_close;\nstatic d_read_t     mydev_read;\nstatic d_write_t    mydev_write;\n\nstatic struct dev_ops mydev_ops = {\n    { \"mydev\", 0, D_MPSAFE },\n    .d_open =   mydev_open,\n    .d_close =  mydev_close,\n    .d_read =   mydev_read,\n    .d_write =  mydev_write,\n};\n\nstatic int\nmydev_open(struct dev_open_args *ap)\n{\n    /* ap-&gt;a_head.a_dev is the device */\n    /* ap-&gt;a_oflags has open flags */\n    /* ap-&gt;a_cred has credentials */\n    return 0;\n}\n\nstatic int\nmydev_read(struct dev_read_args *ap)\n{\n    return uiomove(data, len, ap-&gt;a_uio);\n}\n\n/* Module init */\nstatic int\nmydev_modevent(module_t mod, int type, void *data)\n{\n    static cdev_t dev;\n\n    switch (type) {\n    case MOD_LOAD:\n        dev = make_dev(&amp;mydev_ops, 0, UID_ROOT, GID_WHEEL,\n                       0600, \"mydev\");\n        reference_dev(dev);\n        break;\n    case MOD_UNLOAD:\n        destroy_dev(dev);\n        break;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/devices/#cross-references","title":"Cross-References","text":"<ul> <li>NewBus Framework - Device/driver attachment model</li> <li>Disk Layer - Block device and partition handling</li> <li>Buffer Cache - BIO and buffer management</li> <li>LWKT - Threading and MPSAFE concepts</li> </ul>"},{"location":"sys/kern/disk/","title":"Disk Subsystem","text":"<p>The disk subsystem provides a unified framework for managing block storage devices, including disk registration, slice/partition handling, and I/O routing. It bridges device drivers with the filesystem layer through a multi-level abstraction: whole disk, slices (MBR/GPT partitions), and BSD partitions (within slices).</p> <p>Source files: - <code>sys/kern/subr_disk.c</code> - Core disk management (~1,600 lines) - <code>sys/kern/subr_diskslice.c</code> - Slice management (~900 lines) - <code>sys/kern/subr_diskmbr.c</code> - MBR parsing (~560 lines) - <code>sys/kern/subr_diskgpt.c</code> - GPT parsing (~240 lines) - <code>sys/kern/subr_disklabel32.c</code> - BSD 32-bit disklabel (~660 lines) - <code>sys/kern/subr_disklabel64.c</code> - BSD 64-bit disklabel (~540 lines)</p> <p>Header files: - <code>sys/sys/disk.h</code> - Disk structures - <code>sys/sys/diskslice.h</code> - Slice structures and helpers</p>"},{"location":"sys/kern/disk/#architecture-overview","title":"Architecture Overview","text":"<p>The disk subsystem implements a three-level hierarchy:</p> <pre><code>                 +------------------+\n                 |  Whole Disk      |  (e.g., da0)\n                 |  WHOLE_DISK_SLICE|\n                 +--------+---------+\n                          |\n          +---------------+---------------+\n          |               |               |\n    +-----+-----+   +-----+-----+   +-----+-----+\n    |  Slice 0  |   |  Slice 1  |   |  Slice N  |\n    | (s0/compat)|   |  (s1)     |   |  (sN)     |\n    +-----+-----+   +-----+-----+   +-----------+\n          |               |\n    +-----+-----+   +-----+-----+\n    |Partitions |   |Partitions |\n    | (a-p)     |   | (a-p)     |\n    +-----------+   +-----------+\n</code></pre>"},{"location":"sys/kern/disk/#device-naming","title":"Device Naming","text":"<p>Disk devices follow a hierarchical naming scheme:</p> Device Description <code>da0</code> Whole disk (raw, no interpretation) <code>da0s0</code> Compatibility slice (MBR) or GPT s0 <code>da0s1</code> First slice (MBR partition 1) <code>da0s1a</code> Partition 'a' within slice 1 <code>da0s1c</code> Raw slice 1 (whole slice)"},{"location":"sys/kern/disk/#minor-number-encoding","title":"Minor Number Encoding","text":"<p>Minor numbers encode unit, slice, and partition:</p> <pre><code>/* sys/sys/diskslice.h:240-256 */\nstatic __inline u_int32_t\ndkmakeminor(u_int32_t unit, u_int32_t slice, u_int32_t part)\n{\n    u_int32_t val;\n    val = ((unit &amp; 0x001f) &lt;&lt; 3) | ((unit &amp; 0x01e0) &lt;&lt; 16) |\n          ((slice &amp; 0x000f) &lt;&lt; 16) | ((slice &amp; 0x0070) &lt;&lt; 25) |\n          (part &amp; 0x0007) | ((part &amp; 0x0008) &lt;&lt; 17) |\n          ((part &amp; 0x00F0) &lt;&lt; 21);\n    return(val);\n}\n</code></pre> <p>Bit layout (32 bits): <pre><code>| SL2 | PART3 |UNIT_2 |P| SLICE |  MAJOR?  |  UNIT   |PART |\n</code></pre></p>"},{"location":"sys/kern/disk/#core-data-structures","title":"Core Data Structures","text":""},{"location":"sys/kern/disk/#struct-disk","title":"struct disk","text":"<p>The primary structure representing a disk device:</p> <pre><code>/* sys/sys/disk.h:132-148 */\nstruct disk {\n    struct dev_ops      *d_dev_ops;     /* disk layer dev ops */\n    struct dev_ops      *d_raw_ops;     /* raw device driver ops */\n    u_int               d_flags;\n    int                 d_opencount;    /* current open count */\n    cdev_t              d_rawdev;       /* backing raw device */\n    cdev_t              d_cdev;         /* whole-disk device */\n    struct diskslices   *d_slice;       /* slice table */\n    struct disk_info    d_info;         /* media parameters */\n    const char          *d_disktype;    /* disk type string */\n    LIST_ENTRY(disk)    d_list;         /* global disk list */\n    kdmsg_iocom_t       d_iocom;        /* cluster import/export */\n    int                 d_refs;         /* destruction interlock */\n};\n</code></pre> <p>Flags: - <code>DISKFLAG_LOCK</code> - Disk operations locked - <code>DISKFLAG_WANTED</code> - Someone waiting for lock - <code>DISKFLAG_MARKER</code> - Enumeration marker (not a real disk)</p>"},{"location":"sys/kern/disk/#struct-disk_info","title":"struct disk_info","text":"<p>Media parameters provided by the device driver:</p> <pre><code>/* sys/sys/disk.h:70-99 */\nstruct disk_info {\n    u_int64_t   d_media_size;       /* media size in bytes */\n    u_int64_t   d_media_blocks;     /* media size in sectors */\n    int         d_media_blksize;    /* sector size (bytes) */\n    u_int       d_dsflags;          /* management flags */\n\n    /* Optional geometry (legacy CHS) */\n    u_int       d_type;             /* DTYPE_xxx */\n    u_int       d_nheads;\n    u_int       d_ncylinders;\n    u_int       d_secpertrack;\n    u_int       d_secpercyl;\n    u_int       d_trimflag;\n    char        *d_serialno;        /* serial number string */\n};\n</code></pre> <p>DSO flags (<code>d_dsflags</code>):</p> Flag Value Description <code>DSO_NOLABELS</code> 0x0001 Don't probe for labels <code>DSO_ONESLICE</code> 0x0002 Single slice only <code>DSO_COMPATLABEL</code> 0x0004 Create compatibility label if none <code>DSO_COMPATPARTA</code> 0x0008 Create partition 'a' in compat label <code>DSO_RAWEXTENSIONS</code> 0x0020 Allow raw partition extensions <code>DSO_MBRQUIET</code> 0x0040 Silent MBR probe failures <code>DSO_DEVICEMAPPER</code> 0x0080 Device mapper (use \".\" separator) <code>DSO_RAWPSIZE</code> 0x0100 Use raw device psize on failure"},{"location":"sys/kern/disk/#struct-diskslices","title":"struct diskslices","text":"<p>Container for all slices on a disk:</p> <pre><code>/* sys/sys/diskslice.h:167-177 */\nstruct diskslices {\n    struct cdevsw   *dss_cdevsw;        /* containing device switch */\n    int             dss_first_bsd_slice; /* COMPATIBILITY_SLICE mapped here */\n    u_int           dss_nslices;        /* actual number of slices */\n    u_int           dss_oflags;         /* flags for first open */\n    int             dss_secmult;        /* block-to-sector multiplier */\n    int             dss_secshift;       /* block-to-sector shift (-1 if N/A) */\n    int             dss_secsize;        /* sector size */\n    struct diskslice dss_slices[MAX_SLICES];\n};\n</code></pre>"},{"location":"sys/kern/disk/#struct-diskslice","title":"struct diskslice","text":"<p>Individual slice (MBR partition or GPT entry):</p> <pre><code>/* sys/sys/diskslice.h:142-163 */\nstruct diskslice {\n    cdev_t          ds_dev;             /* device for this slice */\n    u_int64_t       ds_offset;          /* starting sector */\n    u_int64_t       ds_size;            /* number of sectors */\n    u_int32_t       ds_reserved;        /* reserved sectors (label area) */\n    struct uuid     ds_type_uuid;       /* slice type UUID (GPT) */\n    struct uuid     ds_stor_uuid;       /* storage UUID (GPT) */\n    int             ds_type;            /* MBR partition type */\n    int             ds_flags;           /* DSF_ flags */\n    disklabel_t     ds_label;           /* BSD label (if present) */\n    struct disklabel_ops *ds_ops;       /* label operations */\n    void            *ds_devs[MAXPARTITIONS];\n    u_int32_t       ds_openmask[DKMAXPARTITIONS/32];\n    u_char          ds_wlabel;          /* label writable flag */\n    int             ds_ttlopens;        /* total opens */\n};\n</code></pre> <p>Special slices:</p> Constant Value Description <code>COMPATIBILITY_SLICE</code> 0 Compatibility (s0) <code>WHOLE_DISK_SLICE</code> 1 Entire disk <code>BASE_SLICE</code> 2 First real slice (s1) <code>WHOLE_SLICE_PART</code> 255 Entire slice partition"},{"location":"sys/kern/disk/#disk-registration-and-lifecycle","title":"Disk Registration and Lifecycle","text":""},{"location":"sys/kern/disk/#creating-a-disk","title":"Creating a Disk","text":"<p>Device drivers register disks using <code>disk_create()</code>:</p> <pre><code>/* sys/kern/subr_disk.c:664-765 */\ncdev_t\ndisk_create(int unit, struct disk *dp, struct dev_ops *raw_ops)\n{\n    return _disk_create_named(NULL, unit, dp, raw_ops, 0);\n}\n\nstatic cdev_t\n_disk_create_named(const char *name, int unit, struct disk *dp,\n                   struct dev_ops *raw_ops, int clone)\n{\n    cdev_t rawdev;\n    struct dev_ops *dops;\n\n    /* Create raw device (no slice/partition interpretation) */\n    if (name) {\n        rawdev = make_only_dev(raw_ops, dkmakewholedisk(unit),\n                               UID_ROOT, GID_OPERATOR, 0640, \"%s\", name);\n    } else {\n        rawdev = make_only_dev(raw_ops, dkmakewholedisk(unit),\n                               UID_ROOT, GID_OPERATOR, 0640,\n                               \"%s%d\", raw_ops-&gt;head.name, unit);\n    }\n\n    bzero(dp, sizeof(*dp));\n\n    /* Select disk ops based on D_NOEMERGPGR flag */\n    dops = (raw_ops-&gt;head.flags &amp; D_NOEMERGPGR) ? &amp;disk2_ops : &amp;disk1_ops;\n\n    dp-&gt;d_rawdev = rawdev;\n    dp-&gt;d_raw_ops = raw_ops;\n    dp-&gt;d_dev_ops = dops;\n\n    /* Create covering device for slice/partition management */\n    if (clone) {\n        dp-&gt;d_cdev = make_only_dev_covering(...);\n    } else {\n        dp-&gt;d_cdev = make_dev_covering(...);\n    }\n\n    dp-&gt;d_cdev-&gt;si_disk = dp;\n\n    /* Initialize I/O scheduling */\n    dsched_disk_create(dp, name, unit);\n\n    /* Add to global disk list */\n    lwkt_gettoken(&amp;disklist_token);\n    LIST_INSERT_HEAD(&amp;disklist, dp, d_list);\n    lwkt_reltoken(&amp;disklist_token);\n\n    /* Initialize cluster support */\n    disk_iocom_init(dp);\n\n    return dp-&gt;d_rawdev;\n}\n</code></pre>"},{"location":"sys/kern/disk/#setting-disk-information","title":"Setting Disk Information","text":"<p>After creating the disk, drivers provide media parameters:</p> <pre><code>/* sys/kern/subr_disk.c:853-869 */\nvoid\ndisk_setdiskinfo(struct disk *disk, struct disk_info *info)\n{\n    _setdiskinfo(disk, info);\n    disk_msg_send(DISK_DISK_PROBE, disk, NULL);\n}\n\nstatic void\n_setdiskinfo(struct disk *disk, struct disk_info *info)\n{\n    /* Copy info, duplicate serial number */\n    bcopy(info, &amp;disk-&gt;d_info, sizeof(disk-&gt;d_info));\n\n    if (info-&gt;d_serialno &amp;&amp; info-&gt;d_serialno[0]) {\n        info-&gt;d_serialno = kstrdup(info-&gt;d_serialno, M_TEMP);\n        disk_cleanserial(info-&gt;d_serialno);\n        make_dev_alias(disk-&gt;d_cdev, \"serno/%s\", info-&gt;d_serialno);\n    }\n\n    /* Calculate size from blocks or vice versa */\n    if (info-&gt;d_media_size == 0 &amp;&amp; info-&gt;d_media_blocks) {\n        info-&gt;d_media_size = (u_int64_t)info-&gt;d_media_blocks *\n                             info-&gt;d_media_blksize;\n    }\n\n    dsched_disk_update(disk, info);\n}\n</code></pre>"},{"location":"sys/kern/disk/#disk-message-thread","title":"Disk Message Thread","text":"<p>A dedicated kernel thread processes disk operations asynchronously:</p> <pre><code>/* sys/kern/subr_disk.c:500-601 */\nstatic void\ndisk_msg_core(void *arg)\n{\n    struct disk *dp;\n    struct diskslice *sp;\n    disk_msg_t msg;\n\n    lwkt_initport_thread(&amp;disk_msg_port, curthread);\n\n    while (run) {\n        msg = (disk_msg_t)lwkt_waitport(&amp;disk_msg_port, 0);\n\n        switch (msg-&gt;hdr.u.ms_result) {\n        case DISK_DISK_PROBE:\n            dp = (struct disk *)msg-&gt;load;\n            disk_iocom_update(dp);\n            disk_probe(dp, 0);\n            break;\n\n        case DISK_DISK_DESTROY:\n            dp = (struct disk *)msg-&gt;load;\n            disk_iocom_uninit(dp);\n\n            /* Wait for references to drain */\n            while (dp-&gt;d_refs)\n                tsleep(&amp;dp-&gt;d_refs, 0, \"diskdel\", hz / 10);\n            LIST_REMOVE(dp, d_list);\n\n            dsched_disk_destroy(dp);\n            devfs_destroy_related(dp-&gt;d_cdev);\n            destroy_dev(dp-&gt;d_cdev);\n            destroy_only_dev(dp-&gt;d_rawdev);\n            break;\n\n        case DISK_SLICE_REPROBE:\n            dp = (struct disk *)msg-&gt;load;\n            sp = (struct diskslice *)msg-&gt;load2;\n            disk_probe_slice(dp, sp-&gt;ds_dev, dkslice(sp-&gt;ds_dev), 1);\n            break;\n\n        case DISK_DISK_REPROBE:\n            dp = (struct disk *)msg-&gt;load;\n            disk_probe(dp, 1);\n            break;\n        }\n        lwkt_replymsg(&amp;msg-&gt;hdr, 0);\n    }\n}\n</code></pre> <p>Message types:</p> Message Description <code>DISK_DISK_PROBE</code> Initial disk probe (parse MBR/GPT) <code>DISK_DISK_DESTROY</code> Disk removal <code>DISK_SLICE_REPROBE</code> Re-probe single slice <code>DISK_DISK_REPROBE</code> Re-probe entire disk <code>DISK_UNPROBE</code> Remove slice devices <code>DISK_SYNC</code> Synchronization barrier"},{"location":"sys/kern/disk/#slice-and-partition-probing","title":"Slice and Partition Probing","text":""},{"location":"sys/kern/disk/#mbr-parsing","title":"MBR Parsing","text":"<p>MBR (Master Boot Record) parsing handles traditional PC partition tables:</p> <pre><code>/* sys/kern/subr_diskmbr.c:89-322 */\nint\nmbrinit(cdev_t dev, struct disk_info *info, struct diskslices **sspp)\n{\n    struct buf *bp;\n    struct dos_partition *dp;\n    struct dos_partition dpcopy[NDOSPART];\n\n    /* Read sector 0 */\n    bp = getpbuf_mem(NULL);\n    bp-&gt;b_bio1.bio_offset = (off_t)mbr_offset * info-&gt;d_media_blksize;\n    bp-&gt;b_bcount = info-&gt;d_media_blksize;\n    bp-&gt;b_cmd = BUF_CMD_READ;\n    dev_dstrategy(wdev, &amp;bp-&gt;b_bio1);\n\n    if (biowait(&amp;bp-&gt;b_bio1, \"mbrrd\") != 0)\n        return EIO;\n\n    /* Verify magic number */\n    cp = bp-&gt;b_data;\n    if (cp[0x1FE] != 0x55 || cp[0x1FF] != 0xAA)\n        return EINVAL;\n\n    /* Copy partition table */\n    memcpy(&amp;dpcopy[0], cp + DOSPARTOFF, sizeof(dpcopy));\n\n    /* Check for GPT (PMBR) */\n    for (dospart = 0, dp = dp0; dospart &lt; NDOSPART; dospart++, dp++) {\n        if (dospart == 0 &amp;&amp; dp-&gt;dp_typ == DOSPTYP_PMBR) {\n            return gptinit(dev, info, sspp);  /* Switch to GPT */\n        }\n        if (dp-&gt;dp_typ == DOSPTYP_ONTRACK) {\n            /* Ontrack Disk Manager - retry at sector 63 */\n            mbr_offset = 63;\n            goto reread_mbr;\n        }\n    }\n\n    /* Create slice structures */\n    ssp = dsmakeslicestruct(MAX_SLICES, info);\n    *sspp = ssp;\n\n    /* Initialize slices from MBR entries */\n    sp = &amp;ssp-&gt;dss_slices[BASE_SLICE];\n    for (dospart = 0; dospart &lt; NDOSPART; dospart++, dp++, sp++) {\n        mbr_setslice(sname, info, sp, dp, mbr_offset);\n    }\n    ssp-&gt;dss_nslices = BASE_SLICE + NDOSPART;\n\n    /* Handle extended partitions (recursive) */\n    for (dospart = 0; dospart &lt; NDOSPART; dospart++) {\n        if (sp-&gt;ds_type == DOSPTYP_EXT || sp-&gt;ds_type == DOSPTYP_EXTLBA) {\n            mbr_extended(wdev, info, ssp, sp-&gt;ds_offset, sp-&gt;ds_size,\n                         sp-&gt;ds_offset, max_nsectors, max_ntracks,\n                         mbr_offset, 1);\n        }\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/disk/#gpt-parsing","title":"GPT Parsing","text":"<p>GPT (GUID Partition Table) provides modern partition support:</p> <pre><code>/* sys/kern/subr_diskgpt.c:62-229 */\nint\ngptinit(cdev_t dev, struct disk_info *info, struct diskslices **sspp)\n{\n    struct gpt_hdr *gpt;\n    struct gpt_ent *ent;\n\n    /* Read GPT header at LBA 1 */\n    bp1-&gt;b_bio1.bio_offset = info-&gt;d_media_blksize;\n    bp1-&gt;b_bcount = info-&gt;d_media_blksize;\n    bp1-&gt;b_cmd = BUF_CMD_READ;\n    dev_dstrategy(wdev, &amp;bp1-&gt;b_bio1);\n\n    gpt = (void *)bp1-&gt;b_data;\n\n    /* Validate header */\n    len = le32toh(gpt-&gt;hdr_size);\n    if (len &lt; GPT_MIN_HDR_SIZE || len &gt; info-&gt;d_media_blksize)\n        return EINVAL;\n\n    /* Verify CRC32 */\n    crc = le32toh(gpt-&gt;hdr_crc_self);\n    gpt-&gt;hdr_crc_self = 0;\n    if (crc32(gpt, len) != crc)\n        return EINVAL;\n\n    /* Read partition entries */\n    entries = le32toh(gpt-&gt;hdr_entries);\n    entsz = le32toh(gpt-&gt;hdr_entsz);\n    table_lba = le32toh(gpt-&gt;hdr_lba_table);\n\n    bp2-&gt;b_bio1.bio_offset = (off_t)table_lba * info-&gt;d_media_blksize;\n    bp2-&gt;b_bcount = table_blocks * info-&gt;d_media_blksize;\n    dev_dstrategy(wdev, &amp;bp2-&gt;b_bio1);\n\n    /* Create slice structure for up to 128 partitions */\n    ssp = dsmakeslicestruct(BASE_SLICE + 128, info);\n    *sspp = ssp;\n\n    /* Process each GPT entry */\n    for (i = 0; i &lt; entries &amp;&amp; i &lt; 128; ++i) {\n        ent = (void *)((char *)bp2-&gt;b_data + i * entsz);\n\n        /* Convert from little-endian */\n        le_uuid_dec(&amp;ent-&gt;ent_type, &amp;sent.ent_type);\n        le_uuid_dec(&amp;ent-&gt;ent_uuid, &amp;sent.ent_uuid);\n        sent.ent_lba_start = le64toh(ent-&gt;ent_lba_start);\n        sent.ent_lba_end = le64toh(ent-&gt;ent_lba_end);\n\n        if (kuuid_is_nil(&amp;sent.ent_type))\n            continue;\n\n        /* GPT entry 0 -&gt; COMPATIBILITY_SLICE, others -&gt; BASE_SLICE+i-1 */\n        if (i == 0)\n            sp = &amp;ssp-&gt;dss_slices[COMPATIBILITY_SLICE];\n        else\n            sp = &amp;ssp-&gt;dss_slices[BASE_SLICE + i - 1];\n\n        gpt_setslice(sname, info, sp, &amp;sent);\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/disk/#disklabel-probing","title":"Disklabel Probing","text":"<p>BSD disklabels are probed within slices:</p> <pre><code>/* sys/kern/subr_disk.c:184-340 */\nstatic int\ndisk_probe_slice(struct disk *dp, cdev_t dev, int slice, int reprobe)\n{\n    struct disk_info *info = &amp;dp-&gt;d_info;\n    struct diskslice *sp = &amp;dp-&gt;d_slice-&gt;dss_slices[slice];\n    disklabel_ops_t ops;\n    const char *msg;\n\n    /* Try 32-bit disklabel first */\n    ops = &amp;disklabel32_ops;\n    msg = ops-&gt;op_readdisklabel(dev, sp, &amp;sp-&gt;ds_label, info);\n\n    /* If not found, try 64-bit disklabel */\n    if (msg &amp;&amp; !strcmp(msg, \"no disk label\")) {\n        ops = &amp;disklabel64_ops;\n        msg = ops-&gt;op_readdisklabel(dev, sp, &amp;sp-&gt;ds_label, info);\n    }\n\n    if (msg == NULL) {\n        /* Label found - create partition devices */\n        sp-&gt;ds_ops = ops;\n        for (i = 0; i &lt; ops-&gt;op_getnumparts(sp-&gt;ds_label); i++) {\n            ops-&gt;op_loadpartinfo(sp-&gt;ds_label, i, &amp;part);\n            if (part.fstype) {\n                ndev = make_dev_covering(dops, dp-&gt;d_rawdev-&gt;si_ops,\n                        dkmakeminor(dkunit(dp-&gt;d_cdev), slice, i),\n                        UID_ROOT, GID_OPERATOR, 0640,\n                        \"%s%c\", dev-&gt;si_name, 'a' + i);\n                ndev-&gt;si_disk = dp;\n\n                /* Create UUID alias if available */\n                if (!kuuid_is_nil(&amp;part.storage_uuid)) {\n                    snprintf_uuid(uuid_buf, sizeof(uuid_buf),\n                                  &amp;part.storage_uuid);\n                    make_dev_alias(ndev, \"part-by-uuid/%s\", uuid_buf);\n                }\n            }\n        }\n    } else if (info-&gt;d_dsflags &amp; DSO_COMPATLABEL) {\n        /* Create compatibility label */\n        sp-&gt;ds_label = ops-&gt;op_clone_label(info, sp);\n    }\n\n    return (msg ? EINVAL : 0);\n}\n</code></pre>"},{"location":"sys/kern/disk/#disklabel-operations","title":"Disklabel Operations","text":""},{"location":"sys/kern/disk/#disklabel-operations-structure","title":"Disklabel Operations Structure","text":"<p>Both 32-bit and 64-bit disklabels use a common operations interface:</p> <pre><code>/* sys/sys/disklabel.h (conceptual) */\nstruct disklabel_ops {\n    int labelsize;\n\n    const char *(*op_readdisklabel)(cdev_t dev, struct diskslice *sp,\n                                    disklabel_t *lpp, struct disk_info *info);\n    int (*op_setdisklabel)(disklabel_t olp, disklabel_t nlp,\n                           struct diskslices *ssp, struct diskslice *sp,\n                           u_int32_t *openmask);\n    int (*op_writedisklabel)(cdev_t dev, struct diskslices *ssp,\n                             struct diskslice *sp, disklabel_t lp);\n    disklabel_t (*op_clone_label)(struct disk_info *info, struct diskslice *sp);\n    void (*op_adjust_label_reserved)(struct diskslices *ssp, int slice,\n                                     struct diskslice *sp);\n    int (*op_getpartbounds)(struct diskslices *ssp, disklabel_t lp,\n                            u_int32_t part, u_int64_t *start, u_int64_t *blocks);\n    void (*op_loadpartinfo)(disklabel_t lp, u_int32_t part,\n                            struct partinfo *dpart);\n    u_int32_t (*op_getnumparts)(disklabel_t lp);\n    void (*op_makevirginlabel)(disklabel_t lp, struct diskslices *ssp,\n                               struct diskslice *sp, struct disk_info *info);\n    int (*op_getpackname)(disklabel_t lp, char *buf, size_t bytes);\n    void (*op_freedisklabel)(disklabel_t *lpp);\n};\n</code></pre>"},{"location":"sys/kern/disk/#32-bit-disklabel","title":"32-bit Disklabel","text":"<p>Traditional BSD disklabel format (up to 2TB):</p> <pre><code>/* sys/kern/subr_disklabel32.c:647-660 */\nstruct disklabel_ops disklabel32_ops = {\n    .labelsize = sizeof(struct disklabel32),\n    .op_readdisklabel = l32_readdisklabel,\n    .op_setdisklabel = l32_setdisklabel,\n    .op_writedisklabel = l32_writedisklabel,\n    .op_clone_label = l32_clone_label,\n    .op_adjust_label_reserved = l32_adjust_label_reserved,\n    .op_getpartbounds = l32_getpartbounds,\n    .op_loadpartinfo = l32_loadpartinfo,\n    .op_getnumparts = l32_getnumparts,\n    .op_makevirginlabel = l32_makevirginlabel,\n    .op_getpackname = l32_getpackname,\n    .op_freedisklabel = l32_freedisklabel\n};\n</code></pre> <p>32-bit label location: Sector 1 (<code>LABELSECTOR32</code>)</p> <p>Key features: - Up to 16 partitions (a-p) - 32-bit sector addresses (2TB limit) - CHS geometry stored - Checksum-based integrity</p>"},{"location":"sys/kern/disk/#64-bit-disklabel","title":"64-bit Disklabel","text":"<p>DragonFly's native 64-bit disklabel format:</p> <pre><code>/* sys/kern/subr_disklabel64.c:529-542 */\nstruct disklabel_ops disklabel64_ops = {\n    .labelsize = sizeof(struct disklabel64),\n    .op_readdisklabel = l64_readdisklabel,\n    .op_setdisklabel = l64_setdisklabel,\n    .op_writedisklabel = l64_writedisklabel,\n    .op_clone_label = l64_clone_label,\n    .op_adjust_label_reserved = l64_adjust_label_reserved,\n    .op_getpartbounds = l64_getpartbounds,\n    .op_loadpartinfo = l64_loadpartinfo,\n    .op_getnumparts = l64_getnumparts,\n    .op_makevirginlabel = l64_makevirginlabel,\n    .op_getpackname = l64_getpackname,\n    .op_freedisklabel = l64_freedisklabel\n};\n</code></pre> <p>64-bit label location: Offset 0 (sector-agnostic)</p> <p>Key features: - Up to 16 partitions - 64-bit byte offsets (no practical size limit) - UUID-based partition identification - CRC32 integrity check - 1MB-aligned partitions by default - Reserved boot area (32KB)</p> <pre><code>/* sys/kern/subr_disklabel64.c:437-509 */\nstatic void\nl64_makevirginlabel(disklabel_t lpx, struct diskslices *ssp,\n                    struct diskslice *sp, struct disk_info *info)\n{\n    struct disklabel64 *lp = lpx.lab64;\n\n    /* Use 4KB minimum block size for alignment calculations */\n    if ((blksize = info-&gt;d_media_blksize) &lt; 4096)\n        blksize = 4096;\n\n    lp-&gt;d_magic = DISKMAGIC64;\n    lp-&gt;d_align = blksize;\n    lp-&gt;d_npartitions = MAXPARTITIONS64;\n    kern_uuidgen(&amp;lp-&gt;d_stor_uuid, 1);\n\n    /* Reserve space for label (rounded to block size) */\n    ressize = offsetof(struct disklabel64, d_partitions[RESPARTITIONS64]);\n    ressize = (ressize + blkmask) &amp; ~blkmask;\n\n    lp-&gt;d_bbase = ressize;                          /* boot area start */\n    lp-&gt;d_pbase = lp-&gt;d_bbase + BOOT2SIZE64;        /* partition area start */\n    lp-&gt;d_abase = lp-&gt;d_total_size - ressize;       /* backup label */\n\n    /* Align partition boundaries to 1MB (physical alignment) */\n    lp-&gt;d_pbase = ((doffset + lp-&gt;d_pbase + PALIGN_MASK) &amp;\n                   ~(uint64_t)PALIGN_MASK) - doffset;\n    lp-&gt;d_pstop = ((lp-&gt;d_abase - lp-&gt;d_pbase) &amp;\n                   ~(uint64_t)PALIGN_MASK) + lp-&gt;d_pbase;\n}\n</code></pre>"},{"location":"sys/kern/disk/#io-path","title":"I/O Path","text":""},{"location":"sys/kern/disk/#disk-device-operations","title":"Disk Device Operations","text":"<p>The disk layer interposes its own device operations:</p> <pre><code>/* sys/kern/subr_disk.c:136-159 */\nstatic struct dev_ops disk1_ops = {\n    { \"disk\", 0, D_DISK | D_MPSAFE | D_TRACKCLOSE | D_KVABIO },\n    .d_open = diskopen,\n    .d_close = diskclose,\n    .d_read = physread,\n    .d_write = physwrite,\n    .d_ioctl = diskioctl,\n    .d_strategy = diskstrategy,\n    .d_dump = diskdump,\n    .d_psize = diskpsize,\n};\n\n/* Variant without emergency pager */\nstatic struct dev_ops disk2_ops = {\n    { \"disk\", 0, D_DISK | D_MPSAFE | D_TRACKCLOSE | D_KVABIO | D_NOEMERGPGR },\n    /* ... same operations ... */\n};\n</code></pre>"},{"location":"sys/kern/disk/#strategy-routine","title":"Strategy Routine","text":"<p>The disk strategy routine translates slice-relative offsets:</p> <pre><code>/* sys/kern/subr_disk.c:1221-1253 */\nstatic int\ndiskstrategy(struct dev_strategy_args *ap)\n{\n    cdev_t dev = ap-&gt;a_head.a_dev;\n    struct bio *bio = ap-&gt;a_bio;\n    struct bio *nbio;\n    struct disk *dp;\n\n    dp = dev-&gt;si_disk;\n    if (dp == NULL) {\n        bio-&gt;bio_buf-&gt;b_error = ENXIO;\n        bio-&gt;bio_buf-&gt;b_flags |= B_ERROR;\n        biodone(bio);\n        return 0;\n    }\n\n    /*\n     * dscheck() transforms slice-relative offset to absolute offset.\n     * Returns NULL if I/O should not proceed (EOF, error, etc.)\n     */\n    if ((nbio = dscheck(dev, bio, dp-&gt;d_slice)) != NULL) {\n        dev_dstrategy(dp-&gt;d_rawdev, nbio);\n    } else {\n        biodone(bio);\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/disk/#slice-checking-and-translation","title":"Slice Checking and Translation","text":"<p><code>dscheck()</code> validates and translates I/O requests:</p> <pre><code>/* sys/kern/subr_diskslice.c:90-310 */\nstruct bio *\ndscheck(cdev_t dev, struct bio *bio, struct diskslices *ssp)\n{\n    struct buf *bp = bio-&gt;bio_buf;\n    struct diskslice *sp;\n    u_int64_t secno, endsecno, slicerel_secno;\n\n    slice = dkslice(dev);\n    part = dkpart(dev);\n    sp = &amp;ssp-&gt;dss_slices[slice];\n\n    /* Calculate sector number from byte offset */\n    if (ssp-&gt;dss_secshift != -1) {\n        secno = bio-&gt;bio_offset &gt;&gt; (DEV_BSHIFT + ssp-&gt;dss_secshift);\n        nsec = bp-&gt;b_bcount &gt;&gt; (DEV_BSHIFT + ssp-&gt;dss_secshift);\n    } else {\n        secno = bio-&gt;bio_offset / ssp-&gt;dss_secsize;\n        nsec = bp-&gt;b_bcount / ssp-&gt;dss_secsize;\n    }\n\n    /* Handle WHOLE_DISK_SLICE - no label interpretation */\n    if (slice == WHOLE_DISK_SLICE) {\n        if (part &gt;= 128 &amp;&amp; part != WHOLE_SLICE_PART) {\n            /* Encode partition in high bits for raw pass-through */\n            nbio = push_bio(bio);\n            nbio-&gt;bio_offset = bio-&gt;bio_offset | (u_int64_t)part &lt;&lt; 56;\n            return nbio;\n        }\n        endsecno = sp-&gt;ds_size;\n        slicerel_secno = secno;\n    }\n    /* Handle whole-slice partition */\n    else if (part == WHOLE_SLICE_PART) {\n        endsecno = sp-&gt;ds_size;\n        slicerel_secno = secno;\n    }\n    /* Handle labeled partition */\n    else if ((lp = sp-&gt;ds_label).opaque != NULL) {\n        ops = sp-&gt;ds_ops;\n        if (ops-&gt;op_getpartbounds(ssp, lp, part, &amp;slicerel_secno, &amp;endsecno))\n            goto bad;\n        slicerel_secno += secno;\n    }\n    else {\n        /* No label - can't access partition */\n        goto bad;\n    }\n\n    /* Check reserved area (label protection) */\n    if (slicerel_secno &lt; sp-&gt;ds_reserved &amp;&amp; nsec &amp;&amp;\n        bp-&gt;b_cmd == BUF_CMD_WRITE) {\n        if (sp-&gt;ds_wlabel == 0) {\n            bp-&gt;b_error = EROFS;\n            goto error;\n        }\n        sp-&gt;ds_flags |= DSF_REPROBE;\n    }\n\n    /* Handle EOF */\n    if (secno + nsec &gt; endsecno) {\n        if (secno &gt;= endsecno) {\n            bp-&gt;b_resid = bp-&gt;b_bcount;\n            bp-&gt;b_flags |= B_INVAL;\n            return NULL;\n        }\n        /* Truncate to fit */\n        nsec = endsecno - secno;\n        bp-&gt;b_bcount = nsec * ssp-&gt;dss_secsize;\n    }\n\n    /* Translate to absolute offset */\n    nbio = push_bio(bio);\n    nbio-&gt;bio_offset = (off_t)(sp-&gt;ds_offset + slicerel_secno) *\n                       ssp-&gt;dss_secsize;\n    return nbio;\n}\n</code></pre>"},{"location":"sys/kern/disk/#io-ordering","title":"I/O Ordering","text":""},{"location":"sys/kern/disk/#bio-queue-management","title":"BIO Queue Management","text":"<p>The disk layer provides I/O ordering for drive zone cache optimization:</p> <pre><code>/* sys/kern/subr_disk.c:1353-1409 */\nvoid\nbioqdisksort(struct bio_queue_head *bioq, struct bio *bio)\n{\n    switch (bio-&gt;bio_buf-&gt;b_cmd) {\n    case BUF_CMD_READ:\n        if (bioq-&gt;transition) {\n            /*\n             * Insert reads before the first write to prioritize reads.\n             * Periodically bleed writes through to prevent starvation.\n             */\n            TAILQ_INSERT_BEFORE(bioq-&gt;transition, bio, bio_act);\n            ++bioq-&gt;reorder;\n\n            /* Minor interval: trickle small writes */\n            if (bioq-&gt;reorder % bioq_reorder_minor_interval == 0) {\n                bioqwritereorder(bioq);\n                if (bioq-&gt;reorder &gt;= bioq_reorder_burst_interval) {\n                    bioq-&gt;reorder = 0;\n                }\n            }\n        } else {\n            /* No writes queued, append to tail */\n            TAILQ_INSERT_TAIL(&amp;bioq-&gt;queue, bio, bio_act);\n        }\n        break;\n\n    case BUF_CMD_WRITE:\n        /* Writes always append; track transition point */\n        TAILQ_INSERT_TAIL(&amp;bioq-&gt;queue, bio, bio_act);\n        if (bioq-&gt;transition == NULL)\n            bioq-&gt;transition = bio;\n        break;\n\n    default:\n        /* Other requests force ordering */\n        bioq_insert_tail(bioq, bio);\n        break;\n    }\n}\n</code></pre> <p>Reorder tuning parameters:</p> Sysctl Default Description <code>kern.bioq_reorder_burst_interval</code> 60 Burst interval (reads) <code>kern.bioq_reorder_minor_interval</code> 5 Minor bleed interval <code>kern.bioq_reorder_burst_bytes</code> 3000000 Bytes per burst <code>kern.bioq_reorder_minor_bytes</code> 262144 Bytes per minor bleed"},{"location":"sys/kern/disk/#disk-enumeration","title":"Disk Enumeration","text":"<p>Enumerate all registered disks safely:</p> <pre><code>/* sys/kern/subr_disk.c:967-1008 */\nstruct disk *\ndisk_enumerate(struct disk *marker, struct disk *dp)\n{\n    lwkt_gettoken(&amp;disklist_token);\n\n    if (dp) {\n        --dp-&gt;d_refs;\n        dp = LIST_NEXT(marker, d_list);\n        LIST_REMOVE(marker, d_list);\n    } else {\n        bzero(marker, sizeof(*marker));\n        marker-&gt;d_flags = DISKFLAG_MARKER;\n        dp = LIST_FIRST(&amp;disklist);\n    }\n\n    /* Skip markers */\n    while (dp) {\n        if ((dp-&gt;d_flags &amp; DISKFLAG_MARKER) == 0)\n            break;\n        dp = LIST_NEXT(dp, d_list);\n    }\n\n    if (dp) {\n        ++dp-&gt;d_refs;\n        LIST_INSERT_AFTER(dp, marker, d_list);\n    }\n\n    lwkt_reltoken(&amp;disklist_token);\n    return dp;\n}\n\nvoid\ndisk_enumerate_stop(struct disk *marker, struct disk *dp)\n{\n    lwkt_gettoken(&amp;disklist_token);\n    LIST_REMOVE(marker, d_list);\n    if (dp)\n        --dp-&gt;d_refs;\n    lwkt_reltoken(&amp;disklist_token);\n}\n</code></pre> <p>Usage pattern: <pre><code>struct disk marker;\nstruct disk *dp = NULL;\n\nwhile ((dp = disk_enumerate(&amp;marker, dp)) != NULL) {\n    /* Process disk */\n    if (some_condition) {\n        disk_enumerate_stop(&amp;marker, dp);\n        break;\n    }\n}\n</code></pre></p>"},{"location":"sys/kern/disk/#ioctls","title":"Ioctls","text":""},{"location":"sys/kern/disk/#diskslice-ioctls","title":"Disk/Slice Ioctls","text":"<pre><code>/* sys/kern/subr_diskslice.c:365-688 */\nint\ndsioctl(cdev_t dev, u_long cmd, caddr_t data, int flags,\n        struct diskslices **sspp, struct disk_info *info)\n{\n    switch (cmd) {\n    case DIOCGDVIRGIN32:\n    case DIOCGDVIRGIN64:\n        /* Get virgin (default) disklabel */\n        ops-&gt;op_makevirginlabel(lp, ssp, sp, info);\n        return 0;\n\n    case DIOCGDINFO32:\n    case DIOCGDINFO64:\n        /* Read disklabel from disk */\n        if (sp-&gt;ds_label.opaque == NULL)\n            error = dsreadandsetlabel(dev, info-&gt;d_dsflags, ssp, sp, info);\n        if (error == 0)\n            bcopy(sp-&gt;ds_label.opaque, data, ops-&gt;labelsize);\n        return error;\n\n    case DIOCGMEDIASIZE:\n        /* Get media size in bytes */\n        *(off_t *)data = (u_int64_t)sp-&gt;ds_size * info-&gt;d_media_blksize;\n        return 0;\n\n    case DIOCGSECTORSIZE:\n        /* Get sector size */\n        *(u_int *)data = info-&gt;d_media_blksize;\n        return 0;\n\n    case DIOCGPART:\n        /* Get partition info */\n        dpart-&gt;media_offset = (u_int64_t)sp-&gt;ds_offset * info-&gt;d_media_blksize;\n        dpart-&gt;media_size = (u_int64_t)sp-&gt;ds_size * info-&gt;d_media_blksize;\n        dpart-&gt;media_blocks = sp-&gt;ds_size;\n        dpart-&gt;media_blksize = info-&gt;d_media_blksize;\n        return 0;\n\n    case DIOCGSLICEINFO:\n        /* Get all slice info */\n        bcopy(ssp, data, ...);\n        return 0;\n\n    case DIOCSDINFO32:\n    case DIOCSDINFO64:\n        /* Set disklabel in memory */\n        error = ops-&gt;op_setdisklabel(lp, lptmp, ssp, sp, openmask);\n        return error;\n\n    case DIOCWDINFO32:\n    case DIOCWDINFO64:\n        /* Write disklabel to disk */\n        error = ops-&gt;op_writedisklabel(dev, ssp, sp, sp-&gt;ds_label);\n        return error;\n\n    case DIOCSYNCSLICEINFO:\n        /* Reprobe entire disk */\n        disk_msg_send_sync(DISK_DISK_REPROBE, dev-&gt;si_disk, NULL);\n        devfs_config();\n        return 0;\n\n    case DIOCWLABEL:\n        /* Enable/disable label writes */\n        set_ds_wlabel(ssp, slice, *(int *)data != 0);\n        return 0;\n    }\n}\n</code></pre>"},{"location":"sys/kern/disk/#kernel-dumps","title":"Kernel Dumps","text":""},{"location":"sys/kern/disk/#dump-configuration","title":"Dump Configuration","text":"<pre><code>/* sys/kern/subr_disk.c:915-940 */\nint\ndisk_dumpconf(cdev_t dev, u_int onoff)\n{\n    struct dumperinfo di;\n    u_int64_t size, blkno;\n    u_int32_t secsize;\n\n    if (!onoff)\n        return set_dumper(NULL);\n\n    error = disk_dumpcheck(dev, &amp;size, &amp;blkno, &amp;secsize);\n    if (error)\n        return ENXIO;\n\n    bzero(&amp;di, sizeof(di));\n    di.dumper = diskdump;\n    di.priv = dev;\n    di.blocksize = secsize;\n    di.maxiosize = dev-&gt;si_iosize_max;\n    di.mediaoffset = blkno * DEV_BSIZE;\n    di.mediasize = size * DEV_BSIZE;\n\n    return set_dumper(&amp;di);\n}\n</code></pre>"},{"location":"sys/kern/disk/#device-aliases","title":"Device Aliases","text":"<p>The disk layer creates convenient device aliases:</p> Alias Pattern Example Description <code>serno/&lt;serial&gt;</code> <code>serno/WD-12345</code> By serial number <code>serno/&lt;serial&gt;.sN</code> <code>serno/WD-12345.s1</code> Slice by serial <code>serno/&lt;serial&gt;.sNX</code> <code>serno/WD-12345.s1a</code> Partition by serial <code>slice-by-uuid/&lt;uuid&gt;</code> <code>slice-by-uuid/abc...</code> GPT slice by UUID <code>part-by-uuid/&lt;uuid&gt;</code> <code>part-by-uuid/def...</code> Partition by UUID <code>by-label/&lt;packname&gt;</code> <code>by-label/root</code> By disklabel pack name <code>part-by-label/&lt;&gt;.X</code> <code>part-by-label/root.a</code> Partition by label name"},{"location":"sys/kern/disk/#initialization","title":"Initialization","text":"<pre><code>/* sys/kern/subr_disk.c:1540-1564 */\nstatic void\ndisk_init(void)\n{\n    struct thread *td_core;\n\n    /* Create object cache for disk messages */\n    disk_msg_cache = objcache_create(\"disk-msg-cache\", 0, 0,\n                                     NULL, NULL, NULL,\n                                     objcache_malloc_alloc,\n                                     objcache_malloc_free,\n                                     &amp;disk_msg_malloc_args);\n\n    /* Initialize tokens */\n    lwkt_token_init(&amp;disklist_token, \"disks\");\n    lwkt_token_init(&amp;ds_token, \"ds\");\n\n    /* Initialize message drain port */\n    lwkt_initport_replyonly(&amp;disk_dispose_port, disk_msg_autofree_reply);\n\n    /* Create disk message processing thread */\n    lwkt_gettoken(&amp;disklist_token);\n    lwkt_create(disk_msg_core, NULL, &amp;td_core, NULL, 0, -1, \"disk_msg_core\");\n    tsleep(td_core, 0, \"diskcore\", 0);\n    lwkt_reltoken(&amp;disklist_token);\n}\n\nSYSINIT(disk_register, SI_SUB_PRE_DRIVERS, SI_ORDER_FIRST, disk_init, NULL);\n</code></pre>"},{"location":"sys/kern/disk/#io-scheduling-dsched","title":"I/O Scheduling (dsched)","text":"<p>The disk subsystem includes stub entry points for I/O scheduling (<code>dsched</code>):</p> <pre><code>/* sys/kern/kern_dsched.c - stubs only */\nvoid dsched_disk_create(struct disk *dp, const char *head_name, int unit) { }\nvoid dsched_disk_update(struct disk *dp, struct disk_info *info) { }\nvoid dsched_disk_destroy(struct disk *dp) { }\n</code></pre> <p>These functions are called from <code>disk_create()</code>, <code>disk_setdiskinfo()</code>, and <code>disk_destroy()</code> but do nothing. The original dsched framework was removed in November 2015 (commit <code>3573cf7bf6</code>) with this explanation:</p> <p>After consultation, remove dsched from the kernel. The original idea is still valid but the current implementation has had lingering bugs for several years now and we've determined that it's just got its fingers into too many structures.</p> <p>Also, the implementation was designed before SSDs, and doesn't play well with SSDs.</p> <p>Leave various empty entry points in so we can revisit at some future date.</p> <p>The removal deleted ~5,800 lines of code including three I/O schedulers: - BFQ (Budget Fair Queueing) - ~2,100 lines - FQ (Fair Queueing) - ~900 lines - AS (Anticipatory Scheduler) - ~290 lines</p> <p>The stub entry points remain to allow potential future reimplementation.</p>"},{"location":"sys/kern/disk/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Type Description <code>kern.disks</code> string Space-separated list of disk names <code>kern.disk_debug</code> int Enable disk subsystem debugging <code>debug.sizeof.disk</code> int Size of struct disk <code>debug.sizeof.diskslices</code> int Size of struct diskslices"},{"location":"sys/kern/disk/#see-also","title":"See Also","text":"<ul> <li>Devices - Device framework</li> <li>Buffer Cache - Block I/O buffering</li> <li>VFS Operations - Filesystem integration</li> </ul>"},{"location":"sys/kern/firmware/","title":"Firmware Loading","text":"<p>The firmware subsystem provides an interface for loading firmware images into the kernel and making them available to device drivers. Firmware images are typically embedded in kernel modules and can be loaded on-demand or pre-loaded.</p> <p>Source files: - <code>sys/kern/subr_firmware.c</code> - Firmware management (~540 lines)</p> <p>Header files: - <code>sys/sys/firmware.h</code> - Public API</p>"},{"location":"sys/kern/firmware/#overview","title":"Overview","text":"<p>The firmware framework solves a common driver problem: many hardware devices require proprietary microcode or configuration data to operate. This subsystem:</p> <ol> <li>Provides a registry for firmware images with reference counting</li> <li>Supports automatic loading of firmware modules on demand</li> <li>Handles parent/child relationships for multi-image modules</li> <li>Manages unloading of unreferenced firmware</li> </ol>"},{"location":"sys/kern/firmware/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/firmware/#struct-firmware","title":"struct firmware","text":"<p>The user-visible firmware image descriptor:</p> <pre><code>/* sys/sys/firmware.h:51-56 */\nstruct firmware {\n    const char      *name;      /* system-wide name */\n    const uint8_t   *data;      /* location of image */\n    size_t          datasize;   /* size of image in bytes */\n    unsigned int    version;    /* version of the image */\n};\n</code></pre>"},{"location":"sys/kern/firmware/#struct-priv_fw","title":"struct priv_fw","text":"<p>Internal firmware registry entry:</p> <pre><code>/* sys/kern/subr_firmware.c:81-109 */\nstruct priv_fw {\n    int             refcnt;     /* reference count */\n    struct priv_fw  *parent;    /* parent image for subimages */\n    int             flags;      /* FW_UNLOAD flag */\n    linker_file_t   file;       /* module file (if autoloaded) */\n    struct firmware fw;         /* externally visible information */\n};\n</code></pre> <p>State transitions: <pre><code>firmware_register()    --&gt;  fw.name = image_name\n(autoloaded image)     --&gt;  file = module reference\nfirmware_unregister()  --&gt;  fw.name = NULL\n(unload complete)      --&gt;  file = NULL\n</code></pre></p>"},{"location":"sys/kern/firmware/#registry","title":"Registry","text":"<p>The firmware registry uses a static array:</p> <pre><code>/* sys/kern/subr_firmware.c:124-125 */\n#define FIRMWARE_MAX    30\nstatic struct priv_fw firmware_table[FIRMWARE_MAX];\n</code></pre> <p>A slot is in use if either <code>file != NULL</code> or <code>fw.name != NULL</code>:</p> <pre><code>#define FW_INUSE(p)     ((p)-&gt;file != NULL || (p)-&gt;fw.name != NULL)\n</code></pre>"},{"location":"sys/kern/firmware/#public-api","title":"Public API","text":""},{"location":"sys/kern/firmware/#firmware_register","title":"firmware_register()","text":"<p>Register a firmware image with the kernel:</p> <pre><code>/* sys/kern/subr_firmware.c:172-209 */\nconst struct firmware *\nfirmware_register(const char *imagename, const void *data, size_t datasize,\n    unsigned int version, const struct firmware *parent)\n{\n    struct priv_fw *match, *frp;\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n\n    /* Check name is unique and find free slot */\n    match = lookup(imagename, &amp;frp);\n    if (match != NULL) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        kprintf(\"%s: image %s already registered!\\n\", __func__, imagename);\n        return NULL;\n    }\n    if (frp == NULL) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        kprintf(\"%s: cannot register image %s, firmware table full!\\n\",\n            __func__, imagename);\n        return NULL;\n    }\n\n    bzero(frp, sizeof(*frp));\n    frp-&gt;fw.name = imagename;\n    frp-&gt;fw.data = data;\n    frp-&gt;fw.datasize = datasize;\n    frp-&gt;fw.version = version;\n\n    /* Link to parent if specified */\n    if (parent != NULL) {\n        frp-&gt;parent = PRIV_FW(parent);\n        frp-&gt;parent-&gt;refcnt++;\n    }\n\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n    return &amp;frp-&gt;fw;\n}\n</code></pre> <p>Parameters: - <code>imagename</code> - Unique name for the firmware - <code>data</code> - Pointer to firmware data - <code>datasize</code> - Size in bytes - <code>version</code> - Version number - <code>parent</code> - Parent firmware (for multi-image modules)</p>"},{"location":"sys/kern/firmware/#firmware_get","title":"firmware_get()","text":"<p>Look up and optionally load firmware:</p> <pre><code>/* sys/kern/subr_firmware.c:304-347 */\nconst struct firmware *\nfirmware_get(const char *imagename)\n{\n    struct task fwload_task;\n    struct priv_fw *fp;\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    fp = lookup(imagename, NULL);\n    if (fp != NULL)\n        goto found;\n\n    /*\n     * Image not present, try to load the module holding it.\n     */\n    if (caps_priv_check_self(SYSCAP_NOKLD) != 0 || securelevel &gt; 0) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        kprintf(\"%s: insufficient privileges to \"\n            \"load firmware image %s\\n\", __func__, imagename);\n        return NULL;\n    }\n\n    /*\n     * Defer load to a thread with known context.  linker_reference_module\n     * may do filesystem i/o which requires root &amp; current dirs, etc.\n     */\n    if (!cold) {\n        TASK_INIT(&amp;fwload_task, 0, loadimage, __DECONST(void *, imagename));\n        taskqueue_enqueue(firmware_tq, &amp;fwload_task);\n        lksleep(__DECONST(void *, imagename), &amp;firmware_lock, 0, \"fwload\", 0);\n    }\n\n    /* After load attempt, check if image registered */\n    fp = lookup(imagename, NULL);\n    if (fp == NULL) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        return NULL;\n    }\n\nfound:\n    fp-&gt;refcnt++;\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n    return &amp;fp-&gt;fw;\n}\n</code></pre> <p>Auto-loading:</p> <ol> <li>If firmware not in registry, attempt to load kernel module</li> <li>Module loading delegated to taskqueue thread (needs filesystem context)</li> <li>Module calls <code>firmware_register()</code> during initialization</li> <li>Caller blocks until load completes or fails</li> </ol>"},{"location":"sys/kern/firmware/#firmware_put","title":"firmware_put()","text":"<p>Release a firmware reference:</p> <pre><code>/* sys/kern/subr_firmware.c:358-372 */\nvoid\nfirmware_put(const struct firmware *p, int flags)\n{\n    struct priv_fw *fp = PRIV_FW(p);\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    fp-&gt;refcnt--;\n    if (fp-&gt;refcnt == 0) {\n        if (flags &amp; FIRMWARE_UNLOAD)\n            fp-&gt;flags |= FW_UNLOAD;\n        if (fp-&gt;file)\n            taskqueue_enqueue(firmware_tq, &amp;firmware_unload_task);\n    }\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n}\n</code></pre> <p>Flags: - <code>FIRMWARE_UNLOAD</code> (0x0001) - Request unload when refcount reaches zero</p>"},{"location":"sys/kern/firmware/#firmware_unregister","title":"firmware_unregister()","text":"<p>Remove a firmware image from the registry:</p> <pre><code>/* sys/kern/subr_firmware.c:216-250 */\nint\nfirmware_unregister(const char *imagename)\n{\n    struct priv_fw *fp;\n    int err;\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    fp = lookup(imagename, NULL);\n\n    if (fp == NULL) {\n        /* OK - happens on module unload after firmware_put() */\n        err = 0;\n    } else if (fp-&gt;refcnt != 0) {\n        err = EBUSY;  /* still in use */\n    } else {\n        linker_file_t x = fp-&gt;file;  /* preserve for unload */\n\n        if (fp-&gt;parent != NULL)\n            fp-&gt;parent-&gt;refcnt--;\n\n        bzero(fp, sizeof(struct priv_fw));\n        fp-&gt;file = x;  /* restore for unload completion */\n        err = 0;\n    }\n\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n    return err;\n}\n</code></pre>"},{"location":"sys/kern/firmware/#module-loading","title":"Module Loading","text":""},{"location":"sys/kern/firmware/#load-task","title":"Load Task","text":"<p>Firmware loading is performed by a dedicated taskqueue thread:</p> <pre><code>/* sys/kern/subr_firmware.c:252-295 */\nstatic void\nloadimage(void *arg, int npending)\n{\n    char *imagename = arg;\n    struct priv_fw *fp;\n    linker_file_t result;\n    int error;\n\n    /* synchronize with the thread that dispatched us */\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n\n    error = linker_reference_module(imagename, NULL, &amp;result);\n    if (error != 0) {\n        kprintf(\"%s: could not load firmware image, error %d\\n\",\n            imagename, error);\n        goto done;\n    }\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n    fp = lookup(imagename, NULL);\n    if (fp == NULL || fp-&gt;file != NULL) {\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        if (fp == NULL)\n            kprintf(\"%s: firmware image loaded, \"\n                \"but did not register\\n\", imagename);\n        (void) linker_release_module(imagename, NULL, NULL);\n        goto done;\n    }\n    fp-&gt;file = result;  /* record module identity */\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n\ndone:\n    wakeup_one(imagename);  /* wake waiting caller */\n}\n</code></pre>"},{"location":"sys/kern/firmware/#unload-task","title":"Unload Task","text":"<p>Unreferenced autoloaded modules are unloaded by a background task:</p> <pre><code>/* sys/kern/subr_firmware.c:427-475 */\nstatic void\nunloadentry(void *unused1, int unused2)\n{\n    int limit = FIRMWARE_MAX;\n    int i;\n\n    lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n\n    /* Scan table, may need multiple passes for cross-linked images */\n    for (i = 0; i &lt; limit; i++) {\n        struct priv_fw *fp;\n        int err;\n\n        fp = &amp;firmware_table[i % FIRMWARE_MAX];\n        if (fp-&gt;fw.name == NULL || fp-&gt;file == NULL ||\n            fp-&gt;refcnt != 0 || (fp-&gt;flags &amp; FW_UNLOAD) == 0)\n            continue;\n\n        /* Found entry to unload */\n        limit = i + FIRMWARE_MAX;   /* another full round */\n        fp-&gt;flags &amp;= ~FW_UNLOAD;    /* don't retry */\n\n        lockmgr(&amp;firmware_lock, LK_RELEASE);\n        err = linker_release_module(NULL, NULL, fp-&gt;file);\n        lockmgr(&amp;firmware_lock, LK_EXCLUSIVE);\n\n        if (err == 0)\n            fp-&gt;file = NULL;\n    }\n\n    lockmgr(&amp;firmware_lock, LK_RELEASE);\n}\n</code></pre>"},{"location":"sys/kern/firmware/#parentchild-relationships","title":"Parent/Child Relationships","text":"<p>Modules can contain multiple firmware images. One image (typically named after the module) is the parent; others are children:</p> <pre><code>Module: iwn5000fw.ko\n  |\n  +-- \"iwn5000fw\" (parent, master image)\n  |\n  +-- \"iwn5000init\" (child, references parent)\n  |\n  +-- \"iwn5000boot\" (child, references parent)\n</code></pre> <p>Behavior: - Children increment parent's refcount on registration - Children decrement parent's refcount on unregistration - Module cannot unload until all children are released - Auto-loading uses parent name (module name) to find the module</p>"},{"location":"sys/kern/firmware/#usage-example","title":"Usage Example","text":"<p>Driver requesting firmware:</p> <pre><code>static int\nmydriver_attach(device_t dev)\n{\n    const struct firmware *fw;\n\n    fw = firmware_get(\"mydevice_fw\");\n    if (fw == NULL) {\n        device_printf(dev, \"could not load firmware\\n\");\n        return ENXIO;\n    }\n\n    /* Use fw-&gt;data and fw-&gt;datasize */\n    error = load_microcode(dev, fw-&gt;data, fw-&gt;datasize);\n\n    /* Release when done (or keep reference if needed later) */\n    firmware_put(fw, FIRMWARE_UNLOAD);\n\n    return error;\n}\n</code></pre> <p>Firmware module:</p> <pre><code>/* In the firmware module */\nstatic const uint8_t mydevice_fw_data[] = {\n    /* firmware binary data */\n};\n\nstatic int\nmydevice_fw_modevent(module_t mod, int type, void *unused)\n{\n    switch (type) {\n    case MOD_LOAD:\n        return firmware_register(\"mydevice_fw\",\n            mydevice_fw_data, sizeof(mydevice_fw_data),\n            1, NULL) == NULL ? ENOMEM : 0;\n    case MOD_UNLOAD:\n        return firmware_unregister(\"mydevice_fw\");\n    }\n    return EINVAL;\n}\n</code></pre>"},{"location":"sys/kern/firmware/#initialization","title":"Initialization","text":"<pre><code>/* sys/kern/subr_firmware.c:480-504 */\nstatic int\nfirmware_modevent(module_t mod, int type, void *unused)\n{\n    switch (type) {\n    case MOD_LOAD:\n        TASK_INIT(&amp;firmware_unload_task, 0, unloadentry, NULL);\n        lockinit(&amp;firmware_lock, \"firmware table\", 0, LK_CANRECURSE);\n        firmware_tq = taskqueue_create(\"taskqueue_firmware\", M_WAITOK,\n            taskqueue_thread_enqueue, &amp;firmware_tq);\n        (void) taskqueue_start_threads(&amp;firmware_tq, 1, TDPRI_KERN_DAEMON,\n            -1, \"firmware taskq\");\n        return 0;\n\n    case MOD_UNLOAD:\n        /* Mark all for unload, drain, verify empty */\n        ...\n    }\n}\n\nDECLARE_MODULE(firmware, firmware_mod, SI_SUB_DRIVERS, SI_ORDER_FIRST);\n</code></pre>"},{"location":"sys/kern/firmware/#security","title":"Security","text":"<p>Firmware loading is restricted:</p> <pre><code>if (caps_priv_check_self(SYSCAP_NOKLD) != 0 || securelevel &gt; 0) {\n    /* Cannot load firmware - insufficient privileges */\n}\n</code></pre> <ul> <li>Requires <code>SYSCAP_NOKLD</code> capability</li> <li>Blocked when <code>securelevel &gt; 0</code></li> </ul>"},{"location":"sys/kern/firmware/#see-also","title":"See Also","text":"<ul> <li>Devices - Device driver framework</li> <li>NewBus - Device attachment</li> </ul>"},{"location":"sys/kern/initialization/","title":"Kernel Initialization and Bootstrap","text":"<p>Source Files: - <code>sys/kern/init_main.c</code> - Main kernel initialization - <code>sys/kern/init_sysent.c</code> - System call table (generated) - <code>sys/kern/subr_param.c</code> - Kernel parameter computation - <code>sys/kern/kern_environment.c</code> - Kernel environment variables</p> <p>This document describes how the DragonFly BSD kernel boots and initializes, from the hand-off from machine-dependent code through the creation of the first userspace process (init).</p>"},{"location":"sys/kern/initialization/#overview","title":"Overview","text":"<p>DragonFly's kernel initialization follows a carefully orchestrated sequence:</p> <pre><code>Machine-dependent startup (e.g., init386)\n  \u2193\nmi_proc0init() - Initialize proc0/thread0/lwp0\n  \u2193\nmi_startup() - Machine-independent initialization\n  \u2193\nSYSINIT framework - Ordered subsystem initialization\n  \u2193\nCreate init process (PID 1)\n  \u2193\nStart init - exec /sbin/init\n  \u2193\nUserspace initialization begins\n</code></pre> <p>The initialization process relies on the SYSINIT framework, which allows subsystems to register initialization functions that are executed in a well-defined order based on subsystem (SI_SUB_) and order (SI_ORDER_) values.</p>"},{"location":"sys/kern/initialization/#architecture","title":"Architecture","text":""},{"location":"sys/kern/initialization/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/kern/initialization/#process-0-swapper","title":"Process 0 (swapper)","text":"<p>Process 0 is the kernel's initial context, created statically:</p> <pre><code>/* sys/kern/init_main.c:79-91 */\nstatic struct session session0;\nstatic struct pgrp pgrp0;\nstatic struct sigacts sigacts0;\nstatic struct filedesc filedesc0;\nstatic struct plimit limit0;\nstatic struct vmspace vmspace0;\nstatic struct sysreaper initreaper;\n\nstruct proc *initproc;        /* Will point to init (PID 1) */\nstruct proc proc0;            /* Process 0 (swapper) */\nstruct lwp lwp0;              /* LWP 0 */\nstruct thread thread0;        /* Thread 0 */\n</code></pre> <p>These static structures are never freed and serve as the template for forking other processes.</p>"},{"location":"sys/kern/initialization/#sysinit-structure","title":"SYSINIT Structure","text":"<pre><code>struct sysinit {\n    unsigned int    subsystem;    /* SI_SUB_* */\n    unsigned int    order;        /* SI_ORDER_* within subsystem */\n    void            (*func)(void *);\n    void            *udata;\n};\n</code></pre> <p>Subsystem levels (SI_SUB_*): - <code>SI_BOOT1_COPYRIGHT</code> - Print copyright - <code>SI_BOOT1_POST</code> - Early boot tasks - <code>SI_BOOT2_LEAVE_CRIT</code> - Enable interrupts - <code>SI_BOOT2_PROC0</code> - Initialize proc0 - <code>SI_BOOT2_FINISH_SMP</code> - SMP initialization complete - <code>SI_SUB_PROC0_POST</code> - Post-proc0 initialization - <code>SI_SUB_CREATE_INIT</code> - Create init process - <code>SI_SUB_KTHREAD_INIT</code> - Start init process - Many more (see <code>&lt;sys/kernel.h&gt;</code>)</p> <p>Order values (SI_ORDER_*): - <code>SI_ORDER_FIRST</code> - First in subsystem - <code>SI_ORDER_SECOND</code> - Second - <code>SI_ORDER_MIDDLE</code> - Middle - <code>SI_ORDER_ANY</code> - No specific order - <code>SI_ORDER_LAST</code> - Last in subsystem</p>"},{"location":"sys/kern/initialization/#initialization-sequence","title":"Initialization Sequence","text":""},{"location":"sys/kern/initialization/#phase-1-machine-dependent-setup","title":"Phase 1: Machine-Dependent Setup","text":"<p>Before <code>mi_proc0init()</code> is called, the machine-dependent code (e.g., <code>init386()</code> on x86-64) has:</p> <ol> <li>Set up the boot stack</li> <li>Initialized basic CPU structures</li> <li>Set up memory management (paging)</li> <li>Initialized the GDT/IDT</li> <li>Set up the boot CPU's globaldata structure</li> <li>Prepared the initial kernel environment</li> </ol>"},{"location":"sys/kern/initialization/#phase-2-process-0-creation","title":"Phase 2: Process 0 Creation","text":"<p>Function: <code>mi_proc0init()</code> (<code>init_main.c:162</code>)</p> <p>Called from very low-level boot code to initialize CPU #0's structures:</p> <pre><code>void mi_proc0init(struct globaldata *gd, struct user *proc0paddr)\n{\n    /* Initialize thread0 */\n    lwkt_init_thread(&amp;thread0, proc0paddr, LWKT_THREAD_STACK, 0, gd);\n    lwkt_set_comm(&amp;thread0, \"thread0\");\n\n    /* Initialize proc0 */\n    RB_INIT(&amp;proc0.p_lwp_tree);\n    spin_init(&amp;proc0.p_spin, \"iproc_proc0\");\n    lwkt_token_init(&amp;proc0.p_token, \"iproc\");\n\n    /* Initialize lwp0 */\n    lwp0.lwp_tid = 1;\n    proc0.p_lasttid = lwp0.lwp_tid;\n    lwp_rb_tree_RB_INSERT(&amp;proc0.p_lwp_tree, &amp;lwp0);\n    lwp0.lwp_thread = &amp;thread0;\n    lwp0.lwp_proc = &amp;proc0;\n\n    /* Set up user scheduler */\n    proc0.p_usched = usched_init();\n\n    /* Thread0 linkage */\n    thread0.td_flags |= TDF_RUNNING;\n    thread0.td_proc = &amp;proc0;\n    thread0.td_lwp = &amp;lwp0;\n    thread0.td_switch = cpu_lwkt_switch;\n\n    lwkt_schedule_self(curthread);\n}\n</code></pre> <p>At this point: - Thread0 is running on CPU 0 - Proc0 exists but is incomplete - LWKT threading is operational - Interrupts are still disabled (critical section)</p>"},{"location":"sys/kern/initialization/#phase-3-machine-independent-startup","title":"Phase 3: Machine-Independent Startup","text":"<p>Function: <code>mi_startup()</code> (<code>init_main.c:199</code>)</p> <p>This is the main initialization orchestrator:</p> <pre><code>void mi_startup(void)\n{\n    struct sysinit *sip;\n    struct sysinit **sipp, **xipp;\n\n    /* Get sysinit array from linker set */\n    if (sysinit == NULL) {\n        sysinit = SET_BEGIN(sysinit_set);\n        sysinit_end = SET_LIMIT(sysinit_set);\n    }\n\nrestart:\n    /* Bubble sort sysinits by subsystem and order */\n    for (sipp = sysinit; sipp &lt; sysinit_end; sipp++) {\n        for (xipp = sipp + 1; xipp &lt; sysinit_end; xipp++) {\n            if ((*sipp)-&gt;subsystem &lt; (*xipp)-&gt;subsystem ||\n                ((*sipp)-&gt;subsystem == (*xipp)-&gt;subsystem &amp;&amp;\n                 (*sipp)-&gt;order &lt;= (*xipp)-&gt;order))\n                continue;\n            /* Swap */\n            save = *sipp;\n            *sipp = *xipp;\n            *xipp = save;\n        }\n    }\n\n    /* Execute all sysinit functions in order */\n    for (sipp = sysinit; sipp &lt; sysinit_end; sipp++) {\n        sip = *sipp;\n\n        if (sip-&gt;subsystem == SI_SPECIAL_DUMMY)\n            continue;\n        if (sip-&gt;subsystem == SI_SPECIAL_DONE)\n            continue;\n\n        /* Call initialization function */\n        (*(sip-&gt;func))(sip-&gt;udata);\n\n        /* Mark as done */\n        sip-&gt;subsystem = SI_SPECIAL_DONE;\n\n        /* Check if KLDs added more sysinits */\n        if (newsysinit != NULL) {\n            sysinit = newsysinit;\n            sysinit_end = newsysinit_end;\n            newsysinit = NULL;\n            goto restart;\n        }\n    }\n\n    panic(\"Shouldn't get here!\");  /* Scheduler never returns */\n}\n</code></pre> <p>Key aspects:</p> <ol> <li>Sorting: SYSINITs are sorted by subsystem (primary key) and order (secondary key)</li> <li>Execution: Each function is called in order</li> <li>Dynamic loading: If kernel modules (KLDs) are loaded during initialization, their SYSINITs are merged and the process restarts</li> <li>Never returns: The last SYSINIT starts the scheduler, which never returns</li> </ol>"},{"location":"sys/kern/initialization/#phase-4-important-sysinit-stages","title":"Phase 4: Important SYSINIT Stages","text":""},{"location":"sys/kern/initialization/#41-copyright-and-early-boot","title":"4.1 Copyright and Early Boot","text":"<pre><code>/* Print copyright */\nSYSINIT(announce, SI_BOOT1_COPYRIGHT, SI_ORDER_FIRST,\n        print_caddr_t, copyright);\n\n/* Initialize kernel environment */\nSYSINIT(kenv, SI_BOOT1_POST, SI_ORDER_ANY, kenv_init, NULL);\n</code></pre> <p>The <code>kenv_init()</code> function (<code>kern_environment.c:571</code>): - Creates dynamic environment array - Copies static environment from bootloader - Sets up kenv spinlock - Enables dynamic environment operations</p>"},{"location":"sys/kern/initialization/#42-leave-critical-section","title":"4.2 Leave Critical Section","text":"<pre><code>SYSINIT(leavecrit, SI_BOOT2_LEAVE_CRIT, SI_ORDER_ANY,\n        leavecrit, NULL);\n</code></pre> <p>The <code>leavecrit()</code> function (<code>init_main.c:305</code>): - Stabilizes machine interrupt ABI - Enables CPU interrupts - Exits critical section - Allows device probes to work</p> <p>After this point, interrupts are enabled and the system can respond to external events.</p>"},{"location":"sys/kern/initialization/#43-enable-tsleep","title":"4.3 Enable tsleep","text":"<pre><code>SYSINIT(tsleepworks, SI_BOOT2_FINISH_SMP, SI_ORDER_SECOND,\n        tsleepworks, NULL);\n</code></pre> <p>Sets <code>tsleep_now_works = 1</code>, allowing threads to block/sleep.</p>"},{"location":"sys/kern/initialization/#44-initialize-process-0","title":"4.4 Initialize Process 0","text":"<pre><code>SYSINIT(p0init, SI_BOOT2_PROC0, SI_ORDER_FIRST, proc0_init, NULL);\n</code></pre> <p>The <code>proc0_init()</code> function (<code>init_main.c:373</code>) completes proc0 setup:</p> <pre><code>static void proc0_init(void *dummy)\n{\n    struct proc *p = &amp;proc0;\n\n    /* Initialize osrel */\n    p-&gt;p_osrel = osreldate;\n\n    /* Initialize process and pgrp structures */\n    procinit();\n    vm_init2();\n\n    /* Create process 0 (the swapper) */\n    procinsertinit(p);\n    pgrpinsertinit(&amp;pgrp0);\n    LIST_INIT(&amp;pgrp0.pg_members);\n    LIST_INSERT_HEAD(&amp;pgrp0.pg_members, p, p_pglist);\n\n    /* Set up session */\n    pgrp0.pg_session = &amp;session0;\n    session0.s_count = 1;\n    session0.s_leader = p;\n\n    /* Set process attributes */\n    p-&gt;p_flags = P_SYSTEM;\n    p-&gt;p_stat = SACTIVE;\n    lwp0.lwp_stat = LSRUN;\n    p-&gt;p_nice = NZERO;\n    p-&gt;p_sysent = &amp;null_sysvec;\n\n    bcopy(\"swapper\", p-&gt;p_comm, sizeof(\"swapper\"));\n\n    /* Create credentials (root) */\n    uip = uicreate(0);\n    p-&gt;p_ucred = crget();\n    p-&gt;p_ucred-&gt;cr_ruidinfo = uip;\n    p-&gt;p_ucred-&gt;cr_uidinfo = uip;\n    p-&gt;p_ucred-&gt;cr_ngroups = 1;\n\n    /* Create sigacts */\n    p-&gt;p_sigacts = &amp;sigacts0;\n    refcount_init(&amp;p-&gt;p_sigacts-&gt;ps_refcnt, 1);\n    siginit(p);\n\n    /* Create file descriptor table */\n    fdinit_bootstrap(p, &amp;filedesc0, cmask);\n\n    /* Create limits */\n    plimit_init0(&amp;limit0);\n    p-&gt;p_limit = &amp;limit0;\n\n    /* Set up address space */\n    pmap_pinit0(vmspace_pmap(&amp;vmspace0));\n    p-&gt;p_vmspace = &amp;vmspace0;\n    lwp0.lwp_vmspace = p-&gt;p_vmspace;\n    vmspace_initrefs(&amp;vmspace0);\n    vm_map_init(&amp;vmspace0.vm_map, VM_MIN_USER_ADDRESS,\n                VM_MAX_USER_ADDRESS, vmspace_pmap(&amp;vmspace0));\n\n    /* Initialize kqueue */\n    kqueue_init(&amp;lwp0.lwp_kqueue, &amp;filedesc0);\n\n    /* Charge root for one process */\n    chgproccnt(p-&gt;p_ucred-&gt;cr_uidinfo, 1, 0);\n    vm_init_limits(p);\n}\n</code></pre> <p>After this, proc0 is a fully functional kernel process.</p>"},{"location":"sys/kern/initialization/#45-end-cold-boot","title":"4.5 End Cold Boot","text":"<pre><code>SYSINIT(endofcoldboot, SI_SUB_ISWARM, SI_ORDER_ANY,\n        endofcoldboot, NULL);\n</code></pre> <p>Sets <code>cold = 0</code>, indicating the system is no longer in cold boot phase. Device drivers use this flag to determine if they're being initialized at boot or hotplugged later.</p>"},{"location":"sys/kern/initialization/#46-create-init-process","title":"4.6 Create Init Process","text":"<pre><code>SYSINIT(init, SI_SUB_CREATE_INIT, SI_ORDER_FIRST, create_init, NULL);\n</code></pre> <p>The <code>create_init()</code> function (<code>init_main.c:716</code>):</p> <pre><code>static void create_init(const void *udata)\n{\n    int error;\n    struct lwp *lp;\n\n    crit_enter();\n\n    /* Fork process 1 from lwp0 */\n    error = fork1(&amp;lwp0, RFFDG | RFPROC, &amp;initproc);\n    if (error)\n        panic(\"cannot fork init: %d\", error);\n\n    initproc-&gt;p_flags |= P_SYSTEM;\n    reaper_init(initproc, &amp;initreaper);\n\n    lp = ONLY_LWP_IN_PROC(initproc);\n\n    /* Set up fork handler to call start_init() */\n    cpu_set_fork_handler(lp, start_init, NULL);\n\n    crit_exit();\n}\n</code></pre> <p>At this point: - Init process (PID 1) is created but not running - It's marked as a system process - Fork handler is set to <code>start_init()</code></p>"},{"location":"sys/kern/initialization/#47-start-init-process","title":"4.7 Start Init Process","text":"<pre><code>SYSINIT(kickinit, SI_SUB_KTHREAD_INIT, SI_ORDER_FIRST, kick_init, NULL);\n</code></pre> <p>The <code>kick_init()</code> function (<code>init_main.c:737</code>):</p> <pre><code>static void kick_init(const void *udata)\n{\n    start_forked_proc(&amp;lwp0, initproc);\n}\n</code></pre> <p>This makes the init process runnable. When it's scheduled, it will execute <code>start_init()</code>.</p>"},{"location":"sys/kern/initialization/#phase-5-starting-userspace","title":"Phase 5: Starting Userspace","text":"<p>Function: <code>start_init()</code> (<code>init_main.c:556</code>)</p> <p>This function executes in the context of the init process (PID 1):</p> <pre><code>static void start_init(void *dummy, struct trapframe *frame)\n{\n    struct proc *p = curproc;\n    struct mount *mp;\n    struct vnode *vp;\n    char *path, *next;\n    int error;\n\n    /* Get kernel name from environment */\n    env = kgetenv(\"kernelname\");\n    if (env != NULL)\n        strlcpy(kernelname, env, sizeof(kernelname));\n\n    /* Set up root vnode */\n    mp = mountlist_boot_getfirst();\n    if (VFS_ROOT(mp, &amp;vp))\n        panic(\"cannot find root vnode\");\n\n    p-&gt;p_fd-&gt;fd_cdir = vp;\n    vref(p-&gt;p_fd-&gt;fd_cdir);\n    p-&gt;p_fd-&gt;fd_rdir = vp;\n    vref(p-&gt;p_fd-&gt;fd_rdir);\n\n    /* Mount devfs */\n    kprintf(\"Mounting devfs\\n\");\n    vfs_mountroot_devfs();\n\n    /* Allocate stack space for execve args */\n    addr = trunc_page(USRSTACK - PAGE_SIZE);\n    error = vm_map_find(&amp;p-&gt;p_vmspace-&gt;vm_map, NULL, NULL,\n                        0, &amp;addr, PAGE_SIZE, PAGE_SIZE, FALSE,\n                        VM_MAPTYPE_NORMAL, VM_SUBSYS_INIT,\n                        VM_PROT_ALL, VM_PROT_ALL, 0);\n    if (error)\n        panic(\"init: couldn't allocate argument space\");\n\n    /* Try each path in init_path */\n    for (path = init_path; *path != '\\0'; path = next) {\n        /* Parse next path component */\n        while (*path == ':')\n            path++;\n        if (*path == '\\0')\n            break;\n        for (next = path; *next != '\\0' &amp;&amp; *next != ':'; next++)\n            ;\n\n        /* Build argv for exec */\n        /* argv[0] = path (e.g., \"/sbin/init\") */\n        /* argv[1] = boot flags (e.g., \"-s\" for single-user) */\n\n        /* Try to exec */\n        if ((error = sys_execve(&amp;sysmsg, &amp;args)) == 0) {\n            /* Success! Now running /sbin/init */\n            lp-&gt;lwp_proc-&gt;p_usched-&gt;acquire_curproc(lp);\n            return;\n        }\n\n        if (error != ENOENT)\n            kprintf(\"exec %.*s: error %d\\n\",\n                   (int)(next - path), path, error);\n    }\n\n    kprintf(\"init: not found in path %s\\n\", init_path);\n    panic(\"no init\");\n}\n</code></pre> <p>Default init_path: <pre><code>/sbin/init:/sbin/oinit:/sbin/init.bak\n</code></pre></p> <p>This can be overridden with <code>kern.init_path</code> environment variable.</p> <p>Boot flags passed to init: - <code>-s</code>: Single-user mode (if <code>RB_SINGLE</code> set) - <code>-C</code>: Boot from CDROM (if <code>BOOTCDROM</code> defined)</p> <p>Once <code>sys_execve()</code> succeeds: 1. Init's address space is replaced with <code>/sbin/init</code> 2. Init becomes PID 1 in userspace 3. Kernel initialization is complete 4. Userspace initialization begins (init spawns getty, starts services, etc.)</p>"},{"location":"sys/kern/initialization/#kernel-parameters","title":"Kernel Parameters","text":"<p>Source: <code>sys/kern/subr_param.c</code></p>"},{"location":"sys/kern/initialization/#parameter-computation","title":"Parameter Computation","text":"<p>DragonFly computes many kernel parameters at boot time based on available physical memory and tunables. This happens in two phases:</p>"},{"location":"sys/kern/initialization/#phase-1-init-param1-fixed-parameters","title":"Phase 1: Init Param1 - Fixed Parameters","text":"<p>Function: <code>init_param1()</code> (<code>subr_param.c:197</code>)</p> <p>Called very early, before memory-dependent calculations:</p> <pre><code>void init_param1(void)\n{\n    /* Timer frequency */\n    hz = HZ_DEFAULT;  /* Default: 100 Hz */\n    TUNABLE_INT_FETCH(\"kern.hz\", &amp;hz);\n\n    ustick = 1000000 / hz;        /* microseconds per tick */\n    nstick = 1000000000 / hz;     /* nanoseconds per tick */\n\n    /* Statistics clock */\n    stathz = hz + 1;\n    TUNABLE_INT_FETCH(\"kern.stathz\", &amp;stathz);\n\n    /* Profiling clock */\n    profhz = stathz;\n\n    /* NTP tick delta */\n    ntp_default_tick_delta = howmany(30000000, 60 * hz);\n\n    /* Size limits (can be tuned) */\n    maxswzone = VM_SWZONE_SIZE_MAX;\n    TUNABLE_LONG_FETCH(\"kern.maxswzone\", &amp;maxswzone);\n\n    maxbcache = VM_BCACHE_SIZE_MAX;\n    TUNABLE_LONG_FETCH(\"kern.maxbcache\", &amp;maxbcache);\n\n    /* User address space limits */\n    maxtsiz = MAXTSIZ;    /* Max text size */\n    dfldsiz = DFLDSIZ;    /* Default data size */\n    maxdsiz = MAXDSIZ;    /* Max data size */\n    dflssiz = DFLSSIZ;    /* Default stack size */\n    maxssiz = MAXSSIZ;    /* Max stack size */\n    sgrowsiz = SGROWSIZ;  /* Stack growth size */\n    maxthrssiz = MAXTHRSSIZ;  /* Thread stack area */\n\n    /* Each can be overridden via tunable */\n    TUNABLE_QUAD_FETCH(\"kern.maxtsiz\", &amp;maxtsiz);\n    TUNABLE_QUAD_FETCH(\"kern.dfldsiz\", &amp;dfldsiz);\n    /* ... etc ... */\n}\n</code></pre>"},{"location":"sys/kern/initialization/#phase-2-init-param2-memory-scaled-parameters","title":"Phase 2: Init Param2 - Memory-Scaled Parameters","text":"<p>Function: <code>init_param2(int physpages)</code> (<code>subr_param.c:237</code>)</p> <p>Called after physical memory detection:</p> <pre><code>void init_param2(int physpages)\n{\n    size_t limsize;\n\n    /* Calculate memory size in bytes, limited by KVA */\n    limsize = (size_t)physpages * PAGE_SIZE;\n    if (limsize &gt; KvaSize)\n        limsize = KvaSize;\n\n    /* Limit maxswzone to 1/2 of physical memory */\n    if (maxswzone &gt; limsize / 2)\n        maxswzone = limsize / 2;\n\n    limsize /= 1024 * 1024;  /* Convert to MB */\n\n    /* Compute maxusers (affects many other limits) */\n    maxusers = MAXUSERS;\n    TUNABLE_INT_FETCH(\"kern.maxusers\", &amp;maxusers);\n\n    if (maxusers == 0) {\n        /* Auto-compute: ~384 per 3GB */\n        maxusers = limsize / 8;\n        if (maxusers &lt; 32)\n            maxusers = 32;\n    }\n\n    /* Maximum number of processes */\n    maxproc = NPROC;  /* 20 + 16 * maxusers */\n    TUNABLE_INT_FETCH(\"kern.maxproc\", &amp;maxproc);\n\n    if (maxproc &lt; 32)\n        maxproc = 32;\n    if (maxproc &gt; limsize * 40)\n        maxproc = limsize * 40;  /* Prevent kmap exhaustion */\n\n    /* Maximum open files */\n    maxfiles = MAXFILES;  /* maxproc * 16 */\n    TUNABLE_INT_FETCH(\"kern.maxfiles\", &amp;maxfiles);\n\n    if (maxfiles &lt; 128)\n        maxfiles = 128;\n\n    /* Per-user/per-proc limits */\n    maxprocperuid = maxproc / 4;\n    if (maxprocperuid &lt; 128)\n        maxprocperuid = maxproc / 2;\n\n    minfilesperproc = 8;\n    maxfilesperproc = maxfiles / 4;\n    maxfilesperuser = maxfilesperproc * 2;\n    maxfilesrootres = maxfiles / 20;  /* Reserved for root */\n\n    /* POSIX locks */\n    maxposixlocksperuid = MAXPOSIXLOCKSPERUID;  /* maxproc * 4 */\n    TUNABLE_INT_FETCH(\"kern.maxposixlocksperuid\", &amp;maxposixlocksperuid);\n\n    /* Buffer cache */\n    nbuf = NBUF;  /* Usually 0 (auto-sized) */\n    TUNABLE_LONG_FETCH(\"kern.nbuf\", &amp;nbuf);\n\n    /* Callout wheel size */\n    ncallout = 16 + maxproc + maxfiles;\n    if (ncallout &gt; 5*60*hz)  /* Limit to ~5 minutes worth */\n        ncallout = 5*60*hz;\n    TUNABLE_INT_FETCH(\"kern.ncallout\", &amp;ncallout);\n}\n</code></pre>"},{"location":"sys/kern/initialization/#key-computed-parameters","title":"Key Computed Parameters","text":"Parameter Formula Description <code>maxusers</code> <code>limsize / 8</code> (if not set) Tuning knob affecting many limits <code>maxproc</code> <code>20 + 16 * maxusers</code> Maximum processes <code>maxfiles</code> <code>maxproc * 16</code> Maximum open files system-wide <code>maxprocperuid</code> <code>maxproc / 4</code> Max processes per user <code>maxfilesperproc</code> <code>maxfiles / 4</code> Max files per process <code>maxfilesperuser</code> <code>maxfilesperproc * 2</code> Max files per user <code>maxposixlocksperuid</code> <code>maxproc * 4</code> Max POSIX locks per user <code>ncallout</code> <code>16 + maxproc + maxfiles</code> Callout wheel size"},{"location":"sys/kern/initialization/#virtual-machine-detection","title":"Virtual Machine Detection","text":"<p>DragonFly can detect if it's running as a VM guest:</p> <pre><code>enum vmm_guest_type detect_virtual(void)\n{\n    char *sysenv;\n\n    /* Check SMBIOS BIOS vendor */\n    sysenv = kgetenv(\"smbios.bios.vendor\");\n    if (sysenv != NULL) {\n        /* Check for QEMU, Xen, BHYVE, KVM, etc. */\n        for (i = 0; vmm_bnames[i].str != NULL; i++)\n            if (strcmp(sysenv, vmm_bnames[i].str) == 0)\n                return (vmm_bnames[i].type);\n    }\n\n    /* Check SMBIOS system product */\n    sysenv = kgetenv(\"smbios.system.product\");\n    if (sysenv != NULL) {\n        /* Check for VMware, Hyper-V, VirtualBox, etc. */\n        for (i = 0; vmm_pnames[i].str != NULL; i++)\n            if (strcmp(sysenv, vmm_pnames[i].str) == 0)\n                return (vmm_pnames[i].type);\n    }\n\n    return (VMM_GUEST_NONE);\n}\n</code></pre> <p>Detected VM types: - VMM_GUEST_QEMU - VMM_GUEST_XEN - VMM_GUEST_BHYVE - VMM_GUEST_KVM - VMM_GUEST_VMWARE - VMM_GUEST_HYPERV - VMM_GUEST_VBOX - VMM_GUEST_PARALLELS</p> <p>Exposed via <code>sysctl kern.vmm_guest</code>.</p>"},{"location":"sys/kern/initialization/#kernel-environment","title":"Kernel Environment","text":"<p>Source: <code>sys/kern/kern_environment.c</code></p> <p>The kernel environment provides a key-value store passed from the bootloader, similar to Unix environment variables.</p>"},{"location":"sys/kern/initialization/#architecture_1","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph BOOT[\"Boot time\"]\n        LOADER[\"Bootloader (loader(8))Sets up static environment\u2022 kernelname=/boot/kernel\u2022 kern.hz=1000\u2022 etc.\"]\n    end\n\n    subgraph STATIC[\"Static environment\"]\n        SENV[\"(null-terminated strings)name=value\\\\0name2=value2\\\\0\\\\0\"]\n    end\n\n    subgraph DYNAMIC[\"Dynamic environment\"]\n        DENV[\"(kmalloc'd array of strings)kenv_dynp[] = {  'name=value',  'name2=value2',  NULL}+ spinlock protection+ ksetenv/kunsetenv support\"]\n    end\n\n    LOADER --&gt;|\"kern_envp pointer\"| STATIC\n    STATIC --&gt;|\"kenv_init() - SI_BOOT1_POST\"| DYNAMIC\n</code></pre>"},{"location":"sys/kern/initialization/#static-environment","title":"Static Environment","text":"<p>At boot, the bootloader passes a pointer to a static environment:</p> <pre><code>char *kern_envp;  /* Pointer to static environment */\n</code></pre> <p>Format: <pre><code>\"var1=value1\\0var2=value2\\0var3=value3\\0\\0\"\n           ^              ^              ^\n           |              |              |\n         NUL            NUL          Double NUL (end)\n</code></pre></p>"},{"location":"sys/kern/initialization/#dynamic-environment","title":"Dynamic Environment","text":"<p>After <code>kenv_init()</code> (SI_BOOT1_POST), the environment becomes dynamic:</p> <pre><code>/* kern_environment.c:570 */\nstatic void kenv_init(void *dummy)\n{\n    char *cp;\n    int len, i;\n\n    /* Allocate dynamic array (max 512 entries) */\n    kenv_dynp = kmalloc(KENV_DYNMAXNUM * sizeof(char *),\n                        M_KENV, M_WAITOK | M_ZERO);\n\n    /* Copy static environment to dynamic */\n    for (i = 0, cp = kern_envp; cp != NULL; cp = kernenv_next(cp)) {\n        len = strlen(cp) + 1;\n        if (i &lt; (KENV_DYNMAXNUM - 1)) {\n            kenv_dynp[i] = kmalloc(len, M_KENV, M_WAITOK);\n            strcpy(kenv_dynp[i++], cp);\n        } else {\n            kprintf(\"WARNING: kenv: exhausted dynamic storage\\n\");\n        }\n    }\n    kenv_dynp[i] = NULL;\n\n    spin_init(&amp;kenv_dynlock, \"kenvdynlock\");\n    kenv_isdynamic = 1;\n}\n</code></pre> <p>After this: - Static environment is still accessible (read-only) - Dynamic environment is modifiable - Protected by spinlock (<code>kenv_dynlock</code>)</p>"},{"location":"sys/kern/initialization/#api","title":"API","text":""},{"location":"sys/kern/initialization/#get-environment-variable","title":"Get Environment Variable","text":"<pre><code>char *kgetenv(const char *name);\n</code></pre> <p>Returns a kmalloc'd copy of the value, or NULL if not found. Must be freed with <code>kfreeenv()</code>.</p> <p>Example: <pre><code>char *hz_str = kgetenv(\"kern.hz\");\nif (hz_str != NULL) {\n    hz = atoi(hz_str);\n    kfreeenv(hz_str);\n}\n</code></pre></p>"},{"location":"sys/kern/initialization/#set-environment-variable","title":"Set Environment Variable","text":"<pre><code>int ksetenv(const char *name, const char *value);\n</code></pre> <p>Sets or replaces an environment variable. Returns 0 on success, -1 on error.</p> <p>Example: <pre><code>ksetenv(\"debug.verbose\", \"1\");\n</code></pre></p>"},{"location":"sys/kern/initialization/#unset-environment-variable","title":"Unset Environment Variable","text":"<pre><code>int kunsetenv(const char *name);\n</code></pre> <p>Removes an environment variable. Returns 0 on success, -1 if not found.</p>"},{"location":"sys/kern/initialization/#test-environment-variable","title":"Test Environment Variable","text":"<pre><code>int ktestenv(const char *name);\n</code></pre> <p>Returns 1 if variable exists, 0 otherwise.</p>"},{"location":"sys/kern/initialization/#typed-getters","title":"Typed Getters","text":"<pre><code>int kgetenv_string(const char *name, char *data, int size);\nint kgetenv_int(const char *name, int *data);\nint kgetenv_long(const char *name, long *data);\nint kgetenv_ulong(const char *name, unsigned long *data);\nint kgetenv_quad(const char *name, quad_t *data);\n</code></pre> <p>These return 1 on success, 0 if variable not found or parse error.</p> <p>kgetenv_quad() supports size suffixes: - <code>k</code> or <code>K</code>: \u00d7 1024 - <code>m</code> or <code>M</code>: \u00d7 1024\u00b2 - <code>g</code> or <code>G</code>: \u00d7 1024\u00b3 - <code>t</code> or <code>T</code>: \u00d7 1024\u2074</p> <p>Example: <pre><code>quad_t size;\nif (kgetenv_quad(\"vm.kmem_size\", &amp;size)) {\n    /* size now contains value in bytes */\n    /* \"512m\" \u2192 536870912 */\n    /* \"2g\" \u2192 2147483648 */\n}\n</code></pre></p>"},{"location":"sys/kern/initialization/#free-environment-value","title":"Free Environment Value","text":"<pre><code>void kfreeenv(char *env);\n</code></pre> <p>Frees a string returned by <code>kgetenv()</code>.</p>"},{"location":"sys/kern/initialization/#userspace-access","title":"Userspace Access","text":"<p>The <code>kenv(2)</code> system call provides userspace access:</p> <pre><code>int kenv(int action, const char *name, char *value, int len);\n</code></pre> <p>Actions: - <code>KENV_GET</code>: Get variable value - <code>KENV_SET</code>: Set variable (requires SYSCAP_NOKENV_WR) - <code>KENV_UNSET</code>: Unset variable (requires SYSCAP_NOKENV_WR) - <code>KENV_DUMP</code>: Dump all variables</p> <p>Userspace tools: <pre><code># Get variable\nkenv kern.hz\n\n# Set variable\nkenv kern.verbose=1\n\n# Unset variable\nkenv -u debug.trace\n\n# Dump all\nkenv\n</code></pre></p>"},{"location":"sys/kern/initialization/#tunables","title":"Tunables","text":"<p>The TUNABLE macros simplify reading boot-time configuration:</p> <pre><code>/* Definition (from subr_param.c:74) */\nint maxusers;\n\nTUNABLE_INT_FETCH(\"kern.maxusers\", &amp;maxusers);\n</code></pre> <p>TUNABLE types: - <code>TUNABLE_INT_FETCH(path, var)</code> - <code>TUNABLE_LONG_FETCH(path, var)</code> - <code>TUNABLE_ULONG_FETCH(path, var)</code> - <code>TUNABLE_QUAD_FETCH(path, var)</code> - <code>TUNABLE_STR_FETCH(path, var, size)</code></p> <p>These are typically called during <code>init_param1()</code> or <code>init_param2()</code>.</p>"},{"location":"sys/kern/initialization/#common-kernel-environment-variables","title":"Common Kernel Environment Variables","text":"Variable Description Example <code>kern.hz</code> Timer frequency (Hz) <code>1000</code> <code>kern.maxusers</code> Base tuning parameter <code>384</code> <code>kern.maxproc</code> Max processes <code>4096</code> <code>kern.maxfiles</code> Max open files <code>65536</code> <code>kern.ipc.maxsockets</code> Max sockets <code>16384</code> <code>vm.kmem_size</code> Kernel memory size <code>512m</code> <code>vm.kmem_size_max</code> Max kernel memory <code>2g</code> <code>debug.bootverbose</code> Verbose boot messages <code>1</code> <code>init_path</code> Path to init <code>/sbin/init:/sbin/oinit</code> <code>kernelname</code> Kernel path <code>/boot/kernel/kernel</code>"},{"location":"sys/kern/initialization/#system-call-table","title":"System Call Table","text":"<p>Source: <code>sys/kern/init_sysent.c</code> (generated from <code>syscalls.master</code>)</p>"},{"location":"sys/kern/initialization/#structure","title":"Structure","text":"<p>The system call table is an array of <code>struct sysent</code>:</p> <pre><code>struct sysent {\n    int         sy_narg;        /* Number of arguments */\n    int         sy_flags;       /* Flags (reserved) */\n    sy_call_t   *sy_call;       /* System call handler */\n};\n\n/* The table */\nstruct sysent sysent[] = {\n    { 0, 4, (sy_call_t *)sys_xsyscall },    /* 0 = syscall */\n    { AS(exit_args), 4, (sy_call_t *)sys_exit },  /* 1 = exit */\n    { 0, 4, (sy_call_t *)sys_fork },        /* 2 = fork */\n    { AS(read_args), 8, (sy_call_t *)sys_read },  /* 3 = read */\n    { AS(write_args), 8, (sy_call_t *)sys_write },/* 4 = write */\n    /* ... 500+ more ... */\n};\n</code></pre> <p>AS() macro: <pre><code>#define AS(name) (sizeof(struct name) / sizeof(register_t))\n</code></pre></p> <p>Computes the number of register-sized arguments.</p>"},{"location":"sys/kern/initialization/#system-call-dispatch","title":"System Call Dispatch","text":"<p>When userspace issues a system call:</p> <ol> <li>Trap to kernel (syscall instruction)</li> <li>Kernel trap handler extracts syscall number</li> <li>Lookup in <code>sysent[]</code> table</li> <li>Validate syscall number and argument count</li> <li>Copy arguments from userspace</li> <li>Call <code>sy_call()</code> function</li> <li>Return result to userspace</li> </ol> <p>Example syscalls: <pre><code>/* 1 = exit */\n{ AS(exit_args), 4, (sy_call_t *)sys_exit }\n\nstruct exit_args {\n    int rval;  /* Exit status */\n};\n\nint sys_exit(struct sysmsg *sysmsg, const struct exit_args *uap)\n{\n    /* Never returns */\n    exit1(W_EXITCODE(uap-&gt;rval, 0));\n    /* NOTREACHED */\n}\n</code></pre></p> <pre><code>/* 59 = execve */\n{ AS(execve_args), 4, (sy_call_t *)sys_execve }\n\nstruct execve_args {\n    char *fname;      /* Path to executable */\n    char **argv;      /* Argument vector */\n    char **envv;      /* Environment vector */\n};\n\nint sys_execve(struct sysmsg *sysmsg, const struct execve_args *uap)\n{\n    /* Implemented in kern_exec.c */\n    return kern_execve(...);\n}\n</code></pre>"},{"location":"sys/kern/initialization/#obsolete-syscalls","title":"Obsolete Syscalls","text":"<p>Many syscall numbers are marked obsolete:</p> <pre><code>{ 0, 4, (sy_call_t *)sys_nosys },  /* 8 = __nosys */\n{ 0, 4, (sy_call_t *)sys_nosys },  /* 11 = obsolete execv */\n</code></pre> <p>These return <code>ENOSYS</code> (function not implemented).</p>"},{"location":"sys/kern/initialization/#generating-the-table","title":"Generating the Table","text":"<p>The syscall table is generated from <code>sys/kern/syscalls.master</code>:</p> <pre><code>0   STD     { int syscall(void); }\n1   STD     { void exit(int rval); }\n2   STD     { int fork(void); }\n3   STD     { ssize_t read(int fd, void *buf, size_t nbyte); }\n4   STD     { ssize_t write(int fd, const void *buf, size_t nbyte); }\n...\n</code></pre> <p>Build process: <pre><code>cd sys/kern\nmake sysent\n</code></pre></p> <p>This regenerates: - <code>init_sysent.c</code> - syscall table - <code>syscalls.c</code> - syscall names array - <code>sysproto.h</code> - argument structure declarations - <code>sysmsg.h</code> - message structure declarations - <code>sysunion.h</code> - union of all argument structures</p>"},{"location":"sys/kern/initialization/#global-data-initialization","title":"Global Data Initialization","text":"<p>Function: <code>mi_gdinit()</code> (<code>init_main.c:772</code>)</p> <p>Called for each CPU to initialize per-CPU global data:</p> <pre><code>void mi_gdinit(struct globaldata *gd, int cpuid)\n{\n    /* Initialize systimer queue */\n    TAILQ_INIT(&amp;gd-&gt;gd_systimerq);\n\n    /* Set CPU ID */\n    gd-&gt;gd_sysid_alloc = cpuid;\n    gd-&gt;gd_cpuid = cpuid;\n\n    /* Set CPU mask */\n    CPUMASK_ASSBIT(gd-&gt;gd_cpumask, cpuid);\n    gd-&gt;gd_cpumask_simple = CPUMASK_SIMPLE(cpuid);\n    gd-&gt;gd_cpumask_offset = ...;\n\n    /* Initialize LWKT for this CPU */\n    lwkt_gdinit(gd);\n\n    /* Initialize VM map entry reserve */\n    vm_map_entry_reserve_cpu_init(gd);\n\n    /* Initialize sleep queue */\n    if (gd-&gt;gd_cpuid == 0)\n        sleep_early_gdinit(gd);\n    else\n        sleep_gdinit(gd);\n\n    /* Initialize slab allocator for this CPU */\n    slab_gdinit(gd);\n\n    /* Add CPU to global mask */\n    ATOMIC_CPUMASK_ORBIT(usched_global_cpumask, cpuid);\n\n    /* Set up vmstats pointer */\n    gd-&gt;gd_vmstats = vmstats;\n}\n</code></pre> <p>This is called: - Once for CPU 0 during early boot - Once for each additional CPU during SMP initialization</p>"},{"location":"sys/kern/initialization/#kernel-process-map-kpmap","title":"Kernel Process Map (kpmap)","text":"<p>Function: <code>kpmap_init()</code> (<code>init_main.c:744</code>)</p> <p>Initializes a shared read-only page mapped into all processes:</p> <pre><code>static void kpmap_init(const void *udata)\n{\n    /* Allocate page-aligned kpmap structure */\n    kpmap = kmalloc(roundup2(sizeof(*kpmap), PAGE_SIZE),\n                    M_TEMP, M_ZERO | M_WAITOK);\n\n    /* Set up header describing available fields */\n    kpmap-&gt;header[0].type = UKPTYPE_VERSION;\n    kpmap-&gt;header[0].offset = offsetof(struct sys_kpmap, version);\n\n    kpmap-&gt;header[1].type = KPTYPE_UPTICKS;\n    kpmap-&gt;header[1].offset = offsetof(struct sys_kpmap, upticks);\n\n    kpmap-&gt;header[2].type = KPTYPE_TS_UPTIME;\n    kpmap-&gt;header[2].offset = offsetof(struct sys_kpmap, ts_uptime);\n\n    kpmap-&gt;header[3].type = KPTYPE_TS_REALTIME;\n    kpmap-&gt;header[3].offset = offsetof(struct sys_kpmap, ts_realtime);\n\n    kpmap-&gt;header[4].type = KPTYPE_TSC_FREQ;\n    kpmap-&gt;header[4].offset = offsetof(struct sys_kpmap, tsc_freq);\n\n    kpmap-&gt;header[5].type = KPTYPE_TICK_FREQ;\n    kpmap-&gt;header[5].offset = offsetof(struct sys_kpmap, tick_freq);\n\n    kpmap-&gt;header[6].type = KPTYPE_FAST_GTOD;\n    kpmap-&gt;header[6].offset = offsetof(struct sys_kpmap, fast_gtod);\n\n    kpmap-&gt;version = KPMAP_VERSION;\n}\nSYSINIT(kpmapinit, SI_BOOT1_POST, SI_ORDER_FIRST, kpmap_init, NULL);\n</code></pre> <p>This page is later mapped into userspace, allowing fast access to: - System uptime - Real-time clock - TSC frequency - Tick frequency - Fast gettimeofday</p> <p>This avoids syscall overhead for time queries.</p>"},{"location":"sys/kern/initialization/#summary","title":"Summary","text":"<p>DragonFly's kernel initialization follows a well-orchestrated sequence:</p> <ol> <li>Machine-dependent setup - CPU, memory, interrupts</li> <li>mi_proc0init() - Initialize proc0, thread0, lwp0</li> <li>mi_startup() - Execute SYSINIT functions in order</li> <li>proc0_init() - Complete proc0 setup</li> <li>create_init() - Fork init process (PID 1)</li> <li>kick_init() - Start init process</li> <li>start_init() - Exec /sbin/init</li> </ol> <p>Key mechanisms: - SYSINIT framework: Ordered subsystem initialization - Kernel parameters: Computed based on physical memory - Environment: Boot-time configuration key-value store - System call table: Dispatch table for userspace\u2192kernel transitions</p> <p>After initialization completes, the init process becomes the first userspace process, and normal system operation begins.</p>"},{"location":"sys/kern/ipc/","title":"IPC &amp; Sockets Overview","text":"<p>DragonFly BSD provides multiple inter-process communication mechanisms, from low-level kernel primitives to POSIX-standard APIs. This section documents the kernel implementation of these facilities.</p>"},{"location":"sys/kern/ipc/#socket-layer","title":"Socket Layer","text":"<p>The socket abstraction provides a uniform interface for network and local communication:</p> <ul> <li>Mbufs - Network memory buffer management</li> <li>Sockets - Socket API implementation and socket buffers</li> <li>Unix Domain Sockets - Local IPC via filesystem namespace</li> <li>Protocol Dispatch - Protocol family registration and netisr</li> </ul>"},{"location":"sys/kern/ipc/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    USER[\"User Space\"]\n\n    USER --&gt;|\"socket(), bind(), connect(), send(), recv()\"| SOCKET\n\n    subgraph SOCKET[\"Socket Layer\"]\n        STRUCT[\"socketstruct\"]\n        SOCKBUF[\"sockbuf(SSB)\"]\n        PROTOSW[\"protoswdispatch\"]\n        STRUCT --- SOCKBUF\n    end\n\n    PROTOSW --&gt; UNIX[\"UnixDomain\"]\n    PROTOSW --&gt; TCP[\"TCP/IP\"]\n    PROTOSW --&gt; UDP[\"UDP/IP\"]\n</code></pre>"},{"location":"sys/kern/ipc/#pipes","title":"Pipes","text":"<p>Traditional Unix pipes for unidirectional byte streams between related processes:</p> <ul> <li>Pipes - pipe() system call, direct data transfer optimization</li> </ul>"},{"location":"sys/kern/ipc/#message-queues","title":"Message Queues","text":"<p>Two message queue implementations:</p> <ul> <li>POSIX Message Queues - mq_open(), priority-based queuing</li> <li>System V Message Queues - msgget(), msgctl(), msgsnd(), msgrcv()</li> </ul>"},{"location":"sys/kern/ipc/#system-v-ipc","title":"System V IPC","text":"<p>Classic System V IPC primitives:</p> <ul> <li>System V Message Queues - Message passing</li> <li>System V Semaphores - Counting semaphores with undo support</li> <li>System V Shared Memory - Shared memory segments</li> </ul> <p>All System V IPC facilities share common permission checking via <code>ipcperm()</code> and use integer keys for namespace management.</p>"},{"location":"sys/kern/ipc/#comparison","title":"Comparison","text":"Mechanism Scope Data Model Persistence Best For Pipes Related processes Byte stream Process lifetime Simple parent-child Unix Sockets Local system Stream/Datagram Filesystem Local services, fd passing POSIX Mqueue System-wide Messages Kernel (can persist) Priority messaging SysV Mqueue System-wide Messages Kernel Legacy compatibility SysV Semaphores System-wide Counters Kernel Process synchronization SysV Shm System-wide Raw memory Kernel High-bandwidth sharing TCP/UDP Sockets Network Stream/Datagram Connection lifetime Network communication"},{"location":"sys/kern/ipc/#key-source-files","title":"Key Source Files","text":"File Description <code>sys/kern/uipc_socket.c</code> Socket layer core <code>sys/kern/uipc_usrreq.c</code> Unix domain sockets <code>sys/kern/uipc_mbuf.c</code> Mbuf allocation <code>sys/kern/sys_pipe.c</code> Pipe implementation <code>sys/kern/sys_mqueue.c</code> POSIX message queues <code>sys/kern/sysv_msg.c</code> System V messages <code>sys/kern/sysv_sem.c</code> System V semaphores <code>sys/kern/sysv_shm.c</code> System V shared memory"},{"location":"sys/kern/lwkt/","title":"LWKT Threading","text":"<p>LWKT (Lightweight Kernel Threading) is DragonFly BSD's unique message-passing based concurrency model. It is the architectural foundation that distinguishes DragonFly from traditional BSD systems and is essential to understand before exploring any other kernel subsystem.</p>"},{"location":"sys/kern/lwkt/#overview","title":"Overview","text":"<p>LWKT implements a message-passing threading model designed for multiprocessor scalability. Instead of relying primarily on locks to protect shared data, DragonFly uses:</p> <ul> <li>Message passing between threads via message ports</li> <li>Serializing tokens that can be held across blocking operations</li> <li>Per-CPU thread schedulers that minimize cross-CPU synchronization</li> <li>Inter-processor interrupt queues (IPIQs) for cross-CPU communication</li> </ul>"},{"location":"sys/kern/lwkt/#why-lwkt-exists","title":"Why LWKT Exists","text":"<p>Traditional BSD kernels use pervasive locking (mutexes, spinlocks, read-write locks) to protect shared data structures in multiprocessor environments. This approach suffers from:</p> <ul> <li>Lock contention \u2014 Multiple CPUs waiting for the same lock</li> <li>Cache-line bouncing \u2014 Locks ping-pong between CPU caches</li> <li>Priority inversion \u2014 Lower-priority threads holding locks needed by higher-priority threads</li> <li>Deadlock potential \u2014 Complex lock ordering requirements</li> </ul> <p>DragonFly's LWKT addresses these issues by:</p> <ul> <li>Minimizing shared state \u2014 Each CPU has its own scheduler and thread queues</li> <li>Using message passing \u2014 Threads communicate via asynchronous messages instead of shared memory</li> <li>Allowing tokens across sleeps \u2014 Tokens don't have the strict semantics of traditional locks</li> <li>Deferring to thread owner \u2014 Operations on a thread are sent as messages to its owning CPU</li> </ul>"},{"location":"sys/kern/lwkt/#where-lwkt-fits-in-the-architecture","title":"Where LWKT Fits in the Architecture","text":"<p>LWKT is the lowest-level threading abstraction in DragonFly. It sits below:</p> <ul> <li>Process/LWP management (<code>kern_proc.c</code>, <code>kern_fork.c</code>, etc.)</li> <li>CPU scheduling policies (<code>usched_*.c</code>)</li> <li>All kernel subsystems (VFS, VM, networking, etc.)</li> </ul> <p>Everything in the kernel runs in the context of an LWKT thread. Even interrupt handlers run as threads in DragonFly.</p>"},{"location":"sys/kern/lwkt/#key-concepts","title":"Key Concepts","text":""},{"location":"sys/kern/lwkt/#threads-vs-processes","title":"Threads vs Processes","text":"<p>In DragonFly:</p> <ul> <li>Thread (<code>struct thread</code>) \u2014 The basic unit of execution in LWKT</li> <li>LWP (<code>struct lwp</code>) \u2014 Light Weight Process, represents a user-level thread of execution</li> <li>Process (<code>struct proc</code>) \u2014 A collection of LWPs sharing an address space and resources</li> </ul> <p>An LWKT thread may be:</p> <ul> <li>A kernel thread (no associated LWP or process)</li> <li>A user thread (has an associated LWP and process)</li> </ul> <p>All execution happens via LWKT threads. User threads enter the kernel via system calls, traps, or signals and execute in kernel mode using their LWKT thread context.</p>"},{"location":"sys/kern/lwkt/#message-passing","title":"Message Passing","text":"<p>Threads communicate via message ports (<code>lwkt_port</code>). Each thread has an embedded message port (<code>td_msgport</code>).</p> <p>Synchronous messaging: <pre><code>// Send message, block until reply\nlwkt_sendmsg(target_port, &amp;msg);\n</code></pre></p> <p>Asynchronous messaging: <pre><code>// Send message, don't wait for reply\nlwkt_sendmsg_async(target_port, &amp;msg);\n// ... do other work ...\n// Later, check for reply\nlwkt_waitmsg(&amp;msg, 0);\n</code></pre></p> <p>Messages (<code>struct lwkt_msg</code>) contain:</p> <ul> <li>Target and reply ports</li> <li>Result fields (error code, return value)</li> <li>State flags (DONE, REPLY, QUEUED, SYNC, etc.)</li> </ul>"},{"location":"sys/kern/lwkt/#tokens-serialization-without-traditional-locking","title":"Tokens: Serialization Without Traditional Locking","text":"<p>Tokens (<code>struct lwkt_token</code>) are DragonFly's primary synchronization primitive. They differ fundamentally from traditional locks:</p> <p>Traditional locks (mutexes, spinlocks): - Must be released before blocking - Strict acquire/release semantics - Can deadlock if ordering is incorrect - Cause cache-line bouncing</p> <p>DragonFly tokens: - Can be held across blocking operations - Automatically released on sleep, reacquired on wakeup - Cannot deadlock regardless of acquisition order - Serialization only effective while thread is running</p> <p>Example: <pre><code>lwkt_gettoken(&amp;mp-&gt;mnt_token);  // Acquire token\n\n// Can safely sleep here!\n// Token is temporarily released, reacquired on wakeup\ntsleep(wchan, 0, \"wait\", 0);\n\n// Still holding token after wakeup\nlwkt_reltoken(&amp;mp-&gt;mnt_token);\n</code></pre></p> <p>The key insight: tokens provide logical serialization rather than physical lock-holding. If you block, another thread may run and access the same data, but it will also hold the token, maintaining serialization.</p>"},{"location":"sys/kern/lwkt/#token-types","title":"Token Types","text":"<p>Tokens support two acquisition modes:</p> <ul> <li>Exclusive \u2014 Only one thread at a time (TOK_EXCLUSIVE bit set)</li> <li>Shared \u2014 Multiple threads simultaneously (reference count in <code>t_count</code>)</li> </ul> <p>Multiple exclusive acquisitions by the same thread are allowed and tracked.</p>"},{"location":"sys/kern/lwkt/#per-cpu-scheduling","title":"Per-CPU Scheduling","text":"<p>Each CPU has its own thread scheduler:</p> <ul> <li>Thread queues are per-CPU (<code>gd_tdrunq</code>, <code>gd_tdallq</code>)</li> <li>Switching threads on the same CPU requires only a critical section</li> <li>No locks or cross-CPU synchronization for local scheduling</li> </ul> <p>To schedule a thread on another CPU, use IPIQs (see below).</p>"},{"location":"sys/kern/lwkt/#ipiqs-inter-processor-interrupt-queues","title":"IPIQs: Inter-Processor Interrupt Queues","text":"<p>When one CPU needs to operate on a thread owned by another CPU, it sends a message via an IPIQ (<code>struct lwkt_ipiq</code>):</p> <ul> <li>Lock-free circular buffer (FIFO)</li> <li>Source CPU writes functions to execute</li> <li>Target CPU processes them in interrupt context</li> <li>Used for cross-CPU thread migration, scheduling, etc.</li> </ul> <p>Example: To schedule a thread on CPU 1 from CPU 0: 1. CPU 0 writes a scheduling function to CPU 1's IPIQ 2. CPU 0 sends an inter-processor interrupt (IPI) to CPU 1 3. CPU 1 handles the IPI, processes the IPIQ entry 4. CPU 1 adds the thread to its local run queue</p>"},{"location":"sys/kern/lwkt/#critical-sections","title":"Critical Sections","text":"<p>Critical sections prevent preemption and must be used carefully:</p> <pre><code>crit_enter();\n// Cannot be preempted here\n// Keep this SHORT!\ncrit_exit();\n</code></pre> <p>Critical sections do not prevent interrupts, but LWKT threads (including interrupt threads) will not preempt code in a critical section on the same CPU.</p>"},{"location":"sys/kern/lwkt/#thread-ownership","title":"Thread Ownership","text":"<p>A thread is owned by the CPU in its <code>td_gd</code> (globaldata) field. Only the owning CPU can directly manipulate the thread. Other CPUs must use:</p> <ul> <li>IPIQs to send requests to the owning CPU</li> <li>Messaging to communicate with the thread</li> </ul> <p>This ownership model eliminates many locking requirements.</p>"},{"location":"sys/kern/lwkt/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/lwkt/#struct-thread","title":"<code>struct thread</code>","text":"<p>Defined in <code>sys/sys/thread.h</code>. Key fields:</p> <pre><code>struct thread {\n    TAILQ_ENTRY(thread) td_threadq;   // Queue linkage (run/sleep/etc.)\n    TAILQ_ENTRY(thread) td_allq;      // Link in gd_tdallq\n    lwkt_port td_msgport;             // Built-in message port\n\n    struct lwp *td_lwp;                // Associated LWP (if user thread)\n    struct proc *td_proc;              // Associated process (if user thread)\n    struct pcb *td_pcb;                // Process control block, top of kstack\n    struct globaldata *td_gd;          // Owning CPU's globaldata\n\n    const char *td_wmesg;              // Reason for blocking\n    const volatile void *td_wchan;     // Wait channel\n    int td_pri;                        // Priority (0-31, 31=highest)\n    int td_critcount;                  // Critical section nesting count\n    u_int td_flags;                    // TDF_* flags\n\n    char *td_kstack;                   // Kernel stack base\n    int td_kstack_size;                // Kernel stack size\n    char *td_sp;                       // Saved stack pointer for context switch\n\n    thread_t (*td_switch)(struct thread *); // Context switch function\n\n    lwkt_tokref_t td_toks_have;        // Tokens currently held\n    lwkt_tokref_t td_toks_stop;        // Tokens to acquire\n    struct lwkt_tokref td_toks_array[LWKT_MAXTOKENS];\n\n    char td_comm[MAXCOMLEN+1];         // Thread name\n    struct ucred *td_ucred;            // Credentials (synchronized from proc)\n\n    struct md_thread td_mach;          // Machine-dependent state\n};\n</code></pre> <p>Important fields:</p> <ul> <li><code>td_gd</code> \u2014 Identifies the owning CPU</li> <li><code>td_msgport</code> \u2014 Every thread has a built-in message port</li> <li><code>td_toks_have</code> / <code>td_toks_stop</code> \u2014 Token stack for serialization</li> <li><code>td_pri</code> \u2014 Determines scheduling priority</li> <li><code>td_critcount</code> \u2014 Critical section depth (&gt;0 means non-preemptible)</li> <li><code>td_switch</code> \u2014 Function pointer for machine-dependent context switching</li> </ul>"},{"location":"sys/kern/lwkt/#struct-lwkt_msg","title":"<code>struct lwkt_msg</code>","text":"<p>Defined in <code>sys/sys/msgport.h</code>. Messages sent between threads:</p> <pre><code>struct lwkt_msg {\n    TAILQ_ENTRY(lwkt_msg) ms_node;    // Queue linkage\n    lwkt_port_t ms_target_port;       // Current target port\n    lwkt_port_t ms_reply_port;        // Reply sent here\n\n    void (*ms_abortfn)(struct lwkt_msg *);  // Abort handler\n    int ms_flags;                     // MSGF_* flags\n    int ms_error;                     // Error code (0 = success)\n\n    union {\n        void *ms_resultp;             // Pointer result\n        int ms_result;                // Integer result\n        long ms_lresult;              // Long result\n        __int64_t ms_result64;        // 64-bit result\n        __off_t ms_offset;            // Offset result\n    } u;\n\n    void (*ms_receiptfn)(struct lwkt_msg *, lwkt_port_t);\n};\n</code></pre> <p>Message flags (<code>ms_flags</code>):</p> <ul> <li><code>MSGF_DONE</code> \u2014 Message complete</li> <li><code>MSGF_REPLY</code> \u2014 Message is a reply</li> <li><code>MSGF_QUEUED</code> \u2014 Queued on a port</li> <li><code>MSGF_SYNC</code> \u2014 Synchronous (caller blocked)</li> <li><code>MSGF_INTRANSIT</code> \u2014 Being passed via IPI</li> <li><code>MSGF_ABORTABLE</code> \u2014 Can be aborted</li> <li><code>MSGF_PRIORITY</code> \u2014 High-priority message</li> </ul>"},{"location":"sys/kern/lwkt/#struct-lwkt_port","title":"<code>struct lwkt_port</code>","text":"<p>Message ports for receiving messages:</p> <pre><code>struct lwkt_port {\n    lwkt_msg_queue mp_msgq;           // Normal priority messages\n    lwkt_msg_queue mp_msgq_prio;      // High priority messages\n    int mp_flags;                     // Port flags\n    int mp_cpuid;                     // CPU affinity\n\n    union {\n        struct spinlock spin;         // Spinlock-protected port\n        struct lwkt_serialize *serialize;  // Serializer-protected\n        void *data;                   // Or custom data\n    } mp_u;\n\n    struct thread *mpu_td;            // Owning thread (if thread port)\n\n    // Port operations (function pointers):\n    void (*mp_putport)(lwkt_port_t, lwkt_msg_t);\n    int (*mp_waitmsg)(lwkt_msg_t, int);\n    void *(*mp_waitport)(lwkt_port_t, int);\n    void (*mp_replyport)(lwkt_port_t, lwkt_msg_t);\n    int (*mp_dropmsg)(lwkt_port_t, lwkt_msg_t);\n};\n</code></pre>"},{"location":"sys/kern/lwkt/#struct-lwkt_token","title":"<code>struct lwkt_token</code>","text":"<p>Serializing tokens:</p> <pre><code>struct lwkt_token {\n    long t_count;                     // Shared count | EXCLUSIVE | EXCLREQ\n    struct lwkt_tokref *t_ref;        // Exclusive holder reference\n    long t_collisions;                // Contention counter\n    const char *t_desc;               // Descriptive name\n};\n</code></pre> <p>Token states (encoded in <code>t_count</code>):</p> <ul> <li><code>TOK_EXCLUSIVE</code> (bit 0) \u2014 Exclusively held</li> <li><code>TOK_EXCLREQ</code> (bit 1) \u2014 Exclusive request pending</li> <li>Count (bits 2+) \u2014 Number of shared holders (shifted by <code>TOK_INCR</code>)</li> </ul>"},{"location":"sys/kern/lwkt/#struct-lwkt_tokref","title":"<code>struct lwkt_tokref</code>","text":"<p>Token reference (thread's token stack entry):</p> <pre><code>struct lwkt_tokref {\n    lwkt_token_t tr_tok;              // Token being held\n    long tr_count;                    // TOK_EXCLUSIVE or 0\n    struct thread *tr_owner;          // Thread holding this ref\n};\n</code></pre> <p>Each thread has an array of <code>LWKT_MAXTOKENS</code> (32) token references, allowing nested token acquisition.</p>"},{"location":"sys/kern/lwkt/#struct-lwkt_ipiq","title":"<code>struct lwkt_ipiq</code>","text":"<p>Inter-processor interrupt queue:</p> <pre><code>struct lwkt_ipiq {\n    int ip_rindex;                    // Read index (target CPU updates)\n    int ip_xindex;                    // Completion index (target updates)\n    int ip_windex;                    // Write index (source CPU updates)\n    int ip_drain;                     // Drain source limit\n\n    struct {\n        ipifunc3_t func;              // Function to execute\n        void *arg1;                   // First argument\n        int arg2;                     // Second argument\n        char filler[32 - ...];        // Cache-line alignment\n    } ip_info[MAXCPUFIFO];            // Circular buffer (256 entries)\n};\n</code></pre> <p>Lock-free design: - Source CPU writes to <code>ip_windex</code> slot - Target CPU reads from <code>ip_rindex</code> - No locks needed due to single-writer, single-reader pattern</p>"},{"location":"sys/kern/lwkt/#key-functions","title":"Key Functions","text":""},{"location":"sys/kern/lwkt/#thread-management","title":"Thread Management","text":""},{"location":"sys/kern/lwkt/#lwkt_init_thread","title":"<code>lwkt_init_thread()</code>","text":"<p>Initialize a new LWKT thread structure.</p> <ul> <li>Purpose: Set up thread structure, allocate kernel stack, initialize message port</li> <li>Called by: <code>lwkt_alloc_thread()</code>, kernel thread creation routines</li> <li>Parameters: <code>thread_t td, void *stack, int stksize, int flags, struct globaldata *gd</code></li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_alloc_thread","title":"<code>lwkt_alloc_thread()</code>","text":"<p>Allocate and initialize a new LWKT thread.</p> <ul> <li>Purpose: Allocate memory for thread structure and kernel stack</li> <li>Returns: Initialized <code>thread_t</code></li> <li>Called by: <code>lwkt_create()</code>, process/thread creation code</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_switch","title":"<code>lwkt_switch()</code>","text":"<p>Low-level context switch between threads.</p> <ul> <li>Purpose: Save current thread context, restore next thread context</li> <li>Called by: Scheduler when switching threads</li> <li>Critical section: Must be called within a critical section</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_schedule_self","title":"<code>lwkt_schedule_self()</code>","text":"<p>Deschedule current thread (voluntary sleep).</p> <ul> <li>Purpose: Place current thread on specified queue, switch to next runnable thread</li> <li>Called by: <code>tsleep()</code>, <code>lwkt_deschedule_self()</code>, any code voluntarily blocking</li> <li>Note: Thread will resume when rescheduled by another CPU via <code>lwkt_schedule()</code></li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_schedule","title":"<code>lwkt_schedule()</code>","text":"<p>Schedule a thread for execution.</p> <ul> <li>Purpose: Add thread to its CPU's run queue</li> <li>Cross-CPU: If thread is on another CPU, uses IPIQ to send scheduling request</li> <li>Called by: Wakeup routines, thread creation, message completion</li> </ul>"},{"location":"sys/kern/lwkt/#message-passing_1","title":"Message Passing","text":""},{"location":"sys/kern/lwkt/#lwkt_sendmsg","title":"<code>lwkt_sendmsg()</code>","text":"<p>Send a synchronous message (block until reply).</p> <ul> <li>Purpose: Send message to target port, block waiting for reply</li> <li>Returns: Error code from message (<code>ms_error</code>)</li> <li>Typical use: Synchronous RPC-style operations</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_sendmsg_async","title":"<code>lwkt_sendmsg_async()</code>","text":"<p>Send an asynchronous message (don't wait).</p> <ul> <li>Purpose: Initiate message send, return immediately</li> <li>Note: Caller must later call <code>lwkt_waitmsg()</code> or check <code>MSGF_DONE</code></li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_waitmsg","title":"<code>lwkt_waitmsg()</code>","text":"<p>Wait for message completion.</p> <ul> <li>Purpose: Block until message reply is received</li> <li>Parameters: <code>lwkt_msg_t msg, int flags</code></li> <li>Returns: Error code</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_replymsg","title":"<code>lwkt_replymsg()</code>","text":"<p>Reply to a received message.</p> <ul> <li>Purpose: Send reply back to originating port</li> <li>Called by: Message handler after processing request</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_initmsg","title":"<code>lwkt_initmsg()</code>","text":"<p>Initialize a message structure.</p> <ul> <li>Purpose: Set up message for sending</li> <li>Parameters: <code>lwkt_msg_t msg, lwkt_port_t rport, int flags</code></li> </ul>"},{"location":"sys/kern/lwkt/#token-operations","title":"Token Operations","text":""},{"location":"sys/kern/lwkt/#lwkt_gettoken","title":"<code>lwkt_gettoken()</code>","text":"<p>Acquire a token (exclusive by default).</p> <ul> <li>Purpose: Serialize access to protected data</li> <li>Blocking: Spins if token unavailable, may deschedule on contention</li> <li>Can be called multiple times (token stack)</li> <li>Held across sleep: Token automatically released/reacquired</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_reltoken","title":"<code>lwkt_reltoken()</code>","text":"<p>Release a token.</p> <ul> <li>Purpose: Release the most recently acquired token</li> <li>Must match acquisition (tokens released in reverse order of acquisition)</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_gettoken_shared","title":"<code>lwkt_gettoken_shared()</code>","text":"<p>Acquire a shared (read) token.</p> <ul> <li>Purpose: Allow multiple concurrent readers</li> <li>Blocks if: Exclusive holder or exclusive request pending</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_token_pool_lookup","title":"<code>lwkt_token_pool_lookup()</code>","text":"<p>Look up a token from a token pool.</p> <ul> <li>Purpose: Get a token for a specific object from a pool of tokens</li> <li>Used by: Subsystems that need many tokens (e.g., vnodes)</li> </ul>"},{"location":"sys/kern/lwkt/#ipiq-operations","title":"IPIQ Operations","text":""},{"location":"sys/kern/lwkt/#lwkt_send_ipiq","title":"<code>lwkt_send_ipiq()</code>","text":"<p>Send a function to execute on another CPU.</p> <ul> <li>Purpose: Cross-CPU operation request</li> <li>Parameters: <code>globaldata *gd, ipifunc_t func, void *arg</code></li> <li>Non-blocking: Queues function in target CPU's IPIQ</li> <li>Returns: 0 on success, error if IPIQ full</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_process_ipiq","title":"<code>lwkt_process_ipiq()</code>","text":"<p>Process pending IPIQs.</p> <ul> <li>Purpose: Execute functions queued in local CPU's IPIQ</li> <li>Called by: IPI interrupt handler, scheduler</li> <li>Runs in interrupt context</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_synchronous_ipiq","title":"<code>lwkt_synchronous_ipiq()</code>","text":"<p>Send IPIQ and wait for completion.</p> <ul> <li>Purpose: Execute function on remote CPU, wait for it to finish</li> <li>Blocking: Waits for target CPU to process the request</li> </ul>"},{"location":"sys/kern/lwkt/#port-operations","title":"Port Operations","text":""},{"location":"sys/kern/lwkt/#lwkt_initport_thread","title":"<code>lwkt_initport_thread()</code>","text":"<p>Initialize a thread's built-in port.</p> <ul> <li>Purpose: Set up thread's message port for receiving messages</li> <li>Called by: <code>lwkt_init_thread()</code></li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_waitport","title":"<code>lwkt_waitport()</code>","text":"<p>Wait for a message to arrive on a port.</p> <ul> <li>Purpose: Block until message received</li> <li>Returns: Pointer to received message</li> </ul>"},{"location":"sys/kern/lwkt/#lwkt_getport","title":"<code>lwkt_getport()</code>","text":"<p>Get next message from port (non-blocking).</p> <ul> <li>Purpose: Dequeue message if available</li> <li>Returns: Message or NULL if queue empty</li> </ul>"},{"location":"sys/kern/lwkt/#subsystem-interactions","title":"Subsystem Interactions","text":""},{"location":"sys/kern/lwkt/#with-the-scheduler","title":"With the Scheduler","text":"<p>LWKT provides the low-level threading mechanism; the scheduler determines which thread to run:</p> <ul> <li>Scheduler calls <code>lwkt_switch()</code> to context-switch</li> <li>Threads have priorities (<code>td_pri</code>) used by scheduler</li> <li>Per-CPU run queues (<code>gd_tdrunq</code>) hold runnable threads</li> <li>Scheduler policies (<code>usched_dfly</code>, <code>usched_bsd4</code>) implement different algorithms on top of LWKT</li> </ul> <p>See Scheduling for details.</p>"},{"location":"sys/kern/lwkt/#with-processes-and-lwps","title":"With Processes and LWPs","text":"<p>User threads are LWKT threads with attached LWP and process structures:</p> <ul> <li>System calls execute in user thread's LWKT context</li> <li>Process creation (<code>fork()</code>) creates new LWKT threads</li> <li>Thread exit releases LWKT thread structure</li> </ul> <p>See Processes &amp; Threads for details.</p>"},{"location":"sys/kern/lwkt/#with-the-virtual-filesystem-vfs","title":"With the Virtual Filesystem (VFS)","text":"<p>VFS uses tokens extensively for serialization:</p> <ul> <li>Mount points have tokens (<code>mnt_token</code>)</li> <li>Vnodes use token pools</li> <li>Buffer cache operations hold tokens</li> <li>Token semantics allow sleeping during I/O</li> </ul> <p>See VFS for details.</p>"},{"location":"sys/kern/lwkt/#with-device-drivers","title":"With Device Drivers","text":"<p>Drivers often use:</p> <ul> <li>Serializers (<code>lwkt_serialize</code>) for interrupt synchronization</li> <li>Tokens for driver-internal serialization</li> <li>Message ports for async operations</li> </ul> <p>Device interrupt threads run as LWKT threads, allowing uniform scheduling.</p>"},{"location":"sys/kern/lwkt/#with-the-vm-system","title":"With the VM System","text":"<p>VM operations:</p> <ul> <li>Use tokens to protect VM objects and maps</li> <li>May block during page-ins (tokens held across sleep)</li> <li>Page daemon runs as an LWKT kernel thread</li> </ul>"},{"location":"sys/kern/lwkt/#code-flow-examples","title":"Code Flow Examples","text":""},{"location":"sys/kern/lwkt/#example-1-thread-creation-and-scheduling","title":"Example 1: Thread Creation and Scheduling","text":"<pre><code>// Create a new kernel thread\nthread_t td;\n\ntd = lwkt_alloc_thread(NULL, LWKT_THREAD_STACK, -1, TDF_MPSAFE);\nlwkt_init_thread(td, stack, stksize, 0, my_gd);\ntd-&gt;td_flags |= TDF_MPSAFE;\nbcopy(\"mythrd\", td-&gt;td_comm, sizeof(\"mythrd\"));\n\n// Set up to run a function\ncpu_set_thread_handler(td, my_thread_fn, arg);\n\n// Schedule it for execution\nlwkt_schedule(td);  // Will run on td-&gt;td_gd CPU\n</code></pre> <p>Flow: 1. Allocate thread structure 2. Initialize (stack, message port, globaldata) 3. Set up machine state to call <code>my_thread_fn</code> 4. Add to run queue via <code>lwkt_schedule()</code> 5. Scheduler eventually switches to new thread</p>"},{"location":"sys/kern/lwkt/#example-2-synchronous-message-send","title":"Example 2: Synchronous Message Send","text":"<pre><code>struct lwkt_msg msg;\n\n// Initialize message\nlwkt_initmsg(&amp;msg, &amp;my_reply_port, 0);\n\n// Send to target, block until reply\nint error = lwkt_sendmsg(target_port, &amp;msg);\n\n// Message has been processed, result in msg.ms_error or msg.u.*\nif (error == 0) {\n    result = msg.u.ms_result;\n}\n</code></pre> <p>Flow: 1. Caller initializes message with reply port 2. <code>lwkt_sendmsg()</code> calls target port's <code>mp_putport()</code> function 3. Target port queues message (or processes immediately) 4. Caller blocks waiting for reply (<code>MSGF_SYNC</code> set) 5. Target processes message, calls <code>lwkt_replymsg()</code> 6. Reply wakes up caller, caller returns with result</p>"},{"location":"sys/kern/lwkt/#example-3-token-acquisition-across-sleep","title":"Example 3: Token Acquisition Across Sleep","text":"<pre><code>// Acquire token\nlwkt_gettoken(&amp;vp-&gt;v_token);\n\n// Safe to access vnode fields here\n\n// Need to sleep waiting for I/O\ntsleep(&amp;bp-&gt;b_flags, 0, \"biowait\", 0);\n// Token is temporarily released during sleep\n// Token is reacquired before tsleep() returns\n\n// Still holding token, safe to access vnode\nlwkt_reltoken(&amp;vp-&gt;v_token);\n</code></pre> <p>Flow: 1. <code>lwkt_gettoken()</code> acquires token 2. Code accesses protected data 3. <code>tsleep()</code> deschedules thread, saves token stack 4. While asleep, another thread may acquire the same token 5. On wakeup, <code>tsleep()</code> reacquires all tokens before returning 6. Code continues with token held 7. <code>lwkt_reltoken()</code> releases token</p>"},{"location":"sys/kern/lwkt/#example-4-cross-cpu-scheduling-via-ipiq","title":"Example 4: Cross-CPU Scheduling via IPIQ","text":"<p>CPU 0 wants to schedule a thread owned by CPU 1:</p> <pre><code>// On CPU 0, scheduling thread td (owned by CPU 1)\nlwkt_schedule(td);\n</code></pre> <p>Internal flow: 1. <code>lwkt_schedule()</code> checks <code>td-&gt;td_gd</code> (= CPU 1's globaldata) 2. Sees thread is on another CPU 3. Calls <code>lwkt_send_ipiq(cpu1_gd, lwkt_schedule_remote, td)</code> 4. Writes <code>{lwkt_schedule_remote, td}</code> to CPU 1's IPIQ 5. Sends inter-processor interrupt to CPU 1 6. CPU 1 handles IPI, calls <code>lwkt_process_ipiq()</code> 7. <code>lwkt_process_ipiq()</code> executes <code>lwkt_schedule_remote(td)</code> 8. <code>lwkt_schedule_remote()</code> adds <code>td</code> to CPU 1's run queue</p>"},{"location":"sys/kern/lwkt/#traditional-bsd-vs-dragonfly-lwkt","title":"Traditional BSD vs DragonFly LWKT","text":""},{"location":"sys/kern/lwkt/#traditional-bsd-approach","title":"Traditional BSD Approach","text":"<pre><code>// Traditional: Acquire lock, access data, release lock\nmtx_lock(&amp;vp-&gt;v_lock);\n// Cannot sleep here!\n// Access v_data\nmtx_unlock(&amp;vp-&gt;v_lock);\n</code></pre> <p>Problems: - Locks must be released before sleeping - Complex code to handle sleep/wakeup with locks - Lock ordering requirements to avoid deadlock - Cache-line bouncing on multiprocessor systems</p>"},{"location":"sys/kern/lwkt/#dragonfly-lwkt-approach","title":"DragonFly LWKT Approach","text":"<pre><code>// DragonFly: Acquire token, access data, can sleep, release token\nlwkt_gettoken(&amp;vp-&gt;v_token);\n// Can sleep here!\ntsleep(wchan, 0, \"vnode\", 0);\n// Still serialized after wakeup\n// Access v_data\nlwkt_reltoken(&amp;vp-&gt;v_token);\n</code></pre> <p>Advantages: - Tokens held across sleep (simpler code) - No deadlock possibility (tokens can be acquired in any order) - Per-CPU scheduling (no global scheduler lock) - Message passing reduces shared state</p>"},{"location":"sys/kern/lwkt/#multiprocessor-scalability","title":"Multiprocessor Scalability","text":"<p>Traditional approach: - Global scheduler lock contended by all CPUs - Locks on shared data structures (e.g., vnodes, sockets) - Cache coherency overhead</p> <p>LWKT approach: - Per-CPU scheduling (no global lock) - Message passing instead of shared state - Tokens reduce contention (logical serialization) - Thread ownership eliminates many locks</p>"},{"location":"sys/kern/lwkt/#important-notes","title":"Important Notes","text":""},{"location":"sys/kern/lwkt/#token-deadlock-freedom","title":"Token Deadlock Freedom","text":"<p>Tokens cannot deadlock because:</p> <ol> <li>Held across sleep: If you block waiting for a resource, your token is released</li> <li>Any ordering: Tokens can be acquired in any order</li> <li>Priority boosting: Token contention can boost thread priority</li> </ol> <p>Traditional locks can deadlock because they must be held continuously and have strict ordering requirements.</p>"},{"location":"sys/kern/lwkt/#when-to-use-what","title":"When to Use What","text":"<ul> <li>Tokens: Subsystem-level serialization (e.g., entire mount point, vnode)</li> <li>Spinlocks: Very short critical sections, interrupt context</li> <li>Serializers: Device driver interrupt/thread synchronization</li> <li>Messages: Cross-CPU operations, async work queuing</li> </ul>"},{"location":"sys/kern/lwkt/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Critical sections must be short \u2014 Prevent preemption, don't abuse</li> <li>IPIQs are bounded \u2014 Can fill up if target CPU is busy</li> <li>Token contention \u2014 Tracked in <code>t_collisions</code>, can indicate bottleneck</li> </ul>"},{"location":"sys/kern/lwkt/#files","title":"Files","text":"<p>Key source files implementing LWKT:</p> <ul> <li><code>sys/kern/lwkt_thread.c</code> \u2014 Thread management, scheduling, context switching</li> <li><code>sys/kern/lwkt_msgport.c</code> \u2014 Message ports and message passing</li> <li><code>sys/kern/lwkt_token.c</code> \u2014 Serializing tokens</li> <li><code>sys/kern/lwkt_ipiq.c</code> \u2014 Inter-processor interrupt queues</li> <li><code>sys/kern/lwkt_serialize.c</code> \u2014 Serializer helpers (for drivers)</li> </ul> <p>Key header files:</p> <ul> <li><code>sys/sys/thread.h</code> \u2014 <code>struct thread</code>, token structures</li> <li><code>sys/sys/msgport.h</code> \u2014 <code>struct lwkt_msg</code>, <code>struct lwkt_port</code></li> <li><code>sys/sys/thread2.h</code> \u2014 Inline functions, macros</li> </ul>"},{"location":"sys/kern/lwkt/#references","title":"References","text":"<ul> <li>Synchronization \u2014 Other synchronization primitives (spinlocks, mutexes, etc.)</li> <li>Scheduling \u2014 CPU scheduling policies built on LWKT</li> <li>Processes &amp; Threads \u2014 Process and LWP management using LWKT</li> <li>VFS \u2014 Extensive use of tokens for filesystem serialization</li> <li>IPC &amp; Sockets \u2014 Message passing used in socket layer</li> </ul>"},{"location":"sys/kern/lwkt/#further-reading","title":"Further Reading","text":"<p>DragonFly's LWKT is unique among BSD systems. Understanding it is essential for kernel development. Key concepts to remember:</p> <ol> <li>Message passing over shared memory</li> <li>Tokens held across sleep</li> <li>Per-CPU scheduling</li> <li>Thread ownership by CPU</li> <li>Deadlock-free by design</li> </ol> <p>These principles enable DragonFly's superior multiprocessor scalability compared to traditional BSD kernels.</p>"},{"location":"sys/kern/memory/","title":"Memory Allocation","text":"<p>This document describes DragonFly BSD's kernel memory allocation subsystems, which provide efficient, scalable memory management for kernel code.</p>"},{"location":"sys/kern/memory/#overview","title":"Overview","text":"<p>DragonFly BSD implements a sophisticated multi-layered memory allocation system optimized for SMP scalability and low overhead. The system provides several allocators, each optimized for different use cases:</p> <ol> <li>kmalloc/kfree - General-purpose allocation API (<code>sys/kern/kern_slaballoc.c</code>)</li> <li>kmalloc_obj - Type-stable object allocator (<code>sys/kern/kern_kmalloc.c</code>)</li> <li>objcache - Per-CPU object caches with magazine layer (<code>sys/kern/kern_objcache.c</code>)</li> <li>mpipe - Pre-allocated object pools (<code>sys/kern/kern_mpipe.c</code>)</li> <li>Helper allocators - Specialized data structure allocators (alist, blist, etc.)</li> </ol> <p>The system is designed around several key principles:</p> <ul> <li>Per-CPU operation: Most allocations occur without locks, using per-CPU data structures</li> <li>Lock-free fast paths: Common allocations complete without synchronization</li> <li>Magazine/depot architecture: Multi-level caching reduces contention</li> <li>Type stability: Object allocators maintain type information for improved debuggability</li> <li>Lazy IPI: Cross-CPU operations use asynchronous IPI messages</li> </ul>"},{"location":"sys/kern/memory/#architectural-layers","title":"Architectural Layers","text":"<pre><code>flowchart TB\n    KERNEL[\"Kernel Code(drivers, filesystems, network stack, etc.)\"]\n\n    subgraph API[\"API Layer\"]\n        KMALLOC[\"kmalloc/kfree(slab API)\"]\n        KMALLOCOBJ[\"kmalloc_obj(type-stable)\"]\n        OBJCACHE[\"objcache(magazine)\"]\n    end\n\n    SLAB[\"Slab Allocator (kern_slaballoc.c)Per-CPU zones, chunk management, IPI ops\"]\n\n    KMEM[\"kmem_slab_alloc/free\"]\n\n    VM[\"VM System(vm_map, etc)\"]\n\n    KERNEL --&gt; KMALLOC\n    KERNEL --&gt; KMALLOCOBJ\n    KERNEL --&gt; OBJCACHE\n    KMALLOC --&gt; SLAB\n    KMALLOCOBJ --&gt; SLAB\n    OBJCACHE --&gt; SLAB\n    SLAB --&gt; KMEM\n    KMEM --&gt; VM\n</code></pre>"},{"location":"sys/kern/memory/#layer-responsibilities","title":"Layer Responsibilities","text":"<p>API Layer (<code>kern_kmalloc.c</code>): - Provides <code>kmalloc()</code> and <code>kfree()</code> API - Implements kmalloc_obj for type-stable allocations - Manages malloc types and statistics - Routes large allocations directly to VM</p> <p>Slab Allocator (<code>kern_slaballoc.c</code>): - Power-of-2 sized zones (8 bytes to 32KB) - Per-CPU zone management - Lock-free allocation and freeing - Cross-CPU IPI-based operations - Bitmap tracking for allocated chunks</p> <p>Object Cache (<code>kern_objcache.c</code>): - Magazine-based per-CPU caching - Depot layer for magazine exchange - Constructor/destructor support - Configurable limits and policies</p> <p>VM Interface (<code>kmem_slab_alloc/free</code>): - Direct KVA and page allocation - Wiring and mapping management - Large allocation support</p>"},{"location":"sys/kern/memory/#memory-allocation-flags","title":"Memory Allocation Flags","text":"<p>All allocation functions accept flags that control behavior:</p>"},{"location":"sys/kern/memory/#blocking-behavior","title":"Blocking Behavior","text":"Flag Description Can Fail? <code>M_WAITOK</code> Block until memory available No (panics on failure) <code>M_NOWAIT</code> Return NULL immediately if unavailable Yes <code>M_NULLOK</code> Return NULL instead of panicking Yes (with M_WAITOK) <code>M_INTWAIT</code> Like M_WAITOK for interrupt context No <code>M_INTNOWAIT</code> Like M_NOWAIT for interrupt context Yes"},{"location":"sys/kern/memory/#memory-properties","title":"Memory Properties","text":"Flag Description <code>M_ZERO</code> Zero allocated memory <code>M_USE_RESERVE</code> Can use emergency reserves <code>M_USE_INTERRUPT_RESERVE</code> Can exhaust free list (interrupt) <code>M_CACHEALIGN</code> Align to cache line boundary <code>M_POWEROF2</code> Round size to power of 2 <p>Important: The default behavior without <code>M_ZERO</code> leaves memory uninitialized for performance. In <code>INVARIANTS</code> mode, memory may be filled with patterns to detect use-after-free.</p>"},{"location":"sys/kern/memory/#kmallockfree-api","title":"kmalloc/kfree API","text":"<p>The primary allocation interface for general-purpose kernel memory.</p>"},{"location":"sys/kern/memory/#basic-usage","title":"Basic Usage","text":"<pre><code>#include &lt;sys/malloc.h&gt;\n\n/* Define a malloc type */\nMALLOC_DEFINE(M_MYSUBSYS, \"mysubsys\", \"My subsystem allocations\");\n\n/* Allocate memory */\nvoid *ptr = kmalloc(size, M_MYSUBSYS, M_WAITOK);\n\n/* Allocate and zero */\nvoid *ptr = kmalloc(size, M_MYSUBSYS, M_WAITOK | M_ZERO);\n\n/* Non-blocking allocation */\nvoid *ptr = kmalloc(size, M_MYSUBSYS, M_NOWAIT);\nif (ptr == NULL) {\n    /* Handle allocation failure */\n}\n\n/* Free memory */\nkfree(ptr, M_MYSUBSYS);\n</code></pre>"},{"location":"sys/kern/memory/#malloc-types","title":"Malloc Types","text":"<p>Every allocation must specify a <code>malloc_type</code> for tracking and statistics. Types are defined with:</p> <pre><code>MALLOC_DEFINE(type_name, \"short-desc\", \"Long description\");\n</code></pre> <p>Pre-defined types (in <code>sys/kern/kern_slaballoc.c</code>): - <code>M_CACHE</code> - Various dynamically allocated caches - <code>M_DEVBUF</code> - Device driver memory - <code>M_TEMP</code> - Temporary buffers - <code>M_DRM</code> - DRM subsystem allocations</p>"},{"location":"sys/kern/memory/#allocation-size-limits","title":"Allocation Size Limits","text":"<p>The slab allocator handles allocations based on size:</p> Size Range Chunk Size Zones Behavior 1-127 bytes 8 bytes 16 Slab allocation 128-255 16 bytes 8 Slab allocation 256-511 32 bytes 8 Slab allocation 512-1023 64 bytes 8 Slab allocation 1024-2047 128 bytes 8 Slab allocation 2048-4095 256 bytes 8 Slab allocation 4096-8191 512 bytes 8 Slab allocation 8192-16383 1024 bytes 8 Slab allocation 16384-32767 2048 bytes 8 Slab allocation \u2265 ZoneLimit Page-aligned - Direct VM allocation <p>Notes: - Allocations are rounded up to chunk size - Alignment equals chunk size for power-of-2 requests - Large allocations (\u2265 ZoneLimit or &gt;2 pages) bypass slab allocator - ZoneLimit is typically 32KB on systems with \u22651GB RAM</p>"},{"location":"sys/kern/memory/#zero-sized-allocations","title":"Zero-Sized Allocations","text":"<p>DragonFly allows <code>kmalloc(0, ...)</code> for compatibility (some drivers depend on this):</p> <pre><code>void *ptr = kmalloc(0, M_TEMP, M_WAITOK);\n/* Returns special ZERO_LENGTH_PTR (-8), not NULL */\nkfree(ptr, M_TEMP);  /* Safe to free */\n</code></pre>"},{"location":"sys/kern/memory/#string-allocation","title":"String Allocation","text":"<p>Convenience functions for string duplication:</p> <pre><code>/* Duplicate a string */\nchar *str = kstrdup(original, M_TEMP);\nkfree(str, M_TEMP);\n\n/* Duplicate with length limit */\nchar *str = kstrndup(original, maxlen, M_TEMP);\n</code></pre>"},{"location":"sys/kern/memory/#reallocation","title":"Reallocation","text":"<pre><code>/* Resize allocation - may move data */\nvoid *new_ptr = krealloc(old_ptr, new_size, M_TEMP, M_WAITOK);\n\n/* If new size fits in same zone, returns same pointer */\n/* Otherwise allocates new memory and copies data */\n</code></pre> <p>Note: <code>krealloc()</code> does not support <code>M_ZERO</code> flag.</p>"},{"location":"sys/kern/memory/#memory-limits","title":"Memory Limits","text":"<p>Each malloc type has an associated limit (typically 10% of kernel memory):</p> <pre><code>/* Check current limit */\nlong limit = kmalloc_limit(M_MYSUBSYS);\n\n/* Raise limit (never shrinks) */\nkmalloc_raise_limit(M_MYSUBSYS, new_limit_bytes);\n\n/* Set to unlimited */\nkmalloc_set_unlimited(M_MYSUBSYS);\n</code></pre> <p>When a limit is exceeded: - <code>M_WAITOK</code> allocations panic - <code>M_NULLOK</code> allocations return NULL - Prevents runaway memory usage</p>"},{"location":"sys/kern/memory/#dynamic-type-creation","title":"Dynamic Type Creation","text":"<p>For dynamically loaded modules or runtime-created subsystems:</p> <pre><code>struct malloc_type *my_type = NULL;\n\n/* Create malloc type */\nkmalloc_create(&amp;my_type, \"runtime-type\");\n\n/* Use it */\nvoid *ptr = kmalloc(size, my_type, M_WAITOK);\nkfree(ptr, my_type);\n\n/* Destroy when done (must have no outstanding allocations) */\nkmalloc_destroy(&amp;my_type);\n</code></pre>"},{"location":"sys/kern/memory/#type-stable-object-allocator-kmalloc_obj","title":"Type-Stable Object Allocator (kmalloc_obj)","text":"<p>For fixed-size object allocations, DragonFly provides a type-stable allocator optimized for common-sized objects. This allocator provides:</p> <ul> <li>Type stability: All objects of a type can be freed in bulk</li> <li>Per-CPU slabs: 128KB slabs with embedded object management</li> <li>Lock-free operations: Atomic operations for cross-CPU frees</li> <li>Efficient recycling: Two-tier slab management (per-CPU + global)</li> </ul>"},{"location":"sys/kern/memory/#defining-object-types","title":"Defining Object Types","text":"<pre><code>/* Define a type-stable object type */\nstatic struct malloc_type *my_obj_type;\n\nMALLOC_DEFINE_OBJ(my_obj_type, sizeof(struct my_object),\n                  \"myobj\", \"My object type\");\n</code></pre> <p>Alternatively, create at runtime:</p> <pre><code>_kmalloc_create_obj(&amp;my_obj_type, \"my_runtime_obj\",\n                    sizeof(struct my_object));\n</code></pre>"},{"location":"sys/kern/memory/#object-allocation","title":"Object Allocation","text":"<pre><code>struct my_object *obj;\n\n/* Allocate object */\nobj = kmalloc_obj(sizeof(struct my_object), my_obj_type, M_WAITOK);\n\n/* Use object */\nobj-&gt;field1 = value;\nobj-&gt;field2 = value;\n\n/* Free object */\nkfree_obj(obj, my_obj_type);\n</code></pre>"},{"location":"sys/kern/memory/#type-stable-architecture","title":"Type-Stable Architecture","text":"<p>Per-CPU Slab Management:</p> <p>Each malloc type maintains per-CPU state in <code>struct kmalloc_use</code>: - <code>active</code> - Current slab being allocated from - <code>alternate</code> - Backup slab (hot spare) - Per-CPU statistics (<code>memuse</code>, <code>inuse</code>, etc.)</p> <p>Slab Structure (128KB each):</p> <pre><code>flowchart TB\n    subgraph SLAB[\"kmalloc_slab (128KB)\"]\n        HEADER[\"kmalloc_slab (header)\"]\n        FOBJS[\"fobjs[4096] - Circular buffer of free object pointers\u2022 findex: next free index\u2022 nindex: next allocation index\u2022 maxobjs: total capacity\"]\n        OBJECTS[\"Objects (allocated from end of slab)[ obj 1 ][ obj 2 ][ obj 3 ] ... [ obj N ]\"]\n    end\n\n    HEADER --- FOBJS\n    FOBJS --- OBJECTS\n</code></pre> <p>Slab States:</p> <ol> <li>Active/Alternate (per-CPU): Fast allocation, no locks</li> <li>Partial (global): Has free objects, used to refill per-CPU</li> <li>Full (global): All allocated, kept for type-safety</li> <li>Empty (global): All freed, eligible for recycling</li> </ol>"},{"location":"sys/kern/memory/#allocation-fast-path","title":"Allocation Fast Path","text":"<pre><code>/* From kern_kmalloc.c:kmalloc_obj_alloc() */\n1. Try current active slab's fobjs[] array\n2. If empty, swap active \u2194 alternate\n3. If both empty, rotate global partial\u2192active\n4. If no partial slabs, allocate new 128KB slab\n</code></pre> <p>Key Insight: The fobjs[] circular buffer provides O(1) alloc/free with no linked-list manipulation.</p>"},{"location":"sys/kern/memory/#free-fast-path","title":"Free Fast Path","text":"<pre><code>/* From kern_kmalloc.c:kmalloc_obj_free() */\n1. Calculate slab from object pointer: obj &amp; ~(128KB-1)\n2. Atomically store pointer in slab-&gt;fobjs[slab-&gt;findex++]\n3. Update per-CPU statistics\n4. If cross-CPU free, use atomic ops only\n</code></pre> <p>Cross-CPU Free: When freeing to remote CPU's slab, atomic increments ensure thread-safety without explicit locks.</p>"},{"location":"sys/kern/memory/#type-stability-benefits","title":"Type Stability Benefits","text":"<p>When unmounting a filesystem (e.g., tmpfs), all memory from the filesystem's malloc type can be returned:</p> <pre><code>/* During unmount */\nmalloc_uninit(tmpfs_mount_type);\n\n/* This returns ALL slabs for this type to the system */\n/* No fragmentation! All 128KB slabs are freed intact */\n</code></pre> <p>Traditional allocators would have fragmentation preventing full memory reclamation.</p>"},{"location":"sys/kern/memory/#per-cpu-slab-cache","title":"Per-CPU Slab Cache","text":"<p>To avoid expensive <code>kmem_slab_alloc()</code> calls, DragonFly maintains a per-globaldata slab cache:</p> <pre><code>/* In globaldata structure */\nstruct globaldata {\n    ...\n    kmalloc_slab_t gd_kmslab[KMGD_MAXFREESLABS];  /* 128 slabs */\n    int            gd_kmslab_avail;\n    ...\n};\n</code></pre> <p>Benefits: - Fast slab allocation from cache - Amortizes VM allocation cost - Reduces system map contention</p>"},{"location":"sys/kern/memory/#slab-allocator-internals","title":"Slab Allocator Internals","text":"<p>The slab allocator (<code>kern_slaballoc.c</code>) provides the foundation for kmalloc operations.</p>"},{"location":"sys/kern/memory/#zone-structure","title":"Zone Structure","text":"<p>A zone represents a region of memory divided into fixed-size chunks:</p> <pre><code>struct SLZone {\n    int32_t  z_Magic;          /* Magic number validation */\n    int      z_NFree;          /* Number of free chunks */\n    int      z_NMax;           /* Total chunks in zone */\n    struct SLZone *z_Next;     /* List linkage */\n\n    int      z_BaseIndex;      /* Optimization: start search here */\n    int      z_ChunkSize;      /* Size of each chunk */\n    int      z_ZoneIndex;      /* Which zone array (by size) */\n    int      z_Cpu;            /* Owning CPU */\n    struct globaldata *z_CpuGd;/* Owning CPU's globaldata */\n\n    /* Free chunk lists */\n    struct SLChunk *z_LChunks;    /* Local free chunks (LIFO) */\n    struct SLChunk **z_LChunksp;  /* Tail pointer */\n    struct SLChunk *z_RChunks;    /* Remote free chunks (from other CPUs) */\n\n    /* Cross-CPU synchronization */\n    int      z_RSignal;        /* Signal for remote operations */\n    int      z_RCount;         /* Count of in-flight remote operations */\n\n    char     *z_BasePtr;       /* Base of chunk array */\n\n#ifdef INVARIANTS\n    uint32_t z_Bitmap[...];    /* Allocation bitmap for debugging */\n#endif\n};\n</code></pre>"},{"location":"sys/kern/memory/#per-cpu-zone-management","title":"Per-CPU Zone Management","text":"<p>Each CPU maintains separate zone lists in <code>struct SLGlobalData</code>:</p> <pre><code>struct SLGlobalData {\n    SLZoneList ZoneAry[NZONES];      /* Zones by size */\n    SLZoneList FreeZones;            /* Completely free zones */\n    SLZoneList FreeOvZones;          /* Oversized free zones */\n    int        NFreeZones;           /* Count of free zones */\n    int        JunkIndex;            /* Randomization for new zones */\n};\n</code></pre> <p>NZONES is typically 72 (16 for small + 56 for larger sizes).</p>"},{"location":"sys/kern/memory/#zone-size-calculation","title":"Zone Size Calculation","text":"<p>Zone size is determined at boot based on system memory:</p> <pre><code>/* From kmeminit() in kern_slaballoc.c */\nZoneSize = ZALLOC_MIN_ZONE_SIZE;  /* Start at 32KB */\nwhile (ZoneSize &lt; ZALLOC_MAX_ZONE_SIZE &amp;&amp; (ZoneSize &lt;&lt; 1) &lt; usesize)\n    ZoneSize &lt;&lt;= 1;\n\n/* Typically:\n * &lt; 128MB RAM: 32KB zones\n * &gt;= 1GB RAM:  128KB zones (most systems)\n */\n</code></pre> <p>Larger zones reduce per-zone overhead but increase memory slack.</p>"},{"location":"sys/kern/memory/#allocation-algorithm","title":"Allocation Algorithm","text":"<p>The <code>_kmalloc()</code> function (kern_slaballoc.c:814) implements allocation:</p> <pre><code>1. Handle special cases:\n   - Check malloc type limit\n   - Handle size == 0 (return ZERO_LENGTH_PTR)\n\n2. For small allocations (&lt; ZoneLimit):\n   a. Calculate zone index: zoneindex(&amp;size, &amp;align)\n   b. Enter critical section\n   c. Find zone with free chunks:\n      - Check tail of ZoneAry[zi] (most recently used)\n      - If no free chunks, check z_RChunks (remote frees)\n      - If still none, allocate new zone\n   d. Pop chunk from free list or use z_UIndex (never-allocated)\n   e. Update statistics\n   f. Exit critical section\n   g. Zero memory if M_ZERO requested\n\n3. For large allocations (\u2265 ZoneLimit):\n   a. Round size to page boundary\n   b. Call kmem_slab_alloc(size, PAGE_SIZE, flags)\n   c. Mark pages in kernel page table (btokup)\n   d. Return pointer\n</code></pre> <p>Critical section: Uses <code>crit_enter()/crit_exit()</code> not locks. This blocks preemption but allows interrupts.</p>"},{"location":"sys/kern/memory/#free-algorithm","title":"Free Algorithm","text":"<p>The <code>_kfree()</code> function (kern_slaballoc.c:1391) implements deallocation:</p> <pre><code>1. Handle special cases:\n   - NULL pointer (panic)\n   - ZERO_LENGTH_PTR (return immediately)\n   - Check magic number\n\n2. Determine zone or oversized:\n   kup = btokup(ptr);  /* Kernel page table lookup */\n   if (*kup &gt; 0) {\n       /* Oversized allocation */\n       size = *kup &lt;&lt; PAGE_SHIFT;\n       kmem_slab_free(ptr, size);\n       return;\n   }\n\n3. For zone allocations:\n   z = (SLZone *)((uintptr_t)ptr &amp; ZoneMask);\n\n   a. If z-&gt;z_CpuGd != mycpu:\n      /* Cross-CPU free */\n      - Atomic add to z-&gt;z_RChunks list\n      - Send IPI if zone needs reactivation\n\n   b. If local CPU:\n      - Enter critical section\n      - Add chunk to z-&gt;z_LChunks (front, LIFO)\n      - Update z-&gt;z_NFree\n      - If zone becomes fully free, move to FreeZones list\n      - Exit critical section\n</code></pre> <p>LIFO ordering: Recently freed memory is hot in cache, so reallocate it first.</p>"},{"location":"sys/kern/memory/#cross-cpu-operations","title":"Cross-CPU Operations","text":"<p>When CPU A frees memory owned by CPU B:</p> <pre><code>/* CPU A (freeing) */\n1. chunk-&gt;c_Next = z-&gt;z_RChunks;     /* Atomic */\n2. atomic_cmpset_ptr(&amp;z-&gt;z_RChunks, old, chunk);\n3. if (zone was fully allocated) {\n       lwkt_send_ipiq_passive(z-&gt;z_CpuGd, kfree_remote, z);\n   }\n\n/* CPU B (IPI handler) */\nkfree_remote(void *ptr) {\n    z = (SLZone *)ptr;\n    /* Move z-&gt;z_RChunks to z-&gt;z_LChunks */\n    clean_zone_rchunks(z);\n    /* Reactivate zone if it has free chunks */\n    if (z-&gt;z_NFree &gt; 0)\n        TAILQ_INSERT_HEAD(&amp;slgd-&gt;ZoneAry[z-&gt;z_ZoneIndex], z, z_Entry);\n}\n</code></pre> <p>Key optimizations: - Passive IPI: Low priority, batched - z_RSignal flag: Avoids IPI if zone already active - z_RCount: Prevents premature zone destruction</p>"},{"location":"sys/kern/memory/#zone-recycling","title":"Zone Recycling","text":"<p>Completely free zones are kept in the <code>FreeZones</code> list up to a threshold (<code>ZoneRelsThresh</code>, default 32). Beyond that, zones are returned to the VM system:</p> <pre><code>/* In _kmalloc() hysteresis check */\nwhile (slgd-&gt;NFreeZones &gt; ZoneRelsThresh) {\n    z = TAILQ_LAST(&amp;slgd-&gt;FreeZones, SLZoneList);\n    TAILQ_REMOVE(&amp;slgd-&gt;FreeZones, z, z_Entry);\n    --slgd-&gt;NFreeZones;\n    kmem_slab_free(z, ZoneSize);  /* May block */\n}\n</code></pre> <p>Hysteresis: Prevents thrashing by keeping a buffer of free zones.</p>"},{"location":"sys/kern/memory/#chunk-bitmap-invariants","title":"Chunk Bitmap (INVARIANTS)","text":"<p>In debug builds, zones maintain a bitmap to detect double-free and use-after-free:</p> <pre><code>#ifdef INVARIANTS\nstatic void chunk_mark_allocated(SLZone *z, void *chunk) {\n    int bitdex = ((char *)chunk - (char *)z-&gt;z_BasePtr) / z-&gt;z_ChunkSize;\n    KASSERT(!(z-&gt;z_Bitmap[bitdex &gt;&gt; 5] &amp; (1 &lt;&lt; (bitdex &amp; 31))),\n            (\"double allocation\"));\n    z-&gt;z_Bitmap[bitdex &gt;&gt; 5] |= (1 &lt;&lt; (bitdex &amp; 31));\n}\n\nstatic void chunk_mark_free(SLZone *z, void *chunk) {\n    int bitdex = ((char *)chunk - (char *)z-&gt;z_BasePtr) / z-&gt;z_ChunkSize;\n    KASSERT(z-&gt;z_Bitmap[bitdex &gt;&gt; 5] &amp; (1 &lt;&lt; (bitdex &amp; 31)),\n            (\"double free\"));\n    z-&gt;z_Bitmap[bitdex &gt;&gt; 5] &amp;= ~(1 &lt;&lt; (bitdex &amp; 31));\n}\n#endif\n</code></pre>"},{"location":"sys/kern/memory/#memory-pattern-debugging","title":"Memory Pattern Debugging","text":"<pre><code>/* In INVARIANTS mode */\nstatic int use_malloc_pattern = 0;  /* sysctl debug.use_malloc_pattern */\nstatic int use_weird_array = 0;     /* sysctl debug.use_weird_array */\n\n/* On allocation without M_ZERO */\nif (use_malloc_pattern) {\n    for (i = 0; i &lt; size; i += sizeof(int))\n        *(int *)((char *)chunk + i) = -1;  /* 0xFFFFFFFF pattern */\n}\n\n/* On free */\nif (use_weird_array) {\n    bcopy(weirdary, chunk, min(size, sizeof(weirdary)));\n    /* weirdary = { WEIRD_ADDR, ...} = { 0xdeadc0de, ... } */\n}\n</code></pre> <p>Usage: Enable via sysctl to detect: - Use of uninitialized memory (all 0xFF bytes) - Use after free (0xDEADC0DE pattern)</p>"},{"location":"sys/kern/memory/#object-cache-objcache","title":"Object Cache (objcache)","text":"<p>The object cache system (<code>kern_objcache.c</code>) provides a high-level caching layer for frequently allocated objects. It implements a magazine-based architecture similar to Solaris/Illumos slab allocator.</p>"},{"location":"sys/kern/memory/#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph PERCPU[\"Per-CPU Layer (lock-free)\"]\n        subgraph CPU0[\"CPU 0\"]\n            L0[\"loadedmagazine\"]\n            P0[\"previousmagazine\"]\n        end\n        subgraph CPU1[\"CPU 1\"]\n            L1[\"loadedmagazine\"]\n            P1[\"previousmagazine\"]\n        end\n        subgraph CPUN[\"CPU N\"]\n            LN[\"loadedmagazine\"]\n            PN[\"previousmagazine\"]\n        end\n    end\n\n    subgraph DEPOT[\"Depot Layer (locked)Cluster 0 Depot (spinlock protected)\"]\n        FULL[\"Full Magazines\"]\n        EMPTY[\"Empty Magazines\"]\n        STATS[\"unallocated_objects: 1024cluster_limit: 2048\"]\n    end\n\n    subgraph BACKEND[\"Backend Allocator\"]\n        ALLOC[\"objcache_malloc_alloc() / objcache_malloc_free()(or custom allocator)\"]\n    end\n\n    PERCPU --&gt; DEPOT\n    DEPOT --&gt; BACKEND\n</code></pre>"},{"location":"sys/kern/memory/#key-data-structures","title":"Key Data Structures","text":"<pre><code>/* Magazine - array of object pointers */\nstruct magazine {\n    int rounds;              /* Current number of objects */\n    int capacity;            /* Maximum capacity */\n    SLIST_ENTRY(magazine) nextmagazine;\n    void *objects[];         /* Flexible array of object pointers */\n};\n\n/* Per-CPU cache state */\nstruct percpu_objcache {\n    struct magazine *loaded_magazine;    /* Active magazine */\n    struct magazine *previous_magazine;  /* Backup magazine */\n\n    /* Statistics */\n    u_long gets_cumulative;\n    u_long gets_null;\n    u_long allocs_cumulative;  /* Backend allocations */\n    u_long puts_cumulative;\n    u_long gets_exhausted;     /* Hit limit */\n};\n\n/* Depot (shared between CPUs) */\nstruct magazinedepot {\n    struct magazinelist fullmagazines;\n    struct magazinelist emptymagazines;\n    int magcapacity;           /* Objects per magazine */\n    struct spinlock spin;      /* Protects depot */\n\n    int unallocated_objects;   /* Remaining quota */\n    int cluster_limit;         /* Total object limit */\n    int waiting;               /* Waiters for objects */\n};\n\n/* Object cache */\nstruct objcache {\n    objcache_ctor_fn  *ctor;       /* Constructor */\n    objcache_dtor_fn  *dtor;       /* Destructor */\n    void              *privdata;   /* Private data for ctor/dtor */\n\n    objcache_alloc_fn *alloc;      /* Backend allocator */\n    objcache_free_fn  *free;       /* Backend free */\n    void              *allocator_args;\n\n    struct magazinedepot depot[MAXCLUSTERS];  /* MAXCLUSTERS=1 currently */\n    struct percpu_objcache cache_percpu[];    /* Per-CPU caches */\n};\n</code></pre>"},{"location":"sys/kern/memory/#creating-an-object-cache","title":"Creating an Object Cache","text":"<pre><code>/* Simple cache backed by kmalloc */\nstruct objcache *cache;\ncache = objcache_create_simple(M_MYTYPE, sizeof(struct my_object));\n\n/* Cache with constructor/destructor */\nstatic boolean_t my_ctor(void *obj, void *privdata, int ocflags) {\n    struct my_object *o = obj;\n    /* Initialize object fields */\n    o-&gt;refcount = 1;\n    o-&gt;lock = NULL;\n    return TRUE;  /* Success */\n}\n\nstatic void my_dtor(void *obj, void *privdata) {\n    struct my_object *o = obj;\n    /* Clean up before freeing */\n    KKASSERT(o-&gt;refcount == 0);\n}\n\ncache = objcache_create_mbacked(\n    M_MYTYPE,                    /* malloc type */\n    sizeof(struct my_object),    /* object size */\n    1000,                        /* cluster_limit (total objects) */\n    200,                         /* nom_cache (cached objects) */\n    my_ctor,                     /* constructor */\n    my_dtor,                     /* destructor */\n    NULL                         /* privdata */\n);\n</code></pre> <p>Key parameters: - <code>cluster_limit</code>: Maximum total objects (0 = unlimited) - <code>nom_cache</code>: Desired number of cached objects - Magazine capacity is calculated: <code>nom_cache / (ncpus + 1) / 2</code></p>"},{"location":"sys/kern/memory/#allocating-objects","title":"Allocating Objects","text":"<pre><code>struct my_object *obj;\n\n/* Blocking allocation */\nobj = objcache_get(cache, M_WAITOK);\n\n/* Non-blocking allocation */\nobj = objcache_get(cache, M_NOWAIT);\nif (obj == NULL) {\n    /* Handle failure */\n}\n\n/* Use object */\nobj-&gt;data = value;\n\n/* Return to cache */\nobjcache_put(cache, obj);\n</code></pre> <p>Allocation Flow:</p> <pre><code>/* From objcache_get() - kern_objcache.c:427 */\n1. crit_enter();  /* Block preemption */\n\n2. Check loaded magazine:\n   if (loadedmag-&gt;rounds &gt; 0) {\n       obj = loadedmag-&gt;objects[--loadedmag-&gt;rounds];\n       crit_exit();\n       return obj;  /* FAST PATH - no depot access */\n   }\n\n3. Check previous magazine:\n   if (previous-&gt;rounds &gt; 0) {\n       swap(loaded, previous);\n       obj = loadedmag-&gt;objects[--loadedmag-&gt;rounds];\n       crit_exit();\n       return obj;  /* Still fast, no depot */\n   }\n\n4. Both magazines empty - depot exchange:\n   spin_lock(&amp;depot-&gt;spin);\n   if (!SLIST_EMPTY(&amp;depot-&gt;fullmagazines)) {\n       /* Exchange empty for full */\n       emptymag = previous;\n       previous = loaded;\n       loaded = SLIST_FIRST(&amp;depot-&gt;fullmagazines);\n       SLIST_REMOVE_HEAD(&amp;depot-&gt;fullmagazines);\n       SLIST_INSERT_HEAD(&amp;depot-&gt;emptymagazines, emptymag);\n       spin_unlock(&amp;depot-&gt;spin);\n       goto retry;  /* Now have full magazine */\n   }\n\n5. Depot empty - backend allocation:\n   if (depot-&gt;unallocated_objects &gt; 0) {\n       --depot-&gt;unallocated_objects;\n       spin_unlock(&amp;depot-&gt;spin);\n       crit_exit();\n\n       obj = oc-&gt;alloc(oc-&gt;allocator_args, ocflags);\n       if (obj &amp;&amp; !oc-&gt;ctor(obj, oc-&gt;privdata, ocflags)) {\n           oc-&gt;free(obj, oc-&gt;allocator_args);\n           obj = NULL;\n       }\n       return obj;\n   }\n\n6. Limit exceeded:\n   if (ocflags &amp; M_WAITOK) {\n       ssleep(depot, &amp;depot-&gt;spin, 0, \"objcache_get\", 0);\n       goto retry;\n   }\n   return NULL;\n</code></pre>"},{"location":"sys/kern/memory/#returning-objects","title":"Returning Objects","text":"<pre><code>/* From objcache_put() - kern_objcache.c:620 */\n1. crit_enter();\n\n2. Check loaded magazine:\n   if (loadedmag-&gt;rounds &lt; loadedmag-&gt;capacity) {\n       loadedmag-&gt;objects[loadedmag-&gt;rounds++] = obj;\n       crit_exit();\n       return;  /* FAST PATH */\n   }\n\n3. Check previous magazine:\n   if (previous-&gt;rounds &lt; previous-&gt;capacity) {\n       swap(loaded, previous);\n       loadedmag-&gt;objects[loadedmag-&gt;rounds++] = obj;\n       crit_exit();\n       return;\n   }\n\n4. Both magazines full - depot exchange:\n   spin_lock(&amp;depot-&gt;spin);\n   if (!SLIST_EMPTY(&amp;depot-&gt;emptymagazines)) {\n       /* Exchange full for empty */\n       loadedmag = previous;\n       previous = loaded;\n       loaded = SLIST_FIRST(&amp;depot-&gt;emptymagazines);\n       SLIST_REMOVE_HEAD(&amp;depot-&gt;emptymagazines);\n\n       if (MAGAZINE_EMPTY(loadedmag))\n           SLIST_INSERT_HEAD(&amp;depot-&gt;emptymagazines, loadedmag);\n       else\n           SLIST_INSERT_HEAD(&amp;depot-&gt;fullmagazines, loadedmag);\n\n       spin_unlock(&amp;depot-&gt;spin);\n       goto retry;\n   }\n\n5. No empty magazines - free to backend:\n   ++depot-&gt;unallocated_objects;\n   spin_unlock(&amp;depot-&gt;spin);\n   crit_exit();\n\n   oc-&gt;dtor(obj, oc-&gt;privdata);\n   oc-&gt;free(obj, oc-&gt;allocator_args);\n</code></pre>"},{"location":"sys/kern/memory/#magazine-sizing","title":"Magazine Sizing","text":"<p>Magazine capacity is cache-line aligned and bounded:</p> <pre><code>/* From objcache_create() */\n#define MAGAZINE_CAPACITY_MIN  4\n#define MAGAZINE_CAPACITY_MAX  4096\n\nmag_capacity = mag_capacity_align(nom_cache / (ncpus + 1) / 2 + 1);\nif (mag_capacity &gt; MAGAZINE_CAPACITY_MAX)\n    mag_capacity = MAGAZINE_CAPACITY_MAX;\nelse if (mag_capacity &lt; MAGAZINE_CAPACITY_MIN)\n    mag_capacity = MAGAZINE_CAPACITY_MIN;\n\n/* Align to cache line */\nmag_size = __VM_CACHELINE_ALIGN(offsetof(struct magazine, objects[mag_capacity]));\nmag_capacity = (mag_size - MAGAZINE_HDRSIZE) / sizeof(void *);\n</code></pre> <p>Example: For 1000 object cache on 4-CPU system: - <code>nom_cache / (4 + 1) / 2 = 1000 / 5 / 2 = 100</code> objects per magazine - Aligns to cache line, typically 96 or 128 depending on alignment</p>"},{"location":"sys/kern/memory/#dynamic-limit-adjustment","title":"Dynamic Limit Adjustment","text":"<pre><code>/* Adjust cluster limit at runtime */\nobjcache_set_cluster_limit(cache, new_limit);\n\n/* Affects depot-&gt;unallocated_objects:\n * delta = new_limit - old_cluster_limit;\n * depot-&gt;unallocated_objects += delta;\n */\n</code></pre> <p>Use case: Dynamically grow/shrink cache based on load.</p>"},{"location":"sys/kern/memory/#destroying-an-object-cache","title":"Destroying an Object Cache","text":"<pre><code>/* All objects must be returned to cache first */\nobjcache_destroy(cache);\n\n/* Process:\n * 1. Remove from global objcache list\n * 2. Drain all depot magazines (call dtors)\n * 3. Drain all per-CPU magazines\n * 4. Free magazines and objcache structure\n */\n</code></pre>"},{"location":"sys/kern/memory/#backend-allocators","title":"Backend Allocators","text":"<p>Built-in allocators:</p> <pre><code>/* Standard kmalloc backend */\nobjcache_malloc_alloc();    /* Allocates with kmalloc */\nobjcache_malloc_free();     /* Frees with kfree */\n\n/* kmalloc with M_ZERO */\nobjcache_malloc_alloc_zero();\n\n/* No-op backend (pre-allocated objects only) */\nobjcache_nop_alloc();\nobjcache_nop_free();\n</code></pre> <p>Custom backend:</p> <pre><code>void *my_alloc(void *allocator_args, int ocflags) {\n    struct my_alloc_args *args = allocator_args;\n    /* Custom allocation logic */\n    return custom_allocate(args-&gt;param, ocflags &amp; OC_MFLAGS);\n}\n\nvoid my_free(void *obj, void *allocator_args) {\n    /* Custom free logic */\n    custom_free(obj);\n}\n\nstruct my_alloc_args alloc_args = { .param = 42 };\n\ncache = objcache_create(\"mycache\", limit, nom_cache,\n                        ctor, dtor, privdata,\n                        my_alloc, my_free, &amp;alloc_args);\n</code></pre>"},{"location":"sys/kern/memory/#objcache-statistics","title":"Objcache Statistics","text":"<p>Statistics are accessible via <code>sysctl kern.objcache.stats</code>:</p> <pre><code>struct objcache_stats {\n    char oc_name[OBJCACHE_NAMELEN];\n    u_long oc_limit;       /* cluster_limit */\n    u_long oc_requested;   /* Total get requests */\n    u_long oc_allocated;   /* Backend allocations */\n    u_long oc_exhausted;   /* Times limit was hit */\n    u_long oc_failed;      /* Failed allocations */\n    u_long oc_used;        /* Currently allocated */\n    u_long oc_cached;      /* Cached in magazines */\n};\n</code></pre> <p>Example usage: <pre><code>$ sysctl kern.objcache.stats\n# Shows statistics for all objcaches\n</code></pre></p>"},{"location":"sys/kern/memory/#magazine-advantages","title":"Magazine Advantages","text":"<ol> <li>Batch operations: Amortize depot lock acquisition over magazine capacity</li> <li>CPU cache affinity: Recently freed objects are hot in cache</li> <li>Reduced contention: Most operations are per-CPU lock-free</li> <li>Simple: No complex data structures, just object pointer arrays</li> </ol>"},{"location":"sys/kern/memory/#when-to-use-objcache","title":"When to Use Objcache","text":"<p>Use objcache when: - Allocating many same-sized objects - High allocation/free frequency - Constructor/destructor needed - Want detailed statistics - Need fine-grained limit control</p> <p>Use kmalloc_obj when: - Fixed-size objects, simple lifecycle - Want type stability (bulk free) - Less overhead than objcache - No constructor/destructor needed</p> <p>Use plain kmalloc when: - Variable sizes - Infrequent allocations - No special lifecycle requirements</p>"},{"location":"sys/kern/memory/#malloc-pipes-mpipe","title":"Malloc Pipes (mpipe)","text":"<p>Location: <code>sys/kern/kern_mpipe.c</code></p> <p>Malloc pipes provide pre-allocated object pools with automatic growth and callback support. They are primarily used for critical allocations that cannot fail, such as network packet buffers (mbufs).</p>"},{"location":"sys/kern/memory/#architecture_1","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph MPIPE[\"Malloc Pipe (mpipe)\"]\n        FREE[\"Free list (LIFO, cache-hot)obj1 \u2192 obj2 \u2192 obj3 \u2192 ...\"]\n        TOKEN[\"Allocation token (atomic)Lock-free fast path\"]\n        CALLBACK[\"Callback mechanism\u2022 MPF_CALLBACK flag\u2022 Support thread (mpipe_thread)\u2022 Pending request queue\"]\n        CTOR[\"Constructor/deconstructorCalled on alloc/free\"]\n        STATS[\"Statisticstotal, free, array size\"]\n    end\n</code></pre>"},{"location":"sys/kern/memory/#key-features","title":"Key Features","text":"<ol> <li>Pre-allocated pool: Objects allocated at initialization</li> <li>Lock-free allocation: Uses atomic token instead of spinlock</li> <li>Callback support: Can queue requests when pool exhausted</li> <li>LIFO caching: Recently freed objects are cache-hot</li> <li>Constructor/deconstructor: Called on alloc/free for initialization</li> <li>Statistics: Track usage via sysctl</li> </ol>"},{"location":"sys/kern/memory/#api","title":"API","text":""},{"location":"sys/kern/memory/#creating-a-malloc-pipe","title":"Creating a Malloc Pipe","text":"<pre><code>void mpipe_init(malloc_pipe_t mpipe, malloc_type_t type,\n                size_t bytes, int nnom, int nmax,\n                int mpflags,\n                void (*construct)(void *, void *),\n                void (*deconstruct)(void *, void *),\n                void *priv);\n\n/* Parameters:\n * mpipe:      Pointer to malloc_pipe structure\n * type:       Malloc type for accounting\n * bytes:      Size of each object\n * nnom:       Nominal number of objects (initial allocation)\n * nmax:       Maximum number of objects (0 = unlimited)\n * mpflags:    MPF_CALLBACK (enable callback support) | MPF_NOZERO (don't zero)\n * construct:  Constructor called on allocation\n * deconstruct: Deconstructor called on free\n * priv:       Private data passed to construct/deconstruct\n */\n</code></pre> <p>Example: <pre><code>#include &lt;sys/mpipe.h&gt;\n\nstruct malloc_pipe my_pipe;\n\nvoid my_construct(void *obj, void *priv) {\n    struct my_object *o = obj;\n    /* Initialize object fields */\n    o-&gt;magic = MY_MAGIC;\n    o-&gt;refcount = 1;\n}\n\nvoid my_deconstruct(void *obj, void *priv) {\n    struct my_object *o = obj;\n    /* Clean up object before returning to pool */\n    KASSERT(o-&gt;refcount == 0, (\"mpipe: object still referenced\"));\n    o-&gt;magic = 0;\n}\n\nvoid init_my_subsystem(void) {\n    mpipe_init(&amp;my_pipe, M_MYSUBSYS, sizeof(struct my_object),\n               64,      /* Start with 64 objects */\n               1024,    /* Max 1024 objects */\n               MPF_CALLBACK,  /* Enable callback support */\n               my_construct, my_deconstruct, NULL);\n}\n</code></pre></p>"},{"location":"sys/kern/memory/#allocating-from-pipe","title":"Allocating from Pipe","text":"<pre><code>void *mpipe_alloc_waitok(malloc_pipe_t mpipe);\nvoid *mpipe_alloc_nowait(malloc_pipe_t mpipe);\n\n/* mpipe_alloc_waitok():\n *  - Always succeeds (blocks if necessary)\n *  - If pool exhausted and nmax not reached, grows pool\n *  - If nmax reached and MPF_CALLBACK set, queues request\n *  - Constructor called before returning\n *\n * mpipe_alloc_nowait():\n *  - Returns NULL if pool exhausted\n *  - Never blocks\n *  - Constructor still called on success\n */\n</code></pre> <p>Example: <pre><code>struct my_object *obj;\n\n/* In process context (can sleep) */\nobj = mpipe_alloc_waitok(&amp;my_pipe);\n/* obj is never NULL */\n\n/* In interrupt context (cannot sleep) */\nobj = mpipe_alloc_nowait(&amp;my_pipe);\nif (obj == NULL) {\n    /* Handle allocation failure */\n    return ENOMEM;\n}\n\n/* Use object... */\n</code></pre></p>"},{"location":"sys/kern/memory/#freeing-to-pipe","title":"Freeing to Pipe","text":"<pre><code>void mpipe_free(malloc_pipe_t mpipe, void *obj);\n\n/* Process:\n * 1. Call deconstructor on obj\n * 2. Return obj to pipe's free list (LIFO)\n * 3. If MPF_CALLBACK and pending requests, wake support thread\n */\n</code></pre> <p>Example: <pre><code>/* Return object to pool */\nmpipe_free(&amp;my_pipe, obj);\n/* obj is back in pool, ready for reuse */\n</code></pre></p>"},{"location":"sys/kern/memory/#destroying-a-malloc-pipe","title":"Destroying a Malloc Pipe","text":"<pre><code>void mpipe_done(malloc_pipe_t mpipe);\n\n/* Cleans up malloc pipe:\n * - Frees all objects in pool\n * - Deallocates array memory\n * - Asserts that all objects were returned (ary_count == total_count)\n */\n</code></pre>"},{"location":"sys/kern/memory/#callback-mechanism","title":"Callback Mechanism","text":"<p>When <code>MPF_CALLBACK</code> is set, the malloc pipe can handle allocation requests when the pool is exhausted:</p> <pre><code>flowchart TB\n    A[\"1. mpipe_alloc_waitok() called\"]\n    B[\"2. Pool empty, nmax reached\"]\n    C[\"3. Add to pending list(mpipe-&gt;pending_list)\"]\n    D[\"4. tsleep() on mpipe\"]\n    E[\"... waiting ...\"]\n    F[\"5. Another thread calls mpipe_free()\"]\n    G[\"6. mpipe_thread woken\"]\n    H[\"7. Process pending requests\"]\n    I[\"8. Original thread woken\"]\n    J[\"9. Allocation completes\"]\n\n    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H --&gt; I --&gt; J\n</code></pre> <p>mpipe_thread (<code>kern_mpipe.c:277</code>): - Support thread for callback mechanism - Processes <code>mpipe_pending_list</code> - Wakes blocked allocators when objects become available - Only runs when MPF_CALLBACK pipes exist</p>"},{"location":"sys/kern/memory/#internal-structure","title":"Internal Structure","text":"<pre><code>struct malloc_pipe {\n    void    **array;           /* Array of object pointers */\n    int     total_count;       /* Total objects allocated */\n    int     free_count;        /* Objects in array */\n    int     ary_count;         /* Array size */\n    int     max_count;         /* nmax (0 = unlimited) */\n    int     nominal_count;     /* nnom */\n    int     mpflags;           /* MPF_* flags */\n    size_t  bytes;             /* Object size */\n    malloc_type_t type;        /* For accounting */\n    int     token;             /* Atomic allocation token */\n    void    (*construct)(void *, void *);\n    void    (*deconstruct)(void *, void *);\n    void    *priv;\n    /* ... */\n};\n</code></pre>"},{"location":"sys/kern/memory/#allocation-algorithm_1","title":"Allocation Algorithm","text":"<p>Fast path (lock-free): <pre><code>int mpipe_alloc_callback(...) {  /* kern_mpipe.c:103 */\n    /* 1. Try to get token atomically */\n    if (token_test_and_set(&amp;mpipe-&gt;token)) {\n        /* Someone else has token, slow path */\n        goto slow_path;\n    }\n\n    /* 2. We have token, check free list */\n    if (mpipe-&gt;free_count) {\n        /* 3. Pop object from array (LIFO) */\n        obj = mpipe-&gt;array[--mpipe-&gt;free_count];\n        token_clear(&amp;mpipe-&gt;token);\n\n        /* 4. Call constructor */\n        if (mpipe-&gt;construct)\n            mpipe-&gt;construct(obj, mpipe-&gt;priv);\n\n        return obj;\n    }\n\n    /* Pool empty, release token and go to slow path */\n    token_clear(&amp;mpipe-&gt;token);\n\nslow_path:\n    /* Acquire lock, check if need to grow pool... */\n}\n</code></pre></p> <p>Slow path (with lock): - Grow pool if <code>total_count &lt; max_count</code> (or <code>max_count == 0</code>) - If cannot grow and <code>MPF_CALLBACK</code> set, queue request - Otherwise block or return NULL</p>"},{"location":"sys/kern/memory/#statistics","title":"Statistics","text":"<pre><code>/* Per-pipe statistics via mpipe structure */\nstruct malloc_pipe {\n    int total_count;   /* Total objects ever allocated */\n    int free_count;    /* Objects currently in pool */\n    int ary_count;     /* Size of array */\n    /* ... */\n};\n\n/* Can be exposed via sysctl for monitoring */\n</code></pre>"},{"location":"sys/kern/memory/#usage-patterns","title":"Usage Patterns","text":"<p>Network packet buffers (mbufs): <pre><code>static struct malloc_pipe mbuf_pipe;\n\nvoid mbuf_init(void) {\n    mpipe_init(&amp;mbuf_pipe, M_MBUF, MSIZE,\n               256,     /* Start with 256 mbufs */\n               0,       /* Unlimited */\n               MPF_CALLBACK,\n               mbuf_construct, mbuf_deconstruct, NULL);\n}\n\nstruct mbuf *m_get(int how) {\n    if (how == M_WAITOK)\n        return mpipe_alloc_waitok(&amp;mbuf_pipe);\n    else\n        return mpipe_alloc_nowait(&amp;mbuf_pipe);\n}\n\nvoid m_free(struct mbuf *m) {\n    mpipe_free(&amp;mbuf_pipe, m);\n}\n</code></pre></p> <p>Temporary objects with initialization: <pre><code>static struct malloc_pipe temp_pipe;\n\nvoid temp_construct(void *obj, void *priv) {\n    struct temp_obj *t = obj;\n    spin_init(&amp;t-&gt;lock, \"tempobj\");\n    TAILQ_INIT(&amp;t-&gt;list);\n}\n\nvoid temp_deconstruct(void *obj, void *priv) {\n    struct temp_obj *t = obj;\n    spin_uninit(&amp;t-&gt;lock);\n    KASSERT(TAILQ_EMPTY(&amp;t-&gt;list), (\"temp_obj: list not empty\"));\n}\n</code></pre></p>"},{"location":"sys/kern/memory/#when-to-use-malloc-pipes","title":"When to Use Malloc Pipes","text":"<p>Use mpipe when: - Need guaranteed allocations (cannot fail) - High allocation/free frequency - Constructor/deconstructor required - Want to pre-allocate and limit resource usage - Need LIFO cache-hot behavior</p> <p>Use objcache when: - Need detailed statistics and tuning - Want dynamic magazine sizing - More complex caching strategies</p> <p>Use kmalloc when: - Simple allocations without special lifecycle - Variable sizes</p>"},{"location":"sys/kern/memory/#advantages","title":"Advantages","text":"<ol> <li>Pre-allocation: Amortize allocation cost at initialization</li> <li>Lock-free fast path: Uses atomic token, no spinlock contention</li> <li>Cache-hot: LIFO means recently freed objects are hot in CPU cache</li> <li>Simple: Straightforward LIFO array, easy to understand</li> <li>Callback mechanism: Handles exhaustion gracefully</li> </ol>"},{"location":"sys/kern/memory/#limitations","title":"Limitations","text":"<ol> <li>Fixed size: All objects same size</li> <li>Memory overhead: Pre-allocated objects consume memory even if unused</li> <li>Constructor overhead: Called on every allocation</li> <li>No depot: Unlike objcache, no sophisticated caching layers</li> </ol>"},{"location":"sys/kern/memory/#helper-allocators","title":"Helper Allocators","text":""},{"location":"sys/kern/memory/#alist-power-of-2-allocator","title":"alist: Power-of-2 Allocator","text":"<p>Location: <code>sys/kern/subr_alist.c</code></p> <p>The alist allocator manages free blocks using a radix tree and supports only power-of-2 sized allocations. It can handle unlimited address space sizes.</p> <p>Key characteristics: - Power-of-2 only: Can only allocate sizes that are powers of 2 - Power-of-2 aligned: Allocations aligned to their size - Radix tree: Efficient O(log n) operations with hinting - Unlimited size: No fixed maximum range - Bitmap-based: Uses bitmaps in tree nodes</p> <p>Data structure: <pre><code>typedef struct almeta {\n    alist_blk_t bm_bighint;  /* Biggest allocatable block in subtree */\n    u_daddr_t   bm_bitmap;   /* Bitmap of free blocks (power-of-2) */\n} almeta_t;\n\ntypedef struct alist {\n    alist_blk_t bl_radix;    /* Coverage of one meta element */\n    alist_blk_t bl_radix_mask;\n    alist_blk_t bl_radix_ext;\n    alist_blk_t bl_skip;     /* Starting skip (address offset) */\n    alist_blk_t bl_free;     /* Number of free blocks */\n    alist_blk_t bl_blocks;   /* Total blocks managed */\n    alist_blk_t bl_bighint;  /* Biggest hint */\n    almeta_t    *bl_root;    /* Root of radix tree */\n    /* ... */\n} *alist_t;\n</code></pre></p> <p>API: <pre><code>alist_t alist_create(alist_blk_t blocks, struct malloc_type *mtype);\nvoid alist_destroy(alist_t live);\nvoid alist_free(alist_t live, alist_blk_t blkno, alist_blk_t count);\nalist_blk_t alist_alloc(alist_t live, alist_blk_t count);\n</code></pre></p> <p>Example: <pre><code>/* Create alist for 1GB range (2^30 bytes, 4KB blocks = 2^18 blocks) */\nalist_t al = alist_create(262144, M_TEMP);  /* 2^18 blocks */\n\n/* Allocate 16 blocks (must be power-of-2) */\nalist_blk_t blk = alist_alloc(al, 16);\n\n/* Free the blocks */\nalist_free(al, blk, 16);\n\n/* Destroy */\nalist_destroy(al);\n</code></pre></p> <p>Use case: Managing memory ranges that require power-of-2 alignment (e.g., DMA buffers, hardware resource allocation).</p> <p>Algorithm (<code>subr_alist.c:185</code>): - Radix tree with 64-way fanout (ALIST_BMAP_RADIX = 64) - Each meta node has bitmap and bighint - Hinting accelerates allocation (bm_bighint) - Power-of-2 check: <code>count &amp; (count - 1) == 0</code></p>"},{"location":"sys/kern/memory/#blist-bitmap-block-allocator","title":"blist: Bitmap Block Allocator","text":"<p>Location: <code>sys/kern/subr_blist.c</code></p> <p>The blist allocator is a general-purpose bitmap allocator using a radix tree, primarily used for swap space allocation. Unlike alist, it supports arbitrary sizes and ranges.</p> <p>Key characteristics: - General sizes: Allocate any number of blocks - Arbitrary ranges: No power-of-2 restrictions - Fixed maximum: Cannot exceed initial size - Radix tree: Efficient with hinting - Swap allocator: Primary use case</p> <p>Data structure: <pre><code>typedef struct blmeta {\n    daddr_t bm_bighint;   /* Biggest allocatable run in subtree */\n    daddr_t bm_bitmap;    /* Bitmap of free blocks */\n} blmeta_t;\n\ntypedef struct blist {\n    daddr_t bl_blocks;    /* Total blocks managed */\n    daddr_t bl_radix;     /* Coverage of one meta element */\n    daddr_t bl_skip;      /* Starting skip */\n    daddr_t bl_free;      /* Number of free blocks */\n    daddr_t bl_rootblks;  /* daddr_t blks allocated for root */\n    blmeta_t *bl_root;    /* Root of radix tree */\n    /* ... */\n} *blist_t;\n</code></pre></p> <p>API: <pre><code>blist_t blist_create(daddr_t blocks, int flags);\nvoid blist_destroy(blist_t blist);\ndaddr_t blist_alloc(blist_t blist, daddr_t count);\nvoid blist_free(blist_t blist, daddr_t blkno, daddr_t count);\nvoid blist_resize(blist_t *pblist, daddr_t count, int freenew, int flags);\n</code></pre></p> <p>Example: <pre><code>/* Create blist for swap: 10000 blocks */\nblist_t bl = blist_create(10000, M_WAITOK);\n\n/* Allocate 100 blocks (any size) */\ndaddr_t blk = blist_alloc(bl, 100);\nif (blk == SWAPBLK_NONE) {\n    /* Allocation failed */\n}\n\n/* Free the blocks */\nblist_free(bl, blk, 100);\n\n/* Destroy */\nblist_destroy(bl);\n</code></pre></p> <p>Use case: Swap space management (<code>vm/swap_pager.c</code>).</p> <p>Algorithm (<code>subr_blist.c:276</code>): - Radix tree with BLIST_BMAP_RADIX fanout (currently 64) - Each meta has bitmap and bighint - Cursor-based allocation for sequential patterns - Supports resizing (grow or shrink)</p> <p>Differences from alist:</p> Feature alist blist Alignment Power-of-2 only Any Sizes Power-of-2 only Any Max range Unlimited (dynamic) Fixed at creation Primary use General power-of-2 ranges Swap allocation"},{"location":"sys/kern/memory/#sbuf-string-buffers","title":"sbuf: String Buffers","text":"<p>Location: <code>sys/kern/subr_sbuf.c</code></p> <p>The sbuf API provides dynamic string buffers with automatic growth, similar to C++ <code>std::string</code> or Java <code>StringBuilder</code>.</p> <p>Key features: - Dynamic growth: Automatically expands as needed - Safe: Bounds-checked operations - Printf-style: <code>sbuf_printf()</code> for formatted output - Fixed or auto: Can use fixed buffer or auto-allocate - Length tracking: Always know current length</p> <p>Data structure: <pre><code>struct sbuf {\n    char    *s_buf;       /* Buffer itself */\n    int     s_size;       /* Size of buffer */\n    int     s_len;        /* Current length */\n    int     s_flags;      /* Flags (SBUF_*) */\n    /* ... */\n};\n\n/* Flags */\n#define SBUF_FIXEDLEN   0x00000001  /* Fixed length buffer */\n#define SBUF_AUTOEXTEND 0x00000002  /* Auto-extend buffer */\n#define SBUF_OVERFLOWED 0x00000010  /* Buffer overflowed */\n#define SBUF_FINISHED   0x00000020  /* Buffer finalized */\n</code></pre></p> <p>API: <pre><code>/* Create/destroy */\nstruct sbuf *sbuf_new(struct sbuf *s, char *buf, int length, int flags);\nvoid sbuf_delete(struct sbuf *s);\n\n/* Append operations */\nint sbuf_cat(struct sbuf *s, const char *str);\nint sbuf_printf(struct sbuf *s, const char *fmt, ...);\nint sbuf_putc(struct sbuf *s, int c);\nint sbuf_bcpy(struct sbuf *s, const char *buf, size_t len);  /* Copy */\nint sbuf_bcat(struct sbuf *s, const char *buf, size_t len);  /* Append */\n\n/* Finalize and extract */\nint sbuf_finish(struct sbuf *s);  /* NUL-terminate */\nchar *sbuf_data(struct sbuf *s);\nint sbuf_len(struct sbuf *s);\nint sbuf_overflowed(struct sbuf *s);\n\n/* Clear */\nvoid sbuf_clear(struct sbuf *s);\n</code></pre></p> <p>Example - Auto-extending buffer: <pre><code>struct sbuf *sb;\n\n/* Create auto-extending buffer */\nsb = sbuf_new(NULL, NULL, 0, SBUF_AUTOEXTEND);\n\n/* Build string */\nsbuf_printf(sb, \"Process %d: \", proc-&gt;p_pid);\nsbuf_cat(sb, proc-&gt;p_comm);\nsbuf_printf(sb, \" (%d threads)\", proc-&gt;p_nthreads);\n\n/* Finalize */\nsbuf_finish(sb);\n\n/* Use the string */\nprintf(\"%s\\n\", sbuf_data(sb));\n\n/* Clean up */\nsbuf_delete(sb);\n</code></pre></p> <p>Example - Fixed buffer: <pre><code>char buffer[256];\nstruct sbuf sb;\n\n/* Use fixed buffer (no allocation) */\nsbuf_new(&amp;sb, buffer, sizeof(buffer), SBUF_FIXEDLEN);\n\nsbuf_printf(&amp;sb, \"CPU %d: %s\", cpuid, status);\nsbuf_finish(&amp;sb);\n\nif (sbuf_overflowed(&amp;sb)) {\n    printf(\"Buffer too small!\\n\");\n} else {\n    printf(\"%s\\n\", sbuf_data(&amp;sb));\n}\n\nsbuf_delete(&amp;sb);  /* Does not free buffer, just cleans up */\n</code></pre></p> <p>Use cases: - Building complex strings for sysctl output - Generating debug messages - Creating formatted data for userspace - Any string building where final size is unknown</p> <p>Performance note: Auto-extending sbufs reallocate using <code>kmalloc()</code>, which may be expensive for frequently reallocated buffers. Pre-allocate reasonable size if known.</p>"},{"location":"sys/kern/memory/#sglist-scatter-gather-lists","title":"sglist: Scatter-Gather Lists","text":"<p>Location: <code>sys/kern/subr_sglist.c</code></p> <p>The sglist API manages scatter-gather lists for DMA operations. It describes physically discontiguous memory regions as an array of <code>(address, length)</code> pairs.</p> <p>Key features: - Physical addressing: Describes physical memory ranges - DMA-friendly: Standard format for DMA engines - Bounds tracking: Knows remaining space - Utilities: Build from uio, mbufs, physical buffers</p> <p>Data structure: <pre><code>struct sglist_seg {\n    vm_paddr_t ss_paddr;  /* Physical address */\n    size_t     ss_len;    /* Length in bytes */\n};\n\nstruct sglist {\n    struct sglist_seg *sg_segs;  /* Array of segments */\n    int               sg_nseg;   /* Number of segments */\n    int               sg_maxseg; /* Maximum segments */\n    int               sg_refs;   /* Reference count */\n};\n</code></pre></p> <p>API: <pre><code>/* Create/destroy */\nstruct sglist *sglist_alloc(int nsegs, int mflags);\nstruct sglist *sglist_build(void *buf, size_t len, int mflags);\nvoid sglist_free(struct sglist *sg);\nvoid sglist_hold(struct sglist *sg);   /* Increment refcount */\nvoid sglist_release(struct sglist *sg); /* Decrement refcount */\n\n/* Reset for reuse */\nvoid sglist_reset(struct sglist *sg);\n\n/* Append segments */\nint sglist_append(struct sglist *sg, vm_paddr_t paddr, size_t len);\nint sglist_append_mbuf(struct sglist *sg, struct mbuf *m);\nint sglist_append_phys(struct sglist *sg, vm_paddr_t paddr, size_t len);\nint sglist_append_uio(struct sglist *sg, struct uio *uio);\n\n/* Query */\nint sglist_count(void *buf, size_t len);\nsize_t sglist_length(struct sglist *sg);\nint sglist_slice(struct sglist *original, struct sglist **slice,\n                 size_t offset, size_t length, int mflags);\n\n/* Join two sglists */\nint sglist_join(struct sglist *first, struct sglist *second,\n                struct sglist *out);\n</code></pre></p> <p>Example - Build sglist from buffer: <pre><code>void *buf = kmalloc(8192, M_TEMP, M_WAITOK);\nstruct sglist *sg;\n\n/* Build sglist describing physical pages of buf */\nsg = sglist_build(buf, 8192, M_WAITOK);\n\n/* Program DMA engine with sglist */\nfor (int i = 0; i &lt; sg-&gt;sg_nseg; i++) {\n    dma_program_segment(sg-&gt;sg_segs[i].ss_paddr,\n                       sg-&gt;sg_segs[i].ss_len);\n}\n\n/* Clean up */\nsglist_free(sg);\nkfree(buf, M_TEMP);\n</code></pre></p> <p>Example - Build from mbuf chain: <pre><code>struct mbuf *m = /* ... received packet ... */;\nstruct sglist *sg;\n\n/* Allocate sglist with enough segments for mbuf chain */\nint nsegs = /* estimate based on m_length(m) and page size */;\nsg = sglist_alloc(nsegs, M_WAITOK);\n\n/* Append mbuf chain to sglist */\nif (sglist_append_mbuf(sg, m) != 0) {\n    /* Not enough segments */\n    sglist_free(sg);\n    return EFBIG;\n}\n\n/* Use sglist for DMA... */\n\nsglist_free(sg);\n</code></pre></p> <p>Use cases: - Network DMA (NIC drivers) - Storage device DMA - Any hardware requiring scatter-gather I/O - Zero-copy transfers</p> <p>Implementation note (<code>subr_sglist.c:94</code>): - <code>sglist_build()</code> walks virtual address using <code>pmap_extract()</code> to find physical pages - Consecutive physical pages are coalesced into single segment - <code>sglist_append_mbuf()</code> walks mbuf chain, extracting physical addresses</p>"},{"location":"sys/kern/memory/#rbtree-red-black-tree-utilities","title":"rbtree: Red-Black Tree Utilities","text":"<p>Location: <code>sys/kern/subr_rbtree.c</code></p> <p>The rbtree module provides red-black tree helper functions, but DragonFly uses a custom implementation that is largely self-contained in header files (<code>sys/sys/tree.h</code>).</p> <p>Note: This is not a full-featured allocator, but rather utilities for tree-based data structures. Most code uses the <code>RB_*</code> macros from <code>&lt;sys/tree.h&gt;</code> directly.</p> <p>Red-black tree properties: 1. Every node is red or black 2. Root is black 3. Leaves (NIL) are black 4. Red nodes have black children 5. All paths from node to leaves have same black-height</p> <p>sys/tree.h macros: <pre><code>/* Define tree structure */\nRB_HEAD(my_tree, my_node);\n\n/* Define node structure */\nstruct my_node {\n    RB_ENTRY(my_node) entry;\n    int key;\n    /* ... data ... */\n};\n\n/* Generate tree functions */\nRB_GENERATE(my_tree, my_node, entry, my_compare);\n\n/* Use tree */\nstruct my_tree head = RB_INITIALIZER(&amp;head);\n\nRB_INSERT(my_tree, &amp;head, node);\nstruct my_node *found = RB_FIND(my_tree, &amp;head, &amp;lookup);\nRB_REMOVE(my_tree, &amp;head, node);\n\n/* Iteration */\nRB_FOREACH(node, my_tree, &amp;head) {\n    /* ... */\n}\n</code></pre></p> <p>Use cases: - Ordered key-value data structures - Interval trees - Priority queues with O(log n) operations</p> <p>Alternative: DragonFly also provides <code>&lt;sys/queue.h&gt;</code> for simpler lists/queues (TAILQ, LIST, SLIST, STAILQ).</p>"},{"location":"sys/kern/memory/#code-flow-examples","title":"Code Flow Examples","text":""},{"location":"sys/kern/memory/#example-1-simple-kmalloc-allocation","title":"Example 1: Simple kmalloc Allocation","text":"<p>Scenario: Allocate memory for a temporary buffer in process context.</p> <pre><code>void process_data(void) {\n    char *buffer;\n    size_t size = 4096;\n\n    /* Step 1: Call kmalloc */\n    buffer = kmalloc(size, M_TEMP, M_WAITOK | M_ZERO);\n    /* M_WAITOK: Can block (process context)\n     * M_ZERO: Zero-fill the buffer\n     */\n\n    /* Step 2: kmalloc() \u2192 kern_slaballoc.c:kmalloc() */\n    /* - Round size to power-of-2 zone: 4096 \u2192 zone 4096 */\n\n    /* Step 3: Get CPU-local zone structure */\n    /* - cpuid = mycpuid (no lock needed) */\n    /* - zone = &amp;SLZone[cpuid][zid] */\n\n    /* Step 4: Try to allocate from CPU-local zone */\n    /* - Check zone-&gt;z_NFree &gt; 0 */\n    /* - Pop chunk from zone-&gt;z_FreeChunk */\n    /* - Decrement zone-&gt;z_NFree */\n    /* - If M_ZERO, bzero() the buffer */\n\n    /* Step 5: Return buffer to caller */\n\n    /* Use buffer... */\n    bcopy(source, buffer, size);\n\n    /* Step 6: Free buffer */\n    kfree(buffer, M_TEMP);\n\n    /* Step 7: kfree() \u2192 kern_slaballoc.c:kfree() */\n    /* - Find zone based on pointer address */\n    /* - Determine owning CPU from chunk metadata */\n    /* - If local CPU: add to z_FreeChunk (fast) */\n    /* - If remote CPU: add to z_ReleaseChunk and send IPI */\n}\n</code></pre> <p>Flow diagram: <pre><code>kmalloc(4096, M_TEMP, M_WAITOK|M_ZERO)\n  \u2193\nFind zone: zid = 12 (4KB zone)\n  \u2193\nGet CPU-local zone: cpuid=2, zone=&amp;SLZone[2][12]\n  \u2193\nzone-&gt;z_NFree = 5 (chunks available)\n  \u2193\nPop chunk from z_FreeChunk list\n  \u2193\nz_NFree = 4\n  \u2193\nbzero(chunk, 4096)  \u2190 M_ZERO flag\n  \u2193\nReturn chunk to caller\n  \u2193\n... use memory ...\n  \u2193\nkfree(chunk, M_TEMP)\n  \u2193\nFind owning CPU: cpuid=2 (local)\n  \u2193\nAdd chunk to z_FreeChunk (LIFO)\n  \u2193\nz_NFree = 5\n</code></pre></p>"},{"location":"sys/kern/memory/#example-2-cross-cpu-free-with-ipi","title":"Example 2: Cross-CPU Free with IPI","text":"<p>Scenario: Thread on CPU 0 frees memory allocated by thread on CPU 2.</p> <pre><code>/* Thread on CPU 2 allocates */\nchar *buf = kmalloc(512, M_TEMP, M_WAITOK);\n/* Allocated from SLZone[2][zone_512] */\n\n/* Pass buf to thread on CPU 0 (via queue, etc.) */\n\n/* Thread on CPU 0 frees */\nkfree(buf, M_TEMP);\n\n/* Step 1: kfree() determines owning CPU from chunk */\n/* - Chunk metadata indicates CPU 2 */\n\n/* Step 2: Current CPU (0) != owning CPU (2) */\n/* - Cannot directly modify CPU 2's zone */\n\n/* Step 3: Add to remote CPU's release list */\n/* - zone-&gt;z_ReleaseChunk (uses lock) */\n/* - Add chunk to linked list */\n\n/* Step 4: Send IPI to CPU 2 */\n/* - lwkt_send_ipiq(2, ...) */\n\n/* Step 5: CPU 2 receives IPI */\n/* - Interrupt handler runs on CPU 2 */\n\n/* Step 6: Process release list */\n/* - Move chunks from z_ReleaseChunk to z_FreeChunk */\n/* - Update z_NFree */\n/* - Chunks now available for allocation on CPU 2 */\n</code></pre> <p>Flow diagram: <pre><code>CPU 2: kmalloc(512)\n  \u2193\nAllocate from SLZone[2][zone_512]\n  \u2193\nReturn chunk (metadata: owner=CPU2)\n  \u2193\n... pass to CPU 0 ...\n  \u2193\nCPU 0: kfree(chunk)\n  \u2193\nCheck owner: CPU 2 (remote!)\n  \u2193\nAcquire zone-&gt;z_ReleaseChunk lock\n  \u2193\nAdd chunk to z_ReleaseChunk list\n  \u2193\nRelease lock\n  \u2193\nSend IPI to CPU 2\n  \u2193\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nCPU 2: IPI arrives\n  \u2193\nslgd_alloc() or periodic check\n  \u2193\nProcess z_ReleaseChunk list\n  \u2193\nMove chunks to z_FreeChunk\n  \u2193\nz_NFree += count\n  \u2193\nChunks available for reuse on CPU 2\n</code></pre></p> <p>Why IPIs? - Maintain per-CPU zone invariants - Avoid lock contention on allocation (fast path) - Memory eventually returns to owning CPU's pool - Trade-off: Occasional IPI cost vs. lock-free allocation</p>"},{"location":"sys/kern/memory/#example-3-type-stable-object-allocation","title":"Example 3: Type-Stable Object Allocation","text":"<p>Scenario: VFS allocating vnodes with type stability.</p> <pre><code>/* Initialize malloc type for vnodes */\nMALLOC_DEFINE(M_VNODE, \"vnodes\", \"vnodes\");\n\nvoid vnode_init(void) {\n    /* Create type for vnodes */\n    kmalloc_create(&amp;M_VNODE);\n}\n\nstruct vnode *vnode_alloc(void) {\n    struct vnode *vp;\n\n    /* Step 1: Allocate from type-stable allocator */\n    vp = kmalloc_obj(sizeof(struct vnode), M_VNODE, M_WAITOK|M_ZERO);\n\n    /* Step 2: kmalloc_obj() flow */\n    /* - Get per-CPU active slab (objcache-&gt;slabdata[cpuid].active) */\n    /* - Check active-&gt;findex &lt; KMALLOC_SLAB_FOBJS */\n    /* - Pop object from fobjs[] circular buffer */\n    /* - If active slab exhausted, get new slab from partial list */\n\n    /* Step 3: Initialize vnode */\n    vp-&gt;v_type = VNON;\n    lockinit(&amp;vp-&gt;v_lock, \"vnode\", 0, LK_CANRECURSE);\n    TAILQ_INIT(&amp;vp-&gt;v_namecache);\n\n    return vp;\n}\n\nvoid vnode_free(struct vnode *vp) {\n    /* Clean up */\n    lockuninit(&amp;vp-&gt;v_lock);\n\n    /* Step 1: Free to type-stable allocator */\n    kfree_obj(vp, M_VNODE);\n\n    /* Step 2: kfree_obj() flow */\n    /* - Find slab from object address */\n    /* - Get per-CPU active slab */\n    /* - Add object to fobjs[] circular buffer */\n    /* - Update slab-&gt;findex atomically */\n    /* - Object remains in slab, not returned to system */\n}\n\nvoid vnode_unmount_all(struct mount *mp) {\n    /* On unmount: bulk destroy all vnodes for this mount */\n\n    /* Step 1: Walk vnode list, free each */\n    TAILQ_FOREACH(vp, &amp;mp-&gt;mnt_vnodelist, v_mntvnodes) {\n        kfree_obj(vp, M_VNODE);\n    }\n\n    /* Step 2: Type-stable allocator can now bulk-free slabs */\n    /* - All vnodes for mount are freed */\n    /* - Entire slabs may be empty */\n    /* - kmalloc_obj can return slabs to system */\n\n    /* Advantage: Type stability allows efficient bulk operations */\n}\n</code></pre> <p>Slab allocation flow: <pre><code>kmalloc_obj(sizeof(struct vnode), M_VNODE, ...)\n  \u2193\nGet per-CPU objcache: &amp;objcache[mycpuid]\n  \u2193\nGet active slab: objcache-&gt;slabdata[mycpuid].active\n  \u2193\nActive slab has free objects? (findex &lt; KMALLOC_SLAB_FOBJS)\n  \u2193 YES\nPop object: obj = active-&gt;fobjs[findex++]\n  \u2193\nReturn object\n  \u2193\n... use vnode ...\n  \u2193\nkfree_obj(vnode, M_VNODE)\n  \u2193\nFind slab from object address\n  \u2193\nGet per-CPU active slab\n  \u2193\nAdd to circular buffer: active-&gt;fobjs[findex++] = obj\n  \u2193\nObject back in pool (type-stable: not freed to system)\n</code></pre></p>"},{"location":"sys/kern/memory/#example-4-objcache-magazine-exchange","title":"Example 4: Objcache Magazine Exchange","text":"<p>Scenario: High-frequency allocation from objcache.</p> <pre><code>/* Create objcache for connection objects */\nstruct objcache *conn_cache;\n\nvoid conn_construct(void *obj, void *priv) {\n    struct connection *conn = obj;\n    bzero(conn, sizeof(*conn));\n    TAILQ_INIT(&amp;conn-&gt;send_queue);\n}\n\nvoid conn_destruct(void *obj, void *priv) {\n    struct connection *conn = obj;\n    KASSERT(TAILQ_EMPTY(&amp;conn-&gt;send_queue), (\"conn: queue not empty\"));\n}\n\nvoid conn_init(void) {\n    conn_cache = objcache_create(\"connections\",\n                                 1000,  /* cluster_limit */\n                                 0,     /* nom_cache (auto) */\n                                 conn_construct, conn_destruct, NULL,\n                                 objcache_malloc_alloc,\n                                 objcache_malloc_free, NULL);\n}\n\nstruct connection *conn_alloc(void) {\n    /* Step 1: Call objcache_get() */\n    return objcache_get(conn_cache, M_WAITOK);\n}\n\n/* objcache_get() flow - Fast path */\nvoid *objcache_get_fast_path(struct objcache *cache) {\n    struct objcache_cpu *cpudata;\n    struct magazine *mag;\n    void *obj;\n\n    /* Step 1: Get per-CPU data (lock-free) */\n    cpudata = &amp;cache-&gt;cpu[mycpuid];\n\n    /* Step 2: Get loaded magazine */\n    mag = cpudata-&gt;loaded_mag;\n\n    /* Step 3: Check if magazine has objects */\n    if (mag-&gt;rounds &gt; 0) {\n        /* Step 4: Pop object from magazine (lock-free) */\n        obj = mag-&gt;objects[--mag-&gt;rounds];\n\n        /* Step 5: Return object (constructor already called) */\n        return obj;\n    }\n\n    /* Magazine empty, go to slow path */\n    return objcache_get_slow_path(cache);\n}\n\n/* objcache_get() flow - Slow path (magazine empty) */\nvoid *objcache_get_slow_path(struct objcache *cache) {\n    struct objcache_cpu *cpudata = &amp;cache-&gt;cpu[mycpuid];\n    struct magazine *empty_mag, *full_mag;\n\n    /* Step 1: Loaded magazine is empty */\n    empty_mag = cpudata-&gt;loaded_mag;\n\n    /* Step 2: Check previous magazine */\n    if (cpudata-&gt;previous_mag &amp;&amp; cpudata-&gt;previous_mag-&gt;rounds &gt; 0) {\n        /* Step 3: Swap loaded &lt;-&gt; previous */\n        cpudata-&gt;loaded_mag = cpudata-&gt;previous_mag;\n        cpudata-&gt;previous_mag = empty_mag;\n\n        /* Step 4: Pop from newly loaded magazine */\n        struct magazine *mag = cpudata-&gt;loaded_mag;\n        return mag-&gt;objects[--mag-&gt;rounds];\n    }\n\n    /* Step 5: Both magazines empty, need to get from depot */\n    spin_lock(&amp;cache-&gt;depot_lock);\n\n    /* Step 6: Check depot for full magazine */\n    if (!SLIST_EMPTY(&amp;cache-&gt;depot_full)) {\n        full_mag = SLIST_FIRST(&amp;cache-&gt;depot_full);\n        SLIST_REMOVE_HEAD(&amp;cache-&gt;depot_full, link);\n        cache-&gt;depot_full_count--;\n\n        /* Step 7: Return empty magazine to depot */\n        SLIST_INSERT_HEAD(&amp;cache-&gt;depot_empty, empty_mag, link);\n        cache-&gt;depot_empty_count++;\n\n        spin_unlock(&amp;cache-&gt;depot_lock);\n\n        /* Step 8: Install full magazine as loaded */\n        cpudata-&gt;loaded_mag = full_mag;\n\n        /* Step 9: Pop object */\n        return full_mag-&gt;objects[--full_mag-&gt;rounds];\n    }\n\n    spin_unlock(&amp;cache-&gt;depot_lock);\n\n    /* Step 10: Depot empty, allocate from backend */\n    return cache-&gt;alloc(cache-&gt;allocator_args, M_WAITOK);\n}\n</code></pre> <p>Magazine exchange diagram: <pre><code>objcache_get()\n  \u2193\nCheck loaded magazine: rounds = 0 (empty)\n  \u2193\nCheck previous magazine: rounds = 16 (full)\n  \u2193\nSwap loaded \u2194 previous (lock-free)\n  \u2193\nPop from loaded: obj = loaded-&gt;objects[15]\n  \u2193\nloaded-&gt;rounds = 15\n  \u2193\nReturn object\n  \u2193\n... 15 more fast allocations from loaded ...\n  \u2193\nBoth magazines empty\n  \u2193\nAcquire depot_lock\n  \u2193\nCheck depot_full list: has magazines\n  \u2193\nPop full magazine from depot\n  \u2193\nPush empty magazine to depot\n  \u2193\nRelease depot_lock\n  \u2193\nInstall full magazine as loaded\n  \u2193\nPop object from new loaded magazine\n</code></pre></p> <p>Key insight: Most allocations are lock-free from per-CPU magazines. Only when magazines are exhausted do we acquire the depot lock.</p>"},{"location":"sys/kern/memory/#example-5-zone-recycling-and-hysteresis","title":"Example 5: Zone Recycling and Hysteresis","text":"<p>Scenario: System with low memory pressure triggers zone recycling.</p> <pre><code>/* Slab allocator monitors zone usage via:\n * - ZGlobalZone[zid].ZoneRelsThresh (hysteresis threshold)\n * - zone-&gt;z_NFree (free chunks in zone)\n * - zone-&gt;z_NMax (maximum free chunks before recycling)\n */\n\n/* Allocation increases zone pressure */\nvoid allocation_surge(void) {\n    for (int i = 0; i &lt; 1000; i++) {\n        void *p = kmalloc(1024, M_TEMP, M_WAITOK);\n        /* ... use p ... */\n    }\n    /* Many zones now have high z_NFree (lots of free chunks cached) */\n}\n\n/* Periodic zone cleanup (kern_slaballoc.c:slab_cleanup()) */\nvoid slab_cleanup(void) {\n    /* Called periodically (every few seconds) */\n\n    for (int zid = 0; zid &lt; nzones; zid++) {\n        SLGlobalZone *zglobal = &amp;ZGlobalZone[zid];\n\n        /* Step 1: Calculate zone release threshold */\n        /* Based on:\n         * - Total zone memory\n         * - System memory pressure\n         * - Hysteresis to avoid thrashing\n         */\n        int thresh = zglobal-&gt;ZoneRelsThresh;\n\n        /* Step 2: Check each CPU's zone */\n        for (int cpuid = 0; cpuid &lt; ncpus; cpuid++) {\n            SLZone *zone = &amp;SLZone[cpuid][zid];\n\n            /* Step 3: Exceeds threshold? */\n            if (zone-&gt;z_NFree &gt; thresh) {\n                /* Step 4: Recycle excess chunks */\n                int excess = zone-&gt;z_NFree - (thresh / 2);\n\n                /* Step 5: Free excess chunks back to system */\n                for (int i = 0; i &lt; excess; i++) {\n                    void *chunk = zone-&gt;z_FreeChunk;\n                    zone-&gt;z_FreeChunk = *(void **)chunk;\n                    zone-&gt;z_NFree--;\n\n                    /* Step 6: Return page to VM system */\n                    kfree_real(chunk);  /* \u2192 vm_page_free() */\n                }\n            }\n        }\n    }\n}\n\n/* Result: Zone caches are trimmed, memory returned to VM */\n</code></pre> <p>Hysteresis prevents thrashing: <pre><code>Zone free chunks over time:\n\nz_NFree\n  ^\n  |\n200 |         \u2571\u2572              \u2190 Threshold (ZoneRelsThresh)\n  |        \u2571  \u2572\n  |       \u2571    \u2572___\n100 |------\u2571------\u2572___________  \u2190 Recycle to 100\n  |     \u2571              \u2572\n  |    \u2571                \u2572\n  0 |___/________________\u2572___&gt; time\n     Alloc    Recycle    Steady state\n     surge    event\n\nWithout hysteresis:\n- Constant oscillation around threshold\n- Frequent expensive recycle operations\n\nWith hysteresis:\n- Recycle only when significantly above threshold\n- Recycle to midpoint (thresh/2)\n- Stable behavior under varying load\n</code></pre></p>"},{"location":"sys/kern/memory/#best-practices-and-guidelines","title":"Best Practices and Guidelines","text":""},{"location":"sys/kern/memory/#choosing-the-right-allocator","title":"Choosing the Right Allocator","text":"<p>Decision tree:</p> <pre><code>flowchart TB\n    Q[\"What are you allocating?\"]\n\n    Q --&gt; VAR[\"Variable sizes, infrequent\u2192 Use kmalloc/kfree\"]\n    Q --&gt; SAME[\"Same-size, high frequency, no ctor/dtor\u2192 Use kmalloc_obj/kfree_obj\"]\n    Q --&gt; CTOR[\"Same-size, high frequency, need ctor/dtor\u2192 Use objcache\"]\n    Q --&gt; PRE[\"Pre-allocated pool, guaranteed allocation\u2192 Use mpipe\"]\n    Q --&gt; POW[\"Power-of-2 ranges, alignment critical\u2192 Use alist\"]\n    Q --&gt; GEN[\"General ranges (e.g., swap blocks)\u2192 Use blist\"]\n    Q --&gt; STR[\"Building strings dynamically\u2192 Use sbuf\"]\n    Q --&gt; DMA[\"DMA scatter-gather lists\u2192 Use sglist\"]\n</code></pre> <p>Detailed guidelines:</p> Allocator Best For Avoid When kmalloc General allocations, variable sizes, infrequent High-frequency same-size objects kmalloc_obj Fixed-size, high frequency, type stability Need ctor/dtor, variable sizes objcache High frequency, ctor/dtor, statistics Simple allocations, low frequency mpipe Pre-allocated pools, guaranteed success Dynamic sizing, unpredictable counts alist Power-of-2 aligned ranges Non-power-of-2 sizes blist General block ranges (swap) Need unlimited growth sbuf Dynamic string building Fixed-size strings sglist DMA scatter-gather Non-DMA memory management"},{"location":"sys/kern/memory/#allocation-flags-m_waitok-vs-m_nowait","title":"Allocation Flags: M_WAITOK vs. M_NOWAIT","text":"<p>M_WAITOK (can sleep): <pre><code>/* Use in process context where blocking is acceptable */\nvoid process_context_function(void) {\n    void *p = kmalloc(size, M_TEMP, M_WAITOK);\n    /* p is guaranteed to be non-NULL (or panic if truly OOM) */\n\n    /* Advantages:\n     * - Simple error handling (almost always succeeds)\n     * - Can trigger VM pageout to free memory\n     * - No need to check for NULL\n     */\n}\n</code></pre></p> <p>M_NOWAIT (cannot sleep): <pre><code>/* Use in interrupt context or holding locks */\nvoid interrupt_handler(void) {\n    void *p = kmalloc(size, M_TEMP, M_NOWAIT);\n    if (p == NULL) {\n        /* Handle allocation failure */\n        return ENOMEM;\n    }\n    /* Use p... */\n}\n\nvoid holding_spinlock(void) {\n    spin_lock(&amp;lock);\n\n    void *p = kmalloc(size, M_TEMP, M_NOWAIT);\n    if (p == NULL) {\n        spin_unlock(&amp;lock);\n        return ENOMEM;\n    }\n\n    /* Use p... */\n    spin_unlock(&amp;lock);\n}\n</code></pre></p> <p>Guidelines:</p> <ol> <li>Default to M_WAITOK in process context</li> <li>Simpler code (no NULL checks)</li> <li>Better resource utilization</li> <li> <p>Kernel can free memory if needed</p> </li> <li> <p>Use M_NOWAIT when:</p> </li> <li>In interrupt context</li> <li>Holding spinlocks</li> <li>In callback functions (may be called from interrupt)</li> <li> <p>Performance-critical paths where blocking is unacceptable</p> </li> <li> <p>Always check M_NOWAIT return:    <pre><code>/* WRONG */\np = kmalloc(size, M_TEMP, M_NOWAIT);\nbcopy(src, p, size);  /* May crash if p == NULL! */\n\n/* CORRECT */\np = kmalloc(size, M_TEMP, M_NOWAIT);\nif (p == NULL)\n    return ENOMEM;\nbcopy(src, p, size);\n</code></pre></p> </li> <li> <p>M_INTWAIT: Special flag for slightly more aggressive M_NOWAIT</p> </li> <li>Will try harder than M_NOWAIT</li> <li>Still won't block</li> <li>Use in interrupt context when allocation is important</li> </ol>"},{"location":"sys/kern/memory/#memory-leak-prevention","title":"Memory Leak Prevention","text":"<p>Common patterns:</p> <ol> <li> <p>Match alloc/free:    <pre><code>/* WRONG: Leak on error path */\nint process_data(void) {\n    char *buf = kmalloc(size, M_TEMP, M_WAITOK);\n\n    if (some_error())\n        return EINVAL;  /* Leak: forgot to free buf! */\n\n    /* ... use buf ... */\n    kfree(buf, M_TEMP);\n    return 0;\n}\n\n/* CORRECT: Free on all paths */\nint process_data(void) {\n    char *buf = kmalloc(size, M_TEMP, M_WAITOK);\n    int error = 0;\n\n    if (some_error()) {\n        error = EINVAL;\n        goto cleanup;\n    }\n\n    /* ... use buf ... */\n\ncleanup:\n    kfree(buf, M_TEMP);\n    return error;\n}\n</code></pre></p> </li> <li> <p>Use correct malloc type:    <pre><code>/* WRONG: Type mismatch */\np = kmalloc(size, M_VNODE, M_WAITOK);\n/* ... */\nkfree(p, M_TEMP);  /* Statistics corrupted! */\n\n/* CORRECT */\np = kmalloc(size, M_VNODE, M_WAITOK);\n/* ... */\nkfree(p, M_VNODE);\n</code></pre></p> </li> <li> <p>Avoid double-free:    <pre><code>/* WRONG: Double free */\nkfree(p, M_TEMP);\n/* ... */\nkfree(p, M_TEMP);  /* Corruption! */\n\n/* CORRECT: NULL after free */\nkfree(p, M_TEMP);\np = NULL;\n/* Later accidental free is safe */\nif (p != NULL)\n    kfree(p, M_TEMP);\n</code></pre></p> </li> <li> <p>Reference counting:    <pre><code>struct refcounted_obj {\n    int refcount;\n    /* ... data ... */\n};\n\nvoid obj_hold(struct refcounted_obj *obj) {\n    atomic_add_int(&amp;obj-&gt;refcount, 1);\n}\n\nvoid obj_drop(struct refcounted_obj *obj) {\n    if (atomic_fetchadd_int(&amp;obj-&gt;refcount, -1) == 1) {\n        /* Last reference, free object */\n        kfree(obj, M_OBJ);\n    }\n}\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/memory/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Batch allocations:    <pre><code>/* INEFFICIENT: Many small allocations */\nfor (int i = 0; i &lt; 1000; i++) {\n    items[i] = kmalloc(sizeof(item_t), M_TEMP, M_WAITOK);\n}\n\n/* EFFICIENT: One large allocation */\nitem_t *all_items = kmalloc(1000 * sizeof(item_t), M_TEMP, M_WAITOK);\nfor (int i = 0; i &lt; 1000; i++) {\n    items[i] = &amp;all_items[i];\n}\n</code></pre></p> </li> <li> <p>Use object caches for hot paths:    <pre><code>/* INEFFICIENT: kmalloc in packet processing loop */\nvoid process_packet(struct mbuf *m) {\n    struct pkt_ctx *ctx = kmalloc(sizeof(*ctx), M_TEMP, M_NOWAIT);\n    /* ... */\n    kfree(ctx, M_TEMP);\n}\n\n/* EFFICIENT: Use objcache */\nstatic struct objcache *pkt_ctx_cache;\n\nvoid process_packet(struct mbuf *m) {\n    struct pkt_ctx *ctx = objcache_get(pkt_ctx_cache, M_NOWAIT);\n    /* ... */\n    objcache_put(pkt_ctx_cache, ctx);\n}\n</code></pre></p> </li> <li> <p>Avoid allocations in interrupt context:    <pre><code>/* POOR: Allocate in interrupt */\nvoid device_interrupt(void) {\n    struct work *w = kmalloc(sizeof(*w), M_TEMP, M_NOWAIT);\n    if (w == NULL)\n        return;  /* Drop work! */\n    queue_work(w);\n}\n\n/* BETTER: Pre-allocated pool (mpipe) */\nstatic struct malloc_pipe work_pipe;\n\nvoid device_interrupt(void) {\n    struct work *w = mpipe_alloc_nowait(&amp;work_pipe);\n    if (w == NULL)\n        return;\n    queue_work(w);\n}\n</code></pre></p> </li> <li> <p>Cache-aware allocation:    <pre><code>/* Per-CPU data structure: use kmalloc_obj for cache affinity */\nstruct per_cpu_data *pcd = kmalloc_obj(sizeof(*pcd), M_PERCPU, M_WAITOK);\n/* Likely to be allocated from local CPU zone, cache-hot */\n</code></pre></p> </li> <li> <p>Reuse objects when possible:    <pre><code>/* INEFFICIENT: Constant alloc/free */\nvoid process_loop(void) {\n    for (;;) {\n        char *buf = kmalloc(BUFSIZE, M_TEMP, M_WAITOK);\n        process(buf);\n        kfree(buf, M_TEMP);\n    }\n}\n\n/* EFFICIENT: Allocate once */\nvoid process_loop(void) {\n    char *buf = kmalloc(BUFSIZE, M_TEMP, M_WAITOK);\n    for (;;) {\n        process(buf);\n        /* Reuse buf */\n    }\n    kfree(buf, M_TEMP);\n}\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/memory/#debugging-techniques","title":"Debugging Techniques","text":"<p>Enable INVARIANTS: <pre><code>/* In kernel config */\noptions INVARIANTS\n\n/* Enables:\n * - Memory pattern checks (0xdeadc0de, 0x5a5a5a5a)\n * - Chunk bitmap verification\n * - Redzone protection\n * - Double-free detection\n */\n</code></pre></p> <p>Use vmstat -m for malloc statistics: <pre><code>$ vmstat -m\n         Type InUse MemUse HighUse Requests  Size(s)\n       vnode   1234   123K    234K     5678  128,256\n        temp    567    67K     89K    12345  16,32,64,128,256\n       mbuf    8901   890K   1234K   567890  256\n# Shows:\n# - InUse: Currently allocated objects\n# - MemUse: Current memory usage\n# - HighUse: Peak memory usage\n# - Requests: Total allocation requests\n# - Size(s): Zones used\n</code></pre></p> <p>Check objcache statistics: <pre><code>$ sysctl kern.objcache.stats\n# Shows per-cache statistics:\n# - oc_limit: Cluster limit\n# - oc_requested: Total get requests\n# - oc_allocated: Backend allocations\n# - oc_exhausted: Times limit was hit\n# - oc_failed: Failed allocations\n# - oc_used: Currently allocated\n# - oc_cached: Cached in magazines\n</code></pre></p> <p>Memory leak detection: <pre><code>/* Before suspected leak point */\nvmstat -m &gt; before.txt\n\n/* Run code that might leak */\ntest_suspected_leak();\n\n/* After */\nvmstat -m &gt; after.txt\n\n/* Compare */\ndiff before.txt after.txt\n# Look for types with increased InUse counts\n</code></pre></p> <p>Kernel malloc tracing: <pre><code>/* Enable malloc tracing (if compiled with MALLOC_PROFILE) */\nsysctl debug.malloc.trace=1\n\n/* Generate trace */\n/* ... run workload ... */\n\n/* Analyze trace output in kernel log */\ndmesg | grep malloc\n</code></pre></p> <p>KTR (Kernel Trace) for detailed flow: <pre><code>/* In kernel config */\noptions KTR\noptions KTR_MEMORY\n\n/* At runtime */\nsysctl debug.ktr.entries=16384\nsysctl debug.ktr.mask=0x800  /* KTR_MEMORY */\n\n/* Generate trace */\n/* ... run workload ... */\n\n/* Dump trace */\nsysctl debug.ktr.dump\n</code></pre></p> <p>Common memory corruption patterns: <pre><code>/* Use-after-free: INVARIANTS detects 0xdeadc0de pattern */\nkfree(p, M_TEMP);\np-&gt;field = 123;  /* INVARIANTS panic: 0xdeadc0de detected */\n\n/* Buffer overflow: Watch for panics about corrupted chunks */\nchar *buf = kmalloc(10, M_TEMP, M_WAITOK);\nstrcpy(buf, \"this_string_is_too_long\");  /* Corruption! */\n\n/* Double free: INVARIANTS detects chunk already free */\nkfree(p, M_TEMP);\nkfree(p, M_TEMP);  /* INVARIANTS panic: chunk already free */\n</code></pre></p>"},{"location":"sys/kern/memory/#security-considerations","title":"Security Considerations","text":"<ol> <li> <p>Zero sensitive data:    <pre><code>/* Always use M_ZERO for sensitive data */\nstruct password *pwd = kmalloc(sizeof(*pwd), M_SECURE, M_WAITOK|M_ZERO);\n\n/* After use, explicit zero before free */\nbzero(pwd, sizeof(*pwd));\nkfree(pwd, M_SECURE);\n</code></pre></p> </li> <li> <p>Avoid uninitialized data leaks:    <pre><code>/* UNSAFE: May leak kernel data to userspace */\nstruct userdata *ud = kmalloc(sizeof(*ud), M_TEMP, M_WAITOK);\nud-&gt;field1 = value1;\n/* ud-&gt;field2 uninitialized! */\ncopyout(ud, userptr, sizeof(*ud));  /* Leak! */\n\n/* SAFE: Zero or initialize all fields */\nstruct userdata *ud = kmalloc(sizeof(*ud), M_TEMP, M_WAITOK|M_ZERO);\nud-&gt;field1 = value1;\nud-&gt;field2 = value2;\ncopyout(ud, userptr, sizeof(*ud));\n</code></pre></p> </li> <li> <p>Validate sizes from userspace:    <pre><code>/* UNSAFE: User can cause huge allocation */\nsize_t size;\ncopyin(userptr, &amp;size, sizeof(size));\nvoid *buf = kmalloc(size, M_TEMP, M_WAITOK);  /* Danger! */\n\n/* SAFE: Validate and limit size */\nsize_t size;\ncopyin(userptr, &amp;size, sizeof(size));\nif (size &gt; MAX_USER_ALLOC) {\n    return EINVAL;\n}\nvoid *buf = kmalloc(size, M_TEMP, M_WAITOK);\n</code></pre></p> </li> <li> <p>Use M_NULLOK when appropriate:    <pre><code>/* Allow NULL return instead of panic on exhaustion */\nvoid *p = kmalloc(huge_size, M_TEMP, M_WAITOK|M_NULLOK);\nif (p == NULL) {\n    /* Handle gracefully instead of panic */\n    return ENOMEM;\n}\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/memory/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li> <p>Allocating with locks held:    <pre><code>/* WRONG: May deadlock */\nspin_lock(&amp;lock);\np = kmalloc(size, M_TEMP, M_WAITOK);  /* Can't sleep with spinlock! */\nspin_unlock(&amp;lock);\n\n/* CORRECT: Allocate before lock or use M_NOWAIT */\np = kmalloc(size, M_TEMP, M_WAITOK);\nspin_lock(&amp;lock);\n/* ... use p ... */\nspin_unlock(&amp;lock);\n</code></pre></p> </li> <li> <p>Ignoring M_NOWAIT failures:    <pre><code>/* WRONG: No NULL check */\np = kmalloc(size, M_TEMP, M_NOWAIT);\nbcopy(src, p, size);  /* Crash if p == NULL! */\n\n/* CORRECT */\np = kmalloc(size, M_TEMP, M_NOWAIT);\nif (p == NULL)\n    return ENOMEM;\nbcopy(src, p, size);\n</code></pre></p> </li> <li> <p>Type-stable allocator misconceptions:    <pre><code>/* WRONG: Expecting memory to be returned to VM immediately */\nkfree_obj(obj, M_TYPE);\n/* Memory stays in type-stable slab, not freed to VM! */\n\n/* This is intentional: Type stability keeps memory allocated\n * for fast reuse. Use kmalloc if you need memory returned\n * to system immediately.\n */\n</code></pre></p> </li> <li> <p>Objcache magazine sizing:    <pre><code>/* POOR: Cluster limit too small */\ncache = objcache_create(\"foo\", 10, 0, ...);  /* Only 10 objects max */\n/* Under load, will hit limit and fall back to backend frequently */\n\n/* BETTER: Size based on expected load */\ncache = objcache_create(\"foo\", 1000, 0, ...);  /* Allow 1000 cached */\n</code></pre></p> </li> <li> <p>Mixing allocators:    <pre><code>/* WRONG: Allocate with one, free with another */\np = kmalloc_obj(size, M_TYPE, M_WAITOK);\nkfree(p, M_TYPE);  /* Should use kfree_obj! */\n\n/* CORRECT */\np = kmalloc_obj(size, M_TYPE, M_WAITOK);\nkfree_obj(p, M_TYPE);\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/memory/#summary","title":"Summary","text":"<p>DragonFly's memory allocation system provides a sophisticated, multi-layered architecture optimized for different use cases:</p> <ul> <li>kmalloc/kfree: General-purpose, power-of-2 zones, per-CPU optimization</li> <li>kmalloc_obj/kfree_obj: Type-stable, 128KB slabs, bulk operations</li> <li>objcache: Magazine/depot caching, constructor/destructor support</li> <li>mpipe: Pre-allocated pools, guaranteed allocation</li> <li>Helper allocators: alist, blist, sbuf, sglist for specialized needs</li> </ul> <p>Key design principles: 1. Lock-free fast paths: Per-CPU zones, atomic operations 2. Cache affinity: LIFO behavior, per-CPU allocation 3. Hysteresis: Prevent thrashing in zone recycling 4. Layered architecture: Choose the right tool for the job 5. Type stability: Enable efficient bulk operations</p> <p>The system balances performance, memory efficiency, and simplicity, providing appropriate allocators for different allocation patterns from high-frequency small objects to dynamic string building and DMA scatter-gather lists.</p>"},{"location":"sys/kern/newbus/","title":"NewBus Framework","text":"<p>The NewBus framework is DragonFly BSD's device driver infrastructure, providing a unified, object-oriented approach to device management. It handles device enumeration, driver binding, resource allocation, and interrupt management.</p> <p>Source files: - <code>sys/kern/subr_bus.c</code> - Core NewBus implementation (~3,900 lines) - <code>sys/kern/subr_autoconf.c</code> - Auto-configuration hooks - <code>sys/kern/device_if.m</code> - Device interface methods - <code>sys/kern/bus_if.m</code> - Bus interface methods - <code>sys/sys/bus.h</code> - Public data structures and macros - <code>sys/sys/bus_private.h</code> - Internal structures - <code>sys/sys/kobj.h</code> - Kernel object system (method dispatch)</p>"},{"location":"sys/kern/newbus/#overview","title":"Overview","text":"<p>NewBus derives from FreeBSD's NewBus but includes several DragonFly-specific extensions:</p> <ol> <li>CPU Affinity - Interrupt resources include CPU binding</li> <li>Asynchronous Probing - Devices can probe/attach in parallel threads</li> <li>Global Probe Priority - Fine-grained control over probe ordering</li> <li>LWKT Integration - Uses LWKT threads for async operations</li> <li>Serializer Support - Interrupt handlers can use LWKT serializers</li> <li>Threaded Interrupts Only - No fast interrupts; all are threaded</li> </ol>"},{"location":"sys/kern/newbus/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/newbus/#device_t-struct-bsd_device","title":"device_t (struct bsd_device)","text":"<p>The fundamental device representation:</p> <pre><code>struct bsd_device {\n    KOBJ_FIELDS;                        /* kobj ops table */\n\n    /* Device hierarchy */\n    TAILQ_ENTRY(bsd_device) link;       /* list of devices in parent */\n    TAILQ_ENTRY(bsd_device) devlink;    /* global device list */\n    device_t        parent;\n    device_list_t   children;           /* subordinate devices */\n\n    /* Device details */\n    driver_t       *driver;\n    devclass_t      devclass;\n    int             unit;\n    char           *nameunit;           /* e.g., \"em0\" */\n    char           *desc;               /* driver description */\n    int             busy;               /* device_busy() count */\n    device_state_t  state;\n    uint32_t        devflags;           /* API-level flags */\n    u_short         flags;              /* internal flags */\n    u_char          order;              /* attachment order */\n    void           *ivars;              /* bus-specific instance vars */\n    void           *softc;              /* driver-specific data */\n\n    struct sysctl_ctx_list sysctl_ctx;\n    struct sysctl_oid *sysctl_tree;\n};\n</code></pre> <p>Defined in <code>sys/sys/bus_private.h:97-139</code>.</p>"},{"location":"sys/kern/newbus/#device-flags","title":"Device Flags","text":"<p>Internal flags (<code>flags</code> field):</p> Flag Value Description <code>DF_ENABLED</code> 0x0001 Device should be probed/attached <code>DF_FIXEDCLASS</code> 0x0002 Devclass specified at creation <code>DF_WILDCARD</code> 0x0004 Unit originally wildcard (-1) <code>DF_DESCMALLOCED</code> 0x0008 Description was allocated <code>DF_QUIET</code> 0x0010 Don't print verbose attach message <code>DF_DONENOMATCH</code> 0x0020 DEVICE_NOMATCH already called <code>DF_EXTERNALSOFTC</code> 0x0040 softc not allocated by bus <code>DF_ASYNCPROBE</code> 0x0080 Can probe with its own thread <p>Defined in <code>sys/sys/bus_private.h:124-131</code>.</p>"},{"location":"sys/kern/newbus/#device_state_t","title":"device_state_t","text":"<p>Device lifecycle states:</p> <pre><code>typedef enum device_state {\n    DS_NOTPRESENT,    /* not probed or probe failed */\n    DS_ALIVE,         /* probe succeeded */\n    DS_INPROGRESS,    /* attach in progress (async) */\n    DS_ATTACHED,      /* attach method called */\n    DS_BUSY           /* device is open */\n} device_state_t;\n</code></pre> <p>Defined in <code>sys/sys/bus.h:76-82</code>.</p>"},{"location":"sys/kern/newbus/#devclass_t-struct-devclass","title":"devclass_t (struct devclass)","text":"<p>Groups devices by type (e.g., \"pci\", \"em\", \"ahci\"):</p> <pre><code>struct devclass {\n    TAILQ_ENTRY(devclass) link;\n    devclass_t      parent;         /* parent devclass hierarchy */\n    driver_list_t   drivers;        /* drivers for this bus type */\n    char           *name;\n    device_t       *devices;        /* array indexed by unit */\n    int             maxunit;        /* devices array size */\n\n    struct sysctl_ctx_list sysctl_ctx;\n    struct sysctl_oid *sysctl_tree;\n};\n</code></pre> <p>Defined in <code>sys/sys/bus_private.h:58-68</code>.</p>"},{"location":"sys/kern/newbus/#driver_t-kobj_class_t","title":"driver_t (kobj_class_t)","text":"<p>Driver definition using the kobj class system:</p> <pre><code>struct kobj_class {\n    const char     *name;       /* driver name */\n    kobj_method_t  *methods;    /* method table */\n    size_t          size;       /* softc size */\n    kobj_class_t   *baseclasses;/* inheritance */\n    u_int           refs;       /* reference count */\n    kobj_ops_t      ops;        /* compiled methods */\n    u_int           gpri;       /* global probe priority */\n};\n</code></pre> <p>Global Probe Priorities (<code>sys/sys/kobj.h:89-91</code>):</p> Priority Value Description <code>KOBJ_GPRI_ACPI</code> 0x00FF ACPI drivers probe first <code>KOBJ_GPRI_DEFAULT</code> 0x0000 Default priority <code>KOBJ_GPRI_LAST</code> 0x0000 Same as default <p>Defined in <code>sys/sys/kobj.h:62-73</code>.</p>"},{"location":"sys/kern/newbus/#struct-resource","title":"struct resource","text":"<p>Represents allocated system resources:</p> <pre><code>struct resource {\n    TAILQ_ENTRY(resource) r_link;\n    LIST_ENTRY(resource)  r_sharelink;\n    LIST_HEAD(, resource) *r_sharehead;\n    u_long          r_start;        /* first index */\n    u_long          r_end;          /* last index (inclusive) */\n    u_int           r_flags;\n    void           *r_virtual;      /* virtual address */\n    bus_space_tag_t r_bustag;\n    bus_space_handle_t r_bushandle;\n    device_t        r_dev;          /* owning device */\n    struct rman    *r_rm;           /* resource manager */\n    int             r_rid;          /* resource ID */\n};\n</code></pre> <p>Resource Flags (<code>sys/sys/rman.h:45-52</code>):</p> Flag Value Description <code>RF_ALLOCATED</code> 0x0001 Resource reserved <code>RF_ACTIVE</code> 0x0002 Resource activated <code>RF_SHAREABLE</code> 0x0004 Permits sharing <code>RF_TIMESHARE</code> 0x0008 Permits time-division sharing <code>RF_WANTED</code> 0x0010 Someone waiting <code>RF_FIRSTSHARE</code> 0x0020 First in sharing list <code>RF_PREFETCHABLE</code> 0x0040 Memory is prefetchable <code>RF_OPTIONAL</code> 0x0080 For bus_alloc_resources() <p>Defined in <code>sys/sys/rman.h:98-111</code>.</p>"},{"location":"sys/kern/newbus/#resource-types","title":"Resource Types","text":"<pre><code>#define SYS_RES_IRQ     1   /* interrupt lines */\n#define SYS_RES_DRQ     2   /* ISA DMA lines */\n#define SYS_RES_MEMORY  3   /* I/O memory */\n#define SYS_RES_IOPORT  4   /* I/O ports */\n</code></pre> <p>Defined in <code>sys/sys/bus_resource.h:41-44</code>.</p>"},{"location":"sys/kern/newbus/#struct-resource_list_entry","title":"struct resource_list_entry","text":"<p>Bus-specific resource tracking:</p> <pre><code>struct resource_list_entry {\n    SLIST_ENTRY(resource_list_entry) link;\n    int             type;       /* SYS_RES_* */\n    int             rid;        /* resource identifier */\n    struct resource *res;       /* actual resource */\n    u_long          start;\n    u_long          end;\n    u_long          count;\n    int             cpuid;      /* CPU affinity (DragonFly) */\n};\n</code></pre> <p>Defined in <code>sys/sys/bus.h:136-146</code>.</p>"},{"location":"sys/kern/newbus/#device-interface-methods","title":"Device Interface Methods","text":"<p>Core methods that drivers implement (from <code>sys/kern/device_if.m</code>):</p> Method Purpose <code>DEVICE_PROBE</code> Check if driver supports device; return priority <code>DEVICE_IDENTIFY</code> Add child devices to bus (static method) <code>DEVICE_ATTACH</code> Initialize hardware and allocate resources <code>DEVICE_DETACH</code> Remove device and free resources <code>DEVICE_SHUTDOWN</code> Prepare for system shutdown <code>DEVICE_SUSPEND</code> Save state before power management suspend <code>DEVICE_RESUME</code> Restore state after resume <code>DEVICE_QUIESCE</code> FreeBSD compat: prepare for detach <code>DEVICE_REGISTER</code> Return device registration data"},{"location":"sys/kern/newbus/#probe-return-values","title":"Probe Return Values","text":"<p>Drivers return priority values from <code>DEVICE_PROBE</code>:</p> Value Constant Description 0 <code>BUS_PROBE_SPECIFIC</code> Only this driver can use device 0 <code>BUS_PROBE_VENDOR</code> Vendor-supplied driver 0 <code>BUS_PROBE_DEFAULT</code> Standard driver 0 <code>BUS_PROBE_LOW_PRIORITY</code> Lower priority match 0 <code>BUS_PROBE_GENERIC</code> Generic fallback 0 <code>BUS_PROBE_HOOVER</code> Catch-all device <p>Note: In DragonFly, these are all defined as 0. Use <code>gpri</code> for priority ordering instead.</p> <p>See <code>sys/sys/bus.h:476-489</code>.</p>"},{"location":"sys/kern/newbus/#bus-interface-methods","title":"Bus Interface Methods","text":"<p>Methods for bus drivers to manage children (from <code>sys/kern/bus_if.m</code>):</p> Method Purpose <code>BUS_PRINT_CHILD</code> Print device attachment info <code>BUS_PROBE_NOMATCH</code> Called when no driver matches <code>BUS_READ_IVAR</code> / <code>BUS_WRITE_IVAR</code> Read/write instance variables <code>BUS_CHILD_DETACHED</code> Notification of child detach <code>BUS_DRIVER_ADDED</code> New driver added notification <code>BUS_ADD_CHILD</code> Create child device <code>BUS_ALLOC_RESOURCE</code> Allocate system resource <code>BUS_ACTIVATE_RESOURCE</code> Activate resource <code>BUS_DEACTIVATE_RESOURCE</code> Deactivate resource <code>BUS_RELEASE_RESOURCE</code> Free resource <code>BUS_SETUP_INTR</code> Set up interrupt handler <code>BUS_TEARDOWN_INTR</code> Remove interrupt handler <code>BUS_ENABLE_INTR</code> / <code>BUS_DISABLE_INTR</code> Enable/disable interrupt <code>BUS_SET_RESOURCE</code> Set resource range <code>BUS_GET_RESOURCE</code> Get resource range <code>BUS_DELETE_RESOURCE</code> Delete resource <code>BUS_GET_RESOURCE_LIST</code> Get resource list <code>BUS_CHILD_PRESENT</code> Check if child hardware present <code>BUS_CHILD_PNPINFO_STR</code> Get PnP info string <code>BUS_CHILD_LOCATION_STR</code> Get location string <code>BUS_CONFIG_INTR</code> Configure interrupt trigger/polarity <code>BUS_GET_DMA_TAG</code> Get DMA tag"},{"location":"sys/kern/newbus/#device-lifecycle","title":"Device Lifecycle","text":""},{"location":"sys/kern/newbus/#device-creation","title":"Device Creation","text":"<p>device_add_child() - <code>sys/kern/subr_bus.c:1240-1244</code></p> <p>Creates a new device as a child of an existing device:</p> <pre><code>device_t device_add_child(device_t dev, const char *name, int unit);\n</code></pre> <ul> <li><code>name</code> - Driver name to use (NULL for any)</li> <li><code>unit</code> - Unit number (-1 for auto-assignment)</li> </ul> <p>device_add_child_ordered() - <code>sys/kern/subr_bus.c:1246-1281</code></p> <p>Creates a child with explicit attachment order:</p> <pre><code>device_t device_add_child_ordered(device_t dev, int order,\n                                  const char *name, int unit);\n</code></pre> <p>Lower order values are probed first.</p> <p>make_device() - <code>sys/kern/subr_bus.c:1174-1225</code></p> <p>Internal function that: 1. Allocates <code>struct bsd_device</code> 2. Initializes kobj with null_class 3. Sets initial state to <code>DS_NOTPRESENT</code> 4. Adds to global <code>bus_data_devices</code> list</p>"},{"location":"sys/kern/newbus/#probe-and-attach-flow","title":"Probe and Attach Flow","text":"<p>device_probe_and_attach() - <code>sys/kern/subr_bus.c:1961-2013</code></p> <p>Main entry point for device attachment:</p> <ol> <li>Check if already alive</li> <li>Check if enabled</li> <li>Call <code>device_probe_child()</code></li> <li>On probe failure, call <code>BUS_PROBE_NOMATCH</code></li> <li>Print device info if verbose</li> <li>Either attach async or sync</li> </ol> <p>device_probe_child() - <code>sys/kern/subr_bus.c:1403-1489</code></p> <p>Finds the best matching driver:</p> <ol> <li>Get parent's devclass</li> <li>Iterate through devclass hierarchy</li> <li>For each driver:</li> <li>Set driver on device</li> <li>Call <code>DEVICE_PROBE()</code></li> <li>Track best match by priority</li> <li>Set device to best driver</li> <li>Change state to <code>DS_ALIVE</code></li> </ol> <p>device_doattach() - <code>sys/kern/subr_bus.c:2098-2124</code></p> <p>Performs actual attachment:</p> <ol> <li>Initialize sysctl tree</li> <li>Call <code>DEVICE_ATTACH()</code></li> <li>On success: set state to <code>DS_ATTACHED</code>, notify devctl</li> <li>On failure: restore to <code>DS_NOTPRESENT</code></li> </ol>"},{"location":"sys/kern/newbus/#asynchronous-attachment-dragonfly-extension","title":"Asynchronous Attachment (DragonFly Extension)","text":"<p>Enabled by <code>kern.do_async_attach</code> tunable.</p> <p>device_attach_async() - <code>sys/kern/subr_bus.c:2071-2093</code></p> <pre><code>static void\ndevice_attach_async(device_t dev)\n{\n    atomic_add_int(&amp;numasyncthreads, 1);\n    lwkt_create(device_attach_thread, dev, &amp;td, NULL,\n                0, 0, \"%s\", (dev-&gt;desc ? dev-&gt;desc : \"devattach\"));\n}\n</code></pre> <p>Devices must set <code>DF_ASYNCPROBE</code> flag to use async attach.</p>"},{"location":"sys/kern/newbus/#detach","title":"Detach","text":"<p>device_detach() - <code>sys/kern/subr_bus.c:2126-2152</code></p> <ol> <li>Check if busy</li> <li>Call <code>DEVICE_DETACH()</code></li> <li>Notify devctl</li> <li>Call <code>BUS_CHILD_DETACHED()</code></li> <li>Remove from devclass if not fixed</li> <li>Set state to <code>DS_NOTPRESENT</code></li> <li>Clear driver</li> </ol>"},{"location":"sys/kern/newbus/#resource-allocation","title":"Resource Allocation","text":""},{"location":"sys/kern/newbus/#allocating-resources","title":"Allocating Resources","text":"<p>bus_alloc_resource() - <code>sys/kern/subr_bus.c:3290-3298</code></p> <pre><code>struct resource *\nbus_alloc_resource(device_t dev, int type, int *rid,\n                   u_long start, u_long end, u_long count, u_int flags);\n</code></pre> <ul> <li><code>type</code> - <code>SYS_RES_IRQ</code>, <code>SYS_RES_MEMORY</code>, <code>SYS_RES_IOPORT</code>, <code>SYS_RES_DRQ</code></li> <li><code>rid</code> - Resource ID (in/out)</li> <li><code>start</code>, <code>end</code> - Requested range (0, ~0 for any)</li> <li><code>count</code> - Number of units</li> <li><code>flags</code> - <code>RF_*</code> flags</li> </ul> <p>bus_alloc_resource_any() - <code>sys/sys/bus.h:345-349</code> (inline)</p> <p>Convenience wrapper:</p> <pre><code>static __inline struct resource *\nbus_alloc_resource_any(device_t dev, int type, int *rid, u_int flags)\n{\n    return (bus_alloc_resource(dev, type, rid, 0ul, ~0ul, 1, flags));\n}\n</code></pre> <p>bus_alloc_legacy_irq_resource() - <code>sys/kern/subr_bus.c:3300-3307</code></p> <p>DragonFly-specific: Allocates IRQ with CPU affinity:</p> <pre><code>struct resource *\nbus_alloc_legacy_irq_resource(device_t dev, int *rid, u_long irq, u_int flags);\n</code></pre>"},{"location":"sys/kern/newbus/#activating-resources","title":"Activating Resources","text":"<p>bus_activate_resource() - <code>sys/kern/subr_bus.c:3309-3315</code></p> <p>Maps the resource (for memory/I/O port) or enables it (for IRQ).</p> <p>bus_deactivate_resource() - <code>sys/kern/subr_bus.c:3317-3323</code></p> <p>Unmaps or disables the resource.</p>"},{"location":"sys/kern/newbus/#releasing-resources","title":"Releasing Resources","text":"<p>bus_release_resource() - <code>sys/kern/subr_bus.c:3325-3331</code></p> <p>Frees a previously allocated resource.</p>"},{"location":"sys/kern/newbus/#resource-list-management","title":"Resource List Management","text":"<p>Buses use resource lists to track child resources:</p> Function Purpose <code>resource_list_init()</code> Initialize list <code>resource_list_free()</code> Free list and all entries <code>resource_list_add()</code> Add resource to list <code>resource_list_find()</code> Find resource by type/rid <code>resource_list_delete()</code> Remove resource from list <code>resource_list_alloc()</code> Allocate from list <code>resource_list_release()</code> Release resource <p>See <code>sys/kern/subr_bus.c:2592-2746</code>.</p>"},{"location":"sys/kern/newbus/#interrupt-handling","title":"Interrupt Handling","text":""},{"location":"sys/kern/newbus/#interrupt-flags","title":"Interrupt Flags","text":"<pre><code>#define INTR_HIFREQ     0x0040  /* high frequency interrupt */\n#define INTR_CLOCK      0x0080  /* clock interrupt */\n#define INTR_EXCL       0x0100  /* exclusive, non-shared */\n#define INTR_MPSAFE     0x0200  /* handler is MP-safe */\n#define INTR_NOENTROPY  0x0400  /* don't add to entropy pool */\n#define INTR_NOPOLL     0x0800  /* cannot be polled */\n</code></pre> <p>Note: <code>INTR_FAST</code> is no longer supported - all device interrupts are threaded.</p> <p>Defined in <code>sys/sys/bus.h:109-114</code>.</p>"},{"location":"sys/kern/newbus/#interrupt-trigger-and-polarity","title":"Interrupt Trigger and Polarity","text":"<pre><code>enum intr_trigger {\n    INTR_TRIGGER_CONFORM = 0,\n    INTR_TRIGGER_EDGE = 1,\n    INTR_TRIGGER_LEVEL = 2\n};\n\nenum intr_polarity {\n    INTR_POLARITY_CONFORM = 0,\n    INTR_POLARITY_HIGH = 1,\n    INTR_POLARITY_LOW = 2\n};\n</code></pre> <p>Defined in <code>sys/sys/bus.h:116-126</code>.</p>"},{"location":"sys/kern/newbus/#setting-up-interrupts","title":"Setting Up Interrupts","text":"<p>bus_setup_intr() - <code>sys/kern/subr_bus.c:3344-3351</code></p> <pre><code>int bus_setup_intr(device_t dev, struct resource *r, int flags,\n                   driver_intr_t handler, void *arg, void **cookiep,\n                   lwkt_serialize_t serializer);\n</code></pre> <ul> <li><code>r</code> - IRQ resource from <code>bus_alloc_resource()</code></li> <li><code>flags</code> - <code>INTR_*</code> flags</li> <li><code>handler</code> - Interrupt handler function</li> <li><code>arg</code> - Argument passed to handler</li> <li><code>cookiep</code> - Returns cookie for teardown</li> <li><code>serializer</code> - Optional LWKT serializer (DragonFly extension)</li> </ul> <p>bus_setup_intr_descr() - <code>sys/kern/subr_bus.c:3333-3342</code></p> <p>Same as above but with description string for debugging.</p>"},{"location":"sys/kern/newbus/#tearing-down-interrupts","title":"Tearing Down Interrupts","text":"<p>bus_teardown_intr() - <code>sys/kern/subr_bus.c:3353-3359</code></p> <pre><code>int bus_teardown_intr(device_t dev, struct resource *r, void *cookie);\n</code></pre>"},{"location":"sys/kern/newbus/#enablingdisabling-interrupts","title":"Enabling/Disabling Interrupts","text":"<p>bus_enable_intr() - <code>sys/kern/subr_bus.c:3361-3366</code> bus_disable_intr() - <code>sys/kern/subr_bus.c:3368-3375</code></p>"},{"location":"sys/kern/newbus/#devclass-management","title":"Devclass Management","text":""},{"location":"sys/kern/newbus/#findingcreating-devclasses","title":"Finding/Creating Devclasses","text":"<p>devclass_find() - <code>sys/kern/subr_bus.c:789-793</code></p> <p>Finds an existing devclass by name.</p> <p>devclass_create() - <code>sys/kern/subr_bus.c:783-787</code></p> <p>Finds or creates a devclass.</p> <p>devclass_find_internal() - <code>sys/kern/subr_bus.c:738-781</code></p> <p>Internal function that: 1. Searches global <code>devclasses</code> list 2. Creates if not found and <code>create</code> is true 3. Handles parent devclass for inheritance</p>"},{"location":"sys/kern/newbus/#driver-registration","title":"Driver Registration","text":"<p>devclass_add_driver() - <code>sys/kern/subr_bus.c:805-850</code></p> <p>Adds a driver to a devclass:</p> <ol> <li>Instantiate driver kobj class</li> <li>Create devclass for driver name</li> <li>Add to devclass driver list</li> <li>Call <code>BUS_DRIVER_ADDED</code> for existing buses</li> </ol> <p>devclass_delete_driver() - <code>sys/kern/subr_bus.c:852-906</code></p> <p>Removes a driver:</p> <ol> <li>Find driver link</li> <li>Detach all devices using driver</li> <li>Remove from driver list</li> <li>Uninstantiate kobj class</li> </ol>"},{"location":"sys/kern/newbus/#generic-bus-methods","title":"Generic Bus Methods","text":"<p>Standard implementations for bus drivers in <code>sys/kern/subr_bus.c</code>:</p> Function Line Purpose <code>bus_generic_identify</code> 2786 Add child with driver name <code>bus_generic_identify_sameunit</code> 2795 Add child with parent's unit <code>bus_generic_probe</code> 2807 Call DEVICE_IDENTIFY for all drivers <code>bus_generic_attach</code> 2840 Probe/attach all children <code>bus_generic_attach_gpri</code> 2852 Attach with specific priority <code>bus_generic_detach</code> 2864 Detach all children <code>bus_generic_shutdown</code> 2880 Shutdown all children <code>bus_generic_suspend</code> 2891 Suspend all children (with rollback) <code>bus_generic_resume</code> 2910 Resume all children <code>bus_generic_print_child</code> 2958 Print header + footer <code>bus_generic_driver_added</code> 3004 Try attaching unprobed children <code>bus_generic_setup_intr</code> 3016 Propagate to parent <code>bus_generic_teardown_intr</code> 3030 Propagate to parent <code>bus_generic_alloc_resource</code> 3068 Propagate to parent <code>bus_generic_release_resource</code> 3080 Propagate to parent"},{"location":"sys/kern/newbus/#configuration-resource-hints","title":"Configuration Resource Hints","text":"<p>Support for device hints from <code>/boot/loader.conf</code>:</p> <p>resource_int_value() - <code>sys/kern/subr_bus.c:2325-2342</code> resource_long_value() - <code>sys/kern/subr_bus.c:2344-2362</code> resource_string_value() - <code>sys/kern/subr_bus.c:2364-2397</code></p> <pre><code>int resource_int_value(const char *name, int unit,\n                       const char *resname, int *result);\n</code></pre> <p>resource_kenv() - <code>sys/kern/subr_bus.c:2298-2323</code></p> <p>Supports both DragonFly and FreeBSD hint formats: - DragonFly: <code>deviceN.property</code> - FreeBSD: <code>hint.device.N.property</code></p> <p>resource_disabled() - <code>sys/kern/subr_bus.c:3818-3827</code></p> <pre><code>int resource_disabled(const char *name, int unit);\n</code></pre> <p>Checks if <code>deviceN.disabled=1</code> is set.</p>"},{"location":"sys/kern/newbus/#auto-configuration-hooks","title":"Auto-Configuration Hooks","text":"<p>For drivers that need interrupt-driven configuration:</p>"},{"location":"sys/kern/newbus/#struct-intr_config_hook","title":"struct intr_config_hook","text":"<pre><code>struct intr_config_hook {\n    TAILQ_ENTRY(intr_config_hook) ich_links;\n    void    (*ich_func)(void *);\n    void    *ich_arg;\n    const char *ich_desc;\n    int     ich_order;\n    int     ich_ran;\n};\n</code></pre> <p>Defined in <code>sys/sys/kernel.h:475-482</code>.</p>"},{"location":"sys/kern/newbus/#api-functions","title":"API Functions","text":"<p>config_intrhook_establish() - <code>sys/kern/subr_autoconf.c:136-177</code></p> <p>Registers hook for post-interrupt configuration: - Hooks ordered by <code>ich_order</code> - If called after boot, runs immediately</p> <p>config_intrhook_disestablish() - <code>sys/kern/subr_autoconf.c:179-201</code></p> <p>Removes hook and wakes up waiters.</p> <p>run_interrupt_driven_config_hooks() - <code>sys/kern/subr_autoconf.c:64-127</code></p> <p>Called at <code>SI_SUB_INT_CONFIG_HOOKS</code>: - Runs each hook function - Waits for hooks to complete (with timeout warnings) - USB hack: waits extra 5 seconds for USB devices</p>"},{"location":"sys/kern/newbus/#root-bus","title":"Root Bus","text":"<p>The root bus is the top of the device tree:</p> <pre><code>static driver_t root_driver = {\n    \"root\",\n    root_methods,\n    1,  /* no softc */\n};\n\ndevice_t    root_bus;\ndevclass_t  root_devclass;\n</code></pre> <p>Defined in <code>sys/kern/subr_bus.c:3523-3560</code>.</p> <p>root_bus_configure() - <code>sys/kern/subr_bus.c:3562-3603</code></p> <p>Called during boot to: 1. Call <code>bus_generic_probe()</code> for root bus 2. Probe and attach children (typically nexus) 3. Wait for async attaches</p>"},{"location":"sys/kern/newbus/#devctl-device","title":"devctl Device","text":"<p>Character device <code>/dev/devctl</code> for userland notification:</p> <p>Events Sent: - <code>devadded()</code> - Device attached successfully - <code>devremoved()</code> - Device about to detach - <code>devnomatch()</code> - No driver found</p> <p>devctl_notify() - <code>sys/kern/subr_bus.c:529-559</code></p> <p>Standard notification format: <pre><code>!system=&lt;system&gt; subsystem=&lt;subsystem&gt; type=&lt;type&gt; [data]\n</code></pre></p> <p>See <code>sys/kern/subr_bus.c:250-721</code>.</p>"},{"location":"sys/kern/newbus/#sysctl-interface","title":"Sysctl Interface","text":"<p>hw.bus.info - Returns bus generation count hw.bus.devices - Returns device tree</p> <p><code>struct u_device</code> (<code>sys/sys/bus.h:87-100</code>) exported to userspace.</p> <p>See <code>sys/kern/subr_bus.c:3843-3907</code>.</p>"},{"location":"sys/kern/newbus/#driver-definition-macros","title":"Driver Definition Macros","text":"<p>DRIVER_MODULE - <code>sys/sys/bus.h:518-538</code></p> <pre><code>#define DRIVER_MODULE(name, busname, driver, devclass, evh, arg)    \\\n    DRIVER_MODULE_ORDERED(name, busname, driver, &amp;devclass, evh, arg,\\\n                          SI_ORDER_MIDDLE)\n</code></pre> <p>DEVMETHOD / DEVMETHOD_END - <code>sys/sys/bus.h:494-495</code></p> <pre><code>#define DEVMETHOD       KOBJMETHOD\n#define DEVMETHOD_END   KOBJMETHOD_END\n</code></pre>"},{"location":"sys/kern/newbus/#bus-space-access","title":"Bus Space Access","text":"<p>Shorthand macros for <code>bus_space_*</code> functions:</p> <pre><code>bus_read_1(r, o)    /* read 1 byte */\nbus_read_2(r, o)    /* read 2 bytes */\nbus_read_4(r, o)    /* read 4 bytes */\nbus_write_1(r, o, v)\nbus_write_2(r, o, v)\nbus_write_4(r, o, v)\nbus_read_region_N()\nbus_write_region_N()\nbus_set_region_N()\nbus_copy_region_N()\nbus_barrier()\n</code></pre> <p>See <code>sys/sys/bus.h:563-692</code>.</p>"},{"location":"sys/kern/newbus/#example-simple-pci-driver","title":"Example: Simple PCI Driver","text":"<pre><code>#include &lt;sys/param.h&gt;\n#include &lt;sys/kernel.h&gt;\n#include &lt;sys/module.h&gt;\n#include &lt;sys/bus.h&gt;\n#include &lt;bus/pci/pcivar.h&gt;\n\nstatic int\nmydev_probe(device_t dev)\n{\n    if (pci_get_vendor(dev) == 0x1234 &amp;&amp;\n        pci_get_device(dev) == 0x5678) {\n        device_set_desc(dev, \"My Device\");\n        return BUS_PROBE_DEFAULT;\n    }\n    return ENXIO;\n}\n\nstatic int\nmydev_attach(device_t dev)\n{\n    struct mydev_softc *sc = device_get_softc(dev);\n    int rid;\n\n    /* Allocate BAR0 */\n    rid = PCIR_BAR(0);\n    sc-&gt;mem_res = bus_alloc_resource_any(dev, SYS_RES_MEMORY,\n                                         &amp;rid, RF_ACTIVE);\n    if (sc-&gt;mem_res == NULL)\n        return ENXIO;\n\n    /* Allocate interrupt */\n    rid = 0;\n    sc-&gt;irq_res = bus_alloc_resource_any(dev, SYS_RES_IRQ,\n                                         &amp;rid, RF_ACTIVE | RF_SHAREABLE);\n    if (sc-&gt;irq_res == NULL) {\n        bus_release_resource(dev, SYS_RES_MEMORY, PCIR_BAR(0), sc-&gt;mem_res);\n        return ENXIO;\n    }\n\n    /* Setup interrupt handler */\n    bus_setup_intr(dev, sc-&gt;irq_res, INTR_MPSAFE,\n                   mydev_intr, sc, &amp;sc-&gt;irq_cookie, NULL);\n\n    return 0;\n}\n\nstatic int\nmydev_detach(device_t dev)\n{\n    struct mydev_softc *sc = device_get_softc(dev);\n\n    bus_teardown_intr(dev, sc-&gt;irq_res, sc-&gt;irq_cookie);\n    bus_release_resource(dev, SYS_RES_IRQ, 0, sc-&gt;irq_res);\n    bus_release_resource(dev, SYS_RES_MEMORY, PCIR_BAR(0), sc-&gt;mem_res);\n\n    return 0;\n}\n\nstatic device_method_t mydev_methods[] = {\n    DEVMETHOD(device_probe,     mydev_probe),\n    DEVMETHOD(device_attach,    mydev_attach),\n    DEVMETHOD(device_detach,    mydev_detach),\n    DEVMETHOD_END\n};\n\nstatic driver_t mydev_driver = {\n    \"mydev\",\n    mydev_methods,\n    sizeof(struct mydev_softc)\n};\n\nstatic devclass_t mydev_devclass;\n\nDRIVER_MODULE(mydev, pci, mydev_driver, mydev_devclass, NULL, NULL);\n</code></pre>"},{"location":"sys/kern/newbus/#cross-references","title":"Cross-References","text":"<ul> <li>Device Framework - Character/block device layer</li> <li>Resource Management - rman and DMA</li> <li>LWKT - Threading and serializers</li> <li>Synchronization - Locking primitives</li> </ul>"},{"location":"sys/kern/processes/","title":"Process and Thread Management","text":"<p>This document describes the complete lifecycle of processes and threads in DragonFly BSD, from creation through execution to termination and cleanup.</p>"},{"location":"sys/kern/processes/#overview","title":"Overview","text":"<p>DragonFly BSD has a three-level threading model:</p> <ol> <li>Processes (<code>struct proc</code>) - Traditional BSD process containers</li> <li>LWPs (Light Weight Processes, <code>struct lwp</code>) - Kernel-visible threads</li> <li>LWKT threads (<code>struct thread</code>) - Lightweight kernel threads</li> </ol> <p>User processes contain one or more LWPs, each with an associated LWKT thread. Pure kernel threads exist as LWKT threads without an associated process or LWP.</p> <p>Key source files: - <code>kern_proc.c</code> - Process table management, PID allocation, lookups - <code>kern_fork.c</code> - Process/thread creation (fork, rfork, lwp_create) - <code>kern_exec.c</code> - Program execution (execve) - <code>kern_exit.c</code> - Process termination and cleanup - <code>kern_kthread.c</code> - Kernel thread creation</p>"},{"location":"sys/kern/processes/#process-table-and-pid-management","title":"Process Table and PID Management","text":""},{"location":"sys/kern/processes/#data-structures","title":"Data Structures","text":"<p>The kernel maintains several global lists for process tracking:</p> <pre><code>// Active processes - hash table with 256 buckets\n#define ALLPROC_HSIZE 256\nstatic struct procglob allproc_hash[ALLPROC_HSIZE];\n\n// Zombie processes - single list\nstatic struct procglob zombproc_list;\n\n// PID domain tracking (prevents rapid reuse)\n#define PIDDOM_DELAY 10\nstatic pid_t pid_doms[PIDDOM_DELAY];\n</code></pre> <p>Each <code>procglob</code> entry contains: - <code>allproc</code> - List head for active or zombie processes - <code>lock</code> - Spinlock protecting the list</p>"},{"location":"sys/kern/processes/#pid-allocation","title":"PID Allocation","text":"<p>PID allocation (<code>kern_proc.c:proc_getnewpid()</code>) includes security features to prevent predictability:</p> <ol> <li>Random offset - Start PID counter at random value modulo PID_MAX</li> <li>Anti-reuse - Track last 10 PIDs in <code>pid_doms[]</code>, prevent reuse for ~10 seconds</li> <li>Hash bucket optimization - Increment by ALLPROC_HSIZE (256) to keep process in same hash bucket for better cache locality</li> </ol> <pre><code>newpid += ALLPROC_HSIZE;  // Stay in same hash bucket\nif (newpid &gt;= PID_MAX) {\n    newpid = newpid % ALLPROC_HSIZE;\n    newpid += ALLPROC_HSIZE;\n}\n</code></pre> <p>The algorithm checks: - PID not already in use (<code>pfind()</code>) - PID not recently freed (within PIDDOM_DELAY seconds) - PID not in session/process group use</p>"},{"location":"sys/kern/processes/#process-reference-counting","title":"Process Reference Counting","text":"<p>Processes use atomic reference counting via <code>p_lock</code> with multiple flag bits:</p> <pre><code>#define PLOCK_MASK     0x1fffffff  // Reference count\n#define PLOCK_WAITING  0x20000000  // Someone waiting for lock\n#define PLOCK_ZOMB     0x40000000  // Zombie being reaped\n#define PLOCK_WAITRES  0x80000000  // Waiting for resources\n</code></pre> <p>Reference count macros: - <code>PHOLD(p)</code> / <code>PRELE(p)</code> - General purpose reference counting - <code>PHOLDZOMB(p)</code> / <code>PRELEZOMB(p)</code> - Exclusive zombie reaping (sets PLOCK_ZOMB) - <code>PWAITRES(p)</code> / <code>PWAKEUP(p)</code> - Resource wait coordination</p> <p>The reference count prevents a process structure from being freed while in use. Zombie processes remain in the table with p_stat=SZOMB until the parent reaps them via wait().</p>"},{"location":"sys/kern/processes/#process-lookup-functions","title":"Process Lookup Functions","text":"<pre><code>struct proc *pfind(pid_t pid);           // Find active process\nstruct proc *zpfind(pid_t pid);          // Find zombie process\nstruct proc *pgfind(pid_t pgid);         // Find process group leader\nstruct lwp *lwpfind(struct proc *p, lwpid_t lwpid);  // Find LWP by ID\n</code></pre> <p>All lookup functions: - Acquire the appropriate hash bucket spinlock - Return process/LWP with reference count held (PHOLD) - Caller must release with PRELE when done</p>"},{"location":"sys/kern/processes/#process-iteration","title":"Process Iteration","text":"<pre><code>int allproc_scan(int (*callback)(struct proc *, void *), void *data, int flags);\nint zombproc_scan(int (*callback)(struct proc *, void *), void *data, int flags);\n</code></pre> <p>Flags control behavior: - <code>PFSCAN_NOBRK</code> - Don't break on non-zero callback return - <code>PFSCAN_LOCKED</code> - Keep process locked during callback (PHOLD)</p> <p>These functions are used by system utilities (ps, top) and kernel subsystems that need to enumerate all processes.</p>"},{"location":"sys/kern/processes/#process-creation-fork","title":"Process Creation: fork()","text":""},{"location":"sys/kern/processes/#system-call-entry-points","title":"System Call Entry Points","text":"<pre><code>int sys_fork(struct sysmsg *);          // Traditional fork\nint sys_vfork(struct sysmsg *);         // vfork (shared memory)\nint sys_rfork(struct sysmsg *, struct rfork_args *);  // Extended fork\n</code></pre> <p>All fork variants funnel through <code>fork1()</code> with different flag combinations:</p> <p>RFORK flags: - <code>RFPROC</code> - Create new process (required) - <code>RFMEM</code> - Share address space (for vfork or threads) - <code>RFFDG</code> - Share file descriptor table - <code>RFCFDG</code> - Close file descriptors (exclusive with RFFDG) - <code>RFSIGSHARE</code> - Share signal handlers - <code>RFTHREAD</code> - Create thread (LWP) in current process - <code>RFPPWAIT</code> - Parent sleeps until child releases vmspace (vfork) - <code>RFENVG</code> - Create new environment group - <code>RFCENVG</code> - Close environment variables</p>"},{"location":"sys/kern/processes/#fork1-overview","title":"fork1() Overview","text":"<p>The main fork implementation (<code>kern_fork.c:fork1()</code>) performs these steps:</p> <ol> <li>Preparation</li> <li>Validate flags (RFPROC required, mutual exclusions)</li> <li>Check resource limits (RLIMIT_NPROC)</li> <li> <p>Account for new process in uid structure</p> </li> <li> <p>Process structure allocation</p> </li> <li>Allocate <code>struct proc</code> via <code>kmalloc()</code></li> <li>Initialize reference count, locks, lists</li> <li>Insert into <code>allproc</code> hash table</li> <li> <p>Allocate PID via <code>proc_getnewpid()</code></p> </li> <li> <p>Process setup</p> </li> <li>Copy or share credentials based on flags</li> <li>Copy or share file descriptor table</li> <li>Copy or share signal handler table</li> <li> <p>Set process state to SIDL (intermediate)</p> </li> <li> <p>LWP creation (two-phase)</p> </li> <li><code>lwp_fork1()</code> - Allocate LWP structure, prepare for vm_fork</li> <li><code>vm_fork()</code> - Handle address space (copy-on-write or share)</li> <li> <p><code>lwp_fork2()</code> - Finalize LWP, insert into process</p> </li> <li> <p>Finalization</p> </li> <li>Call <code>at_fork()</code> registered callbacks</li> <li>Set up parent/child relationship</li> <li> <p>For RFPPWAIT (vfork): parent sleeps until child execs or exits</p> </li> <li> <p>Activation</p> </li> <li><code>start_forked_proc()</code> transitions child from SIDL to SACTIVE</li> <li>Child LWP becomes schedulable</li> </ol>"},{"location":"sys/kern/processes/#two-phase-lwp-creation","title":"Two-Phase LWP Creation","text":"<p>The fork process splits LWP creation around <code>vm_fork()</code>:</p> <p>Phase 1: lwp_fork1() (<code>kern_fork.c:lwp_fork1()</code>) - Allocates <code>struct lwp</code> - Allocates kernel stack via <code>lwkt_alloc_thread()</code> - Initializes basic fields (proc pointer, flags) - Does not insert into p_lwp_tree yet</p> <p>VM Fork: vm_fork() (in vm subsystem) - Sets up address space for child process - Creates COW (copy-on-write) mappings for non-shared pages - For RFMEM: shares vmspace directly (vfork, threads) - Handles special mappings (shared memory, mmap)</p> <p>Phase 2: lwp_fork2() (<code>kern_fork.c:lwp_fork2()</code>) - Completes machine-dependent setup via <code>cpu_lwp_fork()</code> - Sets initial register state (return value, stack pointer) - Inserts LWP into <code>p_lwp_tree</code> (RB tree indexed by lwpid) - Sets LWP state to LSSTOP (not running yet)</p> <p>This two-phase design ensures the LWP is not visible to the rest of the system (via p_lwp_tree) until the address space is properly set up.</p>"},{"location":"sys/kern/processes/#starting-the-forked-process","title":"Starting the Forked Process","text":"<pre><code>void start_forked_proc(struct lwp *parent, struct proc *child)\n</code></pre> <p>This function transitions the child process to runnable state:</p> <ol> <li>Acquire process token</li> <li>Transition from SIDL to SACTIVE state</li> <li>Transition LWP from LSSTOP to LSRUN</li> <li>Schedule the child's thread via <code>lwkt_schedule()</code></li> <li>Child will return from fork with retval=0 (vs parent getting child PID)</li> </ol>"},{"location":"sys/kern/processes/#vfork-coordination","title":"vfork() Coordination","text":"<p>When <code>RFPPWAIT</code> is set (vfork):</p> <ol> <li>Parent process blocks in <code>fork1()</code> after starting child</li> <li>Parent waits on <code>p_ppwait_cv</code> condition variable</li> <li>Child signals parent when:</li> <li>Child calls exec (in <code>kern_exec.c:exec_new_vmspace()</code>)</li> <li>Child exits (in <code>kern_exit.c:exit1()</code>)</li> <li>Parent wakes up and continues</li> </ol> <p>This ensures the parent doesn't modify shared memory while the child is using it.</p>"},{"location":"sys/kern/processes/#thread-creation-lwp_create","title":"Thread Creation: lwp_create()","text":"<p>User threads (pthreads) are created via:</p> <pre><code>int sys_lwp_create(struct lwp_params *params);\n</code></pre> <p>This creates a new LWP within the current process:</p> <ol> <li>Calls <code>lwp_create1()</code> to allocate LWP structure</li> <li>Does not call <code>vm_fork()</code> - address space is shared</li> <li>Sets up new stack region within shared address space</li> <li>Sets entry point to user-specified function</li> <li>New thread runs in parallel with existing LWPs</li> </ol> <p>LWP creation is much lighter than process creation since: - No new process structure - No PID allocation - No credential/fd/signal copying - No COW setup (memory is shared)</p>"},{"location":"sys/kern/processes/#process-execution-execve","title":"Process Execution: execve()","text":""},{"location":"sys/kern/processes/#system-call-entry","title":"System Call Entry","text":"<pre><code>int sys_execve(struct sysmsg *, struct execve_args *);\n</code></pre> <p>Arguments: - <code>fname</code> - Path to executable - <code>argv</code> - Argument vector (NULL-terminated) - <code>envv</code> - Environment vector (NULL-terminated)</p>"},{"location":"sys/kern/processes/#execution-overview","title":"Execution Overview","text":"<p>The <code>kern_execve()</code> function (<code>kern_exec.c</code>) replaces the current process's address space and execution context with a new program:</p> <ol> <li>Path resolution - Lookup executable via <code>nlookup()</code></li> <li>Permission checks - Verify execute permission, handle setuid/setgid</li> <li>Image identification - Try each image activator to identify format</li> <li>Point of no return - Destroy old address space</li> <li>New address space setup - Load program, build stack</li> <li>State reset - Close files, reset signals, update credentials</li> <li>Entry - Set PC to program entry point and return to usermode</li> </ol>"},{"location":"sys/kern/processes/#preventing-concurrent-execution","title":"Preventing Concurrent Execution","text":"<p>Multi-threaded processes must not exec concurrently:</p> <pre><code>if (p-&gt;p_flags &amp; P_INEXEC) {\n    return EBUSY;  // Another thread is already in exec\n}\np-&gt;p_flags |= P_INEXEC;\n</code></pre> <p>The <code>P_INEXEC</code> flag remains set until exec completes or fails.</p>"},{"location":"sys/kern/processes/#image-activators","title":"Image Activators","text":"<p>The kernel tries each registered image activator in turn:</p> <pre><code>const struct execsw execsw[] = {\n    { exec_elf_imgact, \"ELF\" },\n    { exec_resident_imgact, \"resident\" },\n    { exec_script_imgact, \"#!\" },\n    { NULL, NULL }\n};\n</code></pre> <p>Each activator's function signature:</p> <pre><code>int (*ex_imgact)(struct image_params *imgp);\n</code></pre> <p>Image activator process: 1. Read first page of executable (4KB) 2. Call each activator's <code>ex_imgact()</code> function 3. Activator inspects headers (ELF magic, #! shebang, etc.) 4. Return <code>-1</code> if not recognized, <code>0</code> if accepted, error code if failed 5. First activator to accept the image handles execution</p>"},{"location":"sys/kern/processes/#script-execution","title":"Script Execution (#!)","text":"<p>The script activator (<code>exec_script_imgact()</code>) handles interpreter files:</p> <pre><code>#!/usr/bin/interpreter [args]\nprogram content...\n</code></pre> <p>When detected: 1. Extract interpreter path and optional arguments from first line 2. Construct new argv: <code>[interpreter, args..., scriptname, original_argv...]</code> 3. Recursively call <code>kern_execve()</code> with interpreter as executable 4. Recursion depth limited to prevent infinite loops</p>"},{"location":"sys/kern/processes/#point-of-no-return","title":"Point of No Return","text":"<pre><code>error = exec_new_vmspace(imgp, stack);\n</code></pre> <p>This function (<code>kern_exec.c:exec_new_vmspace()</code>) destroys the old address space:</p> <ol> <li>Create new vmspace structure</li> <li>If process was vfork'd (P_PPWAIT set):</li> <li>Signal parent to wake up via <code>wakeup(p-&gt;p_pptr)</code></li> <li>Drop shared vmspace reference</li> <li>Destroy old vmspace</li> <li>Attach new vmspace to process</li> <li>Cannot fail after this point - process has no valid address space to return to</li> </ol>"},{"location":"sys/kern/processes/#building-the-new-stack","title":"Building the New Stack","text":"<pre><code>error = exec_copyout_strings(imgp, &amp;stack_base);\n</code></pre> <p>This function constructs the user stack in this order (growing downward):</p> <pre><code>High addresses\n+------------------+\n| envp strings     |\n+------------------+\n| argv strings     |\n+------------------+\n| exec path        |\n+------------------+\n| padding          |  (for alignment)\n+------------------+\n| auxv vector      |  (auxiliary vector, ELF only)\n+------------------+\n| NULL             |\n| envp[n-1]        |  (pointers to env strings)\n| ...              |\n| envp[0]          |\n+------------------+\n| NULL             |\n| argv[n-1]        |  (pointers to arg strings)\n| ...              |\n| argv[0]          |\n+------------------+\n| argc             |  (argument count)\n+------------------+  &lt;- stack pointer at program entry\nLow addresses\n</code></pre> <p>Stack gap randomization: - Controlled by <code>exec.stackgap_random</code> sysctl - Randomly subtracts 0 to stackgap_random bytes from stack pointer - Helps prevent return-to-libc attacks</p> <p>Auxiliary vector (auxv): ELF programs receive metadata through auxv entries: - <code>AT_PHDR</code> - Address of program header table - <code>AT_PHENT</code> - Size of program header entry - <code>AT_PHNUM</code> - Number of program headers - <code>AT_PAGESZ</code> - System page size - <code>AT_BASE</code> - Interpreter base address (for dynamic executables) - <code>AT_ENTRY</code> - Program entry point - <code>AT_EXECPATH</code> - Full path to executable</p>"},{"location":"sys/kern/processes/#credential-handling-setuidsetgid","title":"Credential Handling (setuid/setgid)","text":"<p>If the executable has setuid or setgid bits:</p> <pre><code>if (attr.va_mode &amp; (S_ISUID | S_ISGID)) {\n    p-&gt;p_flags |= P_SUGID;  // Mark process as tainted\n    // Update credentials\n    if (attr.va_mode &amp; S_ISUID)\n        change_euid(attr.va_uid);\n    if (attr.va_mode &amp; S_ISGID)\n        change_egid(attr.va_gid);\n}\n</code></pre> <p>P_SUGID security restrictions: - Prevents ptrace() attachment - Prevents core dumps to user directories - Restricts /proc access - Cleared on subsequent exec of non-setuid binary</p>"},{"location":"sys/kern/processes/#file-descriptor-cleanup","title":"File Descriptor Cleanup","text":"<p>During exec, the kernel closes file descriptors marked close-on-exec:</p> <pre><code>fdcloseexec(p);  // Close all FDs with FD_CLOEXEC set\n</code></pre> <p>This is commonly used for: - Pipe file descriptors in shell pipelines - Internal file descriptors that shouldn't be inherited - Library handles that need to be reopened</p>"},{"location":"sys/kern/processes/#signal-handler-reset","title":"Signal Handler Reset","text":"<pre><code>execsigs(p);\n</code></pre> <p>This function resets all signal dispositions: - Signals set to SIG_IGN remain ignored - Signals set to SIG_DFL remain default - Caught signals (custom handlers) reset to SIG_DFL</p> <p>Since the old address space is gone, signal handler function pointers are no longer valid.</p>"},{"location":"sys/kern/processes/#entry-to-new-program","title":"Entry to New Program","text":"<p>After all setup completes:</p> <ol> <li><code>exec_setregs()</code> sets up initial CPU state:</li> <li>Program counter (PC) \u2192 entry point address</li> <li>Stack pointer (SP) \u2192 top of prepared stack</li> <li>Argument registers \u2192 argc, argv (architecture-dependent)</li> <li>Exec system call returns to user mode</li> <li>CPU state causes execution to begin at program entry point</li> </ol> <p>For dynamically linked ELF programs, entry point is actually the dynamic linker (ld-elf.so), which: - Loads shared libraries - Performs relocations - Eventually transfers control to program's actual <code>_start()</code></p>"},{"location":"sys/kern/processes/#process-termination-exit","title":"Process Termination: exit()","text":""},{"location":"sys/kern/processes/#system-call-entry_1","title":"System Call Entry","text":"<pre><code>int sys_exit(struct sysmsg *, struct exit_args *);\nint sys_exit_group(struct sysmsg *, struct exit_group_args *);  // Linux compat\n</code></pre> <p>Both funnel through <code>exit1()</code> with a status code.</p>"},{"location":"sys/kern/processes/#exit1-overview","title":"exit1() Overview","text":"<p>The main exit implementation (<code>kern_exit.c:exit1()</code>) performs these steps:</p> <ol> <li>Exit coordination - Set P_WEXIT flag to prevent concurrent exits</li> <li>Stop other threads - Call <code>killalllwps()</code> to terminate all other LWPs</li> <li>Accounting - Record CPU time and resource usage</li> <li>Cleanup - Close files, release resources, detach from process groups</li> <li>Zombie transition - Move from allproc to zombproc list</li> <li>Parent notification - Send SIGCHLD to parent</li> <li>Thread termination - Current LWP calls <code>lwp_exit()</code> and never returns</li> </ol>"},{"location":"sys/kern/processes/#preventing-concurrent-exit","title":"Preventing Concurrent Exit","text":"<pre><code>if (p-&gt;p_flags &amp; P_WEXIT) {\n    lwkt_reltoken(&amp;p-&gt;p_token);\n    return;  // Already exiting\n}\np-&gt;p_flags |= P_WEXIT;\n</code></pre> <p>Only the first thread to set <code>P_WEXIT</code> performs the full exit sequence. Other threads in <code>exit1()</code> simply return and get cleaned up by <code>killalllwps()</code>.</p>"},{"location":"sys/kern/processes/#killing-all-lwps","title":"Killing All LWPs","text":"<pre><code>killalllwps(int signo);\n</code></pre> <p>This function (<code>kern_exit.c</code>) terminates all other LWPs in the process:</p> <ol> <li>Acquire proc token</li> <li>Iterate through <code>p_lwp_tree</code> (all LWPs in process)</li> <li>For each other LWP:</li> <li>Set LWP state to LSZOMB</li> <li>Post signal if specified</li> <li>Issue <code>lwkt_deschedule()</code> to remove from scheduler</li> <li>Wait for all LWPs to finish deschedule</li> <li>Reap zombie LWPs</li> </ol> <p>LWP reaper mechanism: - Dead LWPs are added to <code>deadlwp_list</code> - <code>deadlwp_task</code> taskqueue entry processes the list - Each dead LWP's kernel stack is freed - LWP structure itself is freed</p> <p>This asynchronous reaping avoids freeing the kernel stack that might still be in use.</p>"},{"location":"sys/kern/processes/#process-cleanup-sequence","title":"Process Cleanup Sequence","text":"<p>After killing all LWPs, <code>exit1()</code> performs:</p> <ol> <li>Session/Process Group Cleanup</li> <li>If session leader: terminate controlling terminal</li> <li>If process group leader: send SIGHUP to foreground processes</li> <li> <p>Remove from process group</p> </li> <li> <p>File Descriptor Cleanup <pre><code>fdfree(p, td);  // Close all open files\n</code></pre></p> </li> <li>Closes all file descriptors</li> <li>Releases file descriptor table</li> <li> <p>Decrements reference counts on file structures</p> </li> <li> <p>Virtual Memory Cleanup <pre><code>if (!--vmspace-&gt;vm_refcnt) {\n    vmspace_dtor(vmspace);  // Last reference, free vmspace\n}\n</code></pre></p> </li> <li>Decrements vmspace reference count</li> <li> <p>If last reference: unmaps all regions, frees page tables</p> </li> <li> <p>IPC Cleanup</p> </li> <li>Remove from System V semaphore undo list</li> <li> <p>Detach from shared memory segments</p> </li> <li> <p>Credential Cleanup <pre><code>crfree(p-&gt;p_ucred);  // Release credential structure\n</code></pre></p> </li> <li> <p>Timer Cleanup</p> </li> <li>Stop ITIMER_REAL, ITIMER_VIRTUAL, ITIMER_PROF timers</li> <li>Remove from timer queues</li> </ol>"},{"location":"sys/kern/processes/#zombie-transition","title":"Zombie Transition","text":"<pre><code>proc_move_allproc_zombie(struct proc *p);\n</code></pre> <p>This function (<code>kern_proc.c</code>) atomically moves the process from active to zombie state:</p> <ol> <li>Remove from <code>allproc_hash[bucket]</code></li> <li>Add to <code>zombproc_list</code></li> <li>Set <code>p-&gt;p_stat = SZOMB</code></li> <li>Leave <code>p_nthreads</code> at 1 (decrement happens after parent reaps)</li> </ol> <p>Zombie state characteristics: - Process structure remains allocated - PID remains allocated (in <code>pid_doms[]</code> anti-reuse list) - Exit status preserved in <code>p-&gt;p_xstat</code> - All other resources freed</p>"},{"location":"sys/kern/processes/#parent-notification","title":"Parent Notification","text":"<p>After transition to zombie:</p> <ol> <li>Increment parent's <code>p_waitgen</code> counter (wake optimization)</li> <li>Send SIGCHLD to parent via <code>ksignal(p-&gt;p_pptr, SIGCHLD)</code></li> <li>Wakeup any threads in <code>wait()</code> via <code>wakeup(p-&gt;p_pptr)</code></li> </ol> <p>The <code>p_waitgen</code> counter allows wait() to quickly detect if any child has changed state without scanning the entire child list.</p>"},{"location":"sys/kern/processes/#orphan-handling","title":"Orphan Handling","text":"<p>If parent has already exited, child is reparented:</p> <pre><code>proc_reparent(struct proc *child, struct proc *parent);\n</code></pre> <p>Reparenting rules: 1. If parent is init (PID 1): child becomes init's child 2. If process has a registered reaper: child moves to reaper 3. Otherwise: child becomes init's child</p> <p>The reaper mechanism allows creating \"sub-init\" processes for containers or process supervision.</p>"},{"location":"sys/kern/processes/#thread-exit","title":"Thread Exit","text":"<p>Finally, the exiting thread calls:</p> <pre><code>lwp_exit(int masterexit, void (*exitfunc)(void *), void *exitarg);\n</code></pre> <p>This function: 1. Sets <code>lp-&gt;lwp_stat = LSZOMB</code> 2. Calls exit function if provided (for cleanup) 3. Calls <code>cpu_lwp_exit()</code> to release machine-dependent resources 4. Calls <code>lwkt_exit()</code> which:    - Deschedules thread    - Marks stack for deferred free    - Switches to new thread    - Never returns</p>"},{"location":"sys/kern/processes/#parent-wait-and-reaping","title":"Parent Wait and Reaping","text":"<p>Parent processes retrieve exit status via wait():</p> <pre><code>int sys_wait4(struct sysmsg *, struct wait4_args *);\n</code></pre> <p>The <code>kern_wait()</code> function (<code>kern_exit.c</code>) performs:</p> <ol> <li> <p>Wait loop optimization <pre><code>int waitgen = p-&gt;p_waitgen;\nwhile (/* no matching child */) {\n    tsleep_interlock(&amp;waitgen, PCATCH);\n    if (waitgen == p-&gt;p_waitgen)\n        tsleep(p, PCATCH | PINTERLOCKED, \"wait\", 0);\n    waitgen = p-&gt;p_waitgen;\n}\n</code></pre>    The waitgen counter avoids spurious wakeups - only sleep if counter hasn't changed.</p> </li> <li> <p>Child scan</p> </li> <li>Iterate through <code>p-&gt;p_children</code> list</li> <li>Match on PID (if specified) or any child (PID -1)</li> <li> <p>Match on process group (if negative PID)</p> </li> <li> <p>Zombie reaping <pre><code>PHOLDZOMB(child);  // Exclusive access, sets PLOCK_ZOMB\n</code></pre></p> </li> <li>Acquire exclusive zombie lock (prevents concurrent wait by other threads)</li> <li>Copy exit status from <code>p-&gt;p_xstat</code></li> <li>Copy resource usage from <code>p-&gt;p_ru</code></li> <li>Remove from parent's <code>p_children</code> list</li> <li> <p>Call <code>proc_finish(child)</code> to free process structure</p> </li> <li> <p>Process finish <pre><code>proc_finish(struct proc *p);\n</code></pre></p> </li> <li>Remove from zombie list</li> <li>Decrement PID domain reference in <code>pid_doms[]</code></li> <li>Free process structure</li> <li>PID becomes available for reuse after PIDDOM_DELAY</li> </ol>"},{"location":"sys/kern/processes/#wait-options","title":"Wait Options","text":"<p>The wait4() system call supports various options:</p> <ul> <li><code>WNOHANG</code> - Return immediately if no child has exited</li> <li><code>WUNTRACED</code> - Also return for stopped children (job control)</li> <li><code>WCONTINUED</code> - Also return for continued children (SIGCONT)</li> <li><code>WLINUXCLONE</code> - Linux compatibility for thread waiting</li> </ul> <p>Without WNOHANG, wait() sleeps until a child changes state or a signal arrives.</p>"},{"location":"sys/kern/processes/#kernel-threads","title":"Kernel Threads","text":""},{"location":"sys/kern/processes/#overview_1","title":"Overview","text":"<p>Kernel threads are lightweight threads that run entirely in kernel mode without an associated user process or LWP. They are used for:</p> <ul> <li>Asynchronous I/O completion</li> <li>Device driver background tasks</li> <li>Network stack processing</li> <li>Virtual memory pageout daemon</li> <li>System maintenance tasks</li> </ul>"},{"location":"sys/kern/processes/#creating-kernel-threads","title":"Creating Kernel Threads","text":"<pre><code>int kthread_create(void (*func)(void *), void *arg,\n                   struct thread **tdp, const char *fmt, ...);\n</code></pre> <p>Parameters: - <code>func</code> - Thread entry point function - <code>arg</code> - Argument passed to function - <code>tdp</code> - Returns pointer to created thread (can be NULL) - <code>fmt</code> - Printf-style format for thread name (appears in ps)</p> <p>Variants:</p> <pre><code>// Create but don't schedule immediately\nint kthread_alloc(void (*func)(void *), void *arg,\n                  struct thread **tdp, const char *fmt, ...);\n\n// Pin to specific CPU\nint kthread_create_cpu(void (*func)(void *), void *arg,\n                       struct thread **tdp, int cpu, const char *fmt, ...);\n</code></pre>"},{"location":"sys/kern/processes/#thread-creation-process","title":"Thread Creation Process","text":"<p>Internal function <code>_kthread_create()</code> (<code>kern_kthread.c</code>):</p> <ol> <li>Allocate thread structure <pre><code>td = lwkt_alloc_thread(NULL, LWKT_THREAD_STACK, cpu, flags);\n</code></pre></li> <li>Allocates <code>struct thread</code></li> <li>Allocates kernel stack (typically 16KB)</li> <li> <p>CPU parameter pins thread to specific CPU, or -1 for any CPU</p> </li> <li> <p>Set up execution context <pre><code>cpu_set_thread_handler(td, kthread_exit, func, arg);\n</code></pre></p> </li> <li>Sets stack pointer to top of kernel stack</li> <li>Sets up return chain: <code>func</code> returns to <code>kthread_exit</code></li> <li> <p>Saves argument for function</p> </li> <li> <p>Initialize thread metadata</p> </li> <li>Copy name to <code>td-&gt;td_comm</code> (visible in ps, top)</li> <li>Share credentials with proc0 (kernel process)</li> <li> <p>Set <code>td-&gt;td_proc = NULL</code> to mark as pure kernel thread</p> </li> <li> <p>Schedule thread <pre><code>lwkt_schedule(td);\n</code></pre></p> </li> <li>Only if <code>schedule_now</code> parameter is true</li> <li>Thread becomes runnable on target CPU</li> <li>Scheduler will run thread when appropriate</li> </ol>"},{"location":"sys/kern/processes/#kernel-thread-lifecycle","title":"Kernel Thread Lifecycle","text":"<p>Entry: - Thread starts executing at <code>func</code> - Runs at elevated privilege (kernel mode) - Has full access to kernel memory and data structures</p> <p>Execution: - Thread can sleep, waiting for events - Thread can acquire locks and access shared data - Thread should check for termination requests</p> <p>Exit: - Thread returns from <code>func</code>, which chains to <code>kthread_exit()</code> - <code>kthread_exit()</code> calls <code>lwkt_exit()</code>:   - Marks thread as exiting   - Deschedules from runqueue   - Marks stack for deferred free   - Switches to another thread   - Never returns</p>"},{"location":"sys/kern/processes/#kernel-process-creation","title":"Kernel Process Creation","text":"<p>The <code>kproc_start()</code> function creates kernel threads via SYSINIT:</p> <pre><code>struct kproc_desc {\n    char *arg0;                        // Thread name\n    void (*func)(void);                // Entry point\n    struct thread **global_threadpp;   // Where to store thread pointer\n};\n\nvoid kproc_start(const void *udata);\n</code></pre> <p>Usage:</p> <pre><code>static struct thread *mythread;\n\nstatic struct kproc_desc my_kproc = {\n    \"mythread\",\n    my_thread_func,\n    &amp;mythread\n};\nSYSINIT(my_init, SI_SUB_KTHREAD_IDLE, SI_ORDER_ANY, kproc_start, &amp;my_kproc);\n</code></pre> <p>This creates the kernel thread during system initialization, after core kernel threads are running.</p>"},{"location":"sys/kern/processes/#kernel-thread-suspension","title":"Kernel Thread Suspension","text":"<p>Kernel threads can voluntarily suspend:</p> <pre><code>int suspend_kproc(struct thread *td, int timo);\nvoid kproc_suspend_loop(void);\n</code></pre> <p>Suspend mechanism:</p> <ol> <li>External code calls <code>suspend_kproc(td, timeout)</code></li> <li>Sets <code>TDF_MP_STOPREQ</code> flag on target thread</li> <li>Wakes target thread (if sleeping)</li> <li>Waits for thread to acknowledge suspension</li> </ol> <p>Thread cooperation:</p> <p>The kernel thread periodically calls: <pre><code>kproc_suspend_loop();  // Check if someone wants us to suspend\n</code></pre></p> <p>If <code>TDF_MP_STOPREQ</code> is set: - Clear request flag - Sleep until <code>TDF_MP_WAKEREQ</code> is set - Clear wake flag and continue</p> <p>This allows kernel threads to be paused for maintenance operations or shutdown.</p>"},{"location":"sys/kern/processes/#process-states-and-transitions","title":"Process States and Transitions","text":""},{"location":"sys/kern/processes/#process-states-p_stat","title":"Process States (p_stat)","text":"<pre><code>#define SIDL    1   // Process being created\n#define SACTIVE 2   // Process is active\n#define SSTOP   3   // Process stopped (job control)\n#define SZOMB   4   // Process is zombie (awaiting reaping)\n</code></pre> <p>State transitions:</p> <pre><code>     fork1()          start_forked_proc()\nNULL --------&gt; SIDL ----------------------&gt; SACTIVE\n                                               |\n                                               | exit1()\n                                               v\n                                            SZOMB -----&gt; freed\n                                               ^      wait4()\n                                               |\n                                           SSTOP\n                                        (via signal)\n</code></pre>"},{"location":"sys/kern/processes/#lwp-states-lwp_stat","title":"LWP States (lwp_stat)","text":"<pre><code>#define LSRUN    1  // Runnable (on or waiting for CPU)\n#define LSSTOP   2  // Stopped (job control, debugging)\n#define LSSLEEP  3  // Sleeping (waiting for event)\n#define LSZOMB   4  // Zombie (terminated but not reaped)\n#define LSSUSPENDED 5  // Suspended (special conditions)\n</code></pre> <p>Typical LWP lifecycle:</p> <pre><code>     lwp_fork2()      scheduler        sleep event\nNULL -----------&gt; LSSTOP --------&gt; LSRUN -----------&gt; LSSLEEP\n                           ^                             |\n                           |        wakeup               |\n                           +-----------------------------+\n\n                  lwp_exit()\n              ---------------&gt; LSZOMB -----&gt; freed\n                                        (deferred)\n</code></pre>"},{"location":"sys/kern/processes/#thread-states-td_flags","title":"Thread States (td_flags)","text":"<p>LWKT threads have numerous flags in <code>td_flags</code>:</p> <pre><code>#define TDF_RUNNING      0x0001  // Thread on CPU\n#define TDF_RUNQ         0x0002  // Thread on runqueue\n#define TDF_TSLEEP       0x0004  // Thread in tsleep\n#define TDF_EXITING      0x0010  // Thread exiting\n#define TDF_SINTR        0x0020  // Sleep is interruptible\n#define TDF_TIMEOUT      0x0040  // Timeout in progress\n// ... many more\n</code></pre> <p>Threads transition between runqueue, running, sleeping states based on scheduler and synchronization events.</p>"},{"location":"sys/kern/processes/#process-relationships","title":"Process Relationships","text":""},{"location":"sys/kern/processes/#parent-child-relationships","title":"Parent-Child Relationships","text":"<p>Each process maintains:</p> <pre><code>struct proc {\n    struct proc *p_pptr;           // Parent process\n    struct proclist p_children;    // List of child processes\n    struct sibling_entry p_sibling; // Sibling list entry\n    // ...\n};\n</code></pre> <p>Invariants: - Every process except proc0 has a parent - A process's children are on its <code>p_children</code> list - Siblings are linked through <code>p_sibling</code></p>"},{"location":"sys/kern/processes/#process-groups-and-sessions","title":"Process Groups and Sessions","text":"<pre><code>struct proc {\n    struct pgrp *p_pgrp;          // Process group\n    pid_t p_pgid;                 // Process group ID\n    // ...\n};\n\nstruct pgrp {\n    struct proclist pg_members;    // Members of this group\n    struct session *pg_session;    // Session containing this group\n    pid_t pg_id;                   // Process group ID\n};\n\nstruct session {\n    struct pgrp_list s_groups;     // Groups in this session\n    struct vnode *s_ttyvp;         // Controlling terminal\n    pid_t s_sid;                   // Session ID\n};\n</code></pre> <p>Hierarchy:</p> <pre><code>Session\n  |\n  +-- Process Group 1\n  |     |\n  |     +-- Process A (leader)\n  |     +-- Process B\n  |     +-- Process C\n  |\n  +-- Process Group 2\n        |\n        +-- Process D (leader)\n        +-- Process E\n</code></pre> <p>Purpose: - Sessions - Isolate groups of related processes (typically one per login) - Process Groups - Job control (foreground/background jobs in shell) - Controlling Terminal - Associated with session, receives signals (SIGINT, SIGQUIT)</p> <p>Process group leaders (PID == PGID) and session leaders (PID == SID) have special responsibilities: - Session leader death causes SIGHUP to all processes in session - Process group leader death can cause controlling terminal loss</p>"},{"location":"sys/kern/processes/#reaper-hierarchy","title":"Reaper Hierarchy","text":"<pre><code>struct proc {\n    struct sysreaper *p_reaper;    // Our reaper (or NULL)\n    // ...\n};\n\nstruct sysreaper {\n    struct proc *p;                // Reaper process\n    int refs;                      // Reference count\n    struct sysreaper *parent;      // Parent reaper\n};\n</code></pre> <p>Reapers provide a mechanism for process supervision:</p> <ul> <li>When a process exits, orphaned children are reparented to the reaper</li> <li>If no reaper, children go to init (PID 1)</li> <li>Allows containers or supervision trees without requiring PID 1</li> </ul> <p>Example hierarchy:</p> <pre><code>init (PID 1)\n  |\n  +-- reaper_daemon (reaper for container)\n        |\n        +-- container_process_1\n        +-- container_process_2\n              |\n              +-- child_process\n</code></pre> <p>If <code>container_process_2</code> exits, <code>child_process</code> is reparented to <code>reaper_daemon</code>, not init.</p>"},{"location":"sys/kern/processes/#process-trees-and-debugging","title":"Process Trees and Debugging","text":""},{"location":"sys/kern/processes/#proc-filesystem","title":"/proc Filesystem","text":"<p>The process filesystem exposes process information:</p> <ul> <li><code>/proc/&lt;pid&gt;/status</code> - Process status</li> <li><code>/proc/&lt;pid&gt;/mem</code> - Process memory (for debugging)</li> <li><code>/proc/&lt;pid&gt;/map</code> - Memory mappings</li> <li><code>/proc/&lt;pid&gt;/cmdline</code> - Command line arguments</li> </ul>"},{"location":"sys/kern/processes/#ptrace-system-call","title":"ptrace() System Call","text":"<pre><code>int sys_ptrace(struct sysmsg *, struct ptrace_args *);\n</code></pre> <p>The ptrace system call allows one process to control another:</p> <p>Operations: - <code>PT_TRACE_ME</code> - Mark process as traceable - <code>PT_ATTACH</code> - Attach to process as debugger - <code>PT_DETACH</code> - Detach from traced process - <code>PT_CONTINUE</code> - Resume execution - <code>PT_STEP</code> - Single-step execution - <code>PT_READ_I/PT_WRITE_I</code> - Read/write instruction space - <code>PT_READ_D/PT_WRITE_D</code> - Read/write data space - <code>PT_GETREGS/PT_SETREGS</code> - Read/write registers</p> <p>Security restrictions: - Cannot trace processes with P_SUGID flag (setuid/setgid) - Cannot trace processes owned by other users (unless root) - Traced process stops on signals for debugger examination</p> <p>Debuggers (gdb, lldb) use ptrace to: - Set breakpoints (write INT3 instruction) - Read/modify memory and registers - Single-step through code - Examine process state after crashes</p>"},{"location":"sys/kern/processes/#resource-limits","title":"Resource Limits","text":"<p>Each process has resource limits inherited from parent:</p> <pre><code>struct proc {\n    struct plimit *p_limit;  // Resource limits\n    struct pstats *p_stats;  // Statistics and timers\n    // ...\n};\n\nstruct plimit {\n    struct rlimit pl_rlimit[RLIM_NLIMITS];\n};\n\nstruct rlimit {\n    rlim_t rlim_cur;  // Current (soft) limit\n    rlim_t rlim_max;  // Maximum (hard) limit\n};\n</code></pre> <p>Standard limits: - <code>RLIMIT_CPU</code> - CPU time (seconds) - <code>RLIMIT_FSIZE</code> - Maximum file size - <code>RLIMIT_DATA</code> - Data segment size - <code>RLIMIT_STACK</code> - Stack size - <code>RLIMIT_CORE</code> - Core file size - <code>RLIMIT_RSS</code> - Resident set size - <code>RLIMIT_MEMLOCK</code> - Locked memory - <code>RLIMIT_NPROC</code> - Number of processes per uid - <code>RLIMIT_NOFILE</code> - Number of open files - <code>RLIMIT_SBSIZE</code> - Socket buffer size</p> <p>Exceeded soft limits typically cause signals (SIGXCPU, SIGXFSZ). Hard limits cannot be exceeded.</p>"},{"location":"sys/kern/processes/#process-accounting","title":"Process Accounting","text":""},{"location":"sys/kern/processes/#resource-usage","title":"Resource Usage","text":"<pre><code>struct rusage {\n    struct timeval ru_utime;   // User CPU time\n    struct timeval ru_stime;   // System CPU time\n    long ru_maxrss;            // Max resident set size\n    long ru_ixrss;             // Shared memory size\n    long ru_idrss;             // Unshared data size\n    long ru_isrss;             // Unshared stack size\n    long ru_minflt;            // Page faults (no I/O)\n    long ru_majflt;            // Page faults (I/O)\n    long ru_nswap;             // Swaps\n    long ru_inblock;           // Block input operations\n    long ru_oublock;           // Block output operations\n    long ru_msgsnd;            // Messages sent\n    long ru_msgrcv;            // Messages received\n    long ru_nsignals;          // Signals received\n    long ru_nvcsw;             // Voluntary context switches\n    long ru_nivcsw;            // Involuntary context switches\n};\n</code></pre> <p>Accumulated statistics are returned to parent via wait4() and can be queried via getrusage().</p>"},{"location":"sys/kern/processes/#time-accounting","title":"Time Accounting","text":"<p>Processes track time in multiple ways:</p> <pre><code>struct pstats {\n    struct timeval p_start;    // Process start time\n    struct rusage p_ru;        // Self resource usage\n    struct rusage p_cru;       // Cumulative child usage\n};\n</code></pre> <p>Time types: - Real time - Wall clock time since start - User time - CPU time in user mode - System time - CPU time in kernel mode</p> <p>Tracked per-process and accumulated across all children.</p>"},{"location":"sys/kern/processes/#summary","title":"Summary","text":"<p>Process and thread management in DragonFly BSD implements a three-level model:</p> <ol> <li>Processes provide resource containers with credentials, address spaces, and file descriptors</li> <li>LWPs provide user-visible threads within processes</li> <li>LWKT threads provide the low-level scheduling primitive</li> </ol> <p>The lifecycle follows: - Creation via fork1() with two-phase LWP initialization around vm_fork() - Execution via kern_execve() with image activators and point-of-no-return design - Termination via exit1() with comprehensive cleanup and zombie transition - Reaping via kern_wait() with waitgen optimization and zombie cleanup</p> <p>Key design features: - PID anti-reuse protection via pid_doms[] array - Reference counting with PHOLD/PRELE and zombie-specific PHOLDZOMB/PRELEZOMB - P_WEXIT coordination to prevent concurrent exit - Two-phase LWP creation to hide partially-initialized state - P_INEXEC flag to serialize exec in multi-threaded processes - Waitgen optimization to avoid spurious wakeups in wait() - Reaper hierarchy for flexible orphan handling</p> <p>All code locations follow the <code>~/s/dragonfly/sys/kern/</code> directory structure as documented.</p>"},{"location":"sys/kern/resources/","title":"Process Resources and Credentials","text":"<p>Source files: <code>kern_descrip.c</code>, <code>kern_plimit.c</code>, <code>kern_resource.c</code>, <code>kern_prot.c</code></p> <p>This document covers the management of process resources, limits, credentials, and file descriptors in DragonFly BSD. These subsystems provide resource accounting, access control, and per-process resource tracking.</p>"},{"location":"sys/kern/resources/#overview","title":"Overview","text":"<p>Process resource management encompasses four major areas:</p> <ol> <li>File descriptors (<code>kern_descrip.c</code>) \u2014 per-process file descriptor tables with thread-local caching</li> <li>Resource limits (<code>kern_plimit.c</code>) \u2014 copy-on-write limit structures (RLIMIT_* values)</li> <li>Resource accounting (<code>kern_resource.c</code>) \u2014 priority management, CPU usage tracking, per-user accounting</li> <li>Credentials (<code>kern_prot.c</code>) \u2014 UID/GID management with atomic copy-on-write semantics</li> </ol> <p>All four interact closely: file descriptors are subject to resource limits (RLIMIT_NOFILE), credentials control access to resources, and resource accounting tracks consumption per user and process.</p>"},{"location":"sys/kern/resources/#file-descriptors-kern_descripc","title":"File Descriptors (<code>kern_descrip.c</code>)","text":""},{"location":"sys/kern/resources/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/resources/#struct-filedesc","title":"<code>struct filedesc</code>","text":"<p>The per-process file descriptor table:</p> <pre><code>struct filedesc {\n    struct file **fd_files;        /* File pointer array */\n    uint32_t fd_cmask;             /* umask for open() */\n    int fd_lastfile;               /* High water mark */\n    int fd_freefile;               /* Hint for next free fd */\n    int fd_nfiles;                 /* Total slots allocated */\n    int fd_refcnt;                 /* Reference count */\n    struct uidinfo *fd_uinfo;      /* Per-user accounting */\n};\n</code></pre> <p>Key points: - <code>fd_files[]</code> is dynamically resized as file descriptors are allocated - Binary tree structure tracks free slots for efficient allocation - Each process owns one <code>filedesc</code>, inherited from parent on fork()</p>"},{"location":"sys/kern/resources/#thread-local-caching-td_fdcache","title":"Thread-local Caching (<code>td_fdcache</code>)","text":"<p>Each thread maintains a small cache to avoid expensive reference counting:</p> <pre><code>struct thread {\n    struct file *td_fdcache[NFDCACHE];  /* Cached file pointers */\n    // NFDCACHE = 16\n};\n</code></pre> <p>Cache modes (stored in low bits of pointer): - Mode 0: Available slot, no reference held - Mode 1: Locked (transitional state during lookup) - Mode 2: Borrowed reference (can reuse without atomic inc/dec)</p> <p>The cache dramatically improves performance for frequently used file descriptors (e.g., stdin/stdout/stderr).</p>"},{"location":"sys/kern/resources/#file-descriptor-allocation","title":"File Descriptor Allocation","text":""},{"location":"sys/kern/resources/#binary-tree-algorithm-fdalloc_locked","title":"Binary Tree Algorithm (<code>fdalloc_locked</code>)","text":"<p>Source: <code>kern_descrip.c:839-1064</code></p> <p>File descriptor slots are organized as a binary tree for O(log n) allocation:</p> <pre><code>flowchart TB\n    HINT[\"fd_freefile (hint)\"]\n    LEFT[\"left subtree\"]\n    RIGHT[\"right subtree\"]\n\n    HINT --&gt; LEFT\n    HINT --&gt; RIGHT\n</code></pre> <p>Key functions: - <code>right_subtree_size(fd, nfiles)</code> \u2014 size of right subtree at fd - <code>right_ancestor(fd, nfiles)</code> \u2014 next right ancestor in tree - <code>left_ancestor(fd, nfiles)</code> \u2014 next left ancestor in tree</p> <p>Allocation logic: 1. Start at <code>fd_freefile</code> (last known free) 2. If slot free, allocate immediately 3. Otherwise, traverse tree using right_ancestor/left_ancestor 4. On failure, grow <code>fd_files[]</code> array</p>"},{"location":"sys/kern/resources/#growing-the-descriptor-table","title":"Growing the Descriptor Table","text":"<p>Source: <code>kern_descrip.c:1066-1227</code></p> <p>When the table is full: 1. Calculate new size: <code>nfiles + max(nfiles/8, 15) + 3</code> 2. Allocate new array (M_FILEDESC) 3. Copy old entries 4. Free old array 5. Update <code>fd_nfiles</code></p> <p>The growth strategy balances memory overhead with reallocation frequency.</p>"},{"location":"sys/kern/resources/#file-descriptor-operations","title":"File Descriptor Operations","text":""},{"location":"sys/kern/resources/#fgetreadfgetwritefget","title":"<code>fgetread</code>/<code>fgetwrite</code>/<code>fget</code>","text":"<p>Source: <code>kern_descrip.c:1492-1650</code></p> <p>Retrieve file pointer for descriptor <code>fd</code>:</p> <ol> <li>Check cache: Look in <code>td_fdcache[]</code> first</li> <li>Fallback: If not cached, search <code>fd_files[]</code></li> <li>Validate: Check bounds, NULL, and file type (read/write)</li> <li>Reference: Increment <code>f_count</code> (borrowed ref avoids this)</li> <li>Cache: Store in <code>td_fdcache[]</code> as mode 2 (borrowed)</li> </ol> <p>Borrowed references: Cache entries hold refs without atomic ops, dramatically improving hot-path performance.</p>"},{"location":"sys/kern/resources/#fdrop-release-file-reference","title":"<code>fdrop</code> \u2014 Release File Reference","text":"<p>Source: <code>kern_descrip.c:2128-2217</code></p> <p>Decrement <code>f_count</code>; if zero, close file:</p> <ol> <li>Atomic decrement of <code>f_count</code></li> <li>If non-zero, return</li> <li>If zero:</li> <li>Call <code>fo_close()</code> (file operations close)</li> <li>Call <code>fdrevoke()</code> to invalidate all cached refs</li> <li>Free file structure</li> </ol> <p>Locking: Uses per-file <code>f_spin</code> spinlock for atomicity.</p>"},{"location":"sys/kern/resources/#dupdup2fcntlf_dupfd","title":"<code>dup</code>/<code>dup2</code>/<code>fcntl(F_DUPFD)</code>","text":"<p>Source: <code>kern_descrip.c:299-447</code></p> <p>Duplicate file descriptor:</p> <ul> <li><code>dup(old)</code> \u2014 allocate lowest free fd, copy file pointer</li> <li><code>dup2(old, new)</code> \u2014 force specific fd, close <code>new</code> if open</li> <li><code>fcntl(F_DUPFD, minfd)</code> \u2014 allocate fd &gt;= minfd</li> </ul> <p>Flags: - <code>DUP_FIXED</code> \u2014 dup2 behavior (specific fd) - <code>DUP_VARIABLE</code> \u2014 dup behavior (any free fd) - <code>DUP_CLOEXEC</code> \u2014 set close-on-exec flag</p> <p>Cache invalidation: <code>fclearcache()</code> clears all thread caches when closing or revoking.</p>"},{"location":"sys/kern/resources/#file-descriptor-limits","title":"File Descriptor Limits","text":"<p>Source: <code>kern_descrip.c:1229-1333</code></p> <p>Two limits apply:</p> <ol> <li>Per-process: <code>RLIMIT_NOFILE</code> (default 1024, hard max 1,048,576)</li> <li>Per-user: <code>maxfilesperuser</code> (default 80% of system max)</li> </ol> <p>Enforcement: - <code>fdalloc()</code> checks both limits before allocation - <code>chgproccnt()</code> updates per-user <code>ui_proccnt</code> counter (via uidinfo) - <code>chgopenfiles()</code> updates per-user <code>ui_openfiles</code> counter</p> <p>System-wide limit: <code>maxfiles</code> (global sysctl, default computed from RAM).</p>"},{"location":"sys/kern/resources/#file-descriptor-revocation-fdrevoke","title":"File Descriptor Revocation (<code>fdrevoke</code>)","text":"<p>Source: <code>kern_descrip.c:2219-2339</code></p> <p>Invalidate all references to a file:</p> <ol> <li>Mark file with <code>FREVOKED</code> flag</li> <li>Iterate all processes' file descriptor tables</li> <li>For each match, set <code>FREVOKED</code> in <code>fd_files[]</code></li> <li>Call <code>fclearcache()</code> to purge thread caches</li> <li>Wake sleeping threads (select/poll)</li> </ol> <p>Use case: Device revocation (e.g., USB device unplugged).</p>"},{"location":"sys/kern/resources/#resource-limits-kern_plimitc","title":"Resource Limits (<code>kern_plimit.c</code>)","text":""},{"location":"sys/kern/resources/#data-structures_1","title":"Data Structures","text":""},{"location":"sys/kern/resources/#struct-plimit","title":"<code>struct plimit</code>","text":"<p>Process resource limits:</p> <pre><code>struct plimit {\n    struct rlimit pl_rlimit[RLIM_NLIMITS]; /* Limit array */\n    int pl_refcnt;                          /* Reference count */\n    uint32_t pl_flags;                      /* Flags */\n};\n\n#define PLIMITF_EXCLUSIVE  0x00000001  /* Private copy (multi-threaded) */\n</code></pre> <p>Limit types (subset of <code>RLIM_NLIMITS</code>): - <code>RLIMIT_CPU</code> \u2014 CPU seconds (enforced in <code>kern_clock.c</code>) - <code>RLIMIT_DATA</code> \u2014 Data segment size - <code>RLIMIT_STACK</code> \u2014 Stack size - <code>RLIMIT_CORE</code> \u2014 Core dump size - <code>RLIMIT_NOFILE</code> \u2014 Open files per process - <code>RLIMIT_VMEM</code> \u2014 Virtual memory - <code>RLIMIT_NPROC</code> \u2014 Processes per user - <code>RLIMIT_SBSIZE</code> \u2014 Socket buffer space per user</p> <p>Each limit has soft (<code>rlim_cur</code>) and hard (<code>rlim_max</code>) values.</p>"},{"location":"sys/kern/resources/#copy-on-write-semantics","title":"Copy-on-Write Semantics","text":"<p>Source: <code>kern_plimit.c:105-212</code></p> <p>Process limits are shared across fork() using reference counting:</p> <pre><code>   parent fork \u2192 child\n      \u2193              \u2193\n   plimit \u2190 pl_refcnt = 2\n</code></pre> <p><code>plimit_fork(struct plimit *olimit)</code></p> <p>Called during fork(): 1. If <code>PLIMITF_EXCLUSIVE</code> set \u2192 allocate private copy 2. Otherwise \u2192 increment <code>pl_refcnt</code>, share with child</p> <p><code>plimit_lwp_fork(struct plimit *olimit)</code></p> <p>Called when creating LWPs (multi-threaded process): 1. Always allocate private copy 2. Set <code>PLIMITF_EXCLUSIVE</code> flag 3. Return new exclusive limit structure</p> <p>Rationale: Multi-threaded processes need private limits to avoid races when multiple LWPs modify limits simultaneously.</p>"},{"location":"sys/kern/resources/#limit-modification","title":"Limit Modification","text":""},{"location":"sys/kern/resources/#plimit_modify","title":"<code>plimit_modify</code>","text":"<p>Source: <code>kern_plimit.c:233-289</code></p> <p>Atomically modify a limit:</p> <ol> <li>If <code>pl_refcnt &gt; 1</code> \u2192 allocate private copy (copy-on-write)</li> <li>Set <code>PLIMITF_EXCLUSIVE</code> if process has LWPs</li> <li>Update limit value</li> <li>Release old limit structure</li> </ol> <p>Locking: Uses process token (<code>&amp;p-&gt;p_token</code>) for atomicity.</p>"},{"location":"sys/kern/resources/#dosetrlimit","title":"<code>dosetrlimit</code>","text":"<p>Source: <code>kern_plimit.c:291-444</code></p> <p>System call handler for <code>setrlimit()</code>:</p> <ol> <li>Validate new limits (soft \u2264 hard)</li> <li>Check permissions (non-root can only lower hard limit)</li> <li>Call <code>plimit_modify()</code> to update</li> <li>Special handling:</li> <li><code>RLIMIT_NOFILE</code> \u2014 update <code>maxfilesperproc</code> cache</li> <li><code>RLIMIT_STACK</code> \u2014 call <code>vm_map_growstack()</code> to adjust stack</li> <li><code>RLIMIT_CPU</code> \u2014 update <code>p_cpulimit</code> (in microseconds)</li> </ol> <p>Permission check: Raising hard limits requires superuser privilege.</p>"},{"location":"sys/kern/resources/#fork-depth-adjustment-plimit_getadjvalue","title":"Fork Depth Adjustment (<code>plimit_getadjvalue</code>)","text":"<p>Source: <code>kern_plimit.c:446-493</code></p> <p>Adjust limits based on chroot depth:</p> <pre><code>static uint64_t plimit_getadjvalue(uint64_t v) {\n    int depth = chroot_visible_vnodes.depth;\n    v -= v * depth * 10 / 100;  /* 10% per chroot level */\n    // Max 50% reduction\n}\n</code></pre> <p>Rationale: Nested chroot environments (e.g., jails within jails) get progressively reduced limits to prevent resource exhaustion.</p>"},{"location":"sys/kern/resources/#cpu-limit-enforcement","title":"CPU Limit Enforcement","text":"<p>The CPU limit is enforced in <code>kern_clock.c:statclock()</code>:</p> <ol> <li>Each clock tick, check <code>p-&gt;p_cpulimit</code></li> <li>If exceeded, send <code>SIGXCPU</code> signal</li> <li>If grace period exceeded, send <code>SIGKILL</code></li> </ol> <p>The limit is stored in microseconds (<code>p_cpulimit</code>) for efficient comparison.</p>"},{"location":"sys/kern/resources/#resource-accounting-kern_resourcec","title":"Resource Accounting (<code>kern_resource.c</code>)","text":""},{"location":"sys/kern/resources/#priority-management","title":"Priority Management","text":""},{"location":"sys/kern/resources/#three-priority-types","title":"Three Priority Types","text":"<ol> <li>Nice priority (<code>p_nice</code>) \u2014 CPU scheduling priority (-20 to +20)</li> <li>I/O priority (<code>p_ionice</code>) \u2014 Disk I/O priority (0 to 20)</li> <li>Real-time priority (<code>lwp_rtprio</code>) \u2014 Real-time scheduling class</li> </ol> <p>Each type is independent and affects different schedulers.</p>"},{"location":"sys/kern/resources/#getprioritysetpriority","title":"<code>getpriority</code>/<code>setpriority</code>","text":"<p>Source: <code>kern_resource.c:84-244</code></p> <p>Adjust nice value:</p> <pre><code>int setpriority(int which, id_t who, int prio) {\n    // which: PRIO_PROCESS, PRIO_PGRP, PRIO_USER\n    // prio: PRIO_MIN (-20) to PRIO_MAX (+20)\n}\n</code></pre> <p>Effects: 1. Update <code>p-&gt;p_nice</code> 2. Call <code>p-&gt;p_usched-&gt;resetpriority(lp)</code> for each LWP 3. Reschedule threads with new priority</p> <p>Permission: Non-root can only increase nice (lower priority).</p>"},{"location":"sys/kern/resources/#ioprio_getioprio_set","title":"<code>ioprio_get</code>/<code>ioprio_set</code>","text":"<p>Source: <code>kern_resource.c:246-386</code></p> <p>Adjust I/O priority:</p> <pre><code>int ioprio_set(int which, int who, int prio) {\n    // prio: IOPRIO_MIN (0) to IOPRIO_MAX (20)\n}\n</code></pre> <p>Effects: 1. Update <code>p-&gt;p_ionice</code> 2. Affects disk scheduler (dsched) I/O ordering</p> <p>Use case: Deprioritize background tasks (e.g., backups).</p>"},{"location":"sys/kern/resources/#rtpriolwp_rtprio","title":"<code>rtprio</code>/<code>lwp_rtprio</code>","text":"<p>Source: <code>kern_resource.c:388-594</code></p> <p>Real-time priority control:</p> <pre><code>struct rtprio {\n    uint16_t type;   /* RTP_PRIO_REALTIME, NORMAL, IDLE, FIFO */\n    uint16_t prio;   /* 0-31 */\n};\n</code></pre> <p>Classes: - <code>RTP_PRIO_REALTIME</code> \u2014 Hard real-time (requires <code>SYSCAP_NOSCHED</code>) - <code>RTP_PRIO_FIFO</code> \u2014 FIFO scheduling - <code>RTP_PRIO_NORMAL</code> \u2014 Time-sharing - <code>RTP_PRIO_IDLE</code> \u2014 Run only when idle</p> <p>Permission: Real-time classes require <code>SYSCAP_NOSCHED</code> capability.</p>"},{"location":"sys/kern/resources/#cpu-time-accounting","title":"CPU Time Accounting","text":""},{"location":"sys/kern/resources/#calcru-calculate-resource-usage","title":"<code>calcru</code> \u2014 Calculate Resource Usage","text":"<p>Source: <code>kern_resource.c:655-743</code></p> <p>Convert tick counters to timeval:</p> <pre><code>void calcru(struct lwp *lp, struct timeval *up, struct timeval *sp) {\n    // Convert td_uticks \u2192 up (user time)\n    // Convert td_sticks \u2192 sp (system time)\n}\n</code></pre> <p>Tick sources: - <code>td_uticks</code> \u2014 User-mode microseconds (updated in statclock) - <code>td_sticks</code> \u2014 Kernel-mode microseconds (updated in statclock) - <code>td_iticks</code> \u2014 Interrupt microseconds</p> <p>Algorithm: 1. Read tick counters atomically 2. Convert ticks to timeval using <code>sys_cputimer-&gt;freq</code> 3. Handle wraparound and monotonicity</p>"},{"location":"sys/kern/resources/#calcru_proc-aggregate-process-usage","title":"<code>calcru_proc</code> \u2014 Aggregate Process Usage","text":"<p>Source: <code>kern_resource.c:745-819</code></p> <p>Sum all LWP statistics into process rusage:</p> <pre><code>void calcru_proc(struct proc *p, struct rusage *ru) {\n    FOREACH_LWP_IN_PROC(lp, p) {\n        calcru(lp, &amp;utv, &amp;stv);\n        timeradd(&amp;ru-&gt;ru_utime, &amp;utv);\n        timeradd(&amp;ru-&gt;ru_stime, &amp;stv);\n    }\n}\n</code></pre> <p>Aggregated fields: - <code>ru_utime</code> \u2014 Total user CPU time - <code>ru_stime</code> \u2014 Total system CPU time - <code>ru_minflt</code> \u2014 Minor page faults - <code>ru_majflt</code> \u2014 Major page faults - <code>ru_inblock</code> \u2014 Block input operations - <code>ru_oublock</code> \u2014 Block output operations</p>"},{"location":"sys/kern/resources/#getrusage-system-call","title":"<code>getrusage</code> System Call","text":"<p>Source: <code>kern_resource.c:821-922</code></p> <p>Retrieve resource usage:</p> <pre><code>int getrusage(int who, struct rusage *rusage) {\n    // who: RUSAGE_SELF, RUSAGE_CHILDREN\n}\n</code></pre> <p>RUSAGE_SELF: Returns current process usage (via <code>calcru_proc</code>).</p> <p>RUSAGE_CHILDREN: Returns accumulated child usage from <code>p-&gt;p_cru</code> (updated in <code>kern_exit.c:wait1()</code> when reaping children).</p>"},{"location":"sys/kern/resources/#per-user-resource-tracking-uidinfo","title":"Per-User Resource Tracking (<code>uidinfo</code>)","text":""},{"location":"sys/kern/resources/#struct-uidinfo","title":"<code>struct uidinfo</code>","text":"<p>Source: <code>kern_resource.c:66-82</code></p> <p>Per-user accounting structure:</p> <pre><code>struct uidinfo {\n    uid_t ui_uid;           /* User ID */\n    int ui_ref;             /* Reference count */\n    int ui_proccnt;         /* Process count */\n    int ui_openfiles;       /* Open file count */\n    int ui_sbsize;          /* Socket buffer bytes */\n    // Hashed in uidinfo_hash\n};\n</code></pre> <p>Use cases: - Enforce <code>RLIMIT_NPROC</code> (processes per user) - Enforce <code>maxfilesperuser</code> (open files per user) - Enforce <code>RLIMIT_SBSIZE</code> (socket buffer space per user)</p>"},{"location":"sys/kern/resources/#chgproccntchgsbsize","title":"<code>chgproccnt</code>/<code>chgsbsize</code>","text":"<p>Source: <code>kern_resource.c:1038-1103</code></p> <p>Atomically adjust per-user counters:</p> <pre><code>int chgproccnt(struct uidinfo *uip, int diff, int max) {\n    // Returns 0 if within limit, 1 if exceeded\n}\n\nint chgsbsize(struct uidinfo *uip, int *hiwat, int to, int max) {\n    // Adjust socket buffer size tracking\n}\n</code></pre> <p>Atomicity: Uses <code>atomic_fetchadd_int()</code> for lock-free updates.</p> <p>Callers: - <code>fork()</code> \u2192 <code>chgproccnt(uip, 1, maxproc)</code> - <code>exit()</code> \u2192 <code>chgproccnt(uip, -1, 0)</code> - <code>socket()</code> \u2192 <code>chgsbsize()</code> for buffer allocation</p>"},{"location":"sys/kern/resources/#credentials-kern_protc","title":"Credentials (<code>kern_prot.c</code>)","text":""},{"location":"sys/kern/resources/#data-structures_2","title":"Data Structures","text":""},{"location":"sys/kern/resources/#struct-ucred","title":"<code>struct ucred</code>","text":"<p>Process credentials:</p> <pre><code>struct ucred {\n    int cr_ref;                /* Reference count */\n    uid_t cr_uid;              /* Effective user ID */\n    uid_t cr_ruid;             /* Real user ID */\n    uid_t cr_svuid;            /* Saved user ID */\n    gid_t cr_gid;              /* Effective group ID */\n    gid_t cr_rgid;             /* Real group ID */\n    gid_t cr_svgid;            /* Saved group ID */\n    gid_t cr_groups[NGROUPS];  /* Supplementary groups */\n    int cr_ngroups;            /* Group count */\n    struct prison *cr_prison;  /* Jail info */\n    // ... capabilities, labels, etc.\n};\n</code></pre> <p>Special credentials: - <code>NOCRED</code> \u2014 No credential (internal use) - <code>FSCRED</code> \u2014 Filesystem credential (never freed)</p>"},{"location":"sys/kern/resources/#atomic-copy-on-write-cratom","title":"Atomic Copy-on-Write (<code>cratom</code>)","text":"<p>Source: <code>kern_prot.c:113-177</code></p> <p>Ensure exclusive credential before modification:</p> <pre><code>struct ucred *cratom(struct ucred **cr) {\n    if ((*cr)-&gt;cr_ref &gt; 1) {\n        // Shared \u2192 allocate private copy\n        struct ucred *ncr = crdup(*cr);\n        crfree(*cr);\n        *cr = ncr;\n    }\n    return *cr;\n}\n</code></pre> <p>Why copy-on-write? - Credentials are shared across fork() and threads - Modification requires atomicity (no races) - Copy-on-write avoids unnecessary duplication</p>"},{"location":"sys/kern/resources/#cratom_proc","title":"<code>cratom_proc</code>","text":"<p>Source: <code>kern_prot.c:179-209</code></p> <p>Process-level atomization:</p> <pre><code>struct ucred *cratom_proc(struct proc *p) {\n    p-&gt;p_ucred = cratom(&amp;p-&gt;p_ucred);\n    return p-&gt;p_ucred;\n}\n</code></pre> <p>Locking: Uses process token (<code>&amp;p-&gt;p_token</code>) for atomicity.</p>"},{"location":"sys/kern/resources/#uidgid-system-calls","title":"UID/GID System Calls","text":""},{"location":"sys/kern/resources/#getuidgeteuidgetgidgetegid","title":"<code>getuid</code>/<code>geteuid</code>/<code>getgid</code>/<code>getegid</code>","text":"<p>Source: <code>kern_prot.c:211-279</code></p> <p>Retrieve credentials:</p> <pre><code>uid_t getuid(void)  { return curthread-&gt;td_ucred-&gt;cr_ruid; }\nuid_t geteuid(void) { return curthread-&gt;td_ucred-&gt;cr_uid; }\n</code></pre> <p>Thread vs. process credentials: - Threads cache <code>td_ucred</code> (pointer to <code>p-&gt;p_ucred</code>) - Always consistent due to copy-on-write semantics</p>"},{"location":"sys/kern/resources/#setuidseteuid","title":"<code>setuid</code>/<code>seteuid</code>","text":"<p>Source: <code>kern_prot.c:281-457</code></p> <p>Change user ID:</p> <pre><code>int setuid(uid_t uid) {\n    // POSIX_APPENDIX_B_4_2_2 semantics\n}\n</code></pre> <p>Semantics (POSIX_APPENDIX_B_4_2_2):</p> <p>For non-superuser: - Can set euid to ruid or svuid only</p> <p>For superuser: - Sets all three: ruid, euid, svuid</p> <p>Implementation: 1. Call <code>cratom_proc()</code> to get exclusive credential 2. Update uid fields 3. Call <code>change_euid()</code> to transfer uidinfo 4. Mark process as <code>P_SUGID</code> (tainted)</p>"},{"location":"sys/kern/resources/#change_euidchange_ruid","title":"<code>change_euid</code>/<code>change_ruid</code>","text":"<p>Source: <code>kern_prot.c:459-550</code></p> <p>Helper functions to change UID with uidinfo transfer:</p> <pre><code>void change_euid(uid_t euid) {\n    struct uidinfo *new_uip = uifind(euid);\n    uireplace(&amp;p-&gt;p_ucred-&gt;cr_uidinfo, new_uip);\n    // Transfer lock file ownership, etc.\n}\n</code></pre> <p>uidinfo management: - <code>uifind(uid)</code> \u2014 Lookup or create uidinfo (refcounted) - <code>uireplace()</code> \u2014 Atomically swap uidinfo pointers - <code>uihold()</code>/<code>uidrop()</code> \u2014 Reference counting</p> <p>Lock file adjustment: <code>lf_count_adjust()</code> transfers file lock ownership.</p>"},{"location":"sys/kern/resources/#setreuidsetregid","title":"<code>setreuid</code>/<code>setregid</code>","text":"<p>Source: <code>kern_prot.c:552-751</code></p> <p>Set real and effective IDs simultaneously:</p> <pre><code>int setreuid(uid_t ruid, uid_t euid) {\n    // Allows swapping ruid \u2194 euid (for setuid programs)\n}\n</code></pre> <p>Use case: Temporarily drop privileges (swap euid and ruid), perform operation, then restore.</p>"},{"location":"sys/kern/resources/#setresuidsetresgid","title":"<code>setresuid</code>/<code>setresgid</code>","text":"<p>Source: <code>kern_prot.c:753-988</code></p> <p>Set real, effective, and saved IDs:</p> <pre><code>int setresuid(uid_t ruid, uid_t euid, uid_t suid) {\n    // Fine-grained control over all three IDs\n}\n</code></pre> <p>Advantage: Provides explicit control over saved UID (needed for some security models).</p>"},{"location":"sys/kern/resources/#setgroupsgetgroups","title":"<code>setgroups</code>/<code>getgroups</code>","text":"<p>Source: <code>kern_prot.c:990-1120</code></p> <p>Manage supplementary groups:</p> <pre><code>int setgroups(int ngroups, gid_t *groups) {\n    // Requires superuser privilege\n}\n</code></pre> <p>Limit: <code>NGROUPS</code> (typically 16-64 depending on configuration).</p>"},{"location":"sys/kern/resources/#permission-checking","title":"Permission Checking","text":""},{"location":"sys/kern/resources/#p_trespass","title":"<code>p_trespass</code>","text":"<p>Source: <code>kern_prot.c:1256-1332</code></p> <p>Check if process A can signal/ptrace process B:</p> <pre><code>int p_trespass(struct ucred *cr1, struct ucred *cr2) {\n    // Returns 0 if allowed, errno otherwise\n}\n</code></pre> <p>Rules: 1. Root can always signal others 2. Same uid \u2192 allowed 3. Same ruid and target not setuid \u2192 allowed 4. Otherwise \u2192 EPERM</p> <p>Security: Prevents unprivileged processes from interfering with setuid processes.</p>"},{"location":"sys/kern/resources/#processsessiongroup-ids","title":"Process/Session/Group IDs","text":""},{"location":"sys/kern/resources/#getpidgetppid","title":"<code>getpid</code>/<code>getppid</code>","text":"<p>Source: <code>kern_prot.c:1415-1478</code></p> <p>Retrieve process/parent IDs:</p> <pre><code>pid_t getpid(void)  { return curproc-&gt;p_pid; }\npid_t getppid(void) { return curproc-&gt;p_pptr-&gt;p_pid; }\n</code></pre>"},{"location":"sys/kern/resources/#setsid-create-new-session","title":"<code>setsid</code> \u2014 Create New Session","text":"<p>Source: <code>kern_prot.c:1532-1597</code></p> <p>Become session leader:</p> <pre><code>pid_t setsid(void) {\n    // Create new session and process group\n}\n</code></pre> <p>Effects: 1. Allocate new session ID (equal to pid) 2. Allocate new process group ID (equal to pid) 3. Detach from controlling terminal 4. Become session leader</p> <p>Restrictions: Cannot be called by process group leader.</p>"},{"location":"sys/kern/resources/#setpgid-set-process-group","title":"<code>setpgid</code> \u2014 Set Process Group","text":"<p>Source: <code>kern_prot.c:1599-1749</code></p> <p>Change process group membership:</p> <pre><code>int setpgid(pid_t pid, pid_t pgid) {\n    // Move process to different process group\n}\n</code></pre> <p>Restrictions: 1. Can only set own pgid or child's pgid before exec 2. Target must be in same session 3. Cannot move across session boundaries</p>"},{"location":"sys/kern/resources/#credential-tainting-setsugid","title":"Credential Tainting (<code>setsugid</code>)","text":"<p>Source: <code>kern_prot.c:1751-1784</code></p> <p>Mark process as tainted (changed credentials):</p> <pre><code>void setsugid(void) {\n    p-&gt;p_flags |= P_SUGID;\n}\n</code></pre> <p>Effects: - Disables core dumps (security) - Disables ptrace attachment - Prevents privilege escalation exploits</p> <p>Callers: - <code>setuid()</code>, <code>setgid()</code> \u2014 after changing credentials - <code>execve()</code> \u2014 when executing setuid binary</p>"},{"location":"sys/kern/resources/#interactions-between-subsystems","title":"Interactions Between Subsystems","text":""},{"location":"sys/kern/resources/#file-descriptors-resource-limits","title":"File Descriptors \u2192 Resource Limits","text":"<p><code>fdalloc()</code> checks <code>RLIMIT_NOFILE</code> before allocating:</p> <pre><code>if (fd &gt;= p-&gt;p_rlimit[RLIMIT_NOFILE].rlim_cur)\n    return EMFILE;\n</code></pre>"},{"location":"sys/kern/resources/#file-descriptors-credentials","title":"File Descriptors \u2192 Credentials","text":"<p>File operations use <code>td_ucred</code> for permission checks:</p> <pre><code>int error = VOP_READ(vp, uio, 0, td-&gt;td_ucred);\n</code></pre>"},{"location":"sys/kern/resources/#resource-limits-fork","title":"Resource Limits \u2192 Fork","text":"<p><code>fork()</code> checks <code>RLIMIT_NPROC</code> before creating child:</p> <pre><code>if (chgproccnt(uip, 1, p-&gt;p_rlimit[RLIMIT_NPROC].rlim_cur) &gt; 0)\n    return EAGAIN;\n</code></pre>"},{"location":"sys/kern/resources/#credentials-uidinfo","title":"Credentials \u2192 uidinfo","text":"<p>Changing UID transfers uidinfo:</p> <pre><code>change_euid(new_uid) {\n    uireplace(&amp;cr-&gt;cr_uidinfo, uifind(new_uid));\n}\n</code></pre> <p>This updates per-user resource tracking atomically.</p>"},{"location":"sys/kern/resources/#key-design-principles","title":"Key Design Principles","text":""},{"location":"sys/kern/resources/#1-copy-on-write-for-sharing","title":"1. Copy-on-Write for Sharing","text":"<p>Both <code>plimit</code> and <code>ucred</code> use reference counting with copy-on-write:</p> <ul> <li>Cheap sharing across fork()</li> <li>Atomic modification when needed</li> <li>No races in multi-threaded processes</li> </ul>"},{"location":"sys/kern/resources/#2-thread-local-caching","title":"2. Thread-Local Caching","text":"<p>File descriptor cache (<code>td_fdcache</code>) avoids expensive atomic operations:</p> <ul> <li>16-entry cache per thread</li> <li>Borrowed references (mode 2) skip refcount ops</li> <li>Cache coherency via <code>fclearcache()</code></li> </ul>"},{"location":"sys/kern/resources/#3-per-user-resource-tracking","title":"3. Per-User Resource Tracking","text":"<p><code>uidinfo</code> provides global accounting per user:</p> <ul> <li>Prevents single user from exhausting system resources</li> <li>Enforced atomically with <code>chgproccnt</code>/<code>chgsbsize</code></li> <li>Hashed for efficient lookup</li> </ul>"},{"location":"sys/kern/resources/#4-binary-tree-allocation","title":"4. Binary Tree Allocation","text":"<p>File descriptor allocation uses binary tree traversal:</p> <ul> <li>O(log n) allocation even with sparse tables</li> <li>Efficient reuse of low-numbered descriptors</li> <li>Minimizes table growth</li> </ul>"},{"location":"sys/kern/resources/#5-limit-enforcement-points","title":"5. Limit Enforcement Points","text":"<p>Resource limits are enforced at allocation points:</p> <ul> <li><code>RLIMIT_NOFILE</code> \u2014 in <code>fdalloc()</code></li> <li><code>RLIMIT_NPROC</code> \u2014 in <code>fork()</code></li> <li><code>RLIMIT_CPU</code> \u2014 in <code>statclock()</code> (kern_clock.c)</li> <li><code>RLIMIT_STACK</code> \u2014 in <code>vm_map_growstack()</code> (sys/vm)</li> </ul>"},{"location":"sys/kern/resources/#summary","title":"Summary","text":"<p>Process resource management in DragonFly BSD provides:</p> <ol> <li>Efficient file descriptor management with thread-local caching and binary tree allocation</li> <li>Copy-on-write resource limits shared across fork() with atomic modification</li> <li>Multi-level priority control (nice, ionice, rtprio) for CPU and I/O scheduling</li> <li>Atomic credential management with POSIX semantics and per-user accounting</li> <li>System-wide resource tracking preventing exhaustion attacks</li> </ol> <p>The design emphasizes: - Atomicity through copy-on-write and tokens - Performance through caching and lock-free algorithms - Isolation through per-user limits and permission checks - Scalability through efficient data structures (binary tree, hash tables)</p> <p>These subsystems interact closely with process lifecycle (fork/exec/exit), virtual filesystem (file operations), and virtual memory (stack limits), forming the foundation of resource management in the kernel.</p>"},{"location":"sys/kern/scheduling/","title":"CPU Scheduling","text":""},{"location":"sys/kern/scheduling/#overview","title":"Overview","text":"<p>DragonFly's CPU scheduling system consists of two distinct layers:</p> <ol> <li>LWKT (Light Weight Kernel Threads) Layer - Low-level thread scheduling that handles all kernel threads and provides the foundation for userland scheduling</li> <li>User Scheduler Layer - Pluggable schedulers that implement policies for userland process scheduling</li> </ol> <p>This document focuses on the user scheduler layer and the sleep/wakeup synchronization primitives that coordinate thread blocking and resumption.</p> <p>Key source files: - <code>kern_sched.c</code> - POSIX real-time scheduling support (ksched) - <code>kern_synch.c</code> - Sleep/wakeup, tsleep/wakeup infrastructure - <code>kern_usched.c</code> - User scheduler registration and management - <code>usched_bsd4.c</code> - BSD4 scheduler (original DragonFly) - <code>usched_dfly.c</code> - DFLY scheduler (message-based, default) - <code>usched_dummy.c</code> - Dummy scheduler (for testing/reference)</p>"},{"location":"sys/kern/scheduling/#architecture","title":"Architecture","text":""},{"location":"sys/kern/scheduling/#two-layer-design","title":"Two-Layer Design","text":"<pre><code>flowchart TB\n    USERLAND[\"Userland Processes (LWPs)\"]\n\n    subgraph USCHED[\"User Scheduler Layer (pluggable)\"]\n        U1[\"usched_bsd4 / usched_dfly\"]\n        U2[\"Run queues per scheduler\"]\n        U3[\"CPU affinity management\"]\n        U4[\"Priority calculations\"]\n    end\n\n    subgraph LWKT[\"LWKT Scheduler (per-CPU)\"]\n        L1[\"All kernel threads\"]\n        L2[\"Thread preemption\"]\n        L3[\"Context switching\"]\n    end\n\n    USERLAND --&gt; USCHED\n    USCHED --&gt; LWKT\n</code></pre>"},{"location":"sys/kern/scheduling/#pluggable-user-schedulers","title":"Pluggable User Schedulers","text":"<p>DragonFly allows multiple user schedulers to coexist. Each process is assigned a scheduler (<code>p-&gt;p_usched</code>), and the system provides a common interface (<code>struct usched</code>) that all schedulers must implement.</p>"},{"location":"sys/kern/scheduling/#sleepwakeup-infrastructure","title":"Sleep/Wakeup Infrastructure","text":""},{"location":"sys/kern/scheduling/#overview-kern_synchc","title":"Overview (kern_synch.c)","text":"<p>The sleep/wakeup mechanism is DragonFly's primary thread synchronization primitive, allowing threads to block waiting for events and be awakened when those events occur.</p> <p>Core functions: - <code>tsleep()</code> - Sleep on an identifier (ident) with timeout and signal handling - <code>wakeup()</code> - Wake all threads sleeping on an identifier - <code>wakeup_one()</code> - Wake one thread sleeping on an identifier - <code>tsleep_interlock()</code> - Prepare to sleep without blocking yet - <code>ssleep()</code>, <code>lksleep()</code>, <code>mtxsleep()</code>, <code>zsleep()</code> - Variants that atomically release locks</p>"},{"location":"sys/kern/scheduling/#sleep-queue-hash-table","title":"Sleep Queue Hash Table","text":"<p>Data structure: <code>struct tslpque</code> (kern_synch.c:68)</p> <pre><code>struct tslpque {\n    TAILQ_HEAD(, thread)  queue;\n    const volatile void   *ident0;\n    const volatile void   *ident1;\n    const volatile void   *ident2;\n    const volatile void   *ident3;\n};\n</code></pre> <p>Each CPU maintains its own hash table of sleep queues (<code>gd-&gt;gd_tsleep_hash</code>). The hash is computed from the sleep identifier (typically an address):</p> <pre><code>#define LOOKUP(x)  ((((uintptr_t)(x) + ((uintptr_t)(x) &gt;&gt; 18)) ^ \\\n                     LOOKUP_PRIME) % slpque_tablesize)\n#define TCHASHSHIFT(x)  ((x) &gt;&gt; 4)\n</code></pre> <p>The global <code>slpque_cpumasks[]</code> array tracks which CPUs have threads sleeping on each hash bucket, enabling efficient cross-CPU wakeups.</p>"},{"location":"sys/kern/scheduling/#tsleep-flow","title":"tsleep() Flow","text":"<p>Function: <code>tsleep()</code> (kern_synch.c:512)</p> <pre><code>1. Check for delayed wakeups (TDF_DELAYED_WAKEUP)\n2. Handle early boot / panic case (just yield briefly)\n3. Enter critical section\n4. Interlock with sleep queue (if not already done via PINTERLOCKED)\n5. Handle process state (SCORE for coredump)\n6. Check for pending signals (if PCATCH set)\n   - Early return with EINTR/ERESTART if signal pending\n7. Set LWP_SINTR flag (if PCATCH) to allow signal wakeup\n8. Release from user scheduler (p_usched-&gt;release_curproc)\n9. Verify still on sleep queue (race detection)\n10. Deschedule from LWKT (lwkt_deschedule_self)\n11. Set TDF_TSLEEP_DESCHEDULED flag\n12. Setup timeout callout (if timo != 0)\n13. Set lp-&gt;lwp_stat = LSSLEEP\n14. lwkt_switch() - actually sleep\n15. [WOKEN UP]\n16. Cancel timeout (if set)\n17. Remove from sleep queue\n18. Check for signals again (if PCATCH)\n19. Clear LWP_SINTR flag\n20. Set lp-&gt;lwp_stat = LSRUN\n21. Exit critical section\n22. Return error code (0, EWOULDBLOCK, EINTR, ERESTART)\n</code></pre> <p>Key points: - The entire sleep setup runs in a critical section to prevent migration - <code>tsleep_interlock()</code> can be called beforehand to set up the sleep queue while still holding locks - Timeouts are handled by <code>endtsleep()</code> callback - Signal delivery can interrupt sleep (if PCATCH set)</p>"},{"location":"sys/kern/scheduling/#tsleep_interlock-pattern","title":"tsleep_interlock() Pattern","text":"<p>Function: <code>tsleep_interlock()</code> (kern_synch.c:451)</p> <p>This function enables a common synchronization pattern:</p> <pre><code>// Critical pattern:\nmutex_lock(&amp;lock);\n// Check condition\nif (!condition_met) {\n    tsleep_interlock(&amp;condition_ident, 0);  // Setup sleep\n    mutex_unlock(&amp;lock);                     // Release lock\n    tsleep(&amp;condition_ident, PINTERLOCKED, \"wait\", 0);\n    mutex_lock(&amp;lock);\n}\nmutex_unlock(&amp;lock);\n</code></pre> <p>The <code>PINTERLOCKED</code> flag tells <code>tsleep()</code> that the sleep queue interlocking has already been done, preventing races between the lock release and the sleep.</p>"},{"location":"sys/kern/scheduling/#wakeup-mechanism","title":"Wakeup Mechanism","text":"<p>Function: <code>_wakeup()</code> (kern_synch.c:999)</p> <p>Wakeup searches the sleep queue hash bucket for matching threads:</p> <pre><code>1. Enter critical section\n2. Compute hash bucket from ident\n3. Scan local CPU's sleep queue for matching threads\n   - Match on: td-&gt;td_wchan == ident &amp;&amp; td-&gt;td_wdomain == domain\n4. For each match:\n   - Remove from sleep queue (_tsleep_remove)\n   - Set td-&gt;td_wakefromcpu (for scheduler affinity)\n   - Schedule thread (lwkt_schedule) if TDF_TSLEEP_DESCHEDULED\n   - If PWAKEUP_ONE flag, stop after first wakeup\n5. Clean up queue tracking (ident0-3, cpumask bit)\n6. Send IPIs to other CPUs with matching threads\n   - Check slpque_cpumasks[cid] for remote CPUs\n   - Send _wakeup IPI with ident and domain\n7. Exit critical section\n</code></pre> <p>Wakeup variants: - <code>wakeup(ident)</code> - Wake all threads on all CPUs - <code>wakeup_one(ident)</code> - Wake one thread on any CPU - <code>wakeup_mycpu(ident)</code> - Wake threads on current CPU only - <code>wakeup_domain(ident, domain)</code> - Wake threads in specific domain</p>"},{"location":"sys/kern/scheduling/#delayed-wakeup-optimization","title":"Delayed Wakeup Optimization","text":"<p>Functions: <code>wakeup_start_delayed()</code>, <code>wakeup_end_delayed()</code> (kern_synch.c:1266, 1276)</p> <p>For code that performs many wakeups in quick succession, delayed wakeups batch them:</p> <pre><code>wakeup_start_delayed();\n// Multiple wakeups are queued in gd-&gt;gd_delayed_wakeup[0..1]\nwakeup(ident1);\nwakeup(ident2);\n// ...\nwakeup_end_delayed();  // Actually issue the wakeups\n</code></pre> <p>This reduces IPI traffic when many wakeups occur close together.</p>"},{"location":"sys/kern/scheduling/#sleep-queue-domains-pdomain_","title":"Sleep Queue Domains (PDOMAIN_*)","text":"<p>Sleep identifiers can be partitioned into domains to prevent false wakeups:</p> <ul> <li><code>PDOMAIN_UMTX</code> - User mutex domain</li> <li>Default domain (0) - General purpose</li> </ul> <p>Threads sleep and wake within their domain, preventing cross-contamination.</p>"},{"location":"sys/kern/scheduling/#posix-real-time-scheduling-kern_schedc","title":"POSIX Real-Time Scheduling (kern_sched.c)","text":""},{"location":"sys/kern/scheduling/#overview_1","title":"Overview","text":"<p>The <code>ksched</code> module provides POSIX.1b real-time scheduling extensions: - <code>SCHED_FIFO</code> - First-in-first-out realtime - <code>SCHED_RR</code> - Round-robin realtime - <code>SCHED_OTHER</code> - Standard timesharing</p> <p>Key functions: - <code>ksched_setscheduler()</code> - Set scheduling policy and priority - <code>ksched_getscheduler()</code> - Get current scheduling policy - <code>ksched_setparam()</code> / <code>ksched_getparam()</code> - Get/set sched parameters - <code>ksched_yield()</code> - Voluntarily yield CPU - <code>ksched_get_priority_max()</code> / <code>_min()</code> - Query priority ranges - <code>ksched_rr_get_interval()</code> - Get round-robin interval</p>"},{"location":"sys/kern/scheduling/#priority-mapping","title":"Priority Mapping","text":"<p>POSIX requires higher numbers = higher priority, but DragonFly's internal rtprio uses lower numbers = higher priority. The ksched module performs the inversion:</p> <pre><code>#define p4prio_to_rtpprio(P)  (RTP_PRIO_MAX - (P))\n#define rtpprio_to_p4prio(P)  (RTP_PRIO_MAX - (P))\n\n#define P1B_PRIO_MIN  rtpprio_to_p4prio(RTP_PRIO_MAX)\n#define P1B_PRIO_MAX  rtpprio_to_p4prio(RTP_PRIO_MIN)\n</code></pre>"},{"location":"sys/kern/scheduling/#real-time-priority-types","title":"Real-Time Priority Types","text":"<p>Stored in <code>lp-&gt;lwp_rtprio</code>:</p> <ul> <li><code>RTP_PRIO_FIFO</code> - SCHED_FIFO: runs until blocked or preempted by higher priority</li> <li><code>RTP_PRIO_REALTIME</code> - SCHED_RR: round-robin with other same-priority RT threads</li> <li><code>RTP_PRIO_NORMAL</code> - SCHED_OTHER: standard timesharing</li> <li><code>RTP_PRIO_IDLE</code> - Idle priority</li> </ul> <p>Real-time threads (<code>RTP_PRIO_FIFO</code> and <code>RTP_PRIO_REALTIME</code>) always run before normal priority threads.</p>"},{"location":"sys/kern/scheduling/#user-scheduler-management-kern_uschedc","title":"User Scheduler Management (kern_usched.c)","text":""},{"location":"sys/kern/scheduling/#scheduler-registration","title":"Scheduler Registration","text":"<p>Function: <code>usched_ctl()</code> (kern_usched.c:96)</p> <p>Schedulers register/unregister with the system:</p> <pre><code>struct usched {\n    TAILQ_ENTRY(usched) entry;\n    const char *name;\n    const char *desc;\n    void (*usched_register)(void);\n    void (*usched_unregister)(void);\n    void (*acquire_curproc)(struct lwp *);\n    void (*release_curproc)(struct lwp *);\n    void (*setrunqueue)(struct lwp *);\n    void (*schedulerclock)(struct lwp *, sysclock_t, sysclock_t);\n    void (*recalculate)(struct lwp *);\n    void (*resetpriority)(struct lwp *);\n    void (*forking)(struct lwp *parent, struct lwp *child);\n    void (*exiting)(struct lwp *, struct proc *);\n    void (*uload_update)(struct lwp *);\n    void (*setcpumask)(struct lwp *, cpumask_t);\n    void (*yield)(struct lwp *);\n    void (*changedcpu)(struct lwp *);\n};\n</code></pre> <p>Built-in schedulers: - <code>usched_bsd4</code> - Original DragonFly scheduler - <code>usched_dfly</code> - New message-based scheduler (default) - <code>usched_dummy</code> - Minimal reference implementation</p>"},{"location":"sys/kern/scheduling/#scheduler-selection","title":"Scheduler Selection","text":"<p>Function: <code>usched_init()</code> (kern_usched.c:59)</p> <p>At boot, the system selects the default scheduler based on <code>kern.user_scheduler</code> environment variable: - <code>\"dfly\"</code> \u2192 usched_dfly (default) - <code>\"bsd4\"</code> \u2192 usched_bsd4 - <code>\"dummy\"</code> \u2192 usched_dummy</p> <p>Each process inherits its scheduler from its parent on fork. The scheduler can be changed via <code>usched_set(2)</code> syscall.</p>"},{"location":"sys/kern/scheduling/#cpu-affinity-management","title":"CPU Affinity Management","text":"<p>Functions: <code>sys_lwp_setaffinity()</code>, <code>sys_lwp_getaffinity()</code> (kern_usched.c:411, 363)</p> <p>DragonFly allows per-LWP CPU affinity masks (<code>lp-&gt;lwp_cpumask</code>):</p> <pre><code>// Set affinity\ncpumask_t mask;\nCPUMASK_ASSBIT(mask, target_cpu);\nlwp_setaffinity(pid, tid, &amp;mask);\n\n// Get affinity  \nlwp_getaffinity(pid, tid, &amp;mask);\n</code></pre> <p>When an LWP's affinity is changed: 1. Update <code>lp-&gt;lwp_cpumask</code> 2. If current CPU not in new mask, call <code>lwkt_migratecpu()</code> to move thread 3. Call <code>p_usched-&gt;changedcpu(lp)</code> to notify scheduler</p>"},{"location":"sys/kern/scheduling/#usched_set-syscall","title":"usched_set() Syscall","text":"<p>Function: <code>sys_usched_set()</code> (kern_usched.c:184)</p> <p>Commands: - <code>USCHED_SET_SCHEDULER</code> - Change process's scheduler - <code>USCHED_SET_CPU</code> - Pin LWP to specific CPU - <code>USCHED_GET_CPU</code> - Get current CPU - <code>USCHED_ADD_CPU</code> - Add CPU to affinity mask - <code>USCHED_DEL_CPU</code> - Remove CPU from affinity mask - <code>USCHED_SET_CPUMASK</code> - Set full affinity mask - <code>USCHED_GET_CPUMASK</code> - Get affinity mask</p>"},{"location":"sys/kern/scheduling/#scheduler-clock","title":"Scheduler Clock","text":"<p>Function: <code>usched_schedulerclock()</code> (kern_usched.c:152)</p> <p>Called from the system's scheduler clock (hardclock) on each CPU at <code>ESTCPUFREQ</code> (typically 10 Hz). Each registered scheduler's <code>schedulerclock()</code> method is invoked to: - Update per-thread statistics (estcpu, pctcpu) - Detect if round-robin interval expired - Request reschedules as needed</p>"},{"location":"sys/kern/scheduling/#bsd4-scheduler-usched_bsd4c","title":"BSD4 Scheduler (usched_bsd4.c)","text":""},{"location":"sys/kern/scheduling/#overview_2","title":"Overview","text":"<p>The BSD4 scheduler is a traditional BSD-style scheduler with: - 32 run queues per priority class (realtime, normal, idle) - Priority calculated from nice value and CPU usage (estcpu) - Round-robin within each queue - Simple CPU load balancing</p> <p>Priority classes: - Realtime: 0-127 (maps to 32 queues via priorities/4) - Normal: 128-255 - Idle: 256-383 - Thread: 384-511 (kernel threads)</p>"},{"location":"sys/kern/scheduling/#data-structures","title":"Data Structures","text":"<p>Per-CPU state: <code>struct usched_bsd4_pcpu</code> (usched_bsd4.c:131)</p> <pre><code>struct usched_bsd4_pcpu {\n    struct thread  *helper_thread;\n    short          rrcount;        // Round-robin counter\n    short          upri;           // User priority of current process\n    struct lwp     *uschedcp;      // Current scheduled LWP\n    struct lwp     *old_uschedcp;  // Previous LWP\n    cpu_node_t     *cpunode;       // CPU topology node\n};\n</code></pre> <p>Global run queues: - <code>bsd4_queues[32]</code> - Normal priority queues - <code>bsd4_rtqueues[32]</code> - Realtime priority queues - <code>bsd4_idqueues[32]</code> - Idle priority queues - <code>bsd4_queuebits</code>, <code>bsd4_rtqueuebits</code>, <code>bsd4_idqueuebits</code> - Bitmasks indicating non-empty queues</p>"},{"location":"sys/kern/scheduling/#priority-calculation","title":"Priority Calculation","text":"<p>Function: <code>bsd4_resetpriority()</code></p> <p>The normal priority calculation considers: 1. Base priority from nice value (<code>lp-&gt;lwp_rtprio.prio</code>) 2. CPU usage (estcpu): <code>lp-&gt;lwp_estcpu</code> 3. Penalty for batch processes</p> <pre><code>// Simplified priority formula\npri = PRIBASE_NORMAL + (nice * NICEPPQ) + (estcpu / ESTCPUPPQ)\nlwp_priority = min(pri, MAXPRI-1)\n</code></pre> <p>estcpu (estimated CPU usage) is a decay-average: - Incremented on each scheduler clock tick when running - Decayed by factor over time - Used to penalize CPU-bound processes relative to I/O-bound</p>"},{"location":"sys/kern/scheduling/#run-queue-management","title":"Run Queue Management","text":"<p>Function: <code>bsd4_setrunqueue_locked()</code></p> <p>When placing an LWP on the run queue:</p> <pre><code>1. Determine queue index from priority\n   - Realtime/Idle: direct mapping\n   - Normal: (priority - PRIBASE_NORMAL) / PPQ\n2. Add to tail of appropriate queue (FIFO within priority)\n3. Set corresponding bit in queuebits\n4. Increment bsd4_runqcount\n5. Set LWP_MP_ONRUNQ flag\n</code></pre> <p>Function: <code>bsd4_chooseproc_locked()</code></p> <p>Selecting next LWP to run:</p> <pre><code>1. Check realtime queues first (highest priority)\n   - Find first set bit in bsd4_rtqueuebits\n   - Take head of that queue\n2. If no realtime, check normal queues\n   - Find first set bit in bsd4_queuebits  \n   - Take head of that queue\n3. If no normal, check idle queues\n   - Find first set bit in bsd4_idqueuebits\n   - Take head of that queue\n4. Return selected LWP (or NULL if all empty)\n</code></pre>"},{"location":"sys/kern/scheduling/#cpu-selection-heuristics","title":"CPU Selection Heuristics","text":"<p>Function: <code>bsd4_setrunqueue()</code></p> <p>When scheduling an LWP, BSD4 tries to place it intelligently:</p> <ol> <li>Check for free CPUs (not running user processes)</li> <li>Prefer CPUs in same CPU package (cache coherency)</li> <li> <p>Use topology information (<code>cpunode</code>)</p> </li> <li> <p>Check running CPUs</p> </li> <li>Find CPU running lower-priority LWP</li> <li> <p>Use upri (user priority) comparison</p> </li> <li> <p>Round-robin if all busy</p> </li> <li> <p>Use <code>bsd4_scancpu</code> to distribute load</p> </li> <li> <p>Send wakeup IPI to selected CPU if necessary</p> </li> </ol>"},{"location":"sys/kern/scheduling/#acquirerelease-curproc","title":"Acquire/Release Curproc","text":"<p>Function: <code>bsd4_acquire_curproc()</code> (usched_bsd4.c:330)</p> <p>When returning to userland:</p> <pre><code>1. Remove from tsleep queue if necessary\n2. Recalculate estcpu\n3. Handle user_resched request (release and reselect)\n4. Loop until we become dd-&gt;uschedcp:\n   - Try to steal current designation\n   - Or place on runqueue and switch away\n5. Mark CPU as running user process\n</code></pre> <p>Function: <code>bsd4_release_curproc()</code></p> <p>When entering kernel:</p> <pre><code>1. Clear dd-&gt;uschedcp\n2. Call bsd4_select_curproc() to pick new LWP\n3. Mark CPU as not running user process if no LWP selected\n</code></pre>"},{"location":"sys/kern/scheduling/#scheduler-clock_1","title":"Scheduler Clock","text":"<p>Function: <code>bsd4_schedulerclock()</code></p> <p>Called at ESTCPUFREQ for running LWP:</p> <pre><code>1. Increment estcpu (CPU usage accounting)\n2. Increment rrcount (round-robin counter)\n3. If rrcount &gt;= rrinterval:\n   - Reset rrcount\n   - Request user reschedule (need_user_resched)\n   - Triggers round-robin rotation\n</code></pre>"},{"location":"sys/kern/scheduling/#dfly-scheduler-usched_dflyc","title":"DFLY Scheduler (usched_dfly.c)","text":""},{"location":"sys/kern/scheduling/#overview_3","title":"Overview","text":"<p>The DFLY scheduler is DragonFly's modern, message-based scheduler featuring: - Per-CPU run queues (no global lock on fast path) - Sophisticated load balancing with topology awareness - IPC (Inter-Process Communication) affinity detection - NUMA awareness - Proactive load rebalancing</p> <p>Key advantages: - Better scalability on many-CPU systems - Reduced lock contention (per-CPU spinlocks) - Smarter CPU selection for IPC-heavy workloads - Topology-aware scheduling (cores, packages, NUMA nodes)</p>"},{"location":"sys/kern/scheduling/#data-structures_1","title":"Data Structures","text":"<p>Per-CPU state: <code>struct usched_dfly_pcpu</code> (sys/usched_dfly.h)</p> <pre><code>struct usched_dfly_pcpu {\n    struct spinlock spin;             // Per-CPU lock\n    struct thread   *helper_thread;   // Rebalancing helper\n    u_short         scancpu;          // Next CPU to scan\n    u_short         cpuid;\n    u_short         upri;             // Highest user priority\n    u_short         ucount;           // User thread count\n    u_short         uload;            // Load metric\n    int             rrcount;          // Round-robin counter\n    struct lwp      *uschedcp;        // Current user LWP\n    struct lwp      *old_uschedcp;    \n    cpu_node_t      *cpunode;         // Topology node\n\n    // Run queues (32 per priority class)\n    struct lwp_queue queues[NQS];\n    struct lwp_queue rtqueues[NQS];\n    struct lwp_queue idqueues[NQS];\n    u_int32_t       queuebits;\n    u_int32_t       rtqueuebits;\n    u_int32_t       idqueuebits;\n    u_int32_t       runqcount;\n\n    // IPC affinity tracking\n    cpumask_t       ipimask;          // CPUs to send IPI\n};\n</code></pre>"},{"location":"sys/kern/scheduling/#priority-calculation_1","title":"Priority Calculation","text":"<p>Similar to BSD4, but with additional fairness tuning:</p> <pre><code>pri = PRIBASE_NORMAL + \n      (nice * NICEPPQ) + \n      (estcpu / ESTCPUPPQ) +\n      batch_penalty\n</code></pre> <p>The DFLY scheduler uses more sophisticated estcpu decay and better handles bursty workloads.</p>"},{"location":"sys/kern/scheduling/#cpu-selection-algorithm","title":"CPU Selection Algorithm","text":"<p>Function: <code>dfly_choose_best_queue()</code></p> <p>The heart of DFLY scheduling. Uses a weighted scoring system:</p> <pre><code>For each potential target CPU:\n    score = 0\n\n    // Weight1: Prefer keeping thread on current CPU\n    if (cpu == lp-&gt;lwp_thread-&gt;td_gd-&gt;gd_cpuid)\n        score -= weight1\n\n    // Weight2: IPC affinity (wakefromcpu)\n    // Prefer scheduling near the CPU that last woke us\n    if (topology_allows_ipc_optimization(cpu, wakefromcpu))\n        score -= weight2\n\n    // Weight3: Queue length penalty\n    score += dd-&gt;runqcount * weight3\n\n    // Weight4: Availability (other CPU has lower priority thread)\n    if (dd-&gt;upri &gt; our_priority)\n        score -= weight4\n\n    // Weight5: NUMA node memory weighting\n    score += numa_memory_weight(cpu) * weight5\n\n    // Weight6/7: Transfer hysteresis for stability\n\nSelect CPU with lowest (best) score\n</code></pre> <p>Default weights (usched_dfly.c:280-286): - <code>weight1 = 30</code> - Affinity to current CPU - <code>weight2 = 180</code> - IPC locality (strongest) - <code>weight3 = 10</code> - Queue length - <code>weight4 = 120</code> - CPU availability - <code>weight5 = 50</code> - NUMA preference - <code>weight6 = 0</code> - Rebalance hysteresis - <code>weight7 = -100</code> - Idle pull hysteresis</p>"},{"location":"sys/kern/scheduling/#ipc-affinity-detection","title":"IPC Affinity Detection","text":"<p>Key insight: When thread A wakes thread B, they likely have a producer-consumer relationship. Scheduling B near A reduces cache misses and IPC latency.</p> <p>Tracked via: - <code>td-&gt;td_wakefromcpu</code> - CPU that last woke this thread (set in wakeup) - weight2 heuristic advantages scheduling on nearby CPUs</p> <p>The topology-aware logic considers: - Same logical CPU (hyperthreading sibling) - very strong affinity - Same physical package - strong affinity - Same NUMA node - moderate affinity - Different NUMA nodes - no affinity</p>"},{"location":"sys/kern/scheduling/#load-rebalancing","title":"Load Rebalancing","text":"<p>Function: <code>dfly_choose_worst_queue()</code></p> <p>The helper thread (<code>dfly_pcpu[cpu].helper_thread</code>) periodically rebalances:</p> <pre><code>1. Identify overloaded CPU (worst_queue)\n   - High runqcount relative to others\n\n2. Identify underloaded CPU (best_queue)\n   - Low/zero runqcount\n\n3. Transfer LWP from worst to best\n   - Call dfly_changeqcpu_locked()\n   - Move LWP between per-CPU queues\n\n4. Send IPI to target CPU to schedule the LWP\n</code></pre> <p>Rebalancing features (controlled by <code>usched_dfly_features</code>): - <code>0x01</code> - Idle CPU pulling (default on) - <code>0x02</code> - Proactive pushing (default on) - <code>0x04</code> - Rebalancing rover (default on) - <code>0x08</code> - More aggressive pushing (default on)</p>"},{"location":"sys/kern/scheduling/#acquirerelease-curproc_1","title":"Acquire/Release Curproc","text":"<p>Function: <code>dfly_acquire_curproc()</code> (usched_dfly.c:325)</p> <pre><code>1. Quick path: if already uschedcp and no resched needed, return\n2. Remove from tsleep queue if needed\n3. Recalculate estcpu\n4. Handle user_resched: release and reselect\n5. Loop until dd-&gt;uschedcp == lp:\n   - Check if outcast (CPU affinity violation)\n     - If so, migrate to best CPU via dfly_changeqcpu_locked()\n   - Try to become uschedcp\n   - Or place on runqueue and switch away\n</code></pre> <p>Function: <code>dfly_release_curproc()</code></p> <pre><code>1. Acquire per-CPU spinlock\n2. If we are dd-&gt;uschedcp:\n   - Call dfly_select_curproc() to pick new LWP\n   - Consider local runqueue first\n   - May pull from other CPUs if idle\n3. Release spinlock\n</code></pre>"},{"location":"sys/kern/scheduling/#per-cpu-run-queues","title":"Per-CPU Run Queues","text":"<p>Unlike BSD4's global queues, DFLY maintains separate 32-queue arrays on each CPU. This eliminates global lock contention but requires inter-CPU coordination for load balancing.</p> <p>Trade-off: - Pro: Much better scalability, less contention - Con: Requires active rebalancing to prevent imbalance</p> <p>The helper threads and IPC affinity heuristics work together to keep the system balanced without needing a global view.</p>"},{"location":"sys/kern/scheduling/#fork-behavior","title":"Fork Behavior","text":"<p>Function: <code>dfly_forking()</code></p> <p>When a process forks: - Feature <code>0x20</code> (default): Choose best CPU for child based on IPC affinity - Feature <code>0x40</code>: Keep child on current CPU - Feature <code>0x80</code>: Random CPU assignment</p> <p>The default (<code>0x20</code>) recognizes that fork is often followed by exec (in child) or wait (in parent), creating an IPC relationship. The scheduler tries to place the child near the parent for efficient cache sharing.</p>"},{"location":"sys/kern/scheduling/#dummy-scheduler-usched_dummyc","title":"Dummy Scheduler (usched_dummy.c)","text":""},{"location":"sys/kern/scheduling/#purpose","title":"Purpose","text":"<p>The dummy scheduler is a minimal reference implementation demonstrating the scheduler API. It's not suitable for production but useful for: - Understanding the scheduler interface - Testing scheduler infrastructure - Prototyping new scheduler ideas</p>"},{"location":"sys/kern/scheduling/#design","title":"Design","text":"<ul> <li>Single global run queue (<code>dummy_runq</code>)</li> <li>Global spinlock (<code>dummy_spin</code>)</li> <li>No sophisticated CPU selection</li> <li>No priority calculations</li> <li>Simple FIFO scheduling</li> </ul> <p>Key characteristics: - Acquires first available CPU - Helper thread per CPU to accept work - Round-robin at fixed interval - No load balancing heuristics</p> <p>This simplicity makes it easy to understand the flow of <code>acquire_curproc</code>, <code>release_curproc</code>, <code>setrunqueue</code>, etc., without the complexity of real scheduling policies.</p>"},{"location":"sys/kern/scheduling/#scheduler-clock-and-statistics","title":"Scheduler Clock and Statistics","text":""},{"location":"sys/kern/scheduling/#schedcpu-callout","title":"schedcpu() Callout","text":"<p>Function: <code>schedcpu()</code> (kern_synch.c:203)</p> <p>Called once per second on each CPU to update statistics:</p> <pre><code>1. Scan all processes (allproc_scan):\n   - Increment p_swtime (swap time)\n   - For each LWP:\n     - Increment lwp_slptime if sleeping\n     - Recalculate estcpu (if active or slptime &lt; 2)\n     - Decay pctcpu (percentage CPU)\n     - Call p_usched-&gt;recalculate(lp)\n\n2. Check CPU resource limits (schedcpu_resource):\n   - Sum td_sticks + td_uticks for all threads\n   - Call plimit_testcpulimit()\n   - Send SIGXCPU or kill if limit exceeded\n\n3. Wakeup &amp;lbolt and lbolt_syncer (on CPU 0)\n4. Reschedule callout for next second\n</code></pre>"},{"location":"sys/kern/scheduling/#load-average-calculation","title":"Load Average Calculation","text":"<p>Function: <code>loadav()</code> (kern_synch.c:1405)</p> <p>Called every 5 seconds (with randomization) to compute load averages:</p> <pre><code>1. Scan all LWPs (alllwp_scan)\n   - Count runnable LWPs (LSRUN state, not blocked)\n   - Store count in gd-&gt;gd_loadav_nrunnable\n\n2. On CPU 0:\n   - Sum counts from all CPUs\n   - Update averunnable.ldavg[0..2] (1, 5, 15 minute averages)\n   - Use exponential decay with FSCALE fixed-point math\n</code></pre> <p>The load average represents the average number of runnable threads over different time periods, a key system health metric.</p>"},{"location":"sys/kern/scheduling/#cpu-usage-tracking-estcpu-pctcpu","title":"CPU Usage Tracking (estcpu, pctcpu)","text":"<p>estcpu - Estimated CPU usage: - Incremented each scheduler clock tick when LWP is running - Decayed over time (typically 8/10 per second) - Used to calculate dynamic priority - Reset to parent's estcpu on fork (with optional bias)</p> <p>pctcpu - Percentage CPU: - Short-term CPU usage metric (over last second) - Used by ps(1) to display %CPU - Decayed more rapidly than estcpu - Updated by <code>updatepcpu()</code> when sampled</p>"},{"location":"sys/kern/scheduling/#priority-and-scheduling-classes","title":"Priority and Scheduling Classes","text":""},{"location":"sys/kern/scheduling/#priority-ranges","title":"Priority Ranges","text":"<p>DragonFly uses a unified priority space:</p> <pre><code>0-127:     Realtime (highest)\n128-255:   Normal (timesharing)\n256-383:   Idle\n384-511:   Kernel threads\n512+:      Special/NULL\n</code></pre> <p>Internal representation: - <code>lp-&gt;lwp_priority</code> - Calculated scheduling priority (0-511) - <code>lp-&gt;lwp_rtprio.type</code> - Scheduling class (REALTIME, NORMAL, IDLE, etc.) - <code>lp-&gt;lwp_rtprio.prio</code> - Priority within class - <code>td-&gt;td_upri</code> - LWKT priority (negated for proper ordering)</p>"},{"location":"sys/kern/scheduling/#priority-inversion","title":"Priority Inversion","text":"<p>LWKT priorities have inverted sense (lower number = higher priority) compared to user priorities (higher number = higher priority):</p> <pre><code>td-&gt;td_upri = -lp-&gt;lwp_priority;\n</code></pre> <p>This allows LWKT's queue ordering to work correctly.</p>"},{"location":"sys/kern/scheduling/#real-time-scheduling","title":"Real-time Scheduling","text":"<p>Real-time threads (FIFO and RR) have strict priority: - Always run before normal/idle threads - FIFO runs until it blocks or is preempted by higher-priority RT thread - RR is preempted after a quantum (round-robin interval) by same-priority RT threads</p> <p>Caution: Real-time threads can starve normal threads. Use carefully.</p>"},{"location":"sys/kern/scheduling/#nice-value","title":"Nice Value","text":"<p>The traditional Unix nice value (-20 to +19): - Stored in <code>lp-&gt;lwp_rtprio.prio</code> for NORMAL class - Lower nice = higher priority - Maps to priority via: <code>pri += nice * NICEPPQ</code> - Set via <code>setpriority(2)</code> syscall</p>"},{"location":"sys/kern/scheduling/#best-practices","title":"Best Practices","text":""},{"location":"sys/kern/scheduling/#choosing-a-scheduler","title":"Choosing a Scheduler","text":"<p>Use DFLY (default) when: - Many CPUs (&gt;= 8) - IPC-heavy workloads (e.g., build systems) - NUMA systems - Need good scalability</p> <p>Use BSD4 when: - Few CPUs (&lt;= 4) - Simple workloads - Debugging scheduler issues (simpler code) - Prefer traditional BSD behavior</p>"},{"location":"sys/kern/scheduling/#setting-priorities","title":"Setting Priorities","text":"<p>Real-time priorities: - Use sparingly - can starve normal processes - Suitable for hard real-time control tasks - Ensure RT tasks yield or block regularly - Test thoroughly under load</p> <p>Nice values: - Adjust nice for batch jobs (<code>nice +10</code>) - Use negative nice for interactive/important tasks (requires privilege) - Typical range: -5 to +10</p>"},{"location":"sys/kern/scheduling/#cpu-affinity","title":"CPU Affinity","text":"<p>When to use: - Threads with shared data (keep on nearby CPUs) - Real-time tasks (eliminate migration latency) - NUMA systems (pin to node with memory)</p> <p>When NOT to use: - General workloads (scheduler does better job) - Short-lived processes - When load distribution is important</p>"},{"location":"sys/kern/scheduling/#tunables-and-sysctls","title":"Tunables and Sysctls","text":""},{"location":"sys/kern/scheduling/#bsd4-scheduler","title":"BSD4 Scheduler","text":"<ul> <li><code>kern.usched_bsd4_rrinterval</code> - Round-robin interval (default: 10)</li> <li><code>kern.usched_bsd4_decay</code> - estcpu decay rate (default: 8)</li> <li><code>kern.usched_bsd4_batch_time</code> - Batch process threshold</li> <li><code>kern.usched_bsd4_upri_affinity</code> - Affinity threshold</li> <li><code>debug.bsd4_scdebug</code> - Debug PID</li> </ul>"},{"location":"sys/kern/scheduling/#dfly-scheduler","title":"DFLY Scheduler","text":"<ul> <li><code>kern.usched_dfly_weight1</code> - Current CPU affinity (default: 30)</li> <li><code>kern.usched_dfly_weight2</code> - IPC locality (default: 180)</li> <li><code>kern.usched_dfly_weight3</code> - Queue length penalty (default: 10)</li> <li><code>kern.usched_dfly_weight4</code> - CPU availability (default: 120)</li> <li><code>kern.usched_dfly_weight5</code> - NUMA memory (default: 50)</li> <li><code>kern.usched_dfly_features</code> - Feature flags (default: 0x2f)</li> <li><code>kern.usched_dfly_rrinterval</code> - Round-robin interval (default: 10)</li> <li><code>kern.usched_dfly_decay</code> - estcpu decay (default: 8)</li> <li><code>kern.usched_dfly_forkbias</code> - Fork estcpu bias (default: 1)</li> </ul>"},{"location":"sys/kern/scheduling/#general-scheduling","title":"General Scheduling","text":"<ul> <li><code>kern.pctcpu_decay</code> - pctcpu decay rate (default: 10)</li> <li><code>kern.fscale</code> - Fixed-point scale factor (FSCALE = 2048)</li> <li><code>kern.slpque_tablesize</code> - Sleep queue hash table size</li> </ul>"},{"location":"sys/kern/scheduling/#summary","title":"Summary","text":"<p>DragonFly's scheduling system is a sophisticated two-layer design:</p> <ol> <li>Sleep/wakeup provides efficient thread blocking and synchronization</li> <li>Pluggable user schedulers implement diverse scheduling policies</li> <li>BSD4 offers traditional simplicity for smaller systems</li> <li>DFLY provides advanced scalability and topology awareness for modern hardware</li> <li>Extensive tunables allow customization for specific workloads</li> </ol> <p>The system balances: - Responsiveness vs. overhead - Cache affinity vs. load balance - Simplicity vs. scalability</p> <p>Understanding the scheduler is crucial for: - Performance tuning - Real-time system design - Debugging scheduling issues - Kernel development</p> <p>Key takeaways: - Start with DFLY defaults on multi-CPU systems - Use BSD4 for simplicity on small systems - Reserve realtime priorities for critical tasks - Let the scheduler manage CPU affinity for most workloads - Monitor context switches and load average - Tune weights cautiously based on specific problems</p> <p>The DragonFly schedulers represent years of evolution and optimization, providing excellent out-of-the-box performance while remaining tunable for specialized needs.</p>"},{"location":"sys/kern/signals/","title":"Signals","text":"<p>Source file: <code>kern_sig.c</code> (2,701 lines)</p> <p>This document covers signal management in DragonFly BSD, including signal generation, delivery, and handling. Signals are asynchronous notifications delivered to processes or threads to indicate events such as exceptions, terminal I/O, process control, or user-defined events.</p>"},{"location":"sys/kern/signals/#overview","title":"Overview","text":"<p>DragonFly BSD implements POSIX and BSD signal semantics with extensions for:</p> <ol> <li>Per-thread and per-process signals \u2014 Signals can target specific LWPs or the process generally</li> <li>Signal masking \u2014 Per-thread signal masks control delivery</li> <li>Signal actions \u2014 Processes can catch, ignore, or use default handling</li> <li>Process control signals \u2014 STOP/CONT signals for job control</li> <li>Core dumps \u2014 SA_CORE signals generate core files</li> <li>Real-time signals \u2014 Extended signal numbers beyond traditional POSIX</li> <li>Cross-CPU delivery \u2014 IPI-based notification for signals to remote CPUs</li> </ol> <p>Key data structures: - <code>struct sigacts</code> \u2014 Per-process signal actions and state - <code>sigset_t</code> \u2014 Bit set representing signals (32+ signals) - <code>p-&gt;p_siglist</code> \u2014 Process-wide pending signals - <code>lp-&gt;lwp_siglist</code> \u2014 Thread-specific pending signals - <code>lp-&gt;lwp_sigmask</code> \u2014 Per-thread signal mask</p>"},{"location":"sys/kern/signals/#signal-properties","title":"Signal Properties","text":""},{"location":"sys/kern/signals/#signal-categories","title":"Signal Categories","text":"<p>Source: <code>kern_sig.c:119-194</code></p> <p>Each signal has properties encoded in <code>sigproptbl[]</code>:</p> <ul> <li>SA_KILL (<code>0x01</code>) \u2014 Terminates process by default</li> <li>SA_CORE (<code>0x02</code>) \u2014 Terminates and generates core dump</li> <li>SA_STOP (<code>0x04</code>) \u2014 Stops process execution</li> <li>SA_TTYSTOP (<code>0x08</code>) \u2014 Stop from terminal (SIGTSTP/SIGTTIN/SIGTTOU)</li> <li>SA_IGNORE (<code>0x10</code>) \u2014 Ignored by default</li> <li>SA_CONT (<code>0x20</code>) \u2014 Continues stopped process</li> <li>SA_CANTMASK (<code>0x40</code>) \u2014 Cannot be blocked (SIGKILL/SIGSTOP)</li> <li>SA_CKPT (<code>0x80</code>) \u2014 Checkpoint signal (DragonFly extension)</li> </ul> <p>Examples: - <code>SIGKILL</code> \u2014 SA_KILL (cannot be caught or ignored) - <code>SIGSEGV</code> \u2014 SA_KILL | SA_CORE (core dump on segmentation fault) - <code>SIGSTOP</code> \u2014 SA_STOP | SA_CANTMASK (cannot be blocked) - <code>SIGCHLD</code> \u2014 SA_IGNORE (default is to ignore) - <code>SIGCONT</code> \u2014 SA_IGNORE | SA_CONT (resumes stopped processes)</p>"},{"location":"sys/kern/signals/#unmaskable-signals","title":"Unmaskable Signals","text":"<p>Source: <code>kern_sig.c:196, 417</code></p> <p><code>SIGKILL</code> and <code>SIGSTOP</code> cannot be masked, caught, or ignored:</p> <pre><code>sigset_t sigcantmask_mask;  // Contains SIGKILL and SIGSTOP\nSIG_CANTMASK(mask);         // Macro removes unmaskable signals\n</code></pre> <p>This ensures that processes can always be killed or stopped by administrators.</p>"},{"location":"sys/kern/signals/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/signals/#struct-sigacts-per-process-signal-actions","title":"<code>struct sigacts</code> \u2014 Per-Process Signal Actions","text":"<p>Source: <code>sys/signalvar.h:56-73</code></p> <pre><code>struct sigacts {\n    sig_t ps_sigact[_SIG_MAXSIG];      /* Signal handlers */\n    sigset_t ps_catchmask[_SIG_MAXSIG]; /* Masks during handler */\n    struct {\n        int pid;  /* Originating process PID */\n        int uid;  /* Originating process UID */\n    } ps_frominfo[_SIG_MAXSIG];\n\n    sigset_t ps_sigignore;    /* Signals set to SIG_IGN */\n    sigset_t ps_sigcatch;     /* Signals with custom handlers */\n    sigset_t ps_sigonstack;   /* Use alternate signal stack */\n    sigset_t ps_sigintr;      /* Interrupt syscalls (no SA_RESTART) */\n    sigset_t ps_sigreset;     /* Reset to SIG_DFL after catching (SA_RESETHAND) */\n    sigset_t ps_signodefer;   /* Don't mask signal during handler (SA_NODEFER) */\n    sigset_t ps_siginfo;      /* Use siginfo_t (SA_SIGINFO) */\n\n    unsigned int ps_refcnt;\n    int ps_flag;              /* PS_NOCLDSTOP, PS_NOCLDWAIT, PS_CLDSIGIGN */\n};\n</code></pre> <p>Key fields: - <code>ps_sigact[]</code> \u2014 Array of signal handlers (SIG_DFL, SIG_IGN, or function pointer) - <code>ps_catchmask[]</code> \u2014 Signals to block while handler executes - <code>ps_frominfo[]</code> \u2014 Tracks sender's PID/UID for signal provenance</p> <p>Flags: - <code>PS_NOCLDSTOP</code> \u2014 Don't notify parent of child stop/cont (SA_NOCLDSTOP) - <code>PS_NOCLDWAIT</code> \u2014 Don't create zombies, reap immediately (SA_NOCLDWAIT) - <code>PS_CLDSIGIGN</code> \u2014 SIGCHLD handler is SIG_IGN</p>"},{"location":"sys/kern/signals/#signal-masks-sigset_t","title":"Signal Masks (<code>sigset_t</code>)","text":"<p>Source: <code>sys/signalvar.h:91-150</code></p> <p>Bit set representing 32+ signals:</p> <pre><code>typedef struct {\n    unsigned int __bits[_SIG_WORDS];  /* _SIG_WORDS = 4 for 128 signals */\n} sigset_t;\n</code></pre> <p>Operations (macros): - <code>SIGADDSET(set, sig)</code> \u2014 Add signal to set - <code>SIGDELSET(set, sig)</code> \u2014 Remove signal from set - <code>SIGISMEMBER(set, sig)</code> \u2014 Test membership - <code>SIGEMPTYSET(set)</code> \u2014 Clear all signals - <code>SIGFILLSET(set)</code> \u2014 Set all signals - <code>SIGSETOR(set1, set2)</code> \u2014 Union of sets - <code>SIGSETNAND(set1, set2)</code> \u2014 Remove set2 from set1</p> <p>Atomic variants: - <code>SIGADDSET_ATOMIC(set, sig)</code> \u2014 Uses <code>atomic_set_int()</code> for SMP safety - <code>SIGDELSET_ATOMIC(set, sig)</code> \u2014 Uses <code>atomic_clear_int()</code></p> <p>Process vs. Thread signals: - <code>p-&gt;p_siglist</code> \u2014 Process-wide pending signals (any thread can handle) - <code>lp-&gt;lwp_siglist</code> \u2014 Thread-specific pending signals (only this LWP handles) - <code>lp-&gt;lwp_sigmask</code> \u2014 Per-thread signal mask</p>"},{"location":"sys/kern/signals/#signal-actions-sigaction","title":"Signal Actions (<code>sigaction</code>)","text":""},{"location":"sys/kern/signals/#setting-signal-handlers","title":"Setting Signal Handlers","text":""},{"location":"sys/kern/signals/#kern_sigaction","title":"<code>kern_sigaction</code>","text":"<p>Source: <code>kern_sig.c:248-377</code></p> <p>Set signal disposition (handler, mask, flags):</p> <pre><code>int kern_sigaction(int sig, struct sigaction *act, struct sigaction *oact);\n</code></pre> <p>Validation: 1. Check signal number (<code>1 &lt;= sig &lt; _SIG_MAXSIG</code>) 2. Reject attempts to catch <code>SIGKILL</code> or <code>SIGSTOP</code> 3. Acquire <code>p-&gt;p_token</code> for atomicity</p> <p>Installation: 1. Update <code>ps-&gt;ps_sigact[sig]</code> with handler 2. Update <code>ps-&gt;ps_catchmask[sig]</code> with additional mask 3. Set flags in various <code>ps_sig*</code> masks:    - <code>SA_ONSTACK</code> \u2192 <code>ps_sigonstack</code>    - <code>SA_RESTART</code> \u2192 clear <code>ps_sigintr</code>    - <code>SA_RESETHAND</code> \u2192 <code>ps_sigreset</code>    - <code>SA_NODEFER</code> \u2192 <code>ps_signodefer</code>    - <code>SA_SIGINFO</code> \u2192 <code>ps_siginfo</code> 4. Handle <code>SIGCHLD</code> special flags (SA_NOCLDSTOP, SA_NOCLDWAIT)</p> <p>Signal state updates: - If action is <code>SIG_IGN</code> or default-ignore \u2192 remove from pending signals - Update <code>p_sigignore</code> and <code>p_sigcatch</code> sets accordingly - Iterate all LWPs to clear pending instances</p> <p>PID 1 restriction: Process 1 (init) cannot set SA_NOCLDWAIT to prevent zombie accumulation.</p>"},{"location":"sys/kern/signals/#signal-initialization","title":"Signal Initialization","text":""},{"location":"sys/kern/signals/#siginit-process-0-setup","title":"<code>siginit</code> \u2014 Process 0 Setup","text":"<p>Source: <code>kern_sig.c:404-418</code></p> <p>Called during kernel bootstrap:</p> <ol> <li>Initialize <code>p_sigignore</code> with default-ignored signals</li> <li>Set global <code>sigcantmask_mask</code> (SIGKILL + SIGSTOP)</li> </ol>"},{"location":"sys/kern/signals/#execsigs-reset-on-exec","title":"<code>execsigs</code> \u2014 Reset on Exec","text":"<p>Source: <code>kern_sig.c:423-464</code></p> <p>Called by <code>execve()</code> to reset signal state:</p> <ol> <li>Reset all caught signals to SIG_DFL</li> <li>Clear signal stack (reset to user stack)</li> <li>Clear SA_NOCLDWAIT and SA_NOCLDSTOP flags</li> <li>Reset SIGCHLD to SIG_DFL if ignored</li> </ol> <p>Rationale: Exec'd program starts with clean signal state (caught signals don't persist across exec).</p>"},{"location":"sys/kern/signals/#signal-masking","title":"Signal Masking","text":""},{"location":"sys/kern/signals/#kern_sigprocmask","title":"<code>kern_sigprocmask</code>","text":"<p>Source: <code>kern_sig.c:472-509</code></p> <p>Modify thread's signal mask:</p> <pre><code>int kern_sigprocmask(int how, sigset_t *set, sigset_t *oset);\n</code></pre> <p>Operations: - <code>SIG_BLOCK</code> \u2014 Add signals to mask: <code>lwp_sigmask |= *set</code> - <code>SIG_UNBLOCK</code> \u2014 Remove signals from mask: <code>lwp_sigmask &amp;= ~*set</code> - <code>SIG_SETMASK</code> \u2014 Replace mask: <code>lwp_sigmask = *set</code></p> <p>Unmaskable enforcement: <code>SIG_CANTMASK()</code> macro always removes SIGKILL/SIGSTOP.</p> <p>Interlock: After <code>SIG_SETMASK</code>, calls <code>sigirefs_wait()</code> to synchronize with concurrent signal delivery (prevents races with <code>sigsuspend()</code>/<code>ppoll()</code>/<code>pselect()</code>).</p>"},{"location":"sys/kern/signals/#kern_sigpending","title":"<code>kern_sigpending</code>","text":"<p>Source: <code>kern_sig.c:541-548</code></p> <p>Return set of pending signals:</p> <pre><code>int kern_sigpending(sigset_t *set);\n</code></pre> <p>Returns union of process and thread pending signals:</p> <pre><code>*set = lwp_sigpend(lp);  // p-&gt;p_siglist | lp-&gt;lwp_siglist\n</code></pre>"},{"location":"sys/kern/signals/#kern_sigsuspend","title":"<code>kern_sigsuspend</code>","text":"<p>Source: <code>kern_sig.c:573-604</code></p> <p>Atomically change mask and sleep until signal:</p> <pre><code>int kern_sigsuspend(sigset_t *set);\n</code></pre> <p>Algorithm: 1. Save old mask in <code>lp-&gt;lwp_oldsigmask</code> 2. Set <code>LWP_OLDMASK</code> flag (indicates sigsuspend in progress) 3. Acquire <code>lp-&gt;lwp_token</code> (interlocks signal delivery) 4. Set temporary mask: <code>lp-&gt;lwp_sigmask = *set</code> 5. Release token and call <code>sigirefs_wait()</code> to synchronize 6. Sleep in <code>tsleep(ps, PCATCH, \"pause\", 0)</code> 7. Wake on signal delivery, mask restored in <code>postsig()</code></p> <p>Interlock: Holding <code>lp-&gt;lwp_token</code> during mask change ensures signal delivery sees consistent state.</p>"},{"location":"sys/kern/signals/#signal-generation","title":"Signal Generation","text":""},{"location":"sys/kern/signals/#kern_kill-send-signal-to-processthread","title":"<code>kern_kill</code> \u2014 Send Signal to Process/Thread","text":"<p>Source: <code>kern_sig.c:748-857</code></p> <p>Entry point for <code>kill()</code> and <code>lwp_kill()</code> system calls:</p> <pre><code>int kern_kill(int sig, pid_t pid, lwpt_t tid);\n</code></pre> <p>Target selection: - <code>pid &gt; 0, tid == -1</code> \u2014 Signal process (any thread) - <code>pid &gt; 0, tid &gt;= 0</code> \u2014 Signal specific thread - <code>pid == 0</code> \u2014 Signal own process group - <code>pid == -1</code> \u2014 Broadcast to all processes (privileged) - <code>pid &lt; -1</code> \u2014 Signal process group <code>-pid</code></p> <p>Permission check: - <code>CANSIGIO()</code> macro: root or matching UID - <code>p_trespass()</code> for cross-process signals</p> <p>Delivery: - Calls <code>lwpsignal(p, lp, sig)</code> with target process/thread</p>"},{"location":"sys/kern/signals/#ksignal-and-lwpsignal-core-signal-delivery","title":"<code>ksignal</code> and <code>lwpsignal</code> \u2014 Core Signal Delivery","text":"<p>Source: <code>kern_sig.c:1115-1459</code></p> <p>The heart of signal delivery:</p> <pre><code>void ksignal(struct proc *p, int sig);              // Generic process signal\nvoid lwpsignal(struct proc *p, struct lwp *lp, int sig); // Specific or auto-select LWP\n</code></pre> <p>Target selection (if <code>lp == NULL</code>): 1. Check if current preempted thread belongs to <code>p</code> and doesn't mask signal 2. Otherwise call <code>find_lwp_for_signal()</code>:    - Prefer LSRUN (running) threads    - Then LSSLEEP (sleeping with LWP_SINTR)    - Finally LSSTOP (stopped) threads 3. Returns LWP with token held, or NULL if all threads mask signal</p> <p>Signal processing:</p> <ol> <li>Ignored signals:</li> <li>If <code>SIGISMEMBER(p_sigignore, sig)</code> \u2192 discard (unless P_TRACED)</li> <li> <p>Still notify kqueue: <code>KNOTE(&amp;p-&gt;p_klist, NOTE_SIGNAL | sig)</code></p> </li> <li> <p>Continue signals (SA_CONT):</p> </li> <li>Clear all pending STOP signals: <code>SIG_STOPSIGMASK_ATOMIC(p-&gt;p_siglist)</code></li> <li> <p>If process stopped \u2192 call <code>proc_unstop()</code> \u2192 wake all threads</p> </li> <li> <p>Stop signals (SA_STOP):</p> </li> <li>Clear all pending CONT signals: <code>SIG_CONTSIGMASK_ATOMIC(p-&gt;p_siglist)</code></li> <li>TTY stop signals ignored if orphaned process group</li> <li> <p>If default action \u2192 call <code>proc_stop()</code> to stop process</p> </li> <li> <p>Process stopped (p-&gt;p_stat == SSTOP):</p> </li> <li>Add signal to pending list but don't wake (unless SIGKILL or SIGCONT)</li> <li>SIGKILL \u2192 call <code>proc_unstop()</code> to make runnable</li> <li> <p>SIGCONT \u2192 continue process and optionally notify parent</p> </li> <li> <p>Active process:</p> </li> <li>Find suitable LWP (if not already specified)</li> <li>Add signal to <code>lp-&gt;lwp_siglist</code> (thread-specific)</li> <li>Call <code>lwp_signotify(lp)</code> to wake/interrupt thread</li> </ol>"},{"location":"sys/kern/signals/#lwp_signotify-wake-thread-for-signal","title":"<code>lwp_signotify</code> \u2014 Wake Thread for Signal","text":"<p>Source: <code>kern_sig.c:1482-1548</code></p> <p>Notify LWP that signal has arrived:</p> <p>Cases: 1. Preempted on current CPU:    - Call <code>signotify()</code> (sets TDF_SIGPENDING, will check on return to userland)</p> <ol> <li>Sleeping with LWP_SINTR:</li> <li>Thread in <code>tsleep()</code> with <code>PCATCH</code> flag</li> <li>If local CPU \u2192 call <code>setrunnable(lp)</code></li> <li> <p>If remote CPU \u2192 send IPI via <code>lwkt_send_ipiq()</code></p> </li> <li> <p>Sleeping with TDF_SINTR:</p> </li> <li>Thread in <code>lwkt_sleep()</code> with <code>PCATCH</code></li> <li> <p>Same local/remote logic as above</p> </li> <li> <p>Running in userland:</p> </li> <li>Send IPI to remote CPU to knock thread into kernel</li> <li>IPI handler (<code>lwp_signotify_remote</code>) calls <code>signotify()</code></li> </ol> <p>IPI handling: IPIs forward <code>LWPHOLD()</code> reference to target CPU if thread migrates.</p>"},{"location":"sys/kern/signals/#find_lwp_for_signal","title":"<code>find_lwp_for_signal</code>","text":"<p>Source: <code>kern_sig.c:996-1095</code></p> <p>Select best LWP to receive signal:</p> <p>Priority: 1. Current preempted thread (if doesn't mask signal) \u2014 avoids context switch 2. LSRUN thread \u2014 already running, will return to userland soon 3. LSSLEEP thread with LWP_SINTR \u2014 can be interrupted 4. LSSTOP thread \u2014 stopped, will check signal when resumed</p> <p>Locking: Returns LWP with <code>lwp_token</code> held and <code>LWPHOLD()</code> reference.</p>"},{"location":"sys/kern/signals/#signal-delivery-path","title":"Signal Delivery Path","text":""},{"location":"sys/kern/signals/#process-control-signals","title":"Process Control Signals","text":""},{"location":"sys/kern/signals/#proc_stop-stop-all-threads","title":"<code>proc_stop</code> \u2014 Stop All Threads","text":"<p>Source: <code>kern_sig.c:1584-1660</code></p> <p>Called when stop signal arrives (default action):</p> <pre><code>void proc_stop(struct proc *p, int stat);  // stat = SSTOP or SCORE\n</code></pre> <p>Algorithm: 1. Set <code>p-&gt;p_stat = SSTOP</code> (or SCORE for coredump) 2. For each LWP:    - LSSTOP: Already stopped, no action    - LSSLEEP: Set <code>LWP_MP_WSTOP</code>, increment <code>p-&gt;p_nstopped</code>    - LSRUN: Call <code>lwp_signotify()</code> to interrupt 3. If all threads stopped (<code>p-&gt;p_nstopped == p-&gt;p_nthreads</code>):    - Clear <code>P_WAITED</code> flag    - Wake parent: <code>wakeup(parent)</code>    - Send <code>SIGCHLD</code> (unless PS_NOCLDSTOP set)</p> <p>SCORE state: Special state for coredump \u2014 cannot be overridden by SIGCONT until coredump completes.</p> <p>LWP_MP_WSTOP flag: Prevents sleeping threads from incrementing <code>p-&gt;p_nstopped</code> again when they reach <code>tstop()</code>.</p>"},{"location":"sys/kern/signals/#proc_unstop-resume-all-threads","title":"<code>proc_unstop</code> \u2014 Resume All Threads","text":"<p>Source: <code>kern_sig.c:1666-1728</code></p> <p>Called when SIGCONT arrives or SIGKILL overrides stop:</p> <pre><code>void proc_unstop(struct proc *p, int stat);\n</code></pre> <p>Algorithm: 1. Verify <code>p-&gt;p_stat == stat</code> (SSTOP or SCORE) 2. Set <code>p-&gt;p_stat = SACTIVE</code> 3. For each LWP:    - LSRUN: Already running (unexpected but allowed)    - LSSLEEP: Clear <code>LWP_MP_WSTOP</code>, call <code>setrunnable()</code>    - LSSTOP: Call <code>setrunnable()</code> to wake thread</p> <p>Effect: All threads transition from stopped to runnable, will return to userland or continue execution.</p>"},{"location":"sys/kern/signals/#proc_stopwait-wait-for-all-threads-to-stop","title":"<code>proc_stopwait</code> \u2014 Wait for All Threads to Stop","text":"<p>Source: <code>kern_sig.c:1731-1748</code></p> <p>Used before coredump to ensure all threads fully stopped:</p> <pre><code>while (p-&gt;p_nstopped &lt; p-&gt;p_nthreads) {\n    tsleep(&amp;p-&gt;p_nstopped, 0, \"stopwait\", 1);\n}\n</code></pre> <p>Polls until <code>p-&gt;p_nstopped</code> reaches <code>p-&gt;p_nthreads</code>.</p>"},{"location":"sys/kern/signals/#signal-dispatch","title":"Signal Dispatch","text":""},{"location":"sys/kern/signals/#issignal-check-for-pending-signal","title":"<code>issignal</code> \u2014 Check for Pending Signal","text":"<p>Source: <code>kern_sig.c:1979-2248</code></p> <p>Called via <code>CURSIG()</code> macro to check if signal should be delivered:</p> <pre><code>int issignal(struct lwp *lp, int maytrace, int *ptokp);\n</code></pre> <p>Returns: Signal number to handle, or 0 if none.</p> <p>Algorithm:</p> <ol> <li>Quick check without token:</li> <li>Compute <code>mask = lwp_sigpend(lp) &amp; ~lwp_sigmask</code></li> <li>Remove stop signals if <code>P_PPWAIT</code> (vfork parent)</li> <li> <p>If empty \u2192 return 0 (no signal)</p> </li> <li> <p>Acquire token if signal in process list:</p> </li> <li> <p>Recheck mask with token held (double-check pattern)</p> </li> <li> <p>Handle ignored signals:</p> </li> <li> <p>If <code>SIGISMEMBER(p_sigignore, sig)</code> \u2192 delete and continue</p> </li> <li> <p>Tracing (P_TRACED):</p> </li> <li>Stop process: <code>proc_stop(p, SSTOP)</code></li> <li>Call <code>tstop()</code> to block thread</li> <li>Parent can modify signal via ptrace: <code>p-&gt;p_xstat</code></li> <li> <p>If parent clears signal \u2192 continue loop</p> </li> <li> <p>Determine action:</p> </li> <li><code>SIG_DFL</code> \u2014 Default action (check properties)</li> <li><code>SIG_IGN</code> \u2014 Ignored (shouldn't happen, but continue)</li> <li> <p>Custom handler \u2014 return signal number</p> </li> <li> <p>Default action handling:</p> </li> <li>SA_CKPT: Call <code>checkpoint_signal_handler()</code> (DragonFly checkpoint)</li> <li>SA_STOP: Call <code>proc_stop()</code>, delete signal, continue loop</li> <li>SA_IGNORE: Delete signal, continue loop (e.g., SIGCONT)</li> <li> <p>SA_KILL/SA_CORE: Return signal for <code>postsig()</code> to handle</p> </li> <li> <p>Delete signal from pending lists:</p> </li> <li><code>lwp_delsig(lp, sig, haveptok)</code> removes from both <code>p_siglist</code> and <code>lwp_siglist</code></li> </ol> <p>Token management: Carefully tracks whether <code>p-&gt;p_token</code> is held via <code>haveptok</code> flag, returns ownership via <code>ptokp</code> pointer.</p>"},{"location":"sys/kern/signals/#postsig-execute-signal-action","title":"<code>postsig</code> \u2014 Execute Signal Action","text":"<p>Source: <code>kern_sig.c:2260-2360</code></p> <p>Deliver signal to handler or terminate process:</p> <pre><code>void postsig(int sig, int haveptok);\n</code></pre> <p>Called from: Trap handler or syscall return path (userret).</p> <p>Actions:</p> <ol> <li> <p>Virtual kernel: If in vkernel context, switch back to kernel context</p> </li> <li> <p>Delete signal: <code>lwp_delsig(lp, sig, haveptok)</code></p> </li> <li> <p>Notify kqueue: <code>KNOTE(&amp;p-&gt;p_klist, NOTE_SIGNAL | sig)</code></p> </li> <li> <p>Default action (SIG_DFL):</p> </li> <li>Call <code>sigexit(lp, sig)</code> to terminate process</li> <li>Generates core dump if SA_CORE property</li> <li> <p>Does not return</p> </li> <li> <p>Custom handler:</p> </li> <li>Determine mask to restore after handler:<ul> <li>If <code>LWP_OLDMASK</code> set (from sigsuspend) \u2192 use <code>lwp_oldsigmask</code></li> <li>Otherwise \u2192 use current <code>lwp_sigmask</code></li> </ul> </li> <li>Block additional signals: <code>SIGSETOR(lwp_sigmask, ps_catchmask[sig])</code></li> <li>Unless SA_NODEFER \u2192 also block signal itself</li> <li>Reset handler to SIG_DFL if SA_RESETHAND</li> <li>Call platform-specific sendsig: <code>(*sv_sendsig)(action, sig, &amp;returnmask, code)</code></li> <li>Increment <code>ru_nsignals</code> counter</li> </ol> <p>Machine-dependent sendsig: - Sets up signal trampoline on user stack - Modifies user registers to call handler - Stores return mask for <code>sigreturn()</code> syscall</p>"},{"location":"sys/kern/signals/#special-signal-handling","title":"Special Signal Handling","text":""},{"location":"sys/kern/signals/#trapsignal-synchronous-signal-from-trap","title":"<code>trapsignal</code> \u2014 Synchronous Signal from Trap","text":"<p>Source: <code>kern_sig.c:940-985</code></p> <p>Deliver signal caused by trap (e.g., SIGSEGV, SIGFPE):</p> <pre><code>void trapsignal(struct lwp *lp, int sig, u_long code);\n</code></pre> <p>Difference from regular signals: These signals MUST be delivered to the specific LWP that caused the trap (never delivered generically to process).</p> <p>Fast path: If signal is caught and not masked: - Immediately call <code>(*sv_sendsig)()</code> to invoke handler - No need to make signal pending - Update signal mask and apply SA_RESETHAND/SA_NODEFER</p> <p>Slow path: Otherwise call <code>lwpsignal(p, lp, sig)</code> to queue signal.</p> <p>Virtual kernel: If in vkernel emulation, switch back to vkernel context before delivering.</p>"},{"location":"sys/kern/signals/#sigexit-terminate-with-signal","title":"<code>sigexit</code> \u2014 Terminate with Signal","text":"<p>Source: <code>kern_sig.c:2384-2430</code></p> <p>Force process exit due to signal:</p> <pre><code>void sigexit(struct lwp *lp, int sig);\n</code></pre> <p>Steps:</p> <ol> <li> <p>Set <code>p-&gt;p_acflag |= AXSIG</code> (accounting: terminated by signal)</p> </li> <li> <p>Core dump (SA_CORE signals):</p> </li> <li>Stop all threads: <code>proc_stop(p, SCORE)</code> + <code>proc_stopwait(p)</code></li> <li>Call <code>coredump(lp, sig)</code> to write core file</li> <li>If successful \u2192 set <code>WCOREFLAG</code> in exit status</li> <li> <p>Log message: <code>\"pid %d (%s), uid %d: exited on signal %d (core dumped)\"</code></p> </li> <li> <p>Exit: Call <code>exit1(W_EXITCODE(0, sig))</code> \u2014 does not return</p> </li> </ol> <p>Sysctl: <code>kern.logsigexit</code> controls logging (default 1).</p>"},{"location":"sys/kern/signals/#coredump-generate-core-file","title":"<code>coredump</code> \u2014 Generate Core File","text":"<p>Source: <code>kern_sig.c:2515-2678</code></p> <p>Write core dump to filesystem:</p> <pre><code>static int coredump(struct lwp *lp, int sig);\n</code></pre> <p>Algorithm:</p> <ol> <li>Check permissions:</li> <li>If sugid process \u2192 check <code>kern.sugid_coredump</code> sysctl</li> <li> <p>If disabled globally \u2192 check <code>kern.coredump</code> sysctl</p> </li> <li> <p>Expand core filename:</p> </li> <li>Default: <code>\"%N.core\"</code> (process name + \".core\")</li> <li>Supports format specifiers: <code>%N</code> (name), <code>%P</code> (pid), <code>%U</code> (uid)</li> <li> <p>Example: <code>\"/cores/%U/%N-%P\"</code> \u2192 <code>/cores/1000/myapp-1234</code></p> </li> <li> <p>Create core file:</p> </li> <li>Call <code>nlookup_init_at()</code> with expanded path</li> <li>Open with <code>O_CREAT | O_TRUNC | O_NOFOLLOW</code></li> <li> <p>Prevent following symlinks (security)</p> </li> <li> <p>Write core:</p> </li> <li>Call <code>(*p-&gt;p_sysent-&gt;sv_coredump)(lp, sig, vp, limit)</code></li> <li>Platform-specific function writes ELF core format</li> <li> <p>Includes registers, memory segments, thread info</p> </li> <li> <p>Cleanup:</p> </li> <li>Close file</li> <li>Release vnode</li> </ol> <p>Security: Setuid/setgid programs don't dump core by default (prevents password/key leakage).</p>"},{"location":"sys/kern/signals/#process-group-signals","title":"Process Group Signals","text":""},{"location":"sys/kern/signals/#pgsignal-signal-process-group","title":"<code>pgsignal</code> \u2014 Signal Process Group","text":"<p>Source: <code>kern_sig.c:911-929</code></p> <p>Send signal to all members of process group:</p> <pre><code>void pgsignal(struct pgrp *pgrp, int sig, int checkctty);\n</code></pre> <p>Parameters: - <code>pgrp</code> \u2014 Process group structure - <code>sig</code> \u2014 Signal number - <code>checkctty</code> \u2014 If 1, only signal processes with controlling terminal</p> <p>Algorithm: 1. Acquire <code>pgrp-&gt;pg_lock</code> (prevents concurrent fork from missing signal) 2. Iterate <code>pgrp-&gt;pg_members</code> list 3. For each process:    - If <code>checkctty == 0</code> or <code>p-&gt;p_flags &amp; P_CONTROLT</code> \u2192 call <code>ksignal(p, sig)</code></p> <p>Locking: Process group lock ensures that processes forking during signal delivery either: - See the lock and wait \u2192 receive signal after fork completes - Complete fork before lock \u2192 child added to group and receives signal</p>"},{"location":"sys/kern/signals/#wait-for-signal","title":"Wait for Signal","text":""},{"location":"sys/kern/signals/#kern_sigtimedwait-wait-for-specific-signals","title":"<code>kern_sigtimedwait</code> \u2014 Wait for Specific Signals","text":"<p>Source: <code>kern_sig.c:1754-1866</code></p> <p>Wait for signal from specified set (with optional timeout):</p> <pre><code>static int kern_sigtimedwait(sigset_t waitset, siginfo_t *info,\n                               struct timespec *timeout);\n</code></pre> <p>Used by: <code>sigwaitinfo()</code>, <code>sigtimedwait()</code> system calls.</p> <p>Algorithm:</p> <ol> <li>Save current signal mask</li> <li>Compute timeout deadline (if specified)</li> <li>Loop:</li> <li>Check if any signal in <code>waitset</code> is pending: <code>set = lwp_sigpend(lp) &amp; waitset</code></li> <li>If signal found:<ul> <li>Temporarily fill signal mask to block all signals</li> <li>Call <code>issignal(lp, 1, NULL)</code> to process signal</li> <li>If SIGSTOP \u2192 may return 0, retry</li> <li>Delete signal from pending list</li> <li>Return signal number</li> </ul> </li> <li>If no signal and timeout expired \u2192 return EAGAIN</li> <li> <p>Otherwise:</p> <ul> <li>Block all signals in <code>waitset</code>: <code>lwp_sigmask &amp;= ~waitset</code></li> <li>Call <code>sigirefs_wait()</code> to synchronize</li> <li>Sleep: <code>tsleep(&amp;p-&gt;p_sigacts, PCATCH, \"sigwt\", hz)</code></li> <li>Wake on signal delivery (broken by <code>lwpsignal()</code>)</li> <li>Retry loop</li> </ul> </li> <li> <p>Restore original signal mask</p> </li> </ol> <p>Return: Signal number in <code>info-&gt;si_signo</code>, or error code.</p> <p>Note: Signal is consumed (removed from pending list), unlike normal signal delivery which queues for handler.</p>"},{"location":"sys/kern/signals/#kqueue-integration","title":"Kqueue Integration","text":""},{"location":"sys/kern/signals/#filt_sigattachfilt_signal","title":"<code>filt_sigattach</code>/<code>filt_signal</code>","text":"<p>Source: <code>kern_sig.c:84-89, 2562-2614</code></p> <p>Kqueue filter for signal notification:</p> <pre><code>struct filterops sig_filtops = {\n    FILTEROP_MPSAFE,\n    filt_sigattach,\n    filt_sigdetach,\n    filt_signal\n};\n</code></pre> <p>Attach: Register knote on process: <code>KNOTE_INSERT(&amp;p-&gt;p_klist, kn)</code></p> <p>Signal: When signal arrives, <code>lwpsignal()</code> calls: <pre><code>KNOTE(&amp;p-&gt;p_klist, NOTE_SIGNAL | sig);\n</code></pre></p> <p>This wakes any threads waiting in <code>kevent()</code> for signal notification.</p> <p>Use case: Alternative to <code>sigwait()</code> \u2014 allows waiting for signals via kqueue instead of signal-specific APIs.</p>"},{"location":"sys/kern/signals/#signal-interlock-mechanism","title":"Signal Interlock Mechanism","text":""},{"location":"sys/kern/signals/#sigirefs-signal-delivery-interlock","title":"<code>sigirefs</code> \u2014 Signal Delivery Interlock","text":"<p>Source: (referenced throughout <code>kern_sig.c</code>)</p> <p>Prevents races between signal delivery and operations that change the signal mask (sigsuspend, ppoll, pselect):</p> <pre><code>void sigirefs_hold(struct proc *p);     // Increment p-&gt;p_sigirefs\nvoid sigirefs_drop(struct proc *p);     // Decrement p-&gt;p_sigirefs\nvoid sigirefs_wait(struct proc *p);     // Wait for p-&gt;p_sigirefs == 0\n</code></pre> <p>Problem: Without interlock: 1. Thread calls <code>sigsuspend()</code>, changes mask, about to sleep 2. Signal arrives, sees old mask, delivered to process list instead of LWP 3. Thread sleeps, signal not delivered to correct LWP</p> <p>Solution: 1. <code>sigsuspend()</code> calls <code>sigirefs_wait()</code> before sleeping \u2192 waits for pending signal delivery to complete 2. <code>lwpsignal()</code> calls <code>sigirefs_hold()</code> before checking mask \u2192 prevents mask changes during delivery 3. After delivery, <code>sigirefs_drop()</code> releases hold</p> <p>This ensures mask changes and signal delivery don't race.</p>"},{"location":"sys/kern/signals/#syscall-signal-interruption","title":"Syscall Signal Interruption","text":""},{"location":"sys/kern/signals/#iscaught-check-for-interrupting-signal","title":"<code>iscaught</code> \u2014 Check for Interrupting Signal","text":"<p>Source: <code>kern_sig.c:1948-1962</code></p> <p>Check if system call should be interrupted:</p> <pre><code>int iscaught(struct lwp *lp);\n</code></pre> <p>Returns: - <code>EINTR</code> \u2014 Signal interrupts syscall (signal in <code>ps_sigintr</code>) - <code>ERESTART</code> \u2014 Syscall should be restarted (signal not in <code>ps_sigintr</code>, i.e., SA_RESTART) - <code>EWOULDBLOCK</code> \u2014 No signal pending</p> <p>Used by: Long-running syscalls (read, write, sleep) to check for pending signals.</p> <p>Flow: 1. Call <code>CURSIG(lp)</code> \u2192 <code>issignal()</code> 2. If signal found \u2192 check if in <code>ps_sigintr</code> set 3. Return appropriate error code 4. Syscall code checks error and either aborts or restarts</p>"},{"location":"sys/kern/signals/#dragonfly-specific-extensions","title":"DragonFly-Specific Extensions","text":""},{"location":"sys/kern/signals/#checkpoint-signals","title":"Checkpoint Signals","text":"<p>Source: <code>kern_sig.c:162-163</code></p> <p>DragonFly provides checkpoint/resume functionality via signals:</p> <ul> <li>SIGCKPT (<code>32</code>) \u2014 SA_CKPT property, triggers <code>checkpoint_signal_handler()</code></li> <li>SIGCKPTEXIT (<code>33</code>) \u2014 SA_CKPT | SA_KILL, checkpoint and exit</li> </ul> <p>Use case: Save process state to disk, allowing resume later or on different machine.</p>"},{"location":"sys/kern/signals/#cross-cpu-signal-delivery","title":"Cross-CPU Signal Delivery","text":"<p>DragonFly's LWKT threading and per-CPU design requires IPI-based signal notification:</p> <ul> <li><code>lwp_signotify()</code> checks if target LWP is on remote CPU</li> <li>Sends IPI via <code>lwkt_send_ipiq(gd, lwp_signotify_remote, lp)</code></li> <li>Remote CPU calls <code>signotify()</code> or <code>lwkt_schedule()</code> to wake thread</li> </ul> <p>Forwarding: If thread migrates to another CPU before IPI delivery, IPI is forwarded again (with LWPHOLD reference).</p>"},{"location":"sys/kern/signals/#thread-specific-signals","title":"Thread-Specific Signals","text":"<p><code>lwp_kill()</code> syscall allows targeting specific thread (LWP):</p> <pre><code>int lwp_kill(pid_t pid, lwpt_t tid, int sig);\n</code></pre> <p>Restriction: <code>tid</code> cannot be -1 (use <code>kill()</code> for process signals).</p> <p>Use case: Send signal to specific thread in multi-threaded process (e.g., sampling profiler).</p>"},{"location":"sys/kern/signals/#interaction-with-other-subsystems","title":"Interaction with Other Subsystems","text":""},{"location":"sys/kern/signals/#signal-and-fork","title":"Signal and Fork","text":"<p>Source: <code>kern_fork.c</code></p> <p>On <code>fork()</code>: 1. Child inherits <code>p_sigacts</code> (shared, refcounted) 2. Child's <code>p_siglist</code> cleared (no pending signals) 3. Child's <code>lp-&gt;lwp_siglist</code> cleared 4. Child's <code>lp-&gt;lwp_sigmask</code> inherited from parent</p> <p>On <code>rfork(RFPROC)</code>: - Same as fork</p> <p>On <code>rfork(RFTHREAD)</code>: - <code>p_sigacts</code> shared with parent - <code>lwp_sigmask</code> inherited - Signals can target specific thread</p>"},{"location":"sys/kern/signals/#signal-and-exec","title":"Signal and Exec","text":"<p>Source: <code>kern_exec.c</code>, calls <code>execsigs()</code></p> <p>On <code>execve()</code>: 1. Reset all caught signals to SIG_DFL 2. Held signals remain held (preserved in <code>lwp_sigmask</code>) 3. Reset signal stack to user stack 4. Clear SA_NOCLDWAIT / SA_NOCLDSTOP 5. Reset SIGCHLD to SIG_DFL if ignored</p> <p>Rationale: New program image shouldn't inherit old program's signal handlers.</p>"},{"location":"sys/kern/signals/#signal-and-exit","title":"Signal and Exit","text":"<p>Source: <code>kern_exit.c</code></p> <p>On <code>exit()</code>: 1. Parent receives SIGCHLD (unless PS_NOCLDSTOP or process stopped) 2. If SA_NOCLDWAIT set \u2192 child immediately reaped (no zombie) 3. Otherwise \u2192 child becomes zombie, <code>wait()</code> retrieves exit status 4. If signal caused exit \u2192 <code>p-&gt;p_xstat</code> contains signal number | WCOREFLAG</p>"},{"location":"sys/kern/signals/#signal-and-ptrace","title":"Signal and Ptrace","text":"<p>Source: <code>kern_sig.c:2060-2130</code></p> <p>When process traced (P_TRACED): 1. <code>issignal()</code> stops process on every signal 2. Parent debugger notified 3. Debugger can:    - Inspect signal number in <code>p-&gt;p_xstat</code>    - Modify signal (change <code>p-&gt;p_xstat</code>)    - Clear signal (set <code>p-&gt;p_xstat = 0</code>)    - Continue process with <code>PTRACE_CONT</code></p> <p>Use case: Debuggers intercept signals for single-stepping, breakpoints, etc.</p>"},{"location":"sys/kern/signals/#key-algorithms","title":"Key Algorithms","text":""},{"location":"sys/kern/signals/#signal-delivery-decision-tree","title":"Signal Delivery Decision Tree","text":"<pre><code>lwpsignal(p, lp, sig):\n    1. Is signal ignored (p_sigignore)?\n       \u2192 Yes: Discard (notify kqueue)\n       \u2192 No: Continue\n\n    2. Is signal a CONT (SA_CONT)?\n       \u2192 Yes: Clear pending STOP signals\n       \u2192 If process stopped: unstop process\n       \u2192 Continue\n\n    3. Is signal a STOP (SA_STOP)?\n       \u2192 Yes: Clear pending CONT signals\n       \u2192 If default action: stop process, done\n       \u2192 Otherwise: Continue\n\n    4. Is process already stopped (SSTOP)?\n       \u2192 Yes: Add signal to pending list\n       \u2192 If SIGKILL: unstop process\n       \u2192 If SIGCONT and caught: unstop process\n       \u2192 Otherwise: done\n\n    5. Find target LWP:\n       \u2192 If lp == NULL: find_lwp_for_signal()\n       \u2192 If all LWPs mask signal: deliver to process\n\n    6. Add signal to lwp_siglist\n    7. Call lwp_signotify(lp) to wake thread\n</code></pre>"},{"location":"sys/kern/signals/#signal-dispatch-loop","title":"Signal Dispatch Loop","text":"<pre><code>User process trap/syscall return:\n    while (sig = CURSIG(lp)):\n        postsig(sig):\n            if action == SIG_DFL:\n                sigexit(lp, sig)  // Does not return\n            else:\n                Setup signal trampoline\n                Call (*sv_sendsig)(action, sig, &amp;mask, code)\n                Return to userland at handler\n</code></pre> <p>Handler executes, calls <code>sigreturn()</code>, kernel restores original mask and PC, returns to interrupted code.</p>"},{"location":"sys/kern/signals/#performance-considerations","title":"Performance Considerations","text":""},{"location":"sys/kern/signals/#signal-mask-operations","title":"Signal Mask Operations","text":"<p>Signal masks use 128-bit sets (_SIG_WORDS = 4), operations are: - <code>SIGADDSET</code> / <code>SIGDELSET</code> \u2014 O(1) bit operations - <code>SIGEMPTYSET</code> / <code>SIGFILLSET</code> \u2014 O(_SIG_WORDS) loop (4 iterations) - <code>SIGSETOR</code> / <code>SIGSETNAND</code> \u2014 O(_SIG_WORDS) loop</p> <p>Optimization: Atomic variants avoid lock overhead in <code>p-&gt;p_siglist</code>: - <code>SIGADDSET_ATOMIC</code> \u2014 <code>atomic_set_int()</code> - <code>SIGDELSET_ATOMIC</code> \u2014 <code>atomic_clear_int()</code></p>"},{"location":"sys/kern/signals/#signal-delivery-fast-path","title":"Signal Delivery Fast Path","text":"<p><code>issignal()</code> has fast path without token: <pre><code>mask = lwp_sigpend(lp);\nSIGSETNAND(mask, lp-&gt;lwp_sigmask);\nif (SIGISEMPTY(mask))\n    return (0);  // No signal, no token acquired\n</code></pre></p> <p>Only acquires <code>p-&gt;p_token</code> if signal found in process list.</p>"},{"location":"sys/kern/signals/#cross-cpu-delivery","title":"Cross-CPU Delivery","text":"<p><code>lwp_signotify()</code> checks if target LWP is local before sending IPI: <pre><code>if (dtd-&gt;td_gd == mycpu) {\n    setrunnable(lp);  // Local, no IPI\n} else {\n    lwkt_send_ipiq(dtd-&gt;td_gd, lwp_signotify_remote, lp);  // Remote IPI\n}\n</code></pre></p> <p>Avoids IPI overhead for same-CPU signals.</p>"},{"location":"sys/kern/signals/#debugging-and-observability","title":"Debugging and Observability","text":""},{"location":"sys/kern/signals/#signal-logging","title":"Signal Logging","text":"<p>Sysctl: - <code>kern.logsigexit</code> \u2014 Log signal-caused exits (default 1) - <code>kern.coredump</code> \u2014 Enable/disable core dumps (default 1) - <code>kern.sugid_coredump</code> \u2014 Allow setuid/setgid core dumps (default 0)</p> <p>Ktrace: - <code>KTR_PSIG</code> \u2014 Trace signal delivery via <code>ktrace()</code> - Logs: signal number, action, mask, code</p> <p>Log format: <pre><code>pid %d (%s), uid %d: exited on signal %d (core dumped)\n</code></pre></p>"},{"location":"sys/kern/signals/#kqueue-monitoring","title":"Kqueue Monitoring","text":"<p>Register kevent to monitor process signals: <pre><code>struct kevent kev;\nEV_SET(&amp;kev, pid, EVFILT_PROC, EV_ADD, NOTE_SIGNAL, 0, NULL);\nkevent(kq, &amp;kev, 1, NULL, 0, NULL);\n</code></pre></p> <p>Wakes when any signal delivered to process.</p>"},{"location":"sys/kern/signals/#summary","title":"Summary","text":"<p>DragonFly BSD's signal implementation provides:</p> <ol> <li>POSIX compliance with extensions for per-thread signals and checkpointing</li> <li>Efficient signal delivery with fast-path checks and cross-CPU IPI notification</li> <li>Fine-grained control via SA_* flags (RESTART, RESETHAND, NODEFER, SIGINFO)</li> <li>Process control via STOP/CONT signals with accurate thread stopping</li> <li>Ptrace integration for debugging with signal interception</li> <li>Core dump generation with configurable paths and security controls</li> <li>Kqueue integration for event-based signal monitoring</li> </ol> <p>Key design principles:</p> <ul> <li>Per-thread masks allow fine-grained control in multi-threaded processes</li> <li>Copy-on-write sigacts optimize fork() performance</li> <li>Token-based locking ensures atomicity without global signal lock</li> <li>IPI-based notification supports per-CPU LWKT threading model</li> <li>Double-check pattern optimizes fast path (check without lock, recheck with lock)</li> <li>Signal interlock (<code>sigirefs</code>) prevents races with mask-changing operations</li> </ul> <p>The signal subsystem forms the foundation for: - Job control (shell background/foreground) - Process monitoring (parent notification of child events) - Exception handling (trap delivery to user handlers) - Debugging (ptrace signal interception) - Inter-process communication (asynchronous event notification)</p>"},{"location":"sys/kern/synchronization/","title":"Synchronization Primitives","text":"<p>This document describes the synchronization primitives used throughout the DragonFly BSD kernel to coordinate access to shared resources and ensure data consistency across multiple CPUs and threads.</p>"},{"location":"sys/kern/synchronization/#overview","title":"Overview","text":"<p>DragonFly BSD provides a comprehensive hierarchy of synchronization primitives, each optimized for specific use cases. Understanding when to use each primitive is critical for writing correct and performant kernel code.</p> <p>The synchronization primitives, ordered from lowest to highest level:</p> <ol> <li>Tokens - DragonFly's unique per-CPU serialization mechanism (see LWKT Threading)</li> <li>Spinlocks - Low-level busy-wait locks with shared/exclusive support</li> <li>Mutexes (mtx) - Fast persistent locks with sleep capability and async support</li> <li>Lockmgr Locks - Traditional BSD shared/exclusive locks with complex features</li> <li>Condition Variables - Thread coordination using wait/signal patterns</li> <li>Reference Counts - Atomic reference counting for object lifecycle management</li> <li>System References (sysref) - Advanced reference counting with objcache integration</li> </ol>"},{"location":"sys/kern/synchronization/#key-concepts","title":"Key Concepts","text":""},{"location":"sys/kern/synchronization/#dragonflys-unique-approach","title":"DragonFly's Unique Approach","text":"<p>DragonFly BSD differs from traditional BSD and Linux kernels in its synchronization philosophy:</p> <ul> <li>Token-based design: Most high-level kernel operations use LWKT tokens for serialization rather than traditional locks</li> <li>Per-CPU scheduling: Threads are scheduled independently on each CPU, reducing global synchronization overhead</li> <li>Reduced lock contention: By favoring tokens and per-CPU data structures, DragonFly minimizes cache line bouncing</li> </ul>"},{"location":"sys/kern/synchronization/#token-vs-lock-decision-tree","title":"Token vs. Lock Decision Tree","text":"<p>Use Tokens when: - Protecting high-level subsystem state (VFS, VM, etc.) - The critical section may block or call complex functions - Lock ordering is difficult to maintain - The code is not performance-critical</p> <p>Use Spinlocks when: - Critical section is very short (a few instructions) - Cannot sleep or call blocking functions - Need shared (reader) locks for concurrent access - Performance is critical (e.g., per-packet network processing)</p> <p>Use Mutexes when: - Need to block across long-running operations - Need asynchronous lock acquisition with callbacks - Want recursive locking capability - Need to hold lock across function calls that may block</p> <p>Use Lockmgr when: - Need complex lock operations (upgrade, downgrade, cancellation) - Interfacing with traditional BSD code - Need precise control over lock priority and behavior</p>"},{"location":"sys/kern/synchronization/#synchronization-primitives_1","title":"Synchronization Primitives","text":""},{"location":"sys/kern/synchronization/#spinlocks","title":"Spinlocks","text":"<p>Spinlocks are the lowest-level synchronization primitive, providing both exclusive and shared locking with busy-waiting.</p> <p>Source Files: - <code>sys/kern/kern_spinlock.c</code> - Implementation - <code>sys/sys/spinlock.h</code> - Data structures - <code>sys/sys/spinlock2.h</code> - Inline functions</p> <p>Data Structure:</p> <pre><code>struct spinlock {\n    int lock;        /* main spinlock */\n    int update;      /* update counter */\n};\n</code></pre> <p>Lock Field Encoding: - Bits 0-19: Reference count - Bit 31: <code>SPINLOCK_SHARED</code> flag - Bits 20-30: <code>SPINLOCK_EXCLWAIT</code> counter for exclusive waiters</p> <p>Key Functions:</p> <ul> <li><code>spin_init(struct spinlock *spin, const char *desc)</code> - Initialize spinlock</li> <li><code>spin_lock(struct spinlock *spin)</code> - Acquire exclusive lock (sys/kern/kern_spinlock.c:166)</li> <li><code>spin_unlock(struct spinlock *spin)</code> - Release exclusive lock</li> <li><code>spin_lock_shared(struct spinlock *spin)</code> - Acquire shared lock (sys/kern/kern_spinlock.c:276)</li> <li><code>spin_unlock_shared(struct spinlock *spin)</code> - Release shared lock</li> </ul> <p>Critical Implementation Details:</p> <p>Exclusive Lock Acquisition (sys/kern/kern_spinlock.c:166-256): - Increments lock with <code>atomic_fetchadd_int()</code> - On contention, sets <code>SPINLOCK_EXCLWAIT</code> to gain priority - Transfers high bits (EXCLWAIT counter) to low bits when acquiring - Uses exponential backoff to reduce cache bus traffic - Employs TSC-windowing to distribute CPU load</p> <p>Shared Lock Acquisition (sys/kern/kern_spinlock.c:276-372): - Sets <code>SPINLOCK_SHARED</code> flag when granted - Gives priority to exclusive waiters (EXCLWAIT) - Uses TSC-windowing to occasionally bypass EXCLWAIT priority to prevent starvation - No exponential backoff (would hurt shared lock performance)</p> <p>Performance Optimizations: - TSC (timestamp counter) windowing distributes shared lock attempts across CPUs - Exponential backoff prevents cache bus armageddon on multi-socket systems - Automatic downgrade to RDTSC polling in VM guests</p> <p>When to Use: - Protecting per-CPU data structures accessed from multiple contexts - Very short critical sections (microseconds) - Cannot use tokens (e.g., in interrupt context) - Need reader/writer concurrency</p> <p>Restrictions: - Cannot sleep or block - Cannot call functions that might block - Must not hold across function calls unless guaranteed not to block - Keep critical sections extremely short</p>"},{"location":"sys/kern/synchronization/#mutexes-mtx","title":"Mutexes (mtx)","text":"<p>Mutexes provide persistent locks that can be held across blocking operations, with support for asynchronous acquisition and callbacks.</p> <p>Source Files: - <code>sys/kern/kern_mutex.c</code> - Implementation (sys/kern/kern_mutex.c:1-1159) - <code>sys/sys/mutex.h</code> - Data structures - <code>sys/sys/mutex2.h</code> - Inline functions</p> <p>Data Structures:</p> <pre><code>struct mtx {\n    volatile u_int  mtx_lock;     /* lock state */\n    uint32_t        mtx_flags;    /* flags */\n    struct thread   *mtx_owner;   /* exclusive owner */\n    mtx_link_t      *mtx_exlink;  /* exclusive wait list */\n    mtx_link_t      *mtx_shlink;  /* shared wait list */\n    const char      *mtx_ident;   /* identifier */\n} __cachealign;\n\nstruct mtx_link {\n    struct mtx_link *next;\n    struct mtx_link *prev;\n    struct thread   *owner;\n    int             state;\n    void            (*callback)(struct mtx_link *, void *arg, int error);\n    void            *arg;\n};\n</code></pre> <p>Lock State Encoding: - Bit 31 (<code>MTX_EXCLUSIVE</code>): Exclusive lock flag - Bit 30 (<code>MTX_SHWANTED</code>): Shared waiters present - Bit 29 (<code>MTX_EXWANTED</code>): Exclusive waiters present - Bit 28 (<code>MTX_LINKSPIN</code>): Link list manipulation in progress - Bits 0-27: Reference count</p> <p>Key Functions:</p> <ul> <li><code>mtx_init(mtx_t *mtx, const char *ident)</code> - Initialize mutex</li> <li><code>mtx_lock(mtx_t *mtx)</code> - Acquire exclusive lock (sys/kern/kern_mutex.c:202-224)</li> <li><code>mtx_lock_sh(mtx_t *mtx)</code> - Acquire shared lock (sys/kern/kern_mutex.c:360-376)</li> <li><code>mtx_unlock(mtx_t *mtx)</code> - Release lock (sys/kern/kern_mutex.c:630-735)</li> <li><code>mtx_lock_ex_try(mtx_t *mtx)</code> - Try exclusive lock without blocking (sys/kern/kern_mutex.c:486-517)</li> <li><code>mtx_downgrade(mtx_t *mtx)</code> - Convert exclusive to shared (sys/kern/kern_mutex.c:549-583)</li> <li><code>mtx_upgrade_try(mtx_t *mtx)</code> - Try shared to exclusive upgrade (sys/kern/kern_mutex.c:596-623)</li> <li><code>mtx_spinlock(mtx_t *mtx)</code> - Acquire as spinlock (sys/kern/kern_mutex.c:381-413)</li> <li><code>mtx_lock_ex_link(mtx_t *mtx, mtx_link_t *link, int flags, int to)</code> - Async lock with callback</li> </ul> <p>Key Features:</p> <p>Exclusive Priority (sys/kern/kern_mutex.c:120-180): - Exclusive requests set <code>MTX_EXWANTED</code> to prevent new shared locks - Once EXWANTED is set, new shared requests must wait - Prevents shared lock starvation of exclusive requests</p> <p>Asynchronous Locking (sys/kern/kern_mutex.c:169-200): - Caller provides <code>mtx_link_t</code> structure with callback - If lock cannot be acquired immediately, link is queued - Callback invoked when lock is granted - Allows lock acquisition without blocking current thread</p> <p>Link Management (sys/kern/kern_mutex.c:749-876): - Exclusive and shared waiters maintained in separate circular lists - <code>MTX_LINKSPIN</code> prevents concurrent link list manipulation - Link lists allow precise wakeup control and priority ordering</p> <p>When to Use: - Protecting data structures that require blocking operations - Need to hold lock across I/O or memory allocation - Want asynchronous lock acquisition - Need recursive locking capability - Interfacing with code that may block</p> <p>Restrictions: - Heavier weight than spinlocks - Not suitable for very short critical sections - Exclusive lock holder must eventually release</p>"},{"location":"sys/kern/synchronization/#lockmgr-locks","title":"Lockmgr Locks","text":"<p>Lockmgr locks are traditional BSD shared/exclusive locks with extensive features including upgrades, downgrades, timeouts, and cancellation.</p> <p>Source Files: - <code>sys/kern/kern_lock.c</code> - Implementation (sys/kern/kern_lock.c:1-1483) - <code>sys/sys/lock.h</code> - Data structures and API</p> <p>Data Structure:</p> <pre><code>struct lock {\n    u_int           lk_flags;      /* flags */\n    int             lk_timo;       /* timeout */\n    uint64_t        lk_count;      /* state and counts */\n    const char      *lk_wmesg;     /* wait message */\n    struct thread   *lk_lockholder; /* exclusive holder */\n};\n</code></pre> <p>Count Field Encoding (64-bit): - Bit 30 (<code>LKC_EXREQ</code>): Exclusive request pending - Bit 29 (<code>LKC_SHARED</code>): Shared lock(s) granted - Bit 28 (<code>LKC_UPREQ</code>): Upgrade request pending - Bit 27 (<code>LKC_EXREQ2</code>): Multiple exclusive waiters - Bit 26 (<code>LKC_CANCEL</code>): Cancellation active - Bits 0-25 (<code>LKC_XMASK</code>): Exclusive count - Bits 32-63 (<code>LKC_SMASK</code>): Shared count (shifted by 32)</p> <p>Key Functions:</p> <ul> <li><code>lockinit(struct lock *lkp, const char *wmesg, int timo, int flags)</code> - Initialize lock</li> <li><code>lockmgr(struct lock *lkp, u_int flags)</code> - Main lock operation dispatcher</li> <li><code>lockmgr_shared(struct lock *lkp, u_int flags)</code> - Acquire shared lock (sys/kern/kern_lock.c:106-278)</li> <li><code>lockmgr_exclusive(struct lock *lkp, u_int flags)</code> - Acquire exclusive lock (sys/kern/kern_lock.c:283-489)</li> <li><code>lockmgr_upgrade(struct lock *lkp, u_int flags)</code> - Upgrade shared to exclusive (sys/kern/kern_lock.c:575-733)</li> <li><code>lockmgr_downgrade(struct lock *lkp, u_int flags)</code> - Downgrade exclusive to shared (sys/kern/kern_lock.c:497-554)</li> <li><code>lockmgr_release(struct lock *lkp, u_int flags)</code> - Release lock (sys/kern/kern_lock.c:741-966)</li> </ul> <p>Lock Operation Flags:</p> <ul> <li><code>LK_SHARED</code> - Acquire shared lock</li> <li><code>LK_EXCLUSIVE</code> - Acquire exclusive lock</li> <li><code>LK_UPGRADE</code> - Upgrade shared to exclusive (may lose lock temporarily)</li> <li><code>LK_EXCLUPGRADE</code> - Upgrade without releasing (fails if contended)</li> <li><code>LK_DOWNGRADE</code> - Downgrade exclusive to shared</li> <li><code>LK_RELEASE</code> - Release lock</li> <li><code>LK_NOWAIT</code> - Don't sleep, return EBUSY</li> <li><code>LK_CANCELABLE</code> - Lock request can be canceled</li> <li><code>LK_TIMELOCK</code> - Use lock's timeout value</li> <li><code>LK_PCATCH</code> - Catch signals during sleep</li> </ul> <p>Critical Implementation Details:</p> <p>Shared Lock Acquisition (sys/kern/kern_lock.c:106-278): - Blocks if <code>LKC_EXREQ</code> or <code>LKC_UPREQ</code> is set (unless <code>TDF_DEADLKTREAT</code>) - Increments shared count (<code>LKC_SCOUNT</code>) - Waits for <code>LKC_SHARED</code> flag to be set - If <code>undo_shreq()</code> races to zero, may grant UPREQ or EXREQ</p> <p>Exclusive Lock Acquisition (sys/kern/kern_lock.c:283-489): - Sets <code>LKC_EXREQ</code> to block new shared/upgrade requests - If can't set EXREQ (lock held exclusively), sets <code>LKC_EXREQ2</code> aggregation bit - Waits for EXREQ to be cleared (granted) - Granting thread sets <code>lk_lockholder</code> and count</p> <p>Upgrade Operations (sys/kern/kern_lock.c:575-733): - Sets <code>LKC_UPREQ</code> to request upgrade - If only holder (<code>LKC_SMASK == LKC_SCOUNT</code>), immediately converts to exclusive - Otherwise, waits for last shared release to grant upgrade - <code>LK_EXCLUPGRADE</code> fails immediately if another UPREQ or EXREQ exists - Regular <code>LK_UPGRADE</code> falls back to release+acquire if contended</p> <p>Lock Cancellation (sys/kern/kern_lock.c:977-1022): - Exclusive holder can call <code>lockmgr_cancel_beg()</code> to set <code>LKC_CANCEL</code> - All pending/future <code>LK_CANCELABLE</code> requests return <code>ENOLCK</code> - Used to abort operations on locked structures (e.g., vnode reclaim) - Cleared automatically on final release or via <code>lockmgr_cancel_end()</code></p> <p>Priority Handling: - Exclusive requests (EXREQ) have priority over new shared requests - Upgrade requests (UPREQ) have priority over exclusive requests - UPREQ &gt; EXREQ &gt; new shared locks (see <code>undo_shreq()</code> at sys/kern/kern_lock.c:1039-1091)</p> <p>When to Use: - VFS layer and vnode operations (traditional usage) - Need upgrade/downgrade without releasing - Need lock cancellation capability - Interfacing with legacy BSD code - Complex locking scenarios requiring fine control</p> <p>Restrictions: - More complex and heavier than tokens or mutexes - Upgrade operations can fail or temporarily release lock - Cannot use from interrupt context</p>"},{"location":"sys/kern/synchronization/#condition-variables","title":"Condition Variables","text":"<p>Condition variables provide wait/signal coordination between threads, allowing threads to sleep until a condition becomes true.</p> <p>Source Files: - <code>sys/kern/kern_condvar.c</code> - Implementation (sys/kern/kern_condvar.c:1-97) - <code>sys/sys/condvar.h</code> - API</p> <p>Data Structure:</p> <pre><code>struct cv {\n    struct spinlock cv_lock;    /* protects cv_waiters */\n    int             cv_waiters; /* number of waiters */\n    const char      *cv_desc;   /* description */\n};\n</code></pre> <p>Key Functions:</p> <ul> <li><code>cv_init(struct cv *c, const char *desc)</code> - Initialize condition variable (sys/kern/kern_condvar.c:6-12)</li> <li><code>cv_destroy(struct cv *c)</code> - Destroy condition variable</li> <li><code>cv_wait(struct cv *c, struct lock *lk)</code> - Wait with lockmgr lock</li> <li><code>cv_wait_sig(struct cv *c, struct lock *lk)</code> - Wait with signal catching</li> <li><code>cv_timedwait(struct cv *c, struct lock *lk, int timo)</code> - Wait with timeout</li> <li><code>cv_mtx_wait(struct cv *c, struct mtx *mtx)</code> - Wait with mutex</li> <li><code>cv_signal(struct cv *c)</code> - Wake one waiter (sys/kern/kern_condvar.c:75-90)</li> <li><code>cv_broadcast(struct cv *c)</code> - Wake all waiters</li> <li><code>cv_has_waiters(const struct cv *c)</code> - Check if waiters present</li> </ul> <p>Implementation Details:</p> <p>The implementation is deliberately simple:</p> <ol> <li><code>cv_wait()</code> atomically:</li> <li>Increments <code>cv_waiters</code> under <code>cv_lock</code> (sys/kern/kern_condvar.c:34-36)</li> <li>Calls <code>tsleep_interlock()</code> to prepare to sleep</li> <li>Releases the associated lock (lockmgr or mutex)</li> <li> <p>Sleeps on the cv address</p> </li> <li> <p><code>cv_signal()</code> atomically:</p> </li> <li>Decrements <code>cv_waiters</code> under <code>cv_lock</code></li> <li> <p>Calls <code>wakeup_one()</code> if waiters existed (sys/kern/kern_condvar.c:86-88)</p> </li> <li> <p><code>cv_broadcast()</code> atomically:</p> </li> <li>Sets <code>cv_waiters</code> to zero</li> <li>Calls <code>wakeup()</code> to wake all (sys/kern/kern_condvar.c:81-84)</li> </ol> <p>Mutex vs. Lock Versions: - <code>cv_wait(cv, lock)</code> uses <code>lksleep()</code> - releases lockmgr lock - <code>cv_mtx_wait(cv, mtx)</code> uses <code>mtxsleep()</code> - releases mutex (sys/kern/kern_condvar.c:50-72) - Both re-acquire their respective lock before returning</p> <p>When to Use: - Producer/consumer patterns - Thread coordination (wait for condition to become true) - Implementing wait queues - Event notification between threads</p> <p>Typical Pattern:</p> <pre><code>/* Waiter thread */\nmtx_lock(&amp;resource_mtx);\nwhile (!condition_is_true) {\n    cv_wait(&amp;resource_cv, &amp;resource_mtx);\n}\n/* condition is now true and lock is held */\ndo_work();\nmtx_unlock(&amp;resource_mtx);\n\n/* Signaler thread */\nmtx_lock(&amp;resource_mtx);\nmake_condition_true();\ncv_signal(&amp;resource_cv);  /* or cv_broadcast() */\nmtx_unlock(&amp;resource_mtx);\n</code></pre> <p>Restrictions: - Must hold associated lock when calling cv_wait() - Lock is automatically released during sleep - Lock is automatically re-acquired before return - Must hold lock when calling cv_signal/cv_broadcast (not strictly required but recommended)</p>"},{"location":"sys/kern/synchronization/#reference-counting","title":"Reference Counting","text":"<p>Reference counting provides atomic lifecycle management for kernel objects.</p> <p>Source Files: - <code>sys/kern/kern_refcount.c</code> - Implementation (sys/kern/kern_refcount.c:1-78) - <code>sys/sys/refcount.h</code> - Inline functions and API</p> <p>Key Functions:</p> <ul> <li><code>refcount_init(volatile u_int *countp, u_int value)</code> - Initialize counter</li> <li><code>refcount_acquire(volatile u_int *countp)</code> - Add reference (sys/sys/refcount.h:48-52)</li> <li><code>refcount_acquire_n(volatile u_int *countp, u_int n)</code> - Add n references</li> <li><code>refcount_release(volatile u_int *countp)</code> - Drop reference, returns TRUE on last (sys/sys/refcount.h:60-64)</li> <li><code>refcount_release_n(volatile u_int *countp, u_int n)</code> - Drop n references</li> <li><code>refcount_release_wakeup(volatile u_int *countp)</code> - Drop with wakeup support (sys/sys/refcount.h:86-98)</li> <li><code>refcount_wait(volatile u_int *countp, const char *wstr)</code> - Wait for count to reach zero</li> </ul> <p>Implementation Details:</p> <p>All operations use atomic instructions:</p> <ul> <li>Acquire: <code>atomic_add_acq_int(countp, 1)</code></li> <li>Release: <code>atomic_fetchadd_int(countp, -1)</code>, returns old value</li> <li>Release returns TRUE if old value was 1 (last reference)</li> </ul> <p>Waiting Support (<code>REFCNTF_WAITING</code>):</p> <ul> <li>Bit 30 of counter is <code>REFCNTF_WAITING</code> flag</li> <li><code>refcount_wait()</code> sets this flag atomically (sys/kern/kern_refcount.c:58-77)</li> <li><code>refcount_release_wakeup()</code> checks for flag and calls <code>wakeup()</code> on 1-&gt;0 transition (sys/sys/refcount.h:86-98)</li> <li>Allows threads to sleep waiting for count to drop to zero</li> </ul> <p>When to Use: - Managing object lifetimes (vnodes, vm_objects, buffers, etc.) - Shared ownership of kernel structures - Preventing premature deallocation - Lockless lifecycle management</p> <p>Typical Pattern:</p> <pre><code>/* Object creation */\nstruct myobj *obj = kmalloc(...);\nrefcount_init(&amp;obj-&gt;refcnt, 1);  /* creator holds reference */\n\n/* Share object */\nrefcount_acquire(&amp;obj-&gt;refcnt);\npass_to_another_subsystem(obj);\n\n/* Release object */\nif (refcount_release(&amp;obj-&gt;refcnt)) {\n    /* Last reference, free object */\n    cleanup_obj(obj);\n    kfree(obj);\n}\n\n/* Wait for all refs to drop (e.g., during unmount) */\nrefcount_wait(&amp;obj-&gt;refcnt, \"objwait\");\n/* Now safe to free even without lock */\n</code></pre> <p>Restrictions: - Reference count must not overflow (wraps at 2^31-1) - Caller must ensure object validity when acquiring reference - Cannot use to count references if count might be zero (race) - <code>refcount_wait()</code> should be used with <code>refcount_release_wakeup()</code></p>"},{"location":"sys/kern/synchronization/#system-reference-counting-sysref","title":"System Reference Counting (sysref)","text":"<p>System reference counting provides advanced lifecycle management integrated with the objcache allocator and cluster-wide addressing via sysids.</p> <p>Source Files: - <code>sys/kern/kern_sysref.c</code> - Implementation (sys/kern/kern_sysref.c:1-375) - <code>sys/sys/sysref2.h</code> - API and inline functions</p> <p>Data Structures:</p> <pre><code>struct sysref {\n    sysid_t             sysid;      /* cluster-wide unique ID */\n    int                 refcnt;     /* reference count */\n    u_int               flags;      /* SRF_* flags */\n    struct sysref_class *srclass;   /* class descriptor */\n    RB_ENTRY(sysref)    rbnode;     /* red-black tree node */\n};\n\nstruct sysref_class {\n    const char          *name;\n    malloc_type_t       mtype;      /* malloc type */\n    size_t              objsize;    /* object size */\n    size_t              offset;     /* sysref offset in object */\n    size_t              nom_cache;  /* nominal cache size */\n    u_int               flags;\n    objcache_t          *oc;        /* objcache */\n    struct sysref_ops   ops;        /* callbacks */\n    boolean_t           (*ctor)(void *, void *, int);\n    void                (*dtor)(void *, void *);\n};\n</code></pre> <p>Lifecycle States (refcnt):</p> <ul> <li>-0x40000000: Initialization in progress (not yet active)</li> <li>Positive: Active, each reference is +1</li> <li>-0x40000000: Termination in progress</li> <li>0: Freed (not accessible)</li> </ul> <p>Key Functions:</p> <ul> <li><code>sysref_alloc(struct sysref_class *srclass)</code> - Allocate object (sys/kern/kern_sysref.c:132-179)</li> <li><code>sysref_init(struct sysref *sr, struct sysref_class *srclass)</code> - Manual init for static objects</li> <li><code>sysref_activate(struct sysref *sr)</code> - Activate object (sys/kern/kern_sysref.c:273-286)</li> <li><code>sysref_get(struct sysref *sr)</code> - Acquire reference (inline in sys/sysref2.h)</li> <li><code>sysref_put(struct sysref *sr)</code> - Release reference (inline, calls <code>_sysref_put()</code> on special cases)</li> <li><code>sysref_lookup(sysid_t sysid)</code> - Lookup by sysid</li> </ul> <p>Implementation Details:</p> <p>Objcache Integration (sys/kern/kern_sysref.c:193-263):</p> <ul> <li>Constructor allocates sysid and inserts into per-CPU red-black tree</li> <li>Sysid embeds CPU number in low bits for locality</li> <li>If sysid not accessed, destructor just removes from tree (fast path)</li> <li>If sysid accessed (<code>SRF_SYSIDUSED</code>), destructor fully destroys via <code>objcache_dtor()</code></li> </ul> <p>Reference Count Lifecycle (sys/kern/kern_sysref.c:297-360):</p> <ol> <li>Allocation: refcnt = -0x40000000 (init in progress)</li> <li>Activation: refcnt += 0x40000001 (becomes 1, active)</li> <li>Use: refcnt incremented/decremented normally</li> <li>1-&gt;0 Transition: refcnt set to -0x40000000, <code>terminate()</code> callback invoked</li> <li>Termination: refcnt can still be modified during termination</li> <li>Final: -0x40000000 -&gt; 0, object returned to objcache</li> </ol> <p>Cluster Addressing:</p> <ul> <li>Sysid uniquely identifies object across cluster</li> <li>Per-CPU red-black tree allows O(log n) lookup</li> <li>Enables cluster-wide IPC and resource sharing (future)</li> </ul> <p>When to Use: - Managing heavyweight kernel objects (processes, vnodes, etc.) - Need cluster-wide addressing capability - Want objcache integration for performance - Object lifecycle requires termination callback</p> <p>Restrictions: - More complex than simple reference counting - Requires class descriptor setup - Termination callback must release locks - Not suitable for lightweight objects</p>"},{"location":"sys/kern/synchronization/#userland-mutex-support-umtx","title":"Userland Mutex Support (umtx)","text":"<p>Userland mutex support provides system calls for efficient user-space synchronization with kernel sleep/wakeup.</p> <p>Source Files: - <code>sys/kern/kern_umtx.c</code> - Implementation (sys/kern/kern_umtx.c:1-307)</p> <p>Key Functions:</p> <ul> <li><code>sys_umtx_sleep(const int *ptr, int value, int timeout)</code> - Sleep if *ptr == value (sys/kern/kern_umtx.c:109-241)</li> <li><code>sys_umtx_wakeup(const int *ptr, int count)</code> - Wake waiters on ptr (sys/kern/kern_umtx.c:250-306)</li> </ul> <p>Implementation Details:</p> <p>umtx_sleep() (sys/kern/kern_umtx.c:109-241):</p> <ol> <li>Translates user address to physical address via <code>uservtophys()</code></li> <li>Optionally polls for short duration (4000ns) before sleeping</li> <li>Re-checks value hasn't changed</li> <li>Sleeps on physical address with <code>PDOMAIN_UMTX</code></li> <li>Handles discontinuities (COW, paging) with retries</li> <li>Timeout capped at 2 seconds (caller must retry if needed)</li> </ol> <p>umtx_wakeup() (sys/kern/kern_umtx.c:250-306):</p> <ol> <li>Translates user address to physical address</li> <li>Calls <code>wakeup_domain()</code> to wake waiters</li> <li><code>count == 1</code> wakes one, otherwise wakes all</li> </ol> <p>Performance Optimization:</p> <ul> <li>Polls briefly with RDTSC before sleeping (avoids syscall/context switch for short locks)</li> <li>Uses physical addresses as wait channels (handles memory mapping)</li> <li>Caps timeout to avoid tracking page mapping changes</li> </ul> <p>When to Use: - Implementing pthread mutexes and condition variables in userspace - Futex-like operations (Linux compatibility) - Efficient user-space synchronization</p> <p>Restrictions: - User must handle spurious wakeups - Timeout limited to 2 seconds per call - Address must remain valid during sleep</p>"},{"location":"sys/kern/synchronization/#sleep-queues","title":"Sleep Queues","text":"<p>Sleep queues provide a FreeBSD-compatible API for thread blocking and wakeup.</p> <p>Source Files: - <code>sys/kern/subr_sleepqueue.c</code> - Implementation (sys/kern/subr_sleepqueue.c:1-400+)</p> <p>Data Structures:</p> <pre><code>struct sleepqueue_chain {\n    struct spinlock sc_spin;\n    TAILQ_HEAD(, sleepqueue_wchan) sc_wchead;\n    u_int sc_free_count;\n};\n\nstruct sleepqueue_wchan {\n    TAILQ_ENTRY(sleepqueue_wchan) wc_entry;\n    const void *wc_wchan;\n    struct sleepqueue_chain *wc_sc;\n    u_int wc_refs;\n    int wc_type;\n    u_int wc_blocked[SLEEPQ_NRQUEUES];  /* 2 queues per wchan */\n};\n</code></pre> <p>Key Functions:</p> <ul> <li><code>sleepq_lock(const void *wchan)</code> - Lock wait channel (sys/kern/subr_sleepqueue.c:176-216)</li> <li><code>sleepq_release(const void *wchan)</code> - Unlock wait channel (sys/kern/subr_sleepqueue.c:221-239)</li> <li><code>sleepq_add(const void *wchan, struct lock_object *lock, const char *wmesg, int flags, int queue)</code> - Add thread to queue (sys/kern/subr_sleepqueue.c:251-285)</li> <li><code>sleepq_wait(const void *wchan, int pri)</code> - Sleep until woken (sys/kern/subr_sleepqueue.c:386-400)</li> <li><code>sleepq_timedwait(const void *wchan, int pri)</code> - Sleep with timeout</li> <li><code>sleepq_signal(const void *wchan, int flags, int pri, int queue)</code> - Wake one thread</li> <li><code>sleepq_broadcast(const void *wchan, int flags, int pri, int queue)</code> - Wake all threads</li> </ul> <p>Implementation Details:</p> <ul> <li>Global hash table of wait channels (1024 buckets)</li> <li>Each wait channel has 2 sub-queues (typically for different priorities)</li> <li>Uses DragonFly's <code>tsleep()</code>/<code>wakeup()</code> under the hood</li> <li>Maintains blocked counts for each queue</li> <li>Reference counted wait channel structures</li> </ul> <p>When to Use: - FreeBSD compatibility (e.g., Linux KPI emulation) - Prefer native <code>tsleep()</code>/<code>wakeup()</code> for new DragonFly code - Multiple priority levels needed per wait channel</p>"},{"location":"sys/kern/synchronization/#synchronization-primitive-comparison","title":"Synchronization Primitive Comparison","text":"Primitive Can Sleep Shared Locks Recursive Async Priority Use Case Token Yes No Yes No N/A High-level subsystems (VFS, VM) Spinlock No Yes No No Excl &gt; Shared Short critical sections, per-CPU data Mutex Yes Yes Yes Yes Excl &gt; Shared Moderate critical sections, async I/O Lockmgr Yes Yes Yes No Upgrade &gt; Excl &gt; Shared VFS vnodes, complex lock operations Condvar Yes N/A N/A No N/A Thread coordination, wait/signal Refcount N/A N/A N/A No N/A Object lifecycle management Sysref N/A N/A N/A No N/A Complex object lifecycle, objcache Umtx Yes N/A N/A No N/A Userland synchronization"},{"location":"sys/kern/synchronization/#performance-characteristics","title":"Performance Characteristics","text":"<p>Tokens: - Overhead: Low for uncontended, moderate for contested - Cache impact: Low (per-CPU, no atomic ops when acquired) - Best for: High-level, infrequent operations</p> <p>Spinlocks: - Overhead: Very low - Cache impact: High under contention (busy-waiting) - Best for: Very short critical sections, interrupt context</p> <p>Mutexes: - Overhead: Low-to-moderate - Cache impact: Moderate (atomic ops, link management) - Best for: Medium critical sections with potential blocking</p> <p>Lockmgr: - Overhead: Moderate-to-high - Cache impact: Moderate - Best for: Complex locking scenarios, VFS operations</p>"},{"location":"sys/kern/synchronization/#code-examples","title":"Code Examples","text":""},{"location":"sys/kern/synchronization/#example-1-spinlock-protecting-per-packet-metadata","title":"Example 1: Spinlock Protecting Per-Packet Metadata","text":"<pre><code>struct packet_queue {\n    struct spinlock     pq_spin;\n    TAILQ_HEAD(, packet) pq_list;\n    int                 pq_count;\n};\n\nvoid\nenqueue_packet(struct packet_queue *pq, struct packet *pkt)\n{\n    spin_lock(&amp;pq-&gt;pq_spin);\n    TAILQ_INSERT_TAIL(&amp;pq-&gt;pq_list, pkt, pkt_entry);\n    pq-&gt;pq_count++;\n    spin_unlock(&amp;pq-&gt;pq_spin);\n}\n\nstruct packet *\ndequeue_packet(struct packet_queue *pq)\n{\n    struct packet *pkt;\n\n    spin_lock(&amp;pq-&gt;pq_spin);\n    pkt = TAILQ_FIRST(&amp;pq-&gt;pq_list);\n    if (pkt != NULL) {\n        TAILQ_REMOVE(&amp;pq-&gt;pq_list, pkt, pkt_entry);\n        pq-&gt;pq_count--;\n    }\n    spin_unlock(&amp;pq-&gt;pq_spin);\n\n    return pkt;\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#example-2-mutex-with-blocking-operation","title":"Example 2: Mutex with Blocking Operation","text":"<pre><code>struct device_state {\n    struct mtx          ds_mtx;\n    int                 ds_flags;\n    struct bio_queue    ds_bioq;\n};\n\nvoid\ndevice_submit_bio(struct device_state *ds, struct bio *bio)\n{\n    mtx_lock(&amp;ds-&gt;ds_mtx);\n\n    if (ds-&gt;ds_flags &amp; DSF_SUSPENDED) {\n        /* May need to block waiting for resume */\n        while (ds-&gt;ds_flags &amp; DSF_SUSPENDED) {\n            cv_mtx_wait(&amp;ds-&gt;ds_resume_cv, &amp;ds-&gt;ds_mtx);\n        }\n    }\n\n    /* Enqueue bio */\n    TAILQ_INSERT_TAIL(&amp;ds-&gt;ds_bioq, bio, bio_link);\n\n    /* Kick device (may issue I/O, which blocks) */\n    device_start_io(ds);\n\n    mtx_unlock(&amp;ds-&gt;ds_mtx);\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#example-3-condition-variable-waitsignal","title":"Example 3: Condition Variable Wait/Signal","text":"<pre><code>struct work_queue {\n    struct mtx              wq_mtx;\n    struct cv               wq_cv;\n    TAILQ_HEAD(, work_item) wq_items;\n    int                     wq_shutdown;\n};\n\n/* Worker thread */\nvoid\nworker_thread(struct work_queue *wq)\n{\n    struct work_item *item;\n\n    mtx_lock(&amp;wq-&gt;wq_mtx);\n\n    while (!wq-&gt;wq_shutdown) {\n        item = TAILQ_FIRST(&amp;wq-&gt;wq_items);\n        if (item == NULL) {\n            /* No work, sleep until signaled */\n            cv_mtx_wait(&amp;wq-&gt;wq_cv, &amp;wq-&gt;wq_mtx);\n            continue;\n        }\n\n        TAILQ_REMOVE(&amp;wq-&gt;wq_items, item, wi_entry);\n        mtx_unlock(&amp;wq-&gt;wq_mtx);\n\n        /* Process item without holding lock */\n        process_work_item(item);\n\n        mtx_lock(&amp;wq-&gt;wq_mtx);\n    }\n\n    mtx_unlock(&amp;wq-&gt;wq_mtx);\n}\n\n/* Submitter */\nvoid\nsubmit_work(struct work_queue *wq, struct work_item *item)\n{\n    mtx_lock(&amp;wq-&gt;wq_mtx);\n    TAILQ_INSERT_TAIL(&amp;wq-&gt;wq_items, item, wi_entry);\n    cv_signal(&amp;wq-&gt;wq_cv);  /* Wake one worker */\n    mtx_unlock(&amp;wq-&gt;wq_mtx);\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#example-4-reference-counting-for-object-lifecycle","title":"Example 4: Reference Counting for Object Lifecycle","text":"<pre><code>struct cached_object {\n    refcount_t          co_refs;\n    struct spinlock     co_spin;\n    void                *co_data;\n    /* ... */\n};\n\nstruct cached_object *\ncache_lookup(struct cache *cache, uint64_t key)\n{\n    struct cached_object *co;\n\n    spin_lock(&amp;cache-&gt;cache_spin);\n    co = cache_find(cache, key);\n    if (co != NULL) {\n        refcount_acquire(&amp;co-&gt;co_refs);\n    }\n    spin_unlock(&amp;cache-&gt;cache_spin);\n\n    return co;  /* caller now owns reference */\n}\n\nvoid\ncache_release(struct cached_object *co)\n{\n    if (refcount_release(&amp;co-&gt;co_refs)) {\n        /* Last reference, free object */\n        kfree(co-&gt;co_data, M_CACHE);\n        kfree(co, M_CACHE);\n    }\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#example-5-lockmgr-upgrade-operation","title":"Example 5: Lockmgr Upgrade Operation","text":"<pre><code>int\nvnode_truncate(struct vnode *vp, off_t new_size)\n{\n    int error;\n\n    /* Start with shared lock for size check */\n    lockmgr(&amp;vp-&gt;v_lock, LK_SHARED);\n\n    if (vp-&gt;v_size &lt;= new_size) {\n        /* Nothing to do */\n        lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n        return 0;\n    }\n\n    /* Need exclusive lock to modify */\n    error = lockmgr(&amp;vp-&gt;v_lock, LK_UPGRADE);\n    if (error) {\n        /* Lock was released, re-acquire exclusive */\n        return error;\n    }\n\n    /* Now have exclusive lock */\n    vnode_truncate_locked(vp, new_size);\n\n    lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/synchronization/#subsystem-interactions","title":"Subsystem Interactions","text":""},{"location":"sys/kern/synchronization/#relationship-to-lwkt","title":"Relationship to LWKT","text":"<p>All synchronization primitives integrate with LWKT threading:</p> <ul> <li>Critical sections: Spinlocks and tokens use <code>crit_enter()</code>/<code>crit_exit()</code> to prevent preemption</li> <li>Thread scheduling: Blocking locks deschedule via <code>lwkt_deschedule()</code></li> <li>Per-CPU data: Spinlocks often protect per-CPU structures accessed via <code>mycpu</code></li> <li>Token precedence: Tokens are typically acquired before lower-level locks</li> </ul>"},{"location":"sys/kern/synchronization/#relationship-to-sleepwakeup","title":"Relationship to Sleep/Wakeup","text":"<ul> <li>tsleep(): Used by mutexes, lockmgr, and condition variables</li> <li>tsleep_interlock(): Prepares atomic sleep without races</li> <li>wakeup(): Used by cv_signal(), cv_broadcast(), and lock releases</li> <li>Sleep domains: PDOMAIN_UMTX, PDOMAIN_FBSD0, etc. for isolation</li> </ul>"},{"location":"sys/kern/synchronization/#relationship-to-vm","title":"Relationship to VM","text":"<ul> <li>Page faults: Can occur while holding tokens or sleeping locks, but not spinlocks</li> <li>Memory allocation: <code>kmalloc()</code> may block, cannot call while holding spinlock</li> <li>Objcache: Sysref integrates with objcache for efficient allocation</li> <li>COW handling: Umtx uses physical addresses to handle COW transparently</li> </ul>"},{"location":"sys/kern/synchronization/#best-practices","title":"Best Practices","text":""},{"location":"sys/kern/synchronization/#general-guidelines","title":"General Guidelines","text":"<ol> <li>Prefer tokens for high-level code: Unless performance is critical or in interrupt context</li> <li>Keep spinlock critical sections tiny: Measure in instructions, not lines of code</li> <li>Don't hold spinlocks across function calls: Unless function is guaranteed not to block</li> <li>Use shared locks when possible: Reduces contention for read-heavy workloads</li> <li>Avoid lock nesting: If unavoidable, maintain consistent lock order</li> <li>Use condition variables for coordination: Better than polling in a loop</li> </ol>"},{"location":"sys/kern/synchronization/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Holding spinlock across blocking function: Causes panic or deadlock</li> <li>Sleeping with token but not expecting it: Other code may assume token held continuously</li> <li>Reference count overflow: Not checking for wraparound</li> <li>Lock order violation: Acquiring locks in inconsistent order causes deadlock</li> <li>Missing wakeup: Forgetting cv_signal() causes permanent sleep</li> <li>Umtx address changes: COW or munmap while threads sleeping</li> </ol>"},{"location":"sys/kern/synchronization/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>INVARIANTS kernel: Enables lock assertion checking</li> <li>witness: Tracks lock ordering violations (when enabled)</li> <li>KTR tracing: Can trace spinlock contention</li> <li>indefinite_info: Automatic warnings for locks held too long</li> <li>lock_test_mode: Special debugging mode for lockmgr</li> </ol>"},{"location":"sys/kern/synchronization/#see-also","title":"See Also","text":"<ul> <li>LWKT Threading - Thread management and tokens</li> <li>Processes - Process management</li> <li>Scheduling - Thread scheduling</li> <li>Memory Management - VM interactions</li> </ul>"},{"location":"sys/kern/synchronization/#references","title":"References","text":"<ul> <li><code>sys/kern/kern_spinlock.c</code> - Spinlock implementation</li> <li><code>sys/kern/kern_lock.c</code> - Lockmgr implementation  </li> <li><code>sys/kern/kern_mutex.c</code> - Mutex implementation</li> <li><code>sys/kern/kern_condvar.c</code> - Condition variable implementation</li> <li><code>sys/kern/kern_refcount.c</code> - Reference counting</li> <li><code>sys/kern/kern_sysref.c</code> - System reference counting</li> <li><code>sys/kern/kern_umtx.c</code> - Userland mutex support</li> <li><code>sys/kern/subr_sleepqueue.c</code> - Sleep queue implementation</li> </ul>"},{"location":"sys/kern/syscalls/","title":"Syscalls","text":"<p>Documentation in progress. See planning/sys/kern/PLAN.md for details.</p>"},{"location":"sys/kern/syscalls/#overview","title":"Overview","text":"<p>To be documented.</p>"},{"location":"sys/kern/syscalls/#key-files","title":"Key Files","text":"<p>To be listed.</p>"},{"location":"sys/kern/time/","title":"Time and Timer Subsystems","text":""},{"location":"sys/kern/time/#overview","title":"Overview","text":"<p>The DragonFly BSD kernel implements a sophisticated multi-layered timekeeping architecture that provides both system-wide and per-CPU timing facilities. The design separates hardware timer abstraction from software timer services, enabling flexible and efficient time management across different hardware platforms.</p>"},{"location":"sys/kern/time/#architecture-layers","title":"Architecture Layers","text":"<pre><code>flowchart TB\n    USER[\"User Applications(syscalls: gettimeofday, nanosleep, etc)\"]\n\n    REPR[\"Time Representation(timeval, timespec, sbintime_t)\"]\n\n    CALLOUT[\"Callout/Timeout Mechanism (kern_timeout.c)\u2022 Wheel-based per-CPU callout queues\u2022 Frontend (struct callout) / Backend (struct _callout)\"]\n\n    SYSTIMER[\"System Timer Infrastructure (kern_systimer.c)\u2022 Software timers built on hardware abstraction\u2022 Periodic and one-shot timer support\"]\n\n    CLOCK[\"Clock Interrupts (kern_clock.c)\u2022 hardclock() - Main system clock at hz (100Hz)\u2022 statclock() - Statistics clock at stathz (128Hz)\u2022 schedclock() - Scheduler clock at 50Hz\"]\n\n    CPUTIMER[\"Hardware Timer Abstraction (kern_cputimer.c)\u2022 struct cputimer with count() method\u2022 Registration and selection of hardware timers\u2022 Examples: TSC, i8254, ACPI-fast, HPET\"]\n\n    HW[\"Hardware Timers(TSC, HPET, i8254, ACPI-fast, etc)\"]\n\n    USER --&gt; REPR\n    REPR --&gt; CALLOUT\n    CALLOUT --&gt; SYSTIMER\n    SYSTIMER --&gt; CLOCK\n    CLOCK --&gt; CPUTIMER\n    CPUTIMER --&gt; HW\n</code></pre>"},{"location":"sys/kern/time/#key-source-files","title":"Key Source Files","text":"File Lines Purpose <code>kern_clock.c</code> 1866 System clock interrupts, hardclock, statclock, schedclock, NTP adjustment <code>kern_cputimer.c</code> 673 Hardware timer abstraction layer (cputimer) <code>kern_systimer.c</code> 418 Software timer infrastructure built on cputimer <code>kern_timeout.c</code> 1153 Callout/timeout mechanism with wheel-based implementation <code>kern_time.c</code> 1247 Time-related system calls (gettimeofday, nanosleep, etc.) <code>kern_ntptime.c</code> 872 NTP time adjustment with PLL/FLL"},{"location":"sys/kern/time/#design-principles","title":"Design Principles","text":"<ol> <li> <p>Hardware Independence: The cputimer abstraction allows the kernel to work with different hardware timer sources without modifying upper layers.</p> </li> <li> <p>Per-CPU Scalability: Most timing facilities are per-CPU to minimize lock contention and cache line bouncing.</p> </li> <li> <p>Multiple Time Bases: The kernel maintains several time representations:</p> </li> <li>Realtime (<code>gettimeofday</code>): Wall clock time, affected by NTP adjustments</li> <li>Monotonic (<code>clock_gettime(CLOCK_MONOTONIC)</code>): Always increasing, not affected by time adjustments</li> <li> <p>Uptime: Time since boot, not affected by suspend/resume</p> </li> <li> <p>Lazy Updates: Many time globals are updated passively to minimize overhead (e.g., <code>time_second</code>, <code>time_uptime</code>).</p> </li> <li> <p>NTP Integration: Built-in support for Network Time Protocol adjustments using phase-locked loop (PLL) and frequency-locked loop (FLL) algorithms.</p> </li> </ol>"},{"location":"sys/kern/time/#key-concepts","title":"Key Concepts","text":""},{"location":"sys/kern/time/#clock-types-and-frequencies","title":"Clock Types and Frequencies","text":"<p>DragonFly BSD uses three main periodic clock interrupts, each with a different frequency and purpose:</p>"},{"location":"sys/kern/time/#hardclock-main-system-clock","title":"hardclock (Main System Clock)","text":"<ul> <li>Frequency: <code>hz</code> (typically 100 Hz, configurable)</li> <li>Function: <code>hardclock()</code> in kern_clock.c:552</li> <li>Purpose: </li> <li>Updates master tick counter (<code>ticks</code>, <code>sbticks</code>)</li> <li>Advances global time values (<code>time_second</code>, <code>time_uptime</code>)</li> <li>Processes callouts (timeouts)</li> <li>Handles NTP time adjustments</li> <li>Per-CPU scheduling accounting</li> <li>Per-CPU State: <code>gd-&gt;gd_hardclock</code> (struct systimer)</li> </ul> <p>The hardclock is the primary system heartbeat. It runs on each CPU and coordinates time advancement across the system.</p> <pre><code>/*\n * Hardclock runs on each CPU at hz frequency (typically 100Hz).\n * Updates time, processes timeouts, and performs per-CPU bookkeeping.\n */\nstatic void\nhardclock(systimer_t info, int in_ipi, struct intrframe *frame)\n{\n    /* Update tick counters */\n    ticks++;\n    sbticks = mftb();\n\n    /* Update passive time globals */\n    gd-&gt;gd_time_seconds = basetime[0].tv_sec;\n\n    /* Process callouts for this CPU */\n    softclock(info, in_ipi, frame);\n\n    /* Scheduling and accounting */\n    ...\n}\n</code></pre>"},{"location":"sys/kern/time/#statclock-statistics-clock","title":"statclock (Statistics Clock)","text":"<ul> <li>Frequency: <code>stathz</code> (typically 128 Hz, default; or profhz for profiling)</li> <li>Function: <code>statclock()</code> in kern_clock.c:892</li> <li>Purpose:</li> <li>CPU usage statistics (user/system time)</li> <li>Profiling support</li> <li>Randomized to prevent synchronization with hardclock</li> <li>Per-process and per-thread accounting</li> <li>Per-CPU State: <code>gd-&gt;gd_statclock</code> (struct systimer)</li> </ul> <p>The statclock runs at a slightly different frequency than hardclock to provide statistical sampling that doesn't synchronize with the main clock. This prevents phase-locking artifacts in profiling data.</p> <pre><code>/*\n * Statclock collects CPU usage statistics.\n * Frequency is randomized to prevent phase locking with hardclock.\n */\nstatic void\nstatclock(systimer_t info, int in_ipi, struct intrframe *frame)\n{\n    struct thread *td = curthread;\n    struct proc *p = td-&gt;td_proc;\n\n    /* Account CPU time to current thread/process */\n    if (CLKF_USERMODE(frame)) {\n        p-&gt;p_uticks++;\n        if (p-&gt;p_nice &gt; NZERO)\n            cp_time[CP_NICE]++;\n        else\n            cp_time[CP_USER]++;\n    } else {\n        p-&gt;p_sticks++;\n        cp_time[CP_SYS]++;\n    }\n\n    /* Profiling support */\n    if (p-&gt;p_profthreads)\n        addupc_intr(p, CLKF_PC(frame), 1);\n}\n</code></pre>"},{"location":"sys/kern/time/#schedclock-scheduler-clock","title":"schedclock (Scheduler Clock)","text":"<ul> <li>Frequency: 50 Hz (fixed)</li> <li>Function: <code>schedclock()</code> in kern_clock.c:1028</li> <li>Purpose:</li> <li>Scheduler hint processing</li> <li>Thread priority recalculation</li> <li>Load balancing decisions</li> <li>Per-CPU State: <code>gd-&gt;gd_schedclock</code> (struct systimer)</li> </ul> <p>The schedclock runs at a lower frequency than the other clocks, handling less time-critical scheduler operations.</p>"},{"location":"sys/kern/time/#timer-types","title":"Timer Types","text":""},{"location":"sys/kern/time/#cputimer-hardware-timer-abstraction","title":"cputimer (Hardware Timer Abstraction)","text":"<p>The <code>cputimer</code> is a hardware abstraction that provides a consistent interface to various hardware timer sources.</p> <p>Key Operations: - <code>count()</code>: Read current timer value (monotonically increasing) - <code>fromhz(hz)</code>: Convert frequency to timer units - <code>fromus(us)</code>: Convert microseconds to timer units - <code>fromnano(ns)</code>: Convert nanoseconds to timer units</p> <p>Common Hardware Timers: - TSC (Time Stamp Counter): Per-CPU cycle counter on x86 - HPET (High Precision Event Timer): Modern high-resolution timer - i8254: Legacy PC timer (8254 PIT) - ACPI-fast: ACPI power management timer</p> <p>Selection: The kernel selects the best available cputimer based on quality rating (see <code>cputimer_register()</code> in kern_cputimer.c:291).</p>"},{"location":"sys/kern/time/#systimer-software-timer","title":"systimer (Software Timer)","text":"<p>The <code>systimer</code> builds software timer facilities on top of the hardware cputimer abstraction.</p> <p>Types: - Periodic: Fires at regular intervals (used by hardclock, statclock, schedclock) - One-shot: Fires once at a specified time (used internally)</p> <p>Key Functions: - <code>systimer_init_periodic()</code> (kern_systimer.c:292): Create periodic timer - <code>systimer_init_periodic_nq()</code> (kern_systimer.c:317): Create periodic timer without initial queue - <code>systimer_add()</code>: Add timer to active queue - <code>systimer_del()</code>: Remove timer from queue</p> <p>Each CPU maintains its own systimer queue to avoid lock contention.</p>"},{"location":"sys/kern/time/#callout-timeout-mechanism","title":"callout (Timeout Mechanism)","text":"<p>Callouts provide a high-level timeout mechanism for scheduling function calls in the future.</p> <p>Key Features: - Wheel-based algorithm: O(1) insertion and deletion using a timing wheel - Per-CPU wheels: Separate callout wheels per CPU for scalability - Two-tier structure:   - Frontend: <code>struct callout</code> - Embedded in client structures   - Backend: <code>struct _callout</code> - Actual queued element with wheel linkage - Flags support: <code>CALLOUT_MPSAFE</code>, <code>CALLOUT_ACTIVE</code>, etc.</p> <p>Common Use Cases: - Network packet timeouts - Device driver timeouts - Timed events in kernel subsystems</p>"},{"location":"sys/kern/time/#time-bases-and-representation","title":"Time Bases and Representation","text":"<p>DragonFly maintains several time bases for different purposes:</p>"},{"location":"sys/kern/time/#realtime-wall-clock-time","title":"Realtime (Wall Clock Time)","text":"<ul> <li>Source: <code>basetime[]</code> FIFO in kern_clock.c</li> <li>Representation: <code>struct timespec</code> (seconds + nanoseconds)</li> <li>Access: <code>gettimeofday()</code>, <code>clock_gettime(CLOCK_REALTIME)</code></li> <li>Characteristics: Can jump forward/backward with NTP adjustments</li> </ul> <p>The <code>basetime</code> array is a FIFO that allows the NTP code to adjust time without holding locks during the entire hardclock interrupt.</p> <pre><code>/*\n * basetime[] is a FIFO of 16 entries that stores real time.\n * NTP adjustments update the write pointer, hardclock reads\n * from the read pointer.\n */\nstatic struct timespec basetime[BASETIME_ARYSIZE];  /* FIFO */\nstatic int basetime_index;  /* Read pointer */\n</code></pre>"},{"location":"sys/kern/time/#monotonic-time","title":"Monotonic Time","text":"<ul> <li>Source: <code>gd-&gt;gd_cpuclock_base</code> + cputimer count</li> <li>Access: <code>clock_gettime(CLOCK_MONOTONIC)</code>, <code>nanouptime()</code></li> <li>Characteristics: Always increases, never affected by time adjustments</li> </ul> <p>Monotonic time is ideal for measuring intervals because it's immune to clock adjustments.</p>"},{"location":"sys/kern/time/#uptime","title":"Uptime","text":"<ul> <li>Source: <code>time_uptime</code> global, updated in hardclock</li> <li>Access: Direct global variable access (for kernel code)</li> <li>Characteristics: Seconds since boot, low-resolution</li> </ul>"},{"location":"sys/kern/time/#per-cpu-time-state","title":"Per-CPU Time State","text":"<p>Each CPU maintains timing state in its <code>globaldata</code> structure (see <code>sys/sys/globaldata.h</code>):</p> <pre><code>struct globaldata {\n    /* ... */\n\n    /* Passive time values (updated in hardclock) */\n    time_t          gd_time_seconds;    /* Cached time_second */\n    time_t          gd_time_uptime;     /* Cached time_uptime */\n\n    /* High-precision time base */\n    sysclock_t      gd_cpuclock_base;   /* Monotonic base for this CPU */\n\n    /* System clocks (periodic systimers) */\n    struct systimer gd_hardclock;       /* Main clock at hz */\n    struct systimer gd_statclock;       /* Stats clock at stathz */\n    struct systimer gd_schedclock;      /* Scheduler clock at 50Hz */\n\n    /* Callout wheel for this CPU */\n    /* (see struct _callout_tailq in kern_timeout.c) */\n\n    /* ... */\n};\n</code></pre>"},{"location":"sys/kern/time/#ntp-time-adjustment","title":"NTP Time Adjustment","text":"<p>The kernel includes a sophisticated NTP implementation (<code>kern_ntptime.c</code>) that adjusts system time gradually to synchronize with network time sources.</p> <p>Key Components: - PLL (Phase-Locked Loop): Adjusts time based on phase error - FLL (Frequency-Locked Loop): Adjusts frequency based on long-term drift - Adaptive switching: Automatically switches between PLL and FLL based on conditions</p> <p>System Calls: - <code>ntp_adjtime()</code>: User-space interface to NTP adjustment (sys/timex.h) - <code>adjtime()</code>: Legacy BSD interface for time adjustment</p> <p>Time Adjustment Variables: <pre><code>long time_tick;              /* Nominal tick in us */\nlong time_adjtime;           /* Adjustment remaining (us) */\nint64_t time_offset;         /* Current offset for NTP (ns scaled) */\nlong time_freq;              /* Frequency adjustment (scaled ppm) */\nlong time_maxerror;          /* Maximum error (us) */\nlong time_esterror;          /* Estimated error (us) */\nint time_status;             /* Clock status flags */\n</code></pre></p> <p>The NTP code runs as part of hardclock and makes small adjustments to <code>basetime[]</code> to gradually bring the system clock in sync with the reference time source.</p>"},{"location":"sys/kern/time/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/time/#struct-cputimer","title":"struct cputimer","text":"<p>Location: <code>sys/sys/systimer.h</code></p> <p>The <code>cputimer</code> structure provides a hardware-independent interface to various timer sources.</p> <pre><code>struct cputimer {\n    SLIST_ENTRY(cputimer) next;\n    const char *name;           /* Timer name (e.g., \"TSC\", \"HPET\") */\n    int pri;                    /* Priority for selection */\n    int type;                   /* Timer type flags */\n    uint64_t freq;              /* Timer frequency in Hz */\n    sysclock_t (*count)(void);  /* Read current count */\n    sysclock_t (*fromhz)(int);  /* Convert hz to timer units */\n    sysclock_t (*fromus)(int);  /* Convert microseconds to timer units */\n    sysclock_t (*fromnano)(int64_t); /* Convert nanoseconds to timer units */\n    void (*construct)(struct cputimer *, sysclock_t); /* Constructor */\n    void (*destruct)(struct cputimer *);              /* Destructor */\n    sysclock_t freq64_count;    /* For high-precision scaling */\n    int64_t freq64_nsec;        /* For nanosecond conversion */\n};\n</code></pre> <p>Key Fields:</p> <ul> <li><code>name</code>: Human-readable timer name (e.g., \"TSC\", \"HPET\", \"i8254\", \"ACPI-fast\")</li> <li><code>pri</code>: Priority for automatic selection (higher = preferred)</li> <li><code>CPUTIMER_PRI_TSC</code> (1000) - TSC when synchronized</li> <li><code>CPUTIMER_PRI_HPET</code> (900) - HPET</li> <li><code>CPUTIMER_PRI_ACPI</code> (800) - ACPI PM timer</li> <li><code>CPUTIMER_PRI_8254</code> (100) - Legacy 8254 PIT</li> <li><code>CPUTIMER_PRI_DUMMY</code> (-1) - Placeholder</li> <li><code>freq</code>: Timer frequency in Hz (used for time conversions)</li> <li><code>count()</code>: Returns current monotonically-increasing timer count</li> <li><code>fromhz()</code>, <code>fromus()</code>, <code>fromnano()</code>: Convert time units to timer ticks</li> </ul> <p>Global Variables: <pre><code>struct cputimer *sys_cputimer;  /* Currently selected timer */\n</code></pre></p> <p>Referenced in: kern_cputimer.c:54</p>"},{"location":"sys/kern/time/#struct-systimer","title":"struct systimer","text":"<p>Location: <code>sys/sys/systimer.h</code></p> <p>The <code>systimer</code> structure represents a software timer built on top of the cputimer hardware abstraction.</p> <pre><code>struct systimer {\n    TAILQ_ENTRY(systimer) node;     /* Queue linkage */\n    sysclock_t time;                /* Next fire time (absolute) */\n    sysclock_t periodic;            /* Period (0 = one-shot) */\n    sysclock_t which;               /* For load distribution */\n    systimer_func_t func;           /* Callback function */\n    void *data;                     /* Callback argument */\n    int flags;                      /* Flags (periodic, etc.) */\n    struct globaldata *gd;          /* CPU this timer runs on */\n    int freq;                       /* For periodic timers */\n};\n\ntypedef void (*systimer_func_t)(struct systimer *, int, struct intrframe *);\n</code></pre> <p>Key Fields:</p> <ul> <li><code>time</code>: Absolute time when timer should fire (in sysclock_t units)</li> <li><code>periodic</code>: If non-zero, timer fires repeatedly at this interval</li> <li><code>func</code>: Callback function invoked when timer fires</li> <li><code>data</code>: Opaque data passed to callback</li> <li><code>gd</code>: Pointer to globaldata - specifies which CPU owns this timer</li> <li><code>freq</code>: For periodic timers, the desired frequency</li> </ul> <p>Timer Types: - Periodic: Used for hardclock, statclock, schedclock - One-shot: Used internally for callout wheel advancement</p> <p>Per-CPU Queue: Each CPU's <code>globaldata</code> contains a queue of active systimers.</p> <p>Referenced in: kern_systimer.c:292, kern_clock.c:552</p>"},{"location":"sys/kern/time/#struct-callout-and-struct-_callout","title":"struct callout and struct _callout","text":"<p>Location: <code>sys/sys/callout.h</code></p> <p>DragonFly uses a two-tier callout structure:</p>"},{"location":"sys/kern/time/#frontend-struct-callout","title":"Frontend: struct callout","text":"<pre><code>struct callout {\n    struct _callout *lh;        /* Link head (backend pointer) */\n};\n</code></pre> <p>The frontend structure is embedded in client data structures (e.g., network mbuf, driver state). It's lightweight and only contains a pointer to the backend.</p>"},{"location":"sys/kern/time/#backend-struct-_callout","title":"Backend: struct _callout","text":"<p>Location: kern_timeout.c:138 (internal structure)</p> <pre><code>struct _callout {\n    union {\n        TAILQ_ENTRY(_callout) tq;       /* Wheel queue linkage */\n        struct _callout *rq_next;       /* Free list linkage */\n    } c_links;\n    struct callout *c_lh;               /* Back-pointer to frontend */\n    struct softclock_pcpu *c_base;      /* CPU this callout runs on */\n    void (*c_func)(void *);             /* Callback function */\n    void *c_arg;                        /* Callback argument */\n    int c_time;                         /* Wheel position (ticks) */\n    int c_flags;                        /* Flags (see below) */\n};\n</code></pre> <p>Key Fields:</p> <ul> <li><code>c_links.tq</code>: Queue linkage for timing wheel</li> <li><code>c_lh</code>: Back-pointer to the frontend callout structure</li> <li><code>c_base</code>: Per-CPU softclock structure (identifies which CPU)</li> <li><code>c_func</code>: Function to call when timer expires</li> <li><code>c_arg</code>: Argument passed to callback function</li> <li><code>c_time</code>: Absolute tick count when callout should fire</li> <li><code>c_flags</code>: Status and control flags</li> </ul> <p>Callout Flags (defined in sys/sys/callout.h): <pre><code>#define CALLOUT_PENDING         0x0001  /* Callout is on wheel */\n#define CALLOUT_ACTIVE          0x0002  /* Callout is active */\n#define CALLOUT_MPSAFE          0x0008  /* Can run without Giant */\n#define CALLOUT_DID_INIT        0x0010  /* callout_init() called */\n#define CALLOUT_AUTOLOCK        0x0020  /* Automatic locking */\n#define CALLOUT_WAITING         0x0040  /* Thread waiting for callout */\n#define CALLOUT_EXECUTED        0x0080  /* Callout executed */\n</code></pre></p> <p>Design Rationale: The two-tier design allows: 1. Lightweight embedding: Client structures only need one pointer 2. O(1) operations: Checking if callout is pending is just a NULL check 3. Efficient migration: Backend can be moved between CPUs without updating client</p> <p>Referenced in: kern_timeout.c:788 (callout_reset), kern_timeout.c:865 (callout_stop)</p>"},{"location":"sys/kern/time/#callout-wheel-structure","title":"Callout Wheel Structure","text":"<p>Location: kern_timeout.c:132</p> <pre><code>struct softclock_pcpu {\n    struct spinlock spin;               /* Protects callout wheel */\n    struct colist callwheel[BUCKETS];   /* Timing wheel */\n    struct colist calltodo;             /* Overflow list */\n    struct colist *callcursor;          /* Current wheel position */\n    struct _callout *running;           /* Currently executing callout */\n    int softticks;                      /* Local tick counter */\n    int curticks;                       /* Current tick being processed */\n    int isrunning;                      /* Processing in progress */\n    struct thread *thread;              /* Softclock thread */\n};\n</code></pre> <p>Key Fields:</p> <ul> <li><code>callwheel[BUCKETS]</code>: Circular array of callout queues (timing wheel)</li> <li>Default <code>BUCKETS = 512</code> (can be tuned)</li> <li>Each bucket represents one tick</li> <li>Wheel covers 512 ticks (5.12 seconds at 100 Hz)</li> <li><code>calltodo</code>: Overflow list for callouts beyond the wheel range</li> <li><code>callcursor</code>: Points to current wheel bucket being processed</li> <li><code>running</code>: Currently executing callout (for detecting recursion)</li> <li><code>softticks</code>: Local tick counter for this CPU</li> <li><code>thread</code>: Dedicated kernel thread for running callouts</li> </ul> <p>Wheel Algorithm: <pre><code>Bucket index = (current_tick + delta_ticks) % BUCKETS\n\nExample at hz=100 (10ms per tick):\n- Bucket 0: Callouts firing at tick 0, 512, 1024, ...\n- Bucket 1: Callouts firing at tick 1, 513, 1025, ...\n- Bucket N: Callouts firing at tick N, N+512, N+1024, ...\n</code></pre></p> <p>For callouts beyond 512 ticks, they're placed on the overflow list and moved to the wheel as time advances.</p> <p>Per-CPU Design: Each CPU has its own <code>softclock_pcpu</code> structure to avoid lock contention.</p> <p>Referenced in: kern_timeout.c:788, kern_clock.c:712 (softclock processing)</p>"},{"location":"sys/kern/time/#time-representation-types","title":"Time Representation Types","text":"<p>DragonFly uses several time representation types:</p>"},{"location":"sys/kern/time/#struct-timeval-traditional-bsd","title":"struct timeval (Traditional BSD)","text":"<pre><code>struct timeval {\n    time_t      tv_sec;     /* Seconds since epoch */\n    suseconds_t tv_usec;    /* Microseconds [0, 999999] */\n};\n</code></pre> <p>Usage: <code>gettimeofday()</code>, legacy interfaces Resolution: Microsecond (1e-6 seconds) Range: ~68 years (32-bit time_t) or ~292 billion years (64-bit time_t)</p>"},{"location":"sys/kern/time/#struct-timespec-posix","title":"struct timespec (POSIX)","text":"<pre><code>struct timespec {\n    time_t  tv_sec;         /* Seconds since epoch */\n    long    tv_nsec;        /* Nanoseconds [0, 999999999] */\n};\n</code></pre> <p>Usage: <code>clock_gettime()</code>, <code>nanosleep()</code>, modern interfaces Resolution: Nanosecond (1e-9 seconds) Range: Same as timeval but with nanosecond precision</p>"},{"location":"sys/kern/time/#sysclock_t-internal-kernel","title":"sysclock_t (Internal Kernel)","text":"<pre><code>typedef int64_t sysclock_t;    /* System clock counter */\n</code></pre> <p>Usage: Internal timing, cputimer counts Resolution: Depends on hardware timer frequency Range: 2^63 ticks (effectively unlimited for reasonable frequencies) Characteristics: Monotonic, never wraps in practice</p>"},{"location":"sys/kern/time/#sbintime_t-signed-binary-time","title":"sbintime_t (Signed Binary Time)","text":"<pre><code>typedef int64_t sbintime_t;    /* 2^-32 second units */\n</code></pre> <p>Usage: High-precision internal timing Resolution: 2^-32 seconds (~233 picoseconds) Range: ~68 years with picosecond precision Characteristics: Fast binary arithmetic, no division needed</p> <p>Conversion: <pre><code>#define SBT_1S  ((sbintime_t)1 &lt;&lt; 32)  /* One second */\n#define SBT_1MS (SBT_1S / 1000)         /* One millisecond */\n#define SBT_1US (SBT_1S / 1000000)      /* One microsecond */\n#define SBT_1NS (SBT_1S / 1000000000)   /* One nanosecond */\n</code></pre></p>"},{"location":"sys/kern/time/#global-time-variables","title":"Global Time Variables","text":"<p>Location: kern_clock.c</p> <pre><code>/* Master tick counters */\nvolatile int ticks;                     /* Tick counter at hz */\nvolatile int64_t sbticks;               /* Tick counter in sbintime_t */\n\n/* Passive time globals (updated in hardclock) */\ntime_t time_second;                     /* Current realtime (seconds) */\ntime_t time_uptime;                     /* Uptime (seconds since boot) */\n\n/* Real time FIFO for NTP */\n#define BASETIME_ARYSIZE 16\nstatic struct timespec basetime[BASETIME_ARYSIZE];\nstatic volatile int basetime_index;     /* Read index (FIFO) */\n\n/* Boot time reference */\nstruct timespec boottime;               /* Time at boot */\n\n/* NTP adjustment state */\nextern long time_tick;                  /* Nominal tick (us) */\nextern long time_adjtime;               /* Adjustment remaining (us) */\nextern int64_t time_offset;             /* NTP offset (scaled ns) */\nextern long time_freq;                  /* Frequency adjustment (ppm) */\n</code></pre> <p>Key Variables:</p> <ul> <li><code>ticks</code>: Global tick counter, incremented at <code>hz</code> frequency (typically 100 Hz)</li> <li>Used for timeout calculations</li> <li>Wraps after ~497 days at hz=100</li> <li><code>sbticks</code>: High-precision tick counter in sbintime_t format</li> <li><code>time_second</code>: Cached realtime in seconds (low overhead to read)</li> <li><code>time_uptime</code>: Seconds since boot (monotonic)</li> <li><code>basetime[]</code>: FIFO of realtime values (NTP adjusts write end, hardclock reads read end)</li> <li><code>boottime</code>: Reference time when system booted</li> </ul> <p>Thread Safety: Most time globals are updated atomically or use lock-free FIFO mechanisms (basetime).</p>"},{"location":"sys/kern/time/#key-functions","title":"Key Functions","text":""},{"location":"sys/kern/time/#hardware-timer-registration-and-selection","title":"Hardware Timer Registration and Selection","text":""},{"location":"sys/kern/time/#cputimer_register","title":"cputimer_register()","text":"<p>Location: kern_cputimer.c:291</p> <pre><code>void cputimer_register(struct cputimer *timer);\n</code></pre> <p>Registers a hardware timer with the kernel. If the new timer has higher priority than the current timer, it becomes the active timer.</p> <p>Parameters: - <code>timer</code>: Pointer to initialized cputimer structure</p> <p>Selection Logic: <pre><code>if (timer-&gt;pri &gt; sys_cputimer-&gt;pri) {\n    sys_cputimer = timer;  /* Switch to better timer */\n}\n</code></pre></p> <p>Common Priorities: - TSC (synchronized): 1000 - HPET: 900 - ACPI PM Timer: 800 - i8254 PIT: 100</p>"},{"location":"sys/kern/time/#cputimer_select","title":"cputimer_select()","text":"<p>Location: kern_cputimer.c:325</p> <pre><code>void cputimer_select(struct cputimer *timer, int prio);\n</code></pre> <p>Explicitly selects a timer, optionally changing its priority.</p>"},{"location":"sys/kern/time/#system-timer-operations","title":"System Timer Operations","text":""},{"location":"sys/kern/time/#systimer_init_periodic","title":"systimer_init_periodic()","text":"<p>Location: kern_systimer.c:292</p> <pre><code>void systimer_init_periodic(systimer_t info, systimer_func_t func,\n                            void *data, int freq);\n</code></pre> <p>Initializes a periodic software timer.</p> <p>Parameters: - <code>info</code>: Pointer to systimer structure to initialize - <code>func</code>: Callback function to invoke on each timer expiration - <code>data</code>: Opaque pointer passed to callback - <code>freq</code>: Desired frequency in Hz</p> <p>Usage Example (hardclock initialization in kern_clock.c:373): <pre><code>systimer_init_periodic(&amp;gd-&gt;gd_hardclock, hardclock, NULL, hz);\n</code></pre></p>"},{"location":"sys/kern/time/#systimer_init_periodic_nq","title":"systimer_init_periodic_nq()","text":"<p>Location: kern_systimer.c:317</p> <pre><code>void systimer_init_periodic_nq(systimer_t info, systimer_func_t func,\n                               void *data, int freq);\n</code></pre> <p>Same as <code>systimer_init_periodic()</code> but doesn't initially add the timer to the active queue. Must call <code>systimer_add()</code> to activate.</p>"},{"location":"sys/kern/time/#systimer_add","title":"systimer_add()","text":"<p>Location: kern_systimer.c:342</p> <pre><code>void systimer_add(systimer_t info);\n</code></pre> <p>Adds a systimer to the active queue for its CPU.</p>"},{"location":"sys/kern/time/#systimer_del","title":"systimer_del()","text":"<p>Location: kern_systimer.c:368</p> <pre><code>void systimer_del(systimer_t info);\n</code></pre> <p>Removes a systimer from the active queue.</p>"},{"location":"sys/kern/time/#callout-operations","title":"Callout Operations","text":""},{"location":"sys/kern/time/#callout_init","title":"callout_init()","text":"<p>Location: kern_timeout.c:636</p> <pre><code>void callout_init(struct callout *c);\nvoid callout_init_mp(struct callout *c);  /* MP-safe variant */\nvoid callout_init_lk(struct callout *c, struct lock *lk);  /* With lock */\n</code></pre> <p>Initializes a callout structure. Must be called before using the callout.</p> <p>Variants: - <code>callout_init()</code>: Basic initialization - <code>callout_init_mp()</code>: Marks callout as MP-safe (no Giant lock needed) - <code>callout_init_lk()</code>: Associates a lock with the callout (automatic locking)</p> <p>Example: <pre><code>struct callout my_callout;\ncallout_init_mp(&amp;my_callout);\n</code></pre></p>"},{"location":"sys/kern/time/#callout_reset","title":"callout_reset()","text":"<p>Location: kern_timeout.c:788</p> <pre><code>void callout_reset(struct callout *c, int ticks, void (*func)(void *), void *arg);\nvoid callout_reset_bycpu(struct callout *c, int ticks, void (*func)(void *),\n                         void *arg, int cpuid);\n</code></pre> <p>Schedules or reschedules a callout to fire after the specified number of ticks.</p> <p>Parameters: - <code>c</code>: Callout structure (must be initialized) - <code>ticks</code>: Number of ticks until expiration (relative to current time) - <code>func</code>: Callback function - <code>arg</code>: Argument passed to callback</p> <p>Behavior: - If callout is already pending, it's cancelled and rescheduled - Returns immediately; callback executes asynchronously - Thread-safe; uses per-CPU locks</p> <p>Example: <pre><code>/* Fire callback in 5 seconds (assuming hz=100) */\ncallout_reset(&amp;my_callout, 5 * hz, my_timeout_func, my_data);\n</code></pre></p> <p>CPU Assignment: - <code>callout_reset()</code>: Uses current CPU - <code>callout_reset_bycpu()</code>: Explicitly specifies CPU</p>"},{"location":"sys/kern/time/#callout_stop","title":"callout_stop()","text":"<p>Location: kern_timeout.c:865</p> <pre><code>int callout_stop(struct callout *c);\nint callout_stop_sync(struct callout *c);\n</code></pre> <p>Cancels a pending callout.</p> <p>Returns: - Non-zero if callout was pending (successfully cancelled) - Zero if callout was not pending</p> <p>Variants: - <code>callout_stop()</code>: Basic stop, returns immediately - <code>callout_stop_sync()</code>: Waits for callback to complete if currently executing</p> <p>Example: <pre><code>if (callout_stop(&amp;my_callout)) {\n    /* Callout was pending and has been cancelled */\n} else {\n    /* Callout wasn't pending or already fired */\n}\n</code></pre></p>"},{"location":"sys/kern/time/#callout_terminate","title":"callout_terminate()","text":"<p>Location: kern_timeout.c:928</p> <pre><code>void callout_terminate(struct callout *c);\n</code></pre> <p>Terminates a callout, stopping it and waiting for any in-progress execution to complete. Called during cleanup/shutdown.</p>"},{"location":"sys/kern/time/#time-retrieval-functions","title":"Time Retrieval Functions","text":""},{"location":"sys/kern/time/#getmicrotime-getnanotime","title":"getmicrotime() / getnanotime()","text":"<p>Location: kern_time.c:462, kern_time.c:478</p> <pre><code>void getmicrotime(struct timeval *tvp);   /* Realtime, low precision */\nvoid getnanotime(struct timespec *tsp);   /* Realtime, low precision */\n</code></pre> <p>Returns cached realtime with low overhead (no hardware timer read).</p> <p>Precision: Updated only at hardclock frequency (typically 100 Hz, 10ms resolution) Use Case: When exact time isn't critical (logging, coarse timestamps)</p>"},{"location":"sys/kern/time/#microtime-nanotime","title":"microtime() / nanotime()","text":"<p>Location: kern_time.c:494, kern_time.c:510</p> <pre><code>void microtime(struct timeval *tvp);      /* Realtime, high precision */\nvoid nanotime(struct timespec *tsp);      /* Realtime, high precision */\n</code></pre> <p>Returns precise realtime by reading the hardware timer.</p> <p>Precision: Hardware timer resolution (nanosecond range for modern timers) Overhead: Higher than getmicrotime/getnanotime (hardware read required) Use Case: When accurate timestamps are needed</p>"},{"location":"sys/kern/time/#getmicrouptime-getnanouptime","title":"getmicrouptime() / getnanouptime()","text":"<p>Location: kern_time.c:526, kern_time.c:542</p> <pre><code>void getmicrouptime(struct timeval *tvp);  /* Monotonic, low precision */\nvoid getnanouptime(struct timespec *tsp);  /* Monotonic, low precision */\n</code></pre> <p>Returns cached monotonic uptime.</p> <p>Characteristics:  - Monotonically increasing - Not affected by time adjustments - Low overhead (no hardware read)</p>"},{"location":"sys/kern/time/#microuptime-nanouptime","title":"microuptime() / nanouptime()","text":"<p>Location: kern_time.c:558, kern_time.c:574</p> <pre><code>void microuptime(struct timeval *tvp);     /* Monotonic, high precision */\nvoid nanouptime(struct timespec *tsp);     /* Monotonic, high precision */\n</code></pre> <p>Returns precise monotonic uptime by reading hardware timer.</p> <p>Use Case: Measuring time intervals accurately</p>"},{"location":"sys/kern/time/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/time/#sys_clock_gettime","title":"sys_clock_gettime()","text":"<p>Location: kern_time.c:672</p> <pre><code>int sys_clock_gettime(struct sysmsg *sysmsg,\n                      const struct clock_gettime_args *uap);\n</code></pre> <p>POSIX clock_gettime() system call.</p> <p>Supported Clocks: - <code>CLOCK_REALTIME</code>: Realtime (can jump with NTP) - <code>CLOCK_MONOTONIC</code>: Monotonic time (always increasing) - <code>CLOCK_UPTIME</code>: Uptime since boot - <code>CLOCK_VIRTUAL</code>: Per-process virtual time - <code>CLOCK_PROF</code>: Per-process profiling time - <code>CLOCK_SECOND</code>: Low-resolution second counter</p>"},{"location":"sys/kern/time/#sys_nanosleep","title":"sys_nanosleep()","text":"<p>Location: kern_time.c:823</p> <pre><code>int sys_nanosleep(struct sysmsg *sysmsg,\n                  const struct nanosleep_args *uap);\n</code></pre> <p>POSIX nanosleep() - sleep for specified time.</p> <p>Implementation: 1. Converts timespec to ticks 2. Calls <code>tsleep()</code> with timeout 3. Handles early wakeup (signals) 4. Returns remaining time if interrupted</p>"},{"location":"sys/kern/time/#sys_gettimeofday","title":"sys_gettimeofday()","text":"<p>Location: kern_time.c:273</p> <pre><code>int sys_gettimeofday(struct sysmsg *sysmsg,\n                     const struct gettimeofday_args *uap);\n</code></pre> <p>Traditional BSD gettimeofday() system call.</p> <p>Implementation: Calls <code>microtime()</code> to get high-precision realtime.</p>"},{"location":"sys/kern/time/#sys_adjtime-sys_ntp_adjtime","title":"sys_adjtime() / sys_ntp_adjtime()","text":"<p>Location: kern_time.c:370, kern_ntptime.c:260</p> <pre><code>int sys_adjtime(struct sysmsg *sysmsg, const struct adjtime_args *uap);\nint sys_ntp_adjtime(struct sysmsg *sysmsg, const struct ntp_adjtime_args *uap);\n</code></pre> <p>Adjust system time gradually (for time synchronization).</p> <ul> <li><code>adjtime()</code>: Simple time adjustment (legacy BSD)</li> <li><code>ntp_adjtime()</code>: Full NTP adjustment interface</li> </ul>"},{"location":"sys/kern/time/#code-flow-examples","title":"Code Flow Examples","text":""},{"location":"sys/kern/time/#example-1-callout-lifecycle","title":"Example 1: Callout Lifecycle","text":"<p>Complete example of using callouts for a timeout:</p> <pre><code>/* Device driver timeout example */\n\nstruct my_device {\n    struct callout watchdog;\n    int timeout_pending;\n    /* ... other fields ... */\n};\n\n/* Initialize callout (in device attach) */\nvoid\nmy_device_attach(struct my_device *dev)\n{\n    callout_init_mp(&amp;dev-&gt;watchdog);\n    dev-&gt;timeout_pending = 0;\n}\n\n/* Start operation with timeout */\nvoid\nmy_device_start_operation(struct my_device *dev)\n{\n    /* Start hardware operation */\n    my_device_hw_start(dev);\n\n    /* Set 5-second watchdog timeout (hz = 100) */\n    callout_reset(&amp;dev-&gt;watchdog, 5 * hz, my_device_timeout, dev);\n    dev-&gt;timeout_pending = 1;\n}\n\n/* Timeout callback (runs in softclock context) */\nvoid\nmy_device_timeout(void *arg)\n{\n    struct my_device *dev = arg;\n\n    kprintf(\"Device operation timed out!\\n\");\n    dev-&gt;timeout_pending = 0;\n\n    /* Reset hardware */\n    my_device_hw_reset(dev);\n}\n\n/* Operation completed successfully */\nvoid\nmy_device_operation_complete(struct my_device *dev)\n{\n    /* Cancel timeout */\n    if (callout_stop(&amp;dev-&gt;watchdog)) {\n        /* Timeout was pending and successfully cancelled */\n        dev-&gt;timeout_pending = 0;\n    }\n\n    /* Process completion */\n    my_device_handle_completion(dev);\n}\n\n/* Cleanup (in device detach) */\nvoid\nmy_device_detach(struct my_device *dev)\n{\n    /* Stop and wait for any in-progress timeout */\n    callout_terminate(&amp;dev-&gt;watchdog);\n}\n</code></pre> <p>Call Flow: <pre><code>1. Device attach\n   \u251c\u2500&gt; callout_init_mp(&amp;dev-&gt;watchdog)\n   \u2514\u2500&gt; Callout ready for use\n\n2. Start operation\n   \u251c\u2500&gt; my_device_hw_start()\n   \u251c\u2500&gt; callout_reset(&amp;dev-&gt;watchdog, 5*hz, callback, dev)\n   \u2502   \u251c\u2500&gt; Allocate backend _callout\n   \u2502   \u251c\u2500&gt; Calculate expiration: ticks + 5*hz\n   \u2502   \u251c\u2500&gt; Insert into callout wheel bucket\n   \u2502   \u2514\u2500&gt; Mark as CALLOUT_PENDING\n   \u2514\u2500&gt; Return to caller\n\n3a. Normal completion path:\n    \u251c\u2500&gt; Operation completes\n    \u251c\u2500&gt; callout_stop(&amp;dev-&gt;watchdog)\n    \u2502   \u251c\u2500&gt; Remove from wheel\n    \u2502   \u251c\u2500&gt; Clear CALLOUT_PENDING\n    \u2502   \u2514\u2500&gt; Return 1 (was pending)\n    \u2514\u2500&gt; Handle completion\n\n3b. Timeout path:\n    \u251c\u2500&gt; hardclock() runs at hz frequency\n    \u251c\u2500&gt; softclock() processes callout wheel\n    \u251c\u2500&gt; Finds expired callout\n    \u251c\u2500&gt; Calls my_device_timeout(dev)\n    \u2502   \u251c\u2500&gt; Log timeout\n    \u2502   \u2514\u2500&gt; Reset hardware\n    \u2514\u2500&gt; Clear CALLOUT_PENDING\n\n4. Device detach\n   \u251c\u2500&gt; callout_terminate(&amp;dev-&gt;watchdog)\n   \u251c\u2500&gt; Stop callout\n   \u2514\u2500&gt; Wait for completion if running\n</code></pre></p>"},{"location":"sys/kern/time/#example-2-hardclock-processing-flow","title":"Example 2: Hardclock Processing Flow","text":"<p>Location: kern_clock.c:552</p> <pre><code>/*\n * Hardclock runs on each CPU at hz frequency.\n * This is the primary system heartbeat.\n */\nstatic void\nhardclock(systimer_t info, int in_ipi, struct intrframe *frame)\n{\n    globaldata_t gd = mycpu();\n    thread_t td = curthread;\n\n    /* Step 1: Update global tick counters */\n    ++ticks;                        /* Increment master tick counter */\n    sbticks = mftb();               /* Read timebase for sbintime */\n\n    /* Step 2: Update passive time globals from basetime FIFO */\n    gd-&gt;gd_time_seconds = basetime[basetime_index].tv_sec;\n    time_second = gd-&gt;gd_time_seconds;\n    time_uptime = (int)(systimer_count() - systimer_offset) / sys_cputimer-&gt;freq;\n\n    /* Step 3: Handle NTP adjustments (on CPU 0 only) */\n    if (gd-&gt;gd_cpuid == 0) {\n        ntp_update_second(&amp;basetime[basetime_index], ticks);\n    }\n\n    /* Step 4: Process callouts (timeouts) */\n    softclock(info, in_ipi, frame);\n\n    /* Step 5: Per-thread accounting */\n    if (td != &amp;gd-&gt;gd_idlethread) {\n        td-&gt;td_wakefromcpu = -1;\n        if (CLKF_USERMODE(frame)) {\n            ++td-&gt;td_uticks;        /* User mode */\n        } else {\n            ++td-&gt;td_sticks;        /* System mode */\n        }\n    }\n\n    /* Step 6: Scheduler operations (every 10 ticks on DragonFly) */\n    if ((ticks % 10) == 0) {\n        scheduler_tick(td, frame);\n    }\n}\n</code></pre> <p>Flow Diagram: <pre><code>Hardware Timer Interrupt\n        \u2193\nsystimer_intr() [kern_systimer.c:69]\n        \u2193\nhardclock() [kern_clock.c:552]\n        \u251c\u2500&gt; Update ticks, sbticks\n        \u251c\u2500&gt; Update time_second, time_uptime (from basetime FIFO)\n        \u251c\u2500&gt; CPU 0: ntp_update_second()\n        \u2502           \u251c\u2500&gt; Apply PLL/FLL adjustments\n        \u2502           \u2514\u2500&gt; Update basetime[] write index\n        \u251c\u2500&gt; softclock() [kern_clock.c:712]\n        \u2502   \u2514\u2500&gt; Process callout wheel\n        \u2502       \u251c\u2500&gt; Advance wheel cursor\n        \u2502       \u251c\u2500&gt; For each expired callout:\n        \u2502       \u2502   \u251c\u2500&gt; Remove from wheel\n        \u2502       \u2502   \u251c\u2500&gt; Mark as executing\n        \u2502       \u2502   \u251c\u2500&gt; Call callback function\n        \u2502       \u2502   \u2514\u2500&gt; Mark as executed\n        \u2502       \u2514\u2500&gt; Check overflow list (calltodo)\n        \u251c\u2500&gt; Thread accounting (td_uticks/td_sticks)\n        \u2514\u2500&gt; Every 10 ticks: scheduler_tick()\n</code></pre></p>"},{"location":"sys/kern/time/#example-3-callout-wheel-operation","title":"Example 3: Callout Wheel Operation","text":"<p>Wheel Structure (BUCKETS = 512):</p> <p>Current ticks = 1000, Wheel bucket index = ticks % BUCKETS = 1000 % 512 = 488</p> <pre><code>flowchart LR\n    subgraph WHEEL[\"Callout Wheel\"]\n        B488[\"488\"]\n        B489[\"489\"]\n        B490[\"490\"]\n        DOTS[\"...\"]\n        B487[\"487\"]\n    end\n\n    CURSOR[\"Currentcursor\"] -.-&gt; B488\n</code></pre> <p>When <code>callout_reset(&amp;c, 50, func, arg)</code> is called:</p> <ul> <li>Expiration tick = 1000 + 50 = 1050</li> <li>Bucket = 1050 % 512 = 26</li> <li>Insert callout into bucket 26</li> </ul> <p>When ticks reaches 1050:</p> <ul> <li>Cursor advances to bucket 26</li> <li>Callout found and executed</li> </ul> <p>Insertion Algorithm: <pre><code>void\ncallout_reset(struct callout *c, int ticks, void (*func)(void *), void *arg)\n{\n    struct softclock_pcpu *sc = &amp;softclock_pcpu_ary[mycpuid];\n    struct _callout *cc;\n    int expire_tick;\n    int bucket;\n\n    spin_lock(&amp;sc-&gt;spin);\n\n    /* Calculate absolute expiration */\n    expire_tick = sc-&gt;softticks + ticks;\n\n    /* Allocate backend if needed */\n    if (c-&gt;lh == NULL) {\n        cc = callout_alloc(sc);\n        c-&gt;lh = cc;\n        cc-&gt;c_lh = c;\n    } else {\n        cc = c-&gt;lh;\n        /* Remove from current position if pending */\n        if (cc-&gt;c_flags &amp; CALLOUT_PENDING) {\n            TAILQ_REMOVE(&amp;sc-&gt;callwheel[cc-&gt;c_time % BUCKETS], cc, c_links.tq);\n        }\n    }\n\n    /* Setup callout */\n    cc-&gt;c_func = func;\n    cc-&gt;c_arg = arg;\n    cc-&gt;c_time = expire_tick;\n    cc-&gt;c_flags = CALLOUT_PENDING | CALLOUT_ACTIVE;\n\n    /* Insert into appropriate bucket */\n    if (ticks &lt; BUCKETS) {\n        /* Within wheel range */\n        bucket = expire_tick % BUCKETS;\n        TAILQ_INSERT_TAIL(&amp;sc-&gt;callwheel[bucket], cc, c_links.tq);\n    } else {\n        /* Beyond wheel range - use overflow list */\n        TAILQ_INSERT_TAIL(&amp;sc-&gt;calltodo, cc, c_links.tq);\n    }\n\n    spin_unlock(&amp;sc-&gt;spin);\n}\n</code></pre></p> <p>Processing Algorithm (in softclock): <pre><code>void\nsoftclock(systimer_t info, int in_ipi, struct intrframe *frame)\n{\n    struct softclock_pcpu *sc = &amp;softclock_pcpu_ary[mycpuid];\n    struct _callout *cc;\n    int curticks;\n\n    spin_lock(&amp;sc-&gt;spin);\n\n    /* Process all ticks since last run */\n    while (sc-&gt;softticks != ticks) {\n        sc-&gt;curticks = ++sc-&gt;softticks;\n        curticks = sc-&gt;curticks;\n\n        /* Advance cursor to current bucket */\n        sc-&gt;callcursor = &amp;sc-&gt;callwheel[curticks % BUCKETS];\n\n        /* Process all callouts in current bucket */\n        while ((cc = TAILQ_FIRST(sc-&gt;callcursor)) != NULL) {\n            if (cc-&gt;c_time != curticks) {\n                /* Not expired yet (wheel wrapped) */\n                break;\n            }\n\n            /* Remove from wheel */\n            TAILQ_REMOVE(sc-&gt;callcursor, cc, c_links.tq);\n            cc-&gt;c_flags &amp;= ~CALLOUT_PENDING;\n\n            /* Mark as running */\n            sc-&gt;running = cc;\n\n            spin_unlock(&amp;sc-&gt;spin);\n\n            /* Execute callback */\n            cc-&gt;c_func(cc-&gt;c_arg);\n\n            spin_lock(&amp;sc-&gt;spin);\n\n            /* Mark completed */\n            sc-&gt;running = NULL;\n            cc-&gt;c_flags |= CALLOUT_EXECUTED;\n        }\n\n        /* Move callouts from overflow list to wheel if in range */\n        migrate_callouts_from_overflow(sc);\n    }\n\n    spin_unlock(&amp;sc-&gt;spin);\n}\n</code></pre></p>"},{"location":"sys/kern/time/#example-4-ntp-time-adjustment-flow","title":"Example 4: NTP Time Adjustment Flow","text":"<p>Location: kern_ntptime.c, kern_clock.c</p> <pre><code>/*\n * NTP adjustment happens in hardclock on CPU 0\n */\nvoid\nntp_update_second(struct timespec *btp, int ticks)\n{\n    int64_t time_adj;\n\n    /* Phase-Locked Loop (PLL) adjustment */\n    if (time_status &amp; STA_PLL) {\n        time_adj = time_offset / 1000;  /* Scale down */\n\n        /* Apply adjustment limit */\n        if (time_adj &gt; MAXPHASE)\n            time_adj = MAXPHASE;\n        else if (time_adj &lt; -MAXPHASE)\n            time_adj = -MAXPHASE;\n\n        /* Apply adjustment to current second */\n        btp-&gt;tv_nsec += time_adj;\n\n        /* Reduce offset */\n        time_offset -= time_adj * 1000;\n    }\n\n    /* Frequency-Locked Loop (FLL) adjustment */\n    if (time_status &amp; STA_FLL) {\n        /* Adjust frequency based on long-term drift */\n        time_freq += (time_offset / (1LL &lt;&lt; 32));\n    }\n\n    /* Normalize timespec */\n    if (btp-&gt;tv_nsec &gt;= 1000000000) {\n        btp-&gt;tv_nsec -= 1000000000;\n        btp-&gt;tv_sec++;\n    } else if (btp-&gt;tv_nsec &lt; 0) {\n        btp-&gt;tv_nsec += 1000000000;\n        btp-&gt;tv_sec--;\n    }\n\n    /* Update basetime FIFO write pointer */\n    basetime_index = (basetime_index + 1) % BASETIME_ARYSIZE;\n    basetime[basetime_index] = *btp;\n}\n</code></pre> <p>Flow: <pre><code>NTP Daemon (ntpd in userspace)\n        \u2193\nsys_ntp_adjtime() [kern_ntptime.c:260]\n        \u251c\u2500&gt; Lock ntp_lock\n        \u251c\u2500&gt; Update time_offset, time_freq\n        \u251c\u2500&gt; Set time_status (PLL/FLL mode)\n        \u2514\u2500&gt; Unlock ntp_lock\n\nEvery hardclock on CPU 0:\n        \u2193\nhardclock() [kern_clock.c:552]\n        \u251c\u2500&gt; Read basetime[read_index]\n        \u251c\u2500&gt; ntp_update_second(&amp;basetime[write_index])\n        \u2502   \u251c\u2500&gt; Calculate adjustment based on time_offset\n        \u2502   \u251c\u2500&gt; Apply PLL correction (phase)\n        \u2502   \u251c\u2500&gt; Apply FLL correction (frequency)\n        \u2502   \u251c\u2500&gt; Adjust tv_nsec by small amount\n        \u2502   \u251c\u2500&gt; Normalize timespec\n        \u2502   \u2514\u2500&gt; Increment write_index\n        \u2514\u2500&gt; Update time_second from basetime[read_index]\n\nResult: Time gradually converges to NTP reference\n</code></pre></p> <p>Basetime FIFO: <pre><code>basetime[16] = FIFO of timespec values\n\nWrite end (NTP adjusts):     Read end (hardclock uses):\n        \u2193                             \u2193\n[0][1][2][3][4][5][6][7][8][9][10][11][12][13][14][15]\n\n- NTP code writes adjusted time to write_index\n- hardclock reads from read_index\n- FIFO provides lock-free communication\n- 16 entries give ~160ms buffer at hz=100\n</code></pre></p>"},{"location":"sys/kern/time/#best-practices","title":"Best Practices","text":""},{"location":"sys/kern/time/#choosing-timer-facilities","title":"Choosing Timer Facilities","text":"<p>Use callouts when: - Need to schedule a callback in the future - Timeout needed for asynchronous operations - Per-device or per-connection timers</p> <p>Use systimers when: - Need periodic system-wide events - Implementing new clock types - Low-level kernel timing infrastructure</p> <p>Use sleep/tsleep when: - Need to block a thread for a duration - Waiting for an event with timeout - User-space style blocking</p>"},{"location":"sys/kern/time/#callout-usage-guidelines","title":"Callout Usage Guidelines","text":"<ol> <li>Always initialize: Call <code>callout_init()</code> or variant before use</li> <li>MP-safe by default: Use <code>callout_init_mp()</code> for new code</li> <li>Cancel on cleanup: Use <code>callout_terminate()</code> during shutdown</li> <li>Check return values: <code>callout_stop()</code> returns whether callout was pending</li> <li>Avoid long callbacks: Callouts run in softclock context; keep them brief</li> <li>Use appropriate CPU: Consider <code>callout_reset_bycpu()</code> for load distribution</li> </ol>"},{"location":"sys/kern/time/#time-retrieval-guidelines","title":"Time Retrieval Guidelines","text":"<p>For logging/coarse timestamps: <pre><code>getmicrotime(&amp;tv);    /* Low overhead, ~10ms resolution */\n</code></pre></p> <p>For accurate timestamps: <pre><code>microtime(&amp;tv);       /* Higher overhead, hardware precision */\n</code></pre></p> <p>For measuring intervals: <pre><code>microuptime(&amp;tv1);\n/* ... operation ... */\nmicrouptime(&amp;tv2);\ntimevalsub(&amp;tv2, &amp;tv1);  /* tv2 now contains elapsed time */\n</code></pre></p> <p>Why use *uptime variants for intervals?: - Not affected by NTP adjustments - Monotonically increasing - Accurate for performance measurements</p>"},{"location":"sys/kern/time/#ntp-considerations","title":"NTP Considerations","text":"<ul> <li>NTP adjustments are gradual (typically &lt;500ppm frequency adjustment)</li> <li>Realtime can jump backward with <code>settimeofday()</code> but not with NTP</li> <li>For intervals, always use monotonic time (*uptime functions)</li> <li>Maximum NTP adjustment: \u00b1500ms with PLL, more with FLL</li> </ul>"},{"location":"sys/kern/time/#see-also","title":"See Also","text":"<ul> <li>LWKT (Lightweight Kernel Threads) - Thread scheduling and execution</li> <li>Synchronization Primitives - Locks and synchronization used by timers</li> <li>Memory Management - Memory allocation for timer structures</li> </ul>"},{"location":"sys/kern/time/#references","title":"References","text":"<ul> <li><code>sys/kern/kern_clock.c</code> - System clock implementation</li> <li><code>sys/kern/kern_timeout.c</code> - Callout/timeout mechanism</li> <li><code>sys/kern/kern_systimer.c</code> - Software timer infrastructure</li> <li><code>sys/kern/kern_cputimer.c</code> - Hardware timer abstraction</li> <li><code>sys/kern/kern_time.c</code> - Time-related system calls</li> <li><code>sys/kern/kern_ntptime.c</code> - NTP time adjustment</li> <li><code>sys/sys/systimer.h</code> - Timer data structure definitions</li> <li><code>sys/sys/callout.h</code> - Callout API definitions</li> <li><code>sys/sys/time.h</code> - Time representation types</li> </ul>"},{"location":"sys/kern/ipc/mbufs/","title":"Mbuf Memory Buffers","text":"<p>The mbuf (memory buffer) subsystem provides efficient, flexible memory management for network data. Mbufs are the fundamental unit of memory used throughout the DragonFly BSD networking stack for storing packet data, protocol headers, and socket buffers.</p> <p>Source files:</p> <ul> <li><code>sys/kern/uipc_mbuf.c</code> - Core mbuf allocation and manipulation</li> <li><code>sys/kern/uipc_mbuf2.c</code> - Extended mbuf operations and packet tags</li> <li><code>sys/sys/mbuf.h</code> - Structure definitions and macros</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#design-overview","title":"Design Overview","text":"<p>The mbuf system is designed around several key principles:</p> <ol> <li>Fixed-size allocation - Mbufs are a single size (<code>MSIZE</code>), reducing fragmentation</li> <li>Cluster attachment - Large data uses external clusters rather than mbuf chains</li> <li>Per-CPU caching - Object caches provide lock-free allocation on each CPU</li> <li>Reference counting - Clusters can be shared across multiple mbufs</li> <li>Zero-copy optimization - Data can be shared without copying when safe</li> </ol>"},{"location":"sys/kern/ipc/mbufs/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":"<ul> <li>Per-CPU statistics: <code>mbstat[SMP_MAXCPU]</code> and <code>mbtypes[SMP_MAXCPU]</code> arrays with <code>__cachealign</code> for cache-line isolation</li> <li>Object cache integration: Uses DragonFly's <code>objcache(9)</code> for efficient per-CPU allocation</li> <li>Message-passing support: Embedded <code>netmsg</code> structures in mbuf headers for LWKT message passing</li> <li>Jumbo cluster support: Native support for jumbo frames via <code>MJUMPAGESIZE</code> clusters</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/mbufs/#struct-mbuf","title":"struct mbuf","text":"<p>The core mbuf structure (<code>sys/mbuf.h:206</code>):</p> <pre><code>struct mbuf {\n    struct m_hdr m_hdr;\n    union {\n        struct {\n            struct pkthdr MH_pkthdr;    /* M_PKTHDR set */\n            union {\n                struct m_ext MH_ext;    /* M_EXT set */\n                char MH_databuf[MHLEN];\n            } MH_dat;\n        } MH;\n        char M_databuf[MLEN];           /* !M_PKTHDR, !M_EXT */\n    } M_dat;\n};\n</code></pre> <p>Convenience macros provide field access:</p> Macro Field Description <code>m_next</code> <code>m_hdr.mh_next</code> Next buffer in chain <code>m_nextpkt</code> <code>m_hdr.mh_nextpkt</code> Next chain in queue <code>m_data</code> <code>m_hdr.mh_data</code> Pointer to data <code>m_len</code> <code>m_hdr.mh_len</code> Data length in this mbuf <code>m_flags</code> <code>m_hdr.mh_flags</code> Flags (M_EXT, M_PKTHDR, etc.) <code>m_type</code> <code>m_hdr.mh_type</code> Type of data <code>m_pkthdr</code> <code>M_dat.MH.MH_pkthdr</code> Packet header (if M_PKTHDR) <code>m_ext</code> <code>M_dat.MH.MH_dat.MH_ext</code> External storage (if M_EXT)"},{"location":"sys/kern/ipc/mbufs/#struct-m_hdr","title":"struct m_hdr","text":"<p>The mbuf header (<code>sys/mbuf.h:79</code>):</p> <pre><code>struct m_hdr {\n    struct mbuf *mh_next;       /* next buffer in chain */\n    union {\n        struct mbuf *mh_nextpkt;\n        STAILQ_ENTRY(mbuf) mh_stailqpkt;\n    };\n    caddr_t mh_data;            /* location of data */\n    int mh_len;                 /* amount of data */\n    int mh_flags;               /* flags */\n    short mh_type;              /* type of data */\n    short mh_pad;\n    union {\n        struct netmsg_packet mhm_pkt;   /* hardware-&gt;proto msg */\n        struct netmsg_pru_send mhm_snd; /* userspace-&gt;proto msg */\n        struct netmsg_inarp mhm_arp;    /* arpinput msg */\n        struct netmsg_ctlinput mhm_ctl; /* ctlinput msg */\n        struct netmsg_genpkt mhm_gen;   /* generic pkt msg */\n        struct netmsg_forward mhm_fwd;  /* forwarding msg */\n    } mh_msgu;\n};\n</code></pre> <p>The embedded <code>netmsg</code> union enables efficient LWKT message passing without separate allocation.</p>"},{"location":"sys/kern/ipc/mbufs/#struct-pkthdr","title":"struct pkthdr","text":"<p>Packet header for first mbuf in chain (<code>sys/mbuf.h:153</code>):</p> <pre><code>struct pkthdr {\n    struct ifnet *rcvif;        /* receive interface */\n    struct packet_tags tags;    /* list of packet tags */\n    void *header;               /* pointer to packet header */\n    int len;                    /* total packet length */\n    int csum_flags;             /* checksum flags */\n    int csum_data;              /* checksum data */\n    uint16_t csum_iphlen;       /* IP header length */\n    uint8_t csum_thlen;         /* TCP/UDP header length */\n    uint8_t csum_lhlen;         /* link header length */\n    uint16_t tso_segsz;         /* TSO segment size */\n    uint16_t ether_vlantag;     /* VLAN tag */\n    uint16_t hash;              /* packet hash */\n    /* ... additional fields ... */\n    struct pkthdr_pf pf;        /* PF state */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#struct-m_ext","title":"struct m_ext","text":"<p>External storage descriptor (<code>sys/mbuf.h:194</code>):</p> <pre><code>struct m_ext {\n    caddr_t ext_buf;            /* start of buffer */\n    void (*ext_free)(void *);   /* free function */\n    u_int ext_size;             /* size of buffer */\n    void (*ext_ref)(void *);    /* reference function */\n    void *ext_arg;              /* argument for callbacks */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#struct-mbcluster","title":"struct mbcluster","text":"<p>Cluster metadata for reference counting (<code>uipc_mbuf.c:101</code>):</p> <pre><code>struct mbcluster {\n    int32_t mcl_refs;           /* reference count */\n    void *mcl_data;             /* pointer to cluster data */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#struct-mbstat","title":"struct mbstat","text":"<p>Per-CPU statistics (<code>sys/mbuf.h:342</code>):</p> <pre><code>struct mbstat {\n    u_long m_mbufs;         /* mbufs obtained */\n    u_long m_clusters;      /* clusters obtained */\n    u_long m_jclusters;     /* jumbo clusters obtained */\n    u_long m_clfree;        /* free clusters */\n    u_long m_drops;         /* allocation failures */\n    u_long m_wait;          /* times waited for space */\n    u_long m_drain;         /* times drained protocols */\n    u_long m_mcfail;        /* m_copym failures */\n    u_long m_mpfail;        /* m_pullup failures */\n    /* ... size constants ... */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#memory-layout","title":"Memory Layout","text":""},{"location":"sys/kern/ipc/mbufs/#size-constants","title":"Size Constants","text":"Constant Description <code>MSIZE</code> Total mbuf size (256 bytes typical) <code>MLEN</code> Data area in plain mbuf (<code>MSIZE - sizeof(m_hdr)</code>) <code>MHLEN</code> Data area with packet header (<code>MLEN - sizeof(pkthdr)</code>) <code>MCLBYTES</code> Standard cluster size (2048 bytes) <code>MJUMPAGESIZE</code> Jumbo cluster size (PAGE_SIZE) <code>MINCLSIZE</code> Minimum size to use cluster (<code>MHLEN + 1</code>)"},{"location":"sys/kern/ipc/mbufs/#mbuf-variants","title":"Mbuf Variants","text":"<pre><code>Plain mbuf (no M_PKTHDR, no M_EXT):\n+------------------+\n| m_hdr            |\n+------------------+\n| m_dat[MLEN]      |  &lt;- m_data points here\n+------------------+\n\nPacket header mbuf (M_PKTHDR, no M_EXT):\n+------------------+\n| m_hdr            |\n+------------------+\n| pkthdr           |\n+------------------+\n| m_pktdat[MHLEN]  |  &lt;- m_data points here\n+------------------+\n\nMbuf with cluster (M_EXT):\n+------------------+     +--------------------+\n| m_hdr            |     | cluster data       |\n+------------------+     | (MCLBYTES)         |\n| pkthdr (optional)|     |                    |\n+------------------+     |                    |\n| m_ext            |----&gt;|                    |\n+------------------+     +--------------------+\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#mbuf-flags","title":"Mbuf Flags","text":""},{"location":"sys/kern/ipc/mbufs/#core-flags","title":"Core Flags","text":"Flag Value Description <code>M_EXT</code> 0x0001 Has external storage <code>M_PKTHDR</code> 0x0002 Start of record/packet <code>M_EOR</code> 0x0004 End of record <code>M_CLCACHE</code> 0x2000 Allocated from cluster cache <code>M_EXT_CLUSTER</code> 0x4000 Standard cluster (not custom) <code>M_PHCACHE</code> 0x8000 Allocated from packet header cache"},{"location":"sys/kern/ipc/mbufs/#packet-flags","title":"Packet Flags","text":"Flag Value Description <code>M_BCAST</code> 0x0100 Broadcast packet <code>M_MCAST</code> 0x0200 Multicast packet <code>M_FRAG</code> 0x0400 Fragment of larger packet <code>M_FIRSTFRAG</code> 0x0800 First fragment <code>M_LASTFRAG</code> 0x1000 Last fragment <code>M_VLANTAG</code> 0x20000 VLAN tag valid <code>M_HASH</code> 0x100000 Hash field valid"},{"location":"sys/kern/ipc/mbufs/#mbuf-types","title":"Mbuf Types","text":"Type Value Description <code>MT_FREE</code> 0 On free list <code>MT_DATA</code> 1 Dynamic data <code>MT_HEADER</code> 2 Packet header <code>MT_SONAME</code> 3 Socket name <code>MT_CONTROL</code> 5 Control message <code>MT_OOBDATA</code> 6 Out-of-band data"},{"location":"sys/kern/ipc/mbufs/#object-caches","title":"Object Caches","text":"<p>The mbuf subsystem uses eight specialized object caches for efficient allocation (<code>uipc_mbuf.c:91-98</code>):</p> Cache Contents <code>mbuf_cache</code> Plain mbufs <code>mbufphdr_cache</code> Mbufs with packet header <code>mclmeta_cache</code> Standard cluster metadata <code>mjclmeta_cache</code> Jumbo cluster metadata <code>mbufcluster_cache</code> Mbuf + standard cluster <code>mbufphdrcluster_cache</code> Mbuf + pkthdr + standard cluster <code>mbufjcluster_cache</code> Mbuf + jumbo cluster <code>mbufphdrjcluster_cache</code> Mbuf + pkthdr + jumbo cluster <p>Each cache is configured with constructor/destructor functions and uses per-CPU magazines for lock-free fast-path allocation.</p>"},{"location":"sys/kern/ipc/mbufs/#cache-initialization","title":"Cache Initialization","text":"<p>Caches are initialized in <code>mbinit()</code> (<code>uipc_mbuf.c:253</code>):</p> <pre><code>SYSINIT(mbuf, SI_BOOT2_MACHDEP, SI_ORDER_FIRST, mbinit, NULL);\n</code></pre> <p>The <code>mbinit_cluster()</code> function creates cluster caches using <code>SYSINIT</code> at <code>SI_ORDER_ANY</code> to ensure VM is ready.</p>"},{"location":"sys/kern/ipc/mbufs/#allocation-functions","title":"Allocation Functions","text":""},{"location":"sys/kern/ipc/mbufs/#basic-allocation","title":"Basic Allocation","text":"Function Description <code>m_get(how, type)</code> Allocate plain mbuf <code>m_gethdr(how, type)</code> Allocate mbuf with packet header <code>m_getcl(how, type, flags)</code> Allocate mbuf with standard cluster <code>m_getjcl(how, type, flags, size)</code> Allocate mbuf with jumbo cluster <code>m_getl(len, how, type, flags, psize)</code> Allocate appropriate mbuf for length <code>m_getc(len, how, type)</code> Allocate mbuf chain for length <p>The <code>how</code> parameter is either <code>M_WAITOK</code> (can block) or <code>M_NOWAIT</code> (fails immediately if unavailable).</p>"},{"location":"sys/kern/ipc/mbufs/#m_get-m_gethdr","title":"m_get / m_gethdr","text":"<p>Basic mbuf allocation (<code>uipc_mbuf.c:423-457</code>):</p> <pre><code>struct mbuf *\nm_get(int how, int type)\n{\n    struct mbuf *m;\n    int ntries = 0;\n\nretryonce:\n    m = objcache_get(mbuf_cache, MB_OCFLAG(how));\n    if (m == NULL) {\n        if (how == M_WAITOK &amp;&amp; ntries++ == 0) {\n            m_reclaim();\n            goto retryonce;\n        }\n        ++mbstat[mycpu-&gt;gd_cpuid].m_drops;\n        return (NULL);\n    }\n    /* ... initialization ... */\n    return (m);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_getcl","title":"m_getcl","text":"<p>Combined mbuf+cluster allocation (<code>uipc_mbuf.c:517-545</code>):</p> <pre><code>struct mbuf *\nm_getcl(int how, short type, int flags)\n{\n    struct mbuf *m;\n    int ntries = 0;\n\nretryonce:\n    if (flags &amp; M_PKTHDR)\n        m = objcache_get(mbufphdrcluster_cache, MB_OCFLAG(how));\n    else\n        m = objcache_get(mbufcluster_cache, MB_OCFLAG(how));\n    /* ... */\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_getl-inline","title":"m_getl (Inline)","text":"<p>Smart allocation based on length (<code>sys/mbuf.h:583-601</code>):</p> <pre><code>static __inline struct mbuf *\nm_getl(int len, int how, int type, int flags, int *psize)\n{\n    struct mbuf *m;\n    int size;\n\n    if (len &gt;= MINCLSIZE) {\n        m = m_getcl(how, type, flags);\n        size = MCLBYTES;\n    } else if (flags &amp; M_PKTHDR) {\n        m = m_gethdr(how, type);\n        size = MHLEN;\n    } else {\n        m = m_get(how, type);\n        size = MLEN;\n    }\n    if (psize != NULL)\n        *psize = size;\n    return m;\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_getc","title":"m_getc","text":"<p>Allocate chain for specified length (<code>uipc_mbuf.c:1154-1201</code>):</p> <pre><code>struct mbuf *\nm_getc(int len, int how, int type)\n{\n    struct mbuf *nfirst = NULL, *n;\n    int nsize;\n\n    while (len &gt; 0) {\n        n = m_getl(len, how, type, nfirst == NULL ? M_PKTHDR : 0, &amp;nsize);\n        if (n == NULL)\n            goto failed;\n        if (nfirst == NULL)\n            nfirst = n;\n        /* ... chain building ... */\n        len -= nsize;\n    }\n    return (nfirst);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#deallocation-functions","title":"Deallocation Functions","text":""},{"location":"sys/kern/ipc/mbufs/#m_free","title":"m_free","text":"<p>Free single mbuf (<code>uipc_mbuf.c:1307-1456</code>):</p> <pre><code>struct mbuf *\nm_free(struct mbuf *m)\n{\n    struct mbuf *n;\n    struct globaldata *gd = mycpu;\n\n    KASSERT(m-&gt;m_type != MT_FREE, (\"freeing free mbuf %p\", m));\n    --mbtypes[gd-&gt;gd_cpuid].stats[m-&gt;m_type];\n    n = m-&gt;m_next;\n\n    /* Clean up and return to appropriate cache */\n    switch (m-&gt;m_flags &amp; (M_CLCACHE | M_EXT | M_EXT_CLUSTER)) {\n    case M_CLCACHE | M_EXT | M_EXT_CLUSTER:\n        /* Return to combined mbuf+cluster cache if not shared */\n        if (m_sharecount(m) == 1) {\n            m-&gt;m_data = m-&gt;m_ext.ext_buf;\n            objcache_put(mbufcluster_cache, m);\n        } else {\n            /* Cluster shared, must disconnect */\n            m-&gt;m_ext.ext_free(m-&gt;m_ext.ext_arg);\n            objcache_dtor(mbufcluster_cache, m);\n        }\n        break;\n    /* ... other cases ... */\n    }\n    return (n);\n}\n</code></pre> <p>Key handling for shared clusters: when <code>m_sharecount(m) &gt; 1</code>, the mbuf cannot be returned to the combined cache and must be destroyed.</p>"},{"location":"sys/kern/ipc/mbufs/#m_freem","title":"m_freem","text":"<p>Free entire mbuf chain (<code>uipc_mbuf.c:1469-1474</code>):</p> <pre><code>void\nm_freem(struct mbuf *m)\n{\n    while (m)\n        m = m_free(m);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#cluster-reference-counting","title":"Cluster Reference Counting","text":"<p>Clusters use atomic reference counting for safe sharing (<code>uipc_mbuf.c:1263-1296</code>):</p>"},{"location":"sys/kern/ipc/mbufs/#m_mclref","title":"m_mclref","text":"<pre><code>static void\nm_mclref(void *arg)\n{\n    struct mbcluster *mcl = arg;\n    atomic_add_int(&amp;mcl-&gt;mcl_refs, 1);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_mclfree","title":"m_mclfree","text":"<pre><code>static void\nm_mclfree(void *arg)\n{\n    struct mbcluster *mcl = arg;\n    if (atomic_fetchadd_int(&amp;mcl-&gt;mcl_refs, -1) == 1) {\n        --mbstat[mycpu-&gt;gd_cpuid].m_clusters;\n        objcache_put(mclmeta_cache, mcl);\n    }\n}\n</code></pre> <p>The <code>atomic_fetchadd_int()</code> returns the previous value, so a return of 1 means the reference count is now 0.</p>"},{"location":"sys/kern/ipc/mbufs/#m_sharecount","title":"m_sharecount","text":"<p>Check if cluster is shared (<code>uipc_mbuf.c:1040-1049</code>):</p> <pre><code>int\nm_sharecount(struct mbuf *m)\n{\n    if (m-&gt;m_flags &amp; (M_EXT | M_EXT_CLUSTER)) {\n        struct mbcluster *mcl = m-&gt;m_ext.ext_arg;\n        return mcl-&gt;mcl_refs;\n    }\n    return 1;  /* Not external, single reference */\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#chain-manipulation-functions","title":"Chain Manipulation Functions","text":""},{"location":"sys/kern/ipc/mbufs/#m_copym","title":"m_copym","text":"<p>Create read-only copy of mbuf chain (<code>uipc_mbuf.c:1530-1603</code>):</p> <pre><code>struct mbuf *\nm_copym(const struct mbuf *m, int off0, int len, int wait)\n</code></pre> <ul> <li>Copies from offset <code>off0</code> for <code>len</code> bytes (or <code>M_COPYALL</code>)</li> <li>Clusters are shared (reference count incremented), not copied</li> <li>Result is read-only due to shared clusters</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_copypacket","title":"m_copypacket","text":"<p>Optimized full packet copy (<code>uipc_mbuf.c:1614-1665</code>):</p> <pre><code>struct mbuf *\nm_copypacket(struct mbuf *m, int how)\n</code></pre> <ul> <li>Equivalent to <code>m_copym(m, 0, M_COPYALL, how)</code></li> <li>Preserves alignment of first mbuf</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_dup","title":"m_dup","text":"<p>Create writable copy (<code>uipc_mbuf.c:1704-1759</code>):</p> <pre><code>struct mbuf *\nm_dup(struct mbuf *m, int how)\n</code></pre> <ul> <li>Copies all data (clusters are copied, not shared)</li> <li>Result is fully writable</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_cat","title":"m_cat","text":"<p>Concatenate chains (<code>uipc_mbuf.c:1841-1857</code>):</p> <pre><code>void\nm_cat(struct mbuf *m, struct mbuf *n)\n{\n    m = m_last(m);\n    while (n) {\n        if (m-&gt;m_flags &amp; M_EXT ||\n            m-&gt;m_data + m-&gt;m_len + n-&gt;m_len &gt;= &amp;m-&gt;m_dat[MLEN]) {\n            /* Just link chains */\n            m-&gt;m_next = n;\n            return;\n        }\n        /* Copy data into trailing space */\n        bcopy(mtod(n, caddr_t), mtod(m, caddr_t) + m-&gt;m_len, n-&gt;m_len);\n        m-&gt;m_len += n-&gt;m_len;\n        n = m_free(n);\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_adj","title":"m_adj","text":"<p>Trim data from head or tail (<code>uipc_mbuf.c:1859-1928</code>):</p> <pre><code>void\nm_adj(struct mbuf *mp, int req_len)\n</code></pre> <ul> <li>Positive <code>req_len</code>: trim from head</li> <li>Negative <code>req_len</code>: trim from tail</li> <li>Updates <code>m_pkthdr.len</code> if present</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_pullup","title":"m_pullup","text":"<p>Make initial bytes contiguous (<code>uipc_mbuf.c:2103-2159</code>):</p> <pre><code>struct mbuf *\nm_pullup(struct mbuf *n, int len)\n</code></pre> <ul> <li>Ensures first <code>len</code> bytes are in first mbuf's data area</li> <li>Required for protocol header access via casting</li> <li>Frees original and returns NULL on failure</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_pulldown","title":"m_pulldown","text":"<p>Make arbitrary region contiguous (<code>uipc_mbuf2.c:89-234</code>):</p> <pre><code>struct mbuf *\nm_pulldown(struct mbuf *m, int off, int len, int *offp)\n</code></pre> <ul> <li>Makes bytes <code>[off, off+len)</code> contiguous</li> <li>More flexible than <code>m_pullup()</code></li> <li>Returns mbuf containing the region; <code>*offp</code> is offset within that mbuf</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_split","title":"m_split","text":"<p>Partition chain (<code>uipc_mbuf.c:2171-2229</code>):</p> <pre><code>struct mbuf *\nm_split(struct mbuf *m0, int len0, int wait)\n</code></pre> <ul> <li>Returns tail of chain starting at offset <code>len0</code></li> <li>Original chain is truncated to <code>len0</code> bytes</li> <li>May share clusters (result may be read-only)</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_copydata","title":"m_copydata","text":"<p>Copy to linear buffer (<code>uipc_mbuf.c:1671-1697</code>):</p> <pre><code>void\nm_copydata(const struct mbuf *m, int off, int len, void *cp)\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_copyback","title":"m_copyback","text":"<p>Copy from linear buffer (<code>uipc_mbuf.c:2408-2416</code>):</p> <pre><code>void\nm_copyback(struct mbuf *m0, int off, int len, const void *cp)\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#m_defrag","title":"m_defrag","text":"<p>Defragment chain (<code>uipc_mbuf.c:2591-2659</code>):</p> <pre><code>struct mbuf *\nm_defrag(struct mbuf *m0, int how)\n</code></pre> <ul> <li>Creates shortest possible chain</li> <li>Useful before DMA operations requiring few scatter-gather entries</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_unshare","title":"m_unshare","text":"<p>Create writable chain (<code>uipc_mbuf.c:1958-2093</code>):</p> <pre><code>struct mbuf *\nm_unshare(struct mbuf *m0, int how)\n</code></pre> <ul> <li>Replaces shared clusters with private copies</li> <li>Compacts chain where possible</li> <li>Used before encryption/compression that modifies data in place</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#device-interface-functions","title":"Device Interface Functions","text":""},{"location":"sys/kern/ipc/mbufs/#m_devget","title":"m_devget","text":"<p>Copy from device memory to mbuf chain (<code>uipc_mbuf.c:2235-2270</code>):</p> <pre><code>struct mbuf *\nm_devget(void *buf, int len, int offset, struct ifnet *ifp)\n</code></pre> <ul> <li>Creates chain from linear device buffer</li> <li>Sets <code>m_pkthdr.rcvif</code> to receiving interface</li> <li>Leaves room for <code>max_linkhdr</code> in first mbuf</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#m_devpad","title":"m_devpad","text":"<p>Pad packet to minimum length (<code>uipc_mbuf.c:2275-2318</code>):</p> <pre><code>int\nm_devpad(struct mbuf *m, int padto)\n</code></pre> <ul> <li>Pads packet to <code>padto</code> bytes</li> <li>Required for Ethernet minimum frame size</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#packet-tags","title":"Packet Tags","text":"<p>Packet tags attach auxiliary information to mbufs without modifying the mbuf structure.</p>"},{"location":"sys/kern/ipc/mbufs/#struct-m_tag","title":"struct m_tag","text":"<p>Tag structure (<code>sys/mbuf.h:138</code>):</p> <pre><code>struct m_tag {\n    SLIST_ENTRY(m_tag) m_tag_link;  /* List linkage */\n    uint16_t m_tag_id;              /* Tag ID */\n    uint16_t m_tag_len;             /* Data length */\n    uint32_t m_tag_cookie;          /* ABI/Module ID */\n};\n</code></pre> <p>Data follows immediately after the structure.</p>"},{"location":"sys/kern/ipc/mbufs/#tag-functions","title":"Tag Functions","text":"Function Description <code>m_tag_alloc(cookie, type, len, how)</code> Allocate tag with data <code>m_tag_free(t)</code> Free tag <code>m_tag_prepend(m, t)</code> Add tag to mbuf <code>m_tag_unlink(m, t)</code> Remove tag from mbuf <code>m_tag_delete(m, t)</code> Remove and free tag <code>m_tag_delete_chain(m)</code> Free all tags <code>m_tag_locate(m, cookie, type, t)</code> Find tag by cookie/type <code>m_tag_copy(t, how)</code> Copy single tag <code>m_tag_copy_chain(to, from, how)</code> Copy all tags"},{"location":"sys/kern/ipc/mbufs/#tag-usage-example","title":"Tag Usage Example","text":"<pre><code>struct m_tag *tag;\n\n/* Allocate and attach */\ntag = m_tag_alloc(MTAG_ABI_COMPAT, type, sizeof(data), M_NOWAIT);\nif (tag != NULL) {\n    bcopy(&amp;data, tag + 1, sizeof(data));\n    m_tag_prepend(m, tag);\n}\n\n/* Find and use */\ntag = m_tag_locate(m, MTAG_ABI_COMPAT, type, NULL);\nif (tag != NULL) {\n    struct data *dp = (struct data *)(tag + 1);\n    /* use dp */\n}\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#utility-macros","title":"Utility Macros","text":""},{"location":"sys/kern/ipc/mbufs/#data-access","title":"Data Access","text":"<pre><code>mtod(m, t)          /* Cast m-&gt;m_data to type t */\nmtodoff(m, t, off)  /* Cast m-&gt;m_data + off to type t */\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#space-calculation","title":"Space Calculation","text":"<pre><code>M_LEADINGSPACE(m)   /* Bytes available before m_data */\nM_TRAILINGSPACE(m)  /* Bytes available after data */\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#writability-check","title":"Writability Check","text":"<pre><code>M_WRITABLE(m)       /* True if mbuf data is writable */\nM_EXT_WRITABLE(m)   /* True if cluster is writable (sharecount == 1) */\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#alignment","title":"Alignment","text":"<pre><code>M_ALIGN(m, len)     /* Align for 'len' bytes at end of plain mbuf */\nMH_ALIGN(m, len)    /* Align for 'len' bytes at end of pkthdr mbuf */\n</code></pre>"},{"location":"sys/kern/ipc/mbufs/#prepend","title":"Prepend","text":"<pre><code>M_PREPEND(m, plen, how)  /* Prepend 'plen' bytes to mbuf */\n</code></pre> <p>Adjusts <code>m_data</code> and <code>m_len</code>; allocates new mbuf if insufficient leading space.</p>"},{"location":"sys/kern/ipc/mbufs/#statistics-and-debugging","title":"Statistics and Debugging","text":""},{"location":"sys/kern/ipc/mbufs/#per-cpu-statistics","title":"Per-CPU Statistics","text":"<pre><code>extern struct mbstat mbstat[SMP_MAXCPU] __cachealign;\nextern struct mbtypes_stat mbtypes[SMP_MAXCPU] __cachealign;\n</code></pre> <p>Statistics are updated without locking using CPU-local arrays.</p>"},{"location":"sys/kern/ipc/mbufs/#debug-support","title":"Debug Support","text":"<p>When <code>MBUF_DEBUG</code> is defined:</p> <ul> <li><code>m-&gt;m_hdr.mh_lastfunc</code> tracks last function that touched mbuf</li> <li><code>mbuftrackid()</code> records operations</li> <li><code>_m_free()</code> / <code>_m_freem()</code> include caller name</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#sysctl-interface","title":"Sysctl Interface","text":"<ul> <li><code>kern.ipc.nmbclusters</code> - Maximum clusters</li> <li><code>kern.ipc.nmbufs</code> - Maximum mbufs</li> <li><code>kern.ipc.mbuf_wait</code> - Wait count for allocation</li> </ul>"},{"location":"sys/kern/ipc/mbufs/#memory-reclamation","title":"Memory Reclamation","text":"<p>When allocation fails with <code>M_WAITOK</code>, the system attempts reclamation (<code>uipc_mbuf.c:410-421</code>):</p> <pre><code>static void\nm_reclaim(void)\n{\n    struct domain *dp;\n    struct protosw *pr;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        for (pr = dp-&gt;dom_protosw; pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n            if (pr-&gt;pr_drain)\n                (*pr-&gt;pr_drain)();\n        }\n    }\n}\n</code></pre> <p>Protocols implement <code>pr_drain</code> to release cached mbufs.</p>"},{"location":"sys/kern/ipc/mbufs/#see-also","title":"See Also","text":"<ul> <li>Socket Core - Socket buffer management using mbufs</li> <li>VFS Buffer Cache - Similar caching concepts</li> <li>Memory Management - Kernel memory subsystem</li> </ul>"},{"location":"sys/kern/ipc/mqueue/","title":"POSIX Message Queues","text":"<p>POSIX message queues provide named, priority-based message passing between processes. DragonFly's implementation (<code>sys/kern/sys_mqueue.c</code>) follows IEEE Std 1003.1-2001 and was derived from NetBSD.</p>"},{"location":"sys/kern/ipc/mqueue/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/mqueue/#message-queue","title":"Message Queue","text":"<p>The <code>struct mqueue</code> (<code>sys/sys/mqueue.h:77</code>) represents a message queue:</p> <pre><code>struct mqueue {\n    char            mq_name[MQ_NAMELEN];\n    struct lock     mq_mtx;\n    int             mq_send_cv;     /* sleep channel for senders */\n    int             mq_recv_cv;     /* sleep channel for receivers */\n    struct mq_attr  mq_attrib;\n    /* Notification */\n    struct kqinfo   mq_rkq;         /* kqueue for read */\n    struct kqinfo   mq_wkq;         /* kqueue for write */\n    struct sigevent mq_sig_notify;\n    struct proc *   mq_notify_proc;\n    /* Permissions */\n    mode_t          mq_mode;\n    uid_t           mq_euid;\n    gid_t           mq_egid;\n    /* Message storage */\n    u_int           mq_refcnt;\n    TAILQ_HEAD(, mq_msg) mq_head[1 + MQ_PQSIZE];\n    uint32_t        mq_bitmap;\n    LIST_ENTRY(mqueue) mq_list;\n    struct timespec mq_atime;\n    struct timespec mq_mtime;\n    struct timespec mq_btime;\n};\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#message-attributes","title":"Message Attributes","text":"<p>The <code>struct mq_attr</code> (<code>sys/sys/mqueue.h:40</code>) holds queue configuration:</p> <pre><code>struct mq_attr {\n    long    mq_flags;       /* O_NONBLOCK, MQ_UNLINK, MQ_RECEIVE */\n    long    mq_maxmsg;      /* maximum messages in queue */\n    long    mq_msgsize;     /* maximum message size */\n    long    mq_curmsgs;     /* current message count */\n};\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#message-structure","title":"Message Structure","text":"<p>Individual messages use <code>struct mq_msg</code> (<code>sys/sys/mqueue.h:111</code>):</p> <pre><code>struct mq_msg {\n    TAILQ_ENTRY(mq_msg) msg_queue;\n    size_t              msg_len;\n    u_int               msg_prio;\n    int8_t              msg_ptr[1];     /* variable-length data */\n};\n</code></pre> <p>The <code>msg_ptr</code> field uses the struct hack pattern - actual allocation includes space for the message body.</p>"},{"location":"sys/kern/ipc/mqueue/#priority-queue-implementation","title":"Priority Queue Implementation","text":""},{"location":"sys/kern/ipc/mqueue/#constant-time-insertion","title":"Constant-Time Insertion","text":"<p>Messages are stored in priority queues for O(1) insertion. The queue uses 32 priority levels (<code>MQ_PQSIZE = 32</code>) plus one reserved queue:</p> <pre><code>#define MQ_PQSIZE   32      /* number of priority queues */\n#define MQ_PQRESQ   0       /* reserved queue index */\n</code></pre> <p>The <code>mq_head</code> array contains <code>MQ_PQSIZE + 1</code> TAILQ heads. Index 0 is reserved for overflow when <code>mq_prio_max</code> exceeds 32.</p>"},{"location":"sys/kern/ipc/mqueue/#bitmap-tracking","title":"Bitmap Tracking","text":"<p>A 32-bit bitmap (<code>mq_bitmap</code>) tracks which priority queues contain messages:</p> <pre><code>/* Inserting message at priority msg_prio */\nu_int idx = MQ_PQSIZE - msg_prio;\nTAILQ_INSERT_TAIL(&amp;mq-&gt;mq_head[idx], msg, msg_queue);\nmq-&gt;mq_bitmap |= (1 &lt;&lt; --idx);\n</code></pre> <p>The priority-to-index mapping (<code>MQ_PQSIZE - msg_prio</code>) ensures higher priorities map to lower indices, so <code>ffs()</code> (find first set) returns the highest priority queue.</p>"},{"location":"sys/kern/ipc/mqueue/#receiving-highest-priority","title":"Receiving Highest Priority","text":"<p><code>mq_receive1()</code> uses <code>ffs()</code> on the bitmap to find the highest-priority non-empty queue (<code>sys_mqueue.c:685-699</code>):</p> <pre><code>msg = TAILQ_FIRST(&amp;mq-&gt;mq_head[MQ_PQRESQ]);\nif (__predict_true(msg == NULL)) {\n    idx = ffs(mq-&gt;mq_bitmap);\n    msg = TAILQ_FIRST(&amp;mq-&gt;mq_head[idx]);\n}\nTAILQ_REMOVE(&amp;mq-&gt;mq_head[idx], msg, msg_queue);\n\n/* Clear bit if queue now empty */\nif (__predict_true(idx) &amp;&amp; TAILQ_EMPTY(&amp;mq-&gt;mq_head[idx])) {\n    mq-&gt;mq_bitmap &amp;= ~(1 &lt;&lt; --idx);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#reserved-queue","title":"Reserved Queue","text":"<p>If <code>mq_prio_max</code> is increased beyond 32 via sysctl, <code>mqueue_linear_insert()</code> (<code>sys_mqueue.c:204-218</code>) performs linear insertion into <code>MQ_PQRESQ</code>:</p> <pre><code>static inline void\nmqueue_linear_insert(struct mqueue *mq, struct mq_msg *msg)\n{\n    struct mq_msg *mit;\n\n    TAILQ_FOREACH(mit, &amp;mq-&gt;mq_head[MQ_PQRESQ], msg_queue) {\n        if (msg-&gt;msg_prio &gt; mit-&gt;msg_prio)\n            break;\n    }\n    if (mit == NULL)\n        TAILQ_INSERT_TAIL(&amp;mq-&gt;mq_head[MQ_PQRESQ], msg, msg_queue);\n    else\n        TAILQ_INSERT_BEFORE(mit, msg, msg_queue);\n}\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/mqueue/#lock-ordering","title":"Lock Ordering","text":"<p>The implementation uses a two-level locking hierarchy (<code>sys_mqueue.c:33-42</code>):</p> <pre><code>mqlist_mtx          (global list lock)\n  -&gt; mqueue::mq_mtx (per-queue lock)\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#global-list-lock","title":"Global List Lock","text":"<p><code>mqlist_mtx</code> protects: - The global <code>mqueue_head</code> list - Per-process <code>p-&gt;p_mqueue_cnt</code> counter</p>"},{"location":"sys/kern/ipc/mqueue/#per-queue-lock","title":"Per-Queue Lock","text":"<p>Each queue's <code>mq_mtx</code> (a <code>struct lock</code> with <code>LK_CANRECURSE</code>) protects: - Queue attributes (<code>mq_attrib</code>) - Message queues (<code>mq_head[]</code>, <code>mq_bitmap</code>) - Notification state</p>"},{"location":"sys/kern/ipc/mqueue/#blocking-operations","title":"Blocking Operations","text":"<p>Senders and receivers block using <code>lksleep()</code> with the queue lock held:</p> <pre><code>/* Receiver waiting for messages */\nerror = lksleep(&amp;mq-&gt;mq_send_cv, &amp;mq-&gt;mq_mtx, PCATCH, \"mqsend\", t);\n\n/* Sender waiting for space */\nerror = lksleep(&amp;mq-&gt;mq_recv_cv, &amp;mq-&gt;mq_mtx, PCATCH, \"mqrecv\", t);\n</code></pre> <p>Wakeups use <code>wakeup_one()</code> to wake a single waiter.</p>"},{"location":"sys/kern/ipc/mqueue/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/mqueue/#mq_open","title":"mq_open()","text":"<p><code>sys_mq_open()</code> (<code>sys_mqueue.c:414-612</code>) creates or opens a message queue:</p> <ol> <li>Validates access mode flags</li> <li>Copies name from userspace (max <code>MQ_NAMELEN</code> = <code>NAME_MAX + 1</code>)</li> <li>If <code>O_CREAT</code>:</li> <li>Checks per-process limit (<code>mq_open_max</code>)</li> <li>Validates or uses default attributes</li> <li>Allocates new <code>struct mqueue</code></li> <li>Allocates file descriptor (<code>DTYPE_MQUEUE</code>)</li> <li>Looks up existing queue under <code>mqlist_mtx</code></li> <li>If found: checks permissions with <code>vaccess()</code></li> <li>If not found and <code>O_CREAT</code>: inserts new queue into global list</li> <li>Returns descriptor</li> </ol> <p>Default attributes when none specified: <pre><code>attr.mq_maxmsg = mq_def_maxmsg;     /* 32 */\nattr.mq_msgsize = MQ_DEF_MSGSIZE - sizeof(struct mq_msg);  /* ~1000 */\n</code></pre></p>"},{"location":"sys/kern/ipc/mqueue/#mq_send-mq_timedsend","title":"mq_send() / mq_timedsend()","text":"<p><code>mq_send1()</code> (<code>sys_mqueue.c:782-913</code>) sends a message:</p> <ol> <li>Validates priority (&lt; <code>mq_prio_max</code>)</li> <li>Allocates message structure with data</li> <li>Copies data from userspace</li> <li>Acquires queue via <code>mqueue_get()</code></li> <li>Validates message size against <code>mq_msgsize</code></li> <li>If queue full and blocking: sleeps on <code>mq_recv_cv</code></li> <li>Inserts message into appropriate priority queue</li> <li>If notification registered and queue was empty: signals process</li> <li>Increments <code>mq_curmsgs</code>, wakes one receiver</li> </ol>"},{"location":"sys/kern/ipc/mqueue/#mq_receive-mq_timedreceive","title":"mq_receive() / mq_timedreceive()","text":"<p><code>mq_receive1()</code> (<code>sys_mqueue.c:623-725</code>) receives a message:</p> <ol> <li>Acquires queue via <code>mqueue_get()</code></li> <li>Validates buffer size (&gt;= <code>mq_msgsize</code>)</li> <li>If queue empty and blocking: sleeps on <code>mq_send_cv</code></li> <li>Finds highest-priority message via bitmap</li> <li>Removes message from queue</li> <li>Decrements <code>mq_curmsgs</code>, wakes one sender</li> <li>Copies message data and priority to userspace</li> <li>Frees message structure</li> </ol>"},{"location":"sys/kern/ipc/mqueue/#mq_notify","title":"mq_notify()","text":"<p><code>sys_mq_notify()</code> (<code>sys_mqueue.c:956-1002</code>) registers for notification:</p> <pre><code>if (uap-&gt;notification) {\n    if (mq-&gt;mq_notify_proc == NULL) {\n        memcpy(&amp;mq-&gt;mq_sig_notify, &amp;sig, sizeof(struct sigevent));\n        mq-&gt;mq_notify_proc = curproc;\n    } else {\n        error = EBUSY;  /* already registered */\n    }\n} else {\n    mq-&gt;mq_notify_proc = NULL;  /* unregister */\n}\n</code></pre> <p>Only <code>SIGEV_SIGNAL</code> notification is fully implemented. The signal is sent via <code>ksignal()</code> when a message arrives to an empty queue.</p>"},{"location":"sys/kern/ipc/mqueue/#mq_getattr-mq_setattr","title":"mq_getattr() / mq_setattr()","text":"<p><code>sys_mq_getattr()</code> returns current attributes. <code>sys_mq_setattr()</code> only modifies <code>O_NONBLOCK</code>:</p> <pre><code>if (nonblock)\n    mq-&gt;mq_attrib.mq_flags |= O_NONBLOCK;\nelse\n    mq-&gt;mq_attrib.mq_flags &amp;= ~O_NONBLOCK;\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#mq_unlink","title":"mq_unlink()","text":"<p><code>sys_mq_unlink()</code> (<code>sys_mqueue.c:1077-1142</code>) marks a queue for deletion:</p> <ol> <li>Looks up queue by name</li> <li>Checks permissions (owner or root)</li> <li>Sets <code>MQ_UNLINK</code> flag</li> <li>Wakes all waiters</li> <li>If no references: removes from list and destroys</li> <li>Otherwise: last <code>mq_close()</code> destroys it</li> </ol>"},{"location":"sys/kern/ipc/mqueue/#mq_close","title":"mq_close()","text":"<p><code>sys_mq_close()</code> delegates to <code>sys_close()</code>. The actual cleanup happens in <code>mq_close_fop()</code> (<code>sys_mqueue.c:373-408</code>):</p> <pre><code>p-&gt;p_mqueue_cnt--;\nmq-&gt;mq_refcnt--;\n\nif (mq-&gt;mq_notify_proc == p)\n    mq-&gt;mq_notify_proc = NULL;\n\nif (mq-&gt;mq_refcnt == 0 &amp;&amp; (mq-&gt;mq_attrib.mq_flags &amp; MQ_UNLINK)) {\n    LIST_REMOVE(mq, mq_list);\n    destroy = true;\n}\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#timeout-handling","title":"Timeout Handling","text":""},{"location":"sys/kern/ipc/mqueue/#abstimeout2timo","title":"abstimeout2timo()","text":"<p>Converts absolute <code>timespec</code> to relative ticks (<code>sys_mqueue.c:240-259</code>):</p> <pre><code>int\nabstimeout2timo(struct timespec *ts, int *timo)\n{\n    error = itimespecfix(ts);\n    if (error)\n        return error;\n\n    getnanotime(&amp;tsd);\n    timespecsub(ts, &amp;tsd, ts);      /* ts = ts - now */\n\n    if (ts-&gt;tv_sec &lt; 0 || (ts-&gt;tv_sec == 0 &amp;&amp; ts-&gt;tv_nsec &lt;= 0))\n        return ETIMEDOUT;           /* already expired */\n\n    *timo = tstohz(ts);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/ipc/mqueue/#file-operations","title":"File Operations","text":"<p>The <code>mqops</code> structure (<code>sys_mqueue.c:97-106</code>):</p> <pre><code>static struct fileops mqops = {\n    .fo_read = badfo_readwrite,\n    .fo_write = badfo_readwrite,\n    .fo_ioctl = badfo_ioctl,\n    .fo_stat = mq_stat_fop,\n    .fo_close = mq_close_fop,\n    .fo_kqfilter = mq_kqfilter_fop,\n    .fo_shutdown = badfo_shutdown,\n    .fo_seek = badfo_seek\n};\n</code></pre> <p>Note: Direct read/write on the descriptor is not supported - use <code>mq_send()</code>/<code>mq_receive()</code>.</p>"},{"location":"sys/kern/ipc/mqueue/#kqueue-support","title":"kqueue Support","text":"<p><code>mq_kqfilter_fop()</code> supports <code>EVFILT_READ</code> and <code>EVFILT_WRITE</code>:</p> <ul> <li><code>EVFILT_READ</code>: ready when <code>mq_curmsgs &gt; 0</code></li> <li><code>EVFILT_WRITE</code>: ready when <code>mq_curmsgs &lt; mq_maxmsg</code></li> </ul>"},{"location":"sys/kern/ipc/mqueue/#flags","title":"Flags","text":""},{"location":"sys/kern/ipc/mqueue/#user-visible-flags","title":"User-Visible Flags","text":"Flag Usage <code>O_RDONLY</code> Open for receive only <code>O_WRONLY</code> Open for send only <code>O_RDWR</code> Open for send and receive <code>O_CREAT</code> Create queue if not exists <code>O_EXCL</code> Fail if queue exists (with <code>O_CREAT</code>) <code>O_NONBLOCK</code> Non-blocking operations"},{"location":"sys/kern/ipc/mqueue/#internal-flags","title":"Internal Flags","text":"Flag Value Description <code>MQ_UNLINK</code> 0x10000000 Queue marked for deletion <code>MQ_RECEIVE</code> 0x20000000 Receiver is waiting (suppresses notification)"},{"location":"sys/kern/ipc/mqueue/#sysctl-tunables","title":"Sysctl Tunables","text":"Sysctl Default Description <code>kern.mqueue.mq_open_max</code> 512 Max descriptors per process <code>kern.mqueue.mq_prio_max</code> 32 Max message priority <code>kern.mqueue.mq_max_msgsize</code> 16384 Max message size <code>kern.mqueue.mq_def_maxmsg</code> 32 Default max messages per queue <code>kern.mqueue.mq_max_maxmsg</code> 512 Max allowed messages per queue"},{"location":"sys/kern/ipc/mqueue/#resource-limits","title":"Resource Limits","text":"<p>Each process tracks open mqueue descriptors in <code>p-&gt;p_mqueue_cnt</code>. Opening a queue fails with <code>EMFILE</code> if the count reaches <code>mq_open_max</code>.</p>"},{"location":"sys/kern/ipc/mqueue/#source-reference","title":"Source Reference","text":"File Description <code>sys/kern/sys_mqueue.c</code> POSIX message queue implementation <code>sys/sys/mqueue.h</code> Message queue structures and constants"},{"location":"sys/kern/ipc/pipes/","title":"Pipes","text":"<p>Pipes provide unidirectional byte streams for inter-process communication. DragonFly's implementation (<code>sys/kern/sys_pipe.c</code>) replaces the traditional socket-based approach with a high-performance VM-backed design featuring per-CPU caching and busy-wait optimization.</p>"},{"location":"sys/kern/ipc/pipes/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/pipes/#pipe-buffer","title":"Pipe Buffer","text":"<p>Each direction of a pipe uses a <code>struct pipebuf</code> (<code>sys/sys/pipe.h:53</code>):</p> <pre><code>struct pipebuf {\n    struct {\n        struct lwkt_token rlock;\n        size_t      rindex;     /* current read index (FIFO) */\n        int32_t     rip;        /* read-in-progress flag */\n        struct timespec atime;  /* time of last access */\n    } __cachealign;\n    struct {\n        struct lwkt_token wlock;\n        size_t      windex;     /* current write index (FIFO) */\n        int32_t     wip;        /* write-in-progress flag */\n        struct timespec mtime;  /* time of last modify */\n    } __cachealign;\n    size_t      size;           /* size of buffer */\n    caddr_t     buffer;         /* kva of buffer */\n    struct vm_object *object;   /* VM object containing buffer */\n    struct kqinfo   kq;         /* for select/poll/kq */\n    struct sigio    *sigio;     /* async I/O info */\n    uint32_t    state;          /* pipe status flags */\n    int         lticks;         /* timestamp optimization */\n} __cachealign;\n</code></pre> <p>The structure uses <code>__cachealign</code> to separate read and write fields onto different cache lines, reducing false sharing between reader and writer.</p>"},{"location":"sys/kern/ipc/pipes/#pipe-structure","title":"Pipe Structure","text":"<p>The main <code>struct pipe</code> (<code>sys/sys/pipe.h:91</code>) contains two buffers for full-duplex communication:</p> <pre><code>struct pipe {\n    struct pipebuf  bufferA;    /* data storage */\n    struct pipebuf  bufferB;    /* data storage */\n    struct timespec ctime;      /* creation time */\n    struct pipe     *next;      /* per-CPU cache linkage */\n    uint32_t        open_count; /* reference count */\n    uint64_t        inum;       /* inode number */\n} __cachealign;\n</code></pre> <p>Each file descriptor identifies which buffer it reads from using the low bit of <code>fp-&gt;f_data</code>: bit 0 clear reads from <code>bufferA</code>, bit 1 set reads from <code>bufferB</code>.</p>"},{"location":"sys/kern/ipc/pipes/#state-flags","title":"State Flags","text":"<p>Buffer state tracked in <code>pipebuf.state</code> (<code>sys/sys/pipe.h:80-85</code>):</p> Flag Value Description <code>PIPE_ASYNC</code> 0x0004 Async I/O enabled (SIGIO) <code>PIPE_WANTR</code> 0x0008 Reader is sleeping <code>PIPE_WANTW</code> 0x0010 Writer is sleeping <code>PIPE_REOF</code> 0x0040 Read EOF (peer closed write) <code>PIPE_WEOF</code> 0x0080 Write EOF (shutdown) <code>PIPE_CLOSED</code> 0x1000 This side fully closed"},{"location":"sys/kern/ipc/pipes/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/pipes/#pipe-and-pipe2","title":"pipe() and pipe2()","text":"<p><code>sys_pipe()</code> and <code>sys_pipe2()</code> (<code>sys_pipe.c:262-274</code>) create a pipe pair:</p> <pre><code>int sys_pipe(struct sysmsg *sysmsg, const struct pipe_args *uap)\n{\n    return kern_pipe(sysmsg-&gt;sysmsg_fds, 0);\n}\n\nint sys_pipe2(struct sysmsg *sysmsg, const struct pipe2_args *uap)\n{\n    if ((uap-&gt;flags &amp; ~(O_CLOEXEC | O_CLOFORK | O_NONBLOCK)) != 0)\n        return (EINVAL);\n    return kern_pipe(sysmsg-&gt;sysmsg_fds, uap-&gt;flags);\n}\n</code></pre> <p><code>pipe2()</code> accepts flags: <code>O_CLOEXEC</code> (close on exec), <code>O_CLOFORK</code> (close on fork), and <code>O_NONBLOCK</code>.</p>"},{"location":"sys/kern/ipc/pipes/#kern_pipe","title":"kern_pipe()","text":"<p>The core creation logic (<code>sys_pipe.c:276-349</code>):</p> <ol> <li>Allocates a <code>struct pipe</code> via <code>pipe_create()</code></li> <li>Allocates two file descriptors with <code>falloc()</code></li> <li>Configures read-side fd (bit 0 = 0) and write-side fd (bit 0 = 1)</li> <li>Sets <code>f_ops</code> to <code>pipeops</code> for both</li> <li>Activates descriptors with <code>fsetfd()</code></li> </ol> <p>Both file descriptors have <code>FREAD | FWRITE</code> flags set, though traditionally one is the read end and one is the write end.</p>"},{"location":"sys/kern/ipc/pipes/#vm-backed-buffers","title":"VM-Backed Buffers","text":""},{"location":"sys/kern/ipc/pipes/#buffer-allocation","title":"Buffer Allocation","text":"<p><code>pipespace()</code> (<code>sys_pipe.c:359-405</code>) allocates kernel virtual address space backed by a VM object:</p> <pre><code>static int\npipespace(struct pipe *pipe, struct pipebuf *pb, size_t size)\n{\n    size = (size + PAGE_MASK) &amp; ~(size_t)PAGE_MASK;\n    if (size &lt; 16384)\n        size = 16384;\n    if (size &gt; 1024*1024)\n        size = 1024*1024;\n\n    npages = round_page(size) / PAGE_SIZE;\n\n    if (object == NULL || object-&gt;size != npages) {\n        object = vm_object_allocate(OBJT_DEFAULT, npages);\n        buffer = (caddr_t)vm_map_min(kernel_map);\n\n        error = vm_map_find(kernel_map, object, NULL,\n                0, (vm_offset_t *)&amp;buffer, size,\n                PAGE_SIZE, TRUE,\n                VM_MAPTYPE_NORMAL, VM_SUBSYS_PIPE,\n                VM_PROT_ALL, VM_PROT_ALL, 0);\n        /* ... */\n    }\n    pb-&gt;rindex = 0;\n    pb-&gt;windex = 0;\n    return (0);\n}\n</code></pre> <p>Key points: - Buffer size clamped between 16KB and 1MB - Uses <code>OBJT_DEFAULT</code> VM objects (pageable, swap-backed) - Each buffer has an independent <code>vm_object</code> for performance - Default size controlled by sysctl <code>kern.pipe.size</code> (32KB)</p>"},{"location":"sys/kern/ipc/pipes/#per-cpu-pipe-cache","title":"Per-CPU Pipe Cache","text":""},{"location":"sys/kern/ipc/pipes/#cache-design","title":"Cache Design","text":"<p>To reduce allocation overhead, pipes are cached per-CPU (<code>sys_pipe.c:111-118</code>):</p> <pre><code>#define PIPEQ_MAX_CACHE 16      /* per-cpu pipe structure cache */\n\nstatic int pipe_maxcache = PIPEQ_MAX_CACHE;\nstatic struct pipegdlock *pipe_gdlocks;\n</code></pre> <p>The cache lives in <code>globaldata_t</code>: - <code>gd-&gt;gd_pipeq</code> - linked list of cached pipes - <code>gd-&gt;gd_pipeqcount</code> - number of cached pipes</p>"},{"location":"sys/kern/ipc/pipes/#cache-initialization","title":"Cache Initialization","text":"<p><code>pipeinit()</code> (<code>sys_pipe.c:148-177</code>) scales the cache based on system memory:</p> <pre><code>static void\npipeinit(void *dummy)\n{\n    size_t mbytes = kmem_lim_size();\n\n    if (pipe_maxcache == PIPEQ_MAX_CACHE) {\n        if (mbytes &gt;= 7 * 1024)\n            pipe_maxcache *= 2;\n        if (mbytes &gt;= 15 * 1024)\n            pipe_maxcache *= 2;\n    }\n\n    /* Reduce cache on systems with many CPUs */\n    if (ncpus &gt; 64) {\n        pipe_maxcache = pipe_maxcache * 64 / ncpus;\n        if (pipe_maxcache &lt; PIPEQ_MAX_CACHE)\n            pipe_maxcache = PIPEQ_MAX_CACHE;\n    }\n    /* ... */\n}\n</code></pre>"},{"location":"sys/kern/ipc/pipes/#allocation-from-cache","title":"Allocation from Cache","text":"<p><code>pipe_create()</code> (<code>sys_pipe.c:414-448</code>) checks the per-CPU cache first:</p> <pre><code>static int\npipe_create(struct pipe **pipep)\n{\n    globaldata_t gd = mycpu;\n    struct pipe *pipe;\n\n    if ((pipe = gd-&gt;gd_pipeq) != NULL) {\n        gd-&gt;gd_pipeq = pipe-&gt;next;\n        --gd-&gt;gd_pipeqcount;\n        pipe-&gt;next = NULL;\n    } else {\n        pipe = kmalloc(sizeof(*pipe), M_PIPE, M_WAITOK | M_ZERO);\n        pipe-&gt;inum = gd-&gt;gd_anoninum++ * ncpus + gd-&gt;gd_cpuid + 2;\n        lwkt_token_init(&amp;pipe-&gt;bufferA.rlock, \"piper\");\n        lwkt_token_init(&amp;pipe-&gt;bufferA.wlock, \"pipew\");\n        lwkt_token_init(&amp;pipe-&gt;bufferB.rlock, \"piper\");\n        lwkt_token_init(&amp;pipe-&gt;bufferB.wlock, \"pipew\");\n    }\n    /* ... allocate buffer space ... */\n}\n</code></pre> <p>The inode number generation (<code>gd-&gt;gd_anoninum++ * ncpus + gd-&gt;gd_cpuid + 2</code>) ensures unique inums across CPUs without synchronization.</p>"},{"location":"sys/kern/ipc/pipes/#return-to-cache","title":"Return to Cache","text":"<p>When both ends close, <code>pipeclose()</code> (<code>sys_pipe.c:1272-1287</code>) returns the pipe to the cache:</p> <pre><code>if (atomic_fetchadd_int(&amp;pipe-&gt;open_count, -1) == 1) {\n    gd = mycpu;\n    if (gd-&gt;gd_pipeqcount &gt;= pipe_maxcache) {\n        mtx_lock(&amp;pipe_gdlocks[gd-&gt;gd_cpuid].mtx);\n        pipe_free_kmem(rpb);\n        pipe_free_kmem(wpb);\n        mtx_unlock(&amp;pipe_gdlocks[gd-&gt;gd_cpuid].mtx);\n        kfree(pipe, M_PIPE);\n    } else {\n        rpb-&gt;state = 0;\n        wpb-&gt;state = 0;\n        pipe-&gt;next = gd-&gt;gd_pipeq;\n        gd-&gt;gd_pipeq = pipe;\n        ++gd-&gt;gd_pipeqcount;\n    }\n}\n</code></pre> <p>The per-CPU mutex (<code>pipe_gdlocks</code>) serializes access to <code>kernel_map</code> during bulk teardown scenarios (e.g., mass process termination).</p>"},{"location":"sys/kern/ipc/pipes/#read-operation","title":"Read Operation","text":""},{"location":"sys/kern/ipc/pipes/#pipe_read","title":"pipe_read()","text":"<p><code>pipe_read()</code> (<code>sys_pipe.c:453-698</code>) implements reading:</p> <ol> <li>Buffer selection: Determines read buffer based on <code>fp-&gt;f_data</code> bit 0</li> <li>Quick NBIO check: Returns <code>EAGAIN</code> early if non-blocking and buffer empty</li> <li>Serialization: Acquires <code>rlock</code> token and calls <code>pipe_start_uio()</code> to serialize against other readers</li> <li>Copy loop: Reads available data via <code>uiomove()</code></li> </ol> <p>Key features of the read loop:</p> <pre><code>while (uio-&gt;uio_resid) {\n    size = rpb-&gt;windex - rpb-&gt;rindex;\n    cpu_lfence();  /* memory barrier before reading buffer */\n\n    if (size) {\n        rindex = rpb-&gt;rindex &amp; (rpb-&gt;size - 1);\n        nsize = szmin(size, uio-&gt;uio_resid);\n\n        /* Limit to half buffer to avoid ping-pong */\n        if (nsize &gt; (rpb-&gt;size &gt;&gt; 1))\n            nsize = rpb-&gt;size &gt;&gt; 1;\n\n        error = uiomove(&amp;rpb-&gt;buffer[rindex], nsize, uio);\n        rpb-&gt;rindex += nsize;\n\n        /* Wake writer if buffer less than half full */\n        if (size - nsize &lt;= (rpb-&gt;size &gt;&gt; 1))\n            pipesignal(rpb, PIPE_WANTW);\n        continue;\n    }\n    /* ... blocking logic ... */\n}\n</code></pre> <p>The buffer uses power-of-2 sizing, so <code>rindex &amp; (size - 1)</code> computes the circular buffer offset efficiently.</p>"},{"location":"sys/kern/ipc/pipes/#busy-wait-optimization","title":"Busy-Wait Optimization","text":"<p>Before sleeping, the reader busy-waits for a configurable period (<code>sys_pipe.c:599-615</code>):</p> <pre><code>#ifdef _RDTSC_SUPPORTED_\nif (pipe_delay) {\n    int64_t tsc_target;\n    int good = 0;\n\n    tsc_target = tsc_get_target(pipe_delay);\n    while (tsc_test_target(tsc_target) == 0) {\n        cpu_lfence();\n        if (rpb-&gt;windex != rpb-&gt;rindex) {\n            good = 1;\n            break;\n        }\n        cpu_pause();\n    }\n    if (good)\n        continue;\n}\n#endif\n</code></pre> <p>The <code>pipe_delay</code> sysctl (default 4000ns = 4us) trades CPU cycles for reduced IPI/wakeup latency. This is effective for synchronous producer-consumer patterns.</p>"},{"location":"sys/kern/ipc/pipes/#write-operation","title":"Write Operation","text":""},{"location":"sys/kern/ipc/pipes/#pipe_write","title":"pipe_write()","text":"<p><code>pipe_write()</code> (<code>sys_pipe.c:700-987</code>) mirrors the read path:</p> <ol> <li>Buffer selection: Writes to the peer's read buffer</li> <li>EOF check: Returns <code>EPIPE</code> if <code>PIPE_WEOF</code> is set</li> <li>Atomicity: Writes &lt;= <code>PIPE_BUF</code> (512 bytes) are atomic</li> </ol> <pre><code>while (uio-&gt;uio_resid) {\n    space = wpb-&gt;size - (wpb-&gt;windex - wpb-&gt;rindex);\n\n    /* Writes &lt;= PIPE_BUF must be atomic */\n    if ((space &lt; uio-&gt;uio_resid) &amp;&amp; (orig_resid &lt;= PIPE_BUF))\n        space = 0;\n\n    if (space &gt; 0) {\n        /* Limit to half buffer for pipelining */\n        if (space &gt; (wpb-&gt;size &gt;&gt; 1))\n            space = (wpb-&gt;size &gt;&gt; 1);\n\n        /* Handle wraparound */\n        windex = wpb-&gt;windex &amp; (wpb-&gt;size - 1);\n        segsize = wpb-&gt;size - windex;\n        if (segsize &gt; space)\n            segsize = space;\n\n        error = uiomove(&amp;wpb-&gt;buffer[windex], segsize, uio);\n        if (error == 0 &amp;&amp; segsize &lt; space) {\n            segsize = space - segsize;\n            error = uiomove(&amp;wpb-&gt;buffer[0], segsize, uio);\n        }\n\n        cpu_sfence();  /* ensure data visible before windex update */\n        wpb-&gt;windex += space;\n        pipesignal(wpb, PIPE_WANTR);\n        continue;\n    }\n    /* ... blocking logic with busy-wait ... */\n}\n</code></pre> <p>The store fence (<code>cpu_sfence()</code>) ensures buffer contents are visible to readers before <code>windex</code> is updated.</p>"},{"location":"sys/kern/ipc/pipes/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/pipes/#token-based-locking","title":"Token-Based Locking","text":"<p>Each buffer has separate read and write tokens: - <code>rlock</code> - held during reads, protects <code>rindex</code> - <code>wlock</code> - held during writes, protects <code>windex</code></p> <p>This allows concurrent read and write operations on the same buffer.</p>"},{"location":"sys/kern/ipc/pipes/#uio-serialization","title":"UIO Serialization","text":"<p>The <code>rip</code> and <code>wip</code> fields serialize multiple concurrent reads or writes (<code>sys_pipe.c:228-253</code>):</p> <pre><code>static __inline int\npipe_start_uio(int *ipp)\n{\n    int error;\n    while (*ipp) {\n        *ipp = -1;  /* mark as contended */\n        error = tsleep(ipp, PCATCH, \"pipexx\", 0);\n        if (error)\n            return (error);\n    }\n    *ipp = 1;  /* mark as in-progress */\n    return (0);\n}\n\nstatic __inline void\npipe_end_uio(int *ipp)\n{\n    if (*ipp &lt; 0) {\n        *ipp = 0;\n        wakeup(ipp);  /* wake contending thread */\n    } else {\n        *ipp = 0;\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/pipes/#signal-and-wakeup","title":"Signal and Wakeup","text":"<p><code>pipesignal()</code> (<code>sys_pipe.c:188-203</code>) atomically clears wait flags and wakes sleepers:</p> <pre><code>static __inline void\npipesignal(struct pipebuf *pb, uint32_t flags)\n{\n    uint32_t oflags, nflags;\n\n    for (;;) {\n        oflags = pb-&gt;state;\n        cpu_ccfence();\n        nflags = oflags &amp; ~flags;\n        if (atomic_cmpset_int(&amp;pb-&gt;state, oflags, nflags))\n            break;\n    }\n    if (oflags &amp; flags)\n        wakeup(pb);\n}\n</code></pre>"},{"location":"sys/kern/ipc/pipes/#shutdown-and-close","title":"Shutdown and Close","text":""},{"location":"sys/kern/ipc/pipes/#pipe_shutdown","title":"pipe_shutdown()","text":"<p><code>pipe_shutdown()</code> (<code>sys_pipe.c:1113-1183</code>) implements partial close semantics:</p> <pre><code>switch(how) {\ncase SHUT_RDWR:\ncase SHUT_RD:\n    atomic_set_int(&amp;rpb-&gt;state, PIPE_REOF | PIPE_WEOF);\n    /* wake waiters */\n    if (how == SHUT_RD)\n        break;\n    /* fall through */\ncase SHUT_WR:\n    atomic_set_int(&amp;wpb-&gt;state, PIPE_REOF | PIPE_WEOF);\n    /* wake waiters */\n    break;\n}\n</code></pre> <p>This requires all four tokens (both rlock and wlock for both buffers) since it modifies state on both sides.</p>"},{"location":"sys/kern/ipc/pipes/#pipeclose","title":"pipeclose()","text":"<p><code>pipeclose()</code> (<code>sys_pipe.c:1203-1288</code>) handles final cleanup:</p> <ol> <li>Sets <code>PIPE_CLOSED | PIPE_REOF | PIPE_WEOF</code> on own buffer</li> <li>Sets <code>PIPE_REOF | PIPE_WEOF</code> on peer buffer</li> <li>Wakes all waiters on both sides</li> <li>When <code>open_count</code> reaches 0, returns to cache or frees</li> </ol>"},{"location":"sys/kern/ipc/pipes/#file-operations","title":"File Operations","text":"<p>The <code>pipeops</code> structure (<code>sys_pipe.c:89-98</code>):</p> <pre><code>static struct fileops pipeops = {\n    .fo_read = pipe_read, \n    .fo_write = pipe_write,\n    .fo_ioctl = pipe_ioctl,\n    .fo_kqfilter = pipe_kqfilter,\n    .fo_stat = pipe_stat,\n    .fo_close = pipe_close,\n    .fo_shutdown = pipe_shutdown,\n    .fo_seek = badfo_seek\n};\n</code></pre>"},{"location":"sys/kern/ipc/pipes/#supported-ioctls","title":"Supported ioctls","text":"<p><code>pipe_ioctl()</code> (<code>sys_pipe.c:992-1048</code>) supports:</p> ioctl Description <code>FIOASYNC</code> Enable/disable async I/O (SIGIO) <code>FIONREAD</code> Return bytes available for read <code>FIOSETOWN</code> Set owner for SIGIO <code>FIOGETOWN</code> Get owner for SIGIO <code>TIOCSPGRP</code> Set process group (deprecated) <code>TIOCGPGRP</code> Get process group (deprecated)"},{"location":"sys/kern/ipc/pipes/#kqueue-support","title":"kqueue Support","text":"<p><code>pipe_kqfilter()</code> (<code>sys_pipe.c:1290-1325</code>) supports <code>EVFILT_READ</code> and <code>EVFILT_WRITE</code>:</p> <pre><code>switch (kn-&gt;kn_filter) {\ncase EVFILT_READ:\n    kn-&gt;kn_fop = &amp;pipe_rfiltops;\n    break;\ncase EVFILT_WRITE:\n    kn-&gt;kn_fop = &amp;pipe_wfiltops;\n    break;\n}\nknote_insert(&amp;rpb-&gt;kq.ki_note, kn);\n</code></pre> <p>The filter operations are marked <code>FILTEROP_MPSAFE</code> and rely on the knote's <code>KN_PROCESSING</code> flag for synchronization rather than pipe tokens.</p>"},{"location":"sys/kern/ipc/pipes/#sysctl-tunables","title":"Sysctl Tunables","text":"Sysctl Default Description <code>kern.pipe.size</code> 32768 Default buffer size for new pipes <code>kern.pipe.maxcache</code> 16-64 Per-CPU cache size (scaled by memory) <code>kern.pipe.delay</code> 4000 Busy-wait time in nanoseconds"},{"location":"sys/kern/ipc/pipes/#source-reference","title":"Source Reference","text":"File Description <code>sys/kern/sys_pipe.c</code> Pipe implementation <code>sys/sys/pipe.h</code> Pipe structures and flags"},{"location":"sys/kern/ipc/protocol-dispatch/","title":"Protocol Dispatch","text":"<p>The protocol dispatch layer provides the framework for routing socket operations to protocol-specific handlers in DragonFly BSD. It manages protocol registration, domain initialization, and message-based operation dispatch using the LWKT subsystem for multi-processor scalability.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#overview","title":"Overview","text":"<p>DragonFly's protocol dispatch architecture consists of three main components:</p> <ul> <li>Domains - Protocol families (e.g., AF_LOCAL, AF_INET) that group related protocols</li> <li>Protocol Switch - Tables mapping socket types to protocol handlers</li> <li>Network Messages - LWKT messages that dispatch operations to protocol threads</li> </ul> <p>This design allows protocol operations to execute on dedicated threads, avoiding lock contention and enabling parallel processing across CPUs.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#source-files","title":"Source Files","text":"File Description <code>sys/kern/uipc_domain.c</code> Domain management and initialization <code>sys/kern/uipc_proto.c</code> Local domain protocol registration <code>sys/kern/uipc_msg.c</code> Network message dispatch wrappers <code>sys/sys/domain.h</code> Domain structure definition <code>sys/sys/protosw.h</code> Protocol switch structure and flags <code>sys/net/netmsg.h</code> Network message structures"},{"location":"sys/kern/ipc/protocol-dispatch/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#struct-domain","title":"struct domain","text":"<p>The domain structure (<code>sys/sys/domain.h:44</code>) groups protocols by address family:</p> <pre><code>struct domain {\n    int     dom_family;             /* AF_xxx */\n    char    *dom_name;              /* domain name (e.g., \"local\") */\n    void    (*dom_init)(void);      /* initialization routine */\n    int     (*dom_externalize)(struct mbuf *, int, struct thread *);\n                                    /* externalize access rights */\n    void    (*dom_dispose)(struct mbuf *);\n                                    /* dispose of internalized rights */\n    struct  protosw *dom_protosw;   /* protocol switch table start */\n    struct  protosw *dom_protoswNPROTOSW;\n                                    /* protocol switch table end */\n    SLIST_ENTRY(domain) dom_next;   /* next domain in list */\n    int     (*dom_rtattach)(void **, int);\n                                    /* initialize routing table */\n    int     dom_rtoffset;           /* arg to rtattach (sockaddr offset) */\n    int     dom_maxrtkey;           /* for routing layer */\n    void    *(*dom_ifattach)(struct ifnet *);\n                                    /* per-interface attach */\n    void    (*dom_ifdetach)(struct ifnet *, void *);\n                                    /* per-interface detach */\n};\n</code></pre> <p>Key fields:</p> <ul> <li><code>dom_family</code> - Address family identifier (AF_LOCAL, AF_INET, AF_INET6, etc.)</li> <li><code>dom_externalize</code> - Called to externalize access rights in control messages (used by Unix domain sockets for file descriptor passing)</li> <li><code>dom_dispose</code> - Called to dispose of internalized access rights</li> <li><code>dom_protosw</code> / <code>dom_protoswNPROTOSW</code> - Bounds of the protocol switch array for this domain</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#struct-protosw","title":"struct protosw","text":"<p>The protocol switch structure (<code>sys/sys/protosw.h:86</code>) defines protocol behavior:</p> <pre><code>struct protosw {\n    short   pr_type;                /* socket type (SOCK_STREAM, etc.) */\n    const struct domain *pr_domain; /* back pointer to domain */\n    short   pr_protocol;            /* protocol number, if any */\n    short   pr_flags;               /* protocol flags (PR_*) */\n\n    /* Protocol-layer operations (rarely used directly) */\n    void    (*pr_input)(struct mbuf *, ...);\n                                    /* input from below */\n    int     (*pr_output)(struct mbuf *, struct socket *, ...);\n                                    /* output to network */\n    void    (*pr_ctlinput)(int, struct sockaddr *, void *, void *);\n                                    /* control input */\n    int     (*pr_ctloutput)(struct socket *, struct sockopt *);\n                                    /* control output */\n\n    /* Initialization */\n    void    (*pr_init)(void);       /* protocol init */\n\n    /* Timer (deprecated) */\n    void    (*pr_fasttimo)(void);   /* fast timeout */\n    void    (*pr_slowtimo)(void);   /* slow timeout */\n\n    /* Drain excess resources */\n    void    (*pr_drain)(void);\n\n    /* User-request operations */\n    struct  pr_usrreqs *pr_usrreqs;\n};\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#protocol-flags-pr_","title":"Protocol Flags (PR_*)","text":"<p>Protocol flags control dispatch behavior (<code>sys/sys/protosw.h:129</code>):</p> Flag Value Description <code>PR_ATOMIC</code> 0x01 Exchange atomic messages only <code>PR_ADDR</code> 0x02 Addresses given with messages <code>PR_CONNREQUIRED</code> 0x04 Connection required for data transfer <code>PR_WANTRCVD</code> 0x08 Protocol wants <code>pru_rcvd</code> calls <code>PR_RIGHTS</code> 0x10 Protocol supports access rights passing <code>PR_SYNC_PORT</code> 0x20 Use synchronous port (netisr_sync_port) <code>PR_ASYNC_SEND</code> 0x40 Allow asynchronous <code>pru_send</code> <code>PR_ASYNC_RCVD</code> 0x80 Allow asynchronous <code>pru_rcvd</code> <code>PR_MPSAFE</code> 0x0100 Protocol handler is MP-safe <p>Key flag semantics:</p> <ul> <li><code>PR_ATOMIC</code> - Each send/receive operates on complete messages (datagrams)</li> <li><code>PR_CONNREQUIRED</code> - Stream protocols requiring connection before data transfer</li> <li><code>PR_SYNC_PORT</code> - Forces all operations through a single serializing port</li> <li><code>PR_ASYNC_SEND</code> / <code>PR_ASYNC_RCVD</code> - Enable fire-and-forget message dispatch for performance</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#struct-pr_usrreqs","title":"struct pr_usrreqs","text":"<p>User request handlers for socket operations (<code>sys/sys/protosw.h:158</code>):</p> <pre><code>struct pr_usrreqs {\n    void    (*pru_abort)(netmsg_t);         /* abort connection */\n    void    (*pru_accept)(netmsg_t);        /* accept incoming conn */\n    void    (*pru_attach)(netmsg_t);        /* attach protocol */\n    void    (*pru_bind)(netmsg_t);          /* bind to address */\n    void    (*pru_connect)(netmsg_t);       /* connect to peer */\n    void    (*pru_connect2)(netmsg_t);      /* connect two sockets */\n    void    (*pru_control)(netmsg_t);       /* ioctl operations */\n    void    (*pru_detach)(netmsg_t);        /* detach protocol */\n    void    (*pru_disconnect)(netmsg_t);    /* disconnect */\n    void    (*pru_listen)(netmsg_t);        /* listen for connections */\n    void    (*pru_peeraddr)(netmsg_t);      /* get peer address */\n    void    (*pru_rcvd)(netmsg_t);          /* received data consumed */\n    void    (*pru_rcvoob)(netmsg_t);        /* receive OOB data */\n    void    (*pru_send)(netmsg_t);          /* send data */\n    void    (*pru_sense)(netmsg_t);         /* stat-like operation */\n    void    (*pru_shutdown)(netmsg_t);      /* shutdown connection */\n    void    (*pru_sockaddr)(netmsg_t);      /* get local address */\n    void    (*pru_sosend)(struct socket *, struct sockaddr *,\n                          struct uio *, struct mbuf *,\n                          struct mbuf *, int, struct thread *);\n                                            /* optimized send path */\n    void    (*pru_soreceive)(struct socket *, struct sockaddr **,\n                             struct uio *, struct sockbuf *,\n                             struct mbuf **, int *);\n                                            /* optimized receive path */\n    void    (*pru_savefaddr)(struct socket *, const struct sockaddr *);\n                                            /* save foreign address */\n};\n</code></pre> <p>All standard handlers take a <code>netmsg_t</code> parameter, which encapsulates the operation request and allows asynchronous execution.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#network-message-structures","title":"Network Message Structures","text":"<p>Network messages (<code>sys/net/netmsg.h</code>) carry operation requests between threads:</p> <pre><code>struct netmsg_base {\n    struct lwkt_msg     nm_lmsg;        /* LWKT message header */\n    netisr_fn_t         nm_dispatch;    /* dispatch handler */\n    struct socket       *nm_so;         /* associated socket */\n};\n\ntypedef union netmsg *netmsg_t;\n</code></pre> <p>Operation-specific message types extend <code>netmsg_base</code>:</p> <pre><code>struct netmsg_pru_attach {\n    struct netmsg_base  base;\n    int                 nm_proto;       /* protocol number */\n    struct pru_attach_info *nm_ai;      /* attach info */\n};\n\nstruct netmsg_pru_connect {\n    struct netmsg_base  base;\n    struct sockaddr     *nm_nam;        /* target address */\n    struct thread       *nm_td;         /* calling thread */\n    struct mbuf         *nm_m;          /* data mbuf (for sendto) */\n    int                 nm_flags;       /* flags */\n    int                 nm_reconnect;   /* reconnect indicator */\n};\n\nstruct netmsg_pru_send {\n    struct netmsg_base  base;\n    int                 nm_flags;       /* MSG_* flags */\n    int                 nm_priv;        /* privilege level */\n    struct mbuf         *nm_m;          /* data mbuf chain */\n    struct sockaddr     *nm_addr;       /* target address */\n    struct mbuf         *nm_control;    /* control mbuf */\n    struct thread       *nm_td;         /* calling thread */\n};\n\nstruct netmsg_pru_rcvd {\n    struct netmsg_base  base;\n    int                 nm_flags;       /* MSG_* flags */\n    int                 nm_pru_flags;   /* PRUR_* flags */\n};\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#domain-registration","title":"Domain Registration","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#domain_set-macro","title":"DOMAIN_SET Macro","text":"<p>Domains register themselves using the <code>DOMAIN_SET()</code> macro (<code>sys/sys/domain.h:74</code>):</p> <pre><code>#define DOMAIN_SET(name)                                        \\\n    SYSINIT(domain_add_ ## name, SI_SUB_PROTO_DOMAIN,           \\\n            SI_ORDER_FIRST, net_add_domain, &amp;name ## domain)\n\n/* Example from uipc_proto.c */\nDOMAIN_SET(local);\n</code></pre> <p>This creates a SYSINIT entry that calls <code>net_add_domain()</code> during the <code>SI_SUB_PROTO_DOMAIN</code> initialization phase.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#domain-registration-flow","title":"Domain Registration Flow","text":"<pre><code>boot\n \u2502\n \u251c\u2500\u25ba SI_SUB_PROTO_DOMAIN phase\n \u2502       \u2502\n \u2502       \u251c\u2500\u25ba net_add_domain(&amp;localdomain)\n \u2502       \u251c\u2500\u25ba net_add_domain(&amp;inetdomain)\n \u2502       \u251c\u2500\u25ba net_add_domain(&amp;inet6domain)\n \u2502       \u2514\u2500\u25ba ... (other domains)\n \u2502\n \u2514\u2500\u25ba SI_SUB_PROTO_END phase\n         \u2502\n         \u2514\u2500\u25ba net_init_domains()\n                 \u2502\n                 \u2514\u2500\u25ba For each registered domain:\n                         net_init_domain(dom)\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#net_add_domain","title":"net_add_domain()","text":"<p>Adds a domain to the global list (<code>sys/kern/uipc_domain.c:99</code>):</p> <pre><code>void net_add_domain(void *data)\n{\n    struct domain *dp = data;\n\n    crit_enter();\n    SLIST_INSERT_HEAD(&amp;domains, dp, dom_next);\n    crit_exit();\n}\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#net_init_domain","title":"net_init_domain()","text":"<p>Initializes a domain and its protocols (<code>sys/kern/uipc_domain.c:110</code>):</p> <pre><code>static void net_init_domain(struct domain *dp)\n{\n    struct protosw *pr;\n    u_char pr_flags[256];       /* Track protocol flags by protocol number */\n    int warn_deprecation;\n\n    /* Skip if no protocols */\n    if (dp-&gt;dom_protosw == NULL)\n        return;\n\n    /* Check for deprecated timer callbacks */\n    warn_deprecation = 0;\n\n    /* Initialize each protocol in the domain */\n    for (pr = dp-&gt;dom_protosw; pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n        /* Fill in default user request handlers if not specified */\n        pr_usrreqs_init(pr);\n\n        /* Track and validate protocol flags */\n        if (pr-&gt;pr_protocol &amp;&amp; pr-&gt;pr_protocol &lt; 256) {\n            if (pr_flags[pr-&gt;pr_protocol])\n                kprintf(\"domain %s: duplicate proto %d\\n\",\n                        dp-&gt;dom_name, pr-&gt;pr_protocol);\n            pr_flags[pr-&gt;pr_protocol] = pr-&gt;pr_flags;\n        }\n\n        /* Call protocol's init function */\n        if (pr-&gt;pr_init)\n            (*pr-&gt;pr_init)();\n\n        /* Deprecated: timer callbacks */\n        if (pr-&gt;pr_fasttimo || pr-&gt;pr_slowtimo)\n            warn_deprecation = 1;\n    }\n\n    /* Call domain's init function */\n    if (dp-&gt;dom_init)\n        (*dp-&gt;dom_init)();\n\n    if (warn_deprecation)\n        kprintf(\"domain %s: pr_fasttimo or pr_slowtimo \"\n                \"not longer supported\\n\", dp-&gt;dom_name);\n}\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#pr_usrreqs_init","title":"pr_usrreqs_init()","text":"<p>Fills in default handlers for unspecified operations (<code>sys/kern/uipc_domain.c:70</code>):</p> <pre><code>static void pr_usrreqs_init(struct protosw *pr)\n{\n    struct pr_usrreqs *pu = pr-&gt;pr_usrreqs;\n\n    if (pu == NULL) {\n        pr-&gt;pr_usrreqs = &amp;pru_default_notsupp;\n        return;\n    }\n\n    /* Fill in default \"not supported\" handlers */\n    if (pu-&gt;pru_abort == NULL)\n        pu-&gt;pru_abort = pr_generic_notsupp;\n    if (pu-&gt;pru_accept == NULL)\n        pu-&gt;pru_accept = pr_generic_notsupp;\n    if (pu-&gt;pru_attach == NULL)\n        pu-&gt;pru_attach = pr_generic_notsupp;\n    /* ... (similar for all other handlers) ... */\n}\n</code></pre> <p>The default handler <code>pr_generic_notsupp()</code> returns <code>EOPNOTSUPP</code> for unsupported operations.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#protocol-lookup","title":"Protocol Lookup","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#pffindtype","title":"pffindtype()","text":"<p>Finds a protocol by family and socket type (<code>sys/kern/uipc_domain.c:171</code>):</p> <pre><code>struct protosw *pffindtype(int family, int type)\n{\n    struct domain *dp;\n    struct protosw *pr;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        if (dp-&gt;dom_family == family) {\n            /* Scan protocol switch table */\n            for (pr = dp-&gt;dom_protosw;\n                 pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n                if (pr-&gt;pr_type &amp;&amp; pr-&gt;pr_type == type)\n                    return pr;\n            }\n        }\n    }\n    return NULL;\n}\n</code></pre> <p>Used when creating a socket with <code>protocol=0</code> (default protocol for the type).</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#pffindproto","title":"pffindproto()","text":"<p>Finds a protocol by family, protocol number, and type (<code>sys/kern/uipc_domain.c:194</code>):</p> <pre><code>struct protosw *pffindproto(int family, int protocol, int type)\n{\n    struct domain *dp;\n    struct protosw *pr;\n    struct protosw *maybe = NULL;\n\n    if (family == 0)\n        return NULL;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        if (dp-&gt;dom_family == family) {\n            for (pr = dp-&gt;dom_protosw;\n                 pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n                if (pr-&gt;pr_protocol == protocol) {\n                    if (pr-&gt;pr_type == type)\n                        return pr;\n                    if (type == SOCK_RAW &amp;&amp; pr-&gt;pr_type == 0 &amp;&amp;\n                        maybe == NULL)\n                        maybe = pr;  /* Wildcard match for raw */\n                }\n            }\n        }\n    }\n    return maybe;\n}\n</code></pre> <p>Allows wildcard matching for <code>SOCK_RAW</code> sockets when exact type match fails.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#network-message-dispatch","title":"Network Message Dispatch","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#message-dispatch-model","title":"Message Dispatch Model","text":"<p>Socket operations use LWKT messages for thread-safe protocol dispatch:</p> <pre><code>User Space\n    \u2502\n    \u25bc\nSocket Layer (sosend, soreceive, etc.)\n    \u2502\n    \u25bc\nso_pru_*() Wrappers (uipc_msg.c)\n    \u2502\n    \u251c\u2500\u25ba Build netmsg_pru_* structure\n    \u2502\n    \u251c\u2500\u25ba Set dispatch handler (nm_dispatch)\n    \u2502\n    \u2514\u2500\u25ba Send to protocol's message port\n            \u2502\n            \u251c\u2500\u25ba lwkt_domsg()     [synchronous]\n            \u2502       \u2514\u2500\u25ba Wait for reply\n            \u2502\n            \u2514\u2500\u25ba lwkt_sendmsg()   [asynchronous]\n                    \u2514\u2500\u25ba Fire and forget\n            \u2502\n            \u25bc\nProtocol Thread\n    \u2502\n    \u2514\u2500\u25ba nm_dispatch(netmsg)\n            \u2502\n            \u2514\u2500\u25ba pru_handler(netmsg)\n                    \u2502\n                    \u2514\u2500\u25ba lwkt_replymsg() [when done]\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#synchronous-dispatch","title":"Synchronous Dispatch","text":"<p>Most operations use synchronous dispatch (<code>sys/kern/uipc_msg.c:163</code>):</p> <pre><code>int so_pru_attach(struct socket *so, int proto, struct pru_attach_info *ai)\n{\n    struct netmsg_pru_attach msg;\n    int error;\n\n    /* Initialize message */\n    netmsg_init(&amp;msg.base, so, &amp;netisr_adone_rport,\n                0, so-&gt;so_proto-&gt;pr_usrreqs-&gt;pru_attach);\n    msg.nm_proto = proto;\n    msg.nm_ai = ai;\n\n    /* Send and wait for reply */\n    error = lwkt_domsg(so-&gt;so_port, &amp;msg.base.lmsg, 0);\n    return error;\n}\n</code></pre> <p>The <code>lwkt_domsg()</code> call: 1. Sends the message to <code>so-&gt;so_port</code> (protocol thread's port) 2. Blocks until the protocol handler calls <code>lwkt_replymsg()</code> 3. Returns the error code from the reply</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#asynchronous-dispatch","title":"Asynchronous Dispatch","text":"<p>Performance-critical operations support async dispatch (<code>sys/kern/uipc_msg.c:285</code>):</p> <pre><code>int so_pru_send(struct socket *so, int flags, struct mbuf *m,\n                struct sockaddr *addr, struct mbuf *control,\n                struct thread *td)\n{\n    struct netmsg_pru_send msg;\n    int error;\n\n    /* Initialize message */\n    netmsg_init(&amp;msg.base, so, &amp;netisr_adone_rport,\n                0, so-&gt;so_proto-&gt;pr_usrreqs-&gt;pru_send);\n    msg.nm_flags = flags;\n    msg.nm_m = m;\n    msg.nm_addr = addr;\n    msg.nm_control = control;\n    msg.nm_td = td;\n\n    /* Check if async send is allowed */\n    if (so-&gt;so_proto-&gt;pr_flags &amp; PR_ASYNC_SEND) {\n        /* Async path: don't wait for completion */\n        lwkt_sendmsg(so-&gt;so_port, &amp;msg.base.lmsg);\n        return 0;\n    }\n\n    /* Sync path */\n    error = lwkt_domsg(so-&gt;so_port, &amp;msg.base.lmsg, 0);\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#direct-execution","title":"Direct Execution","text":"<p>For same-CPU optimization, direct variants bypass message passing (<code>sys/kern/uipc_msg.c:421</code>):</p> <pre><code>int so_pru_attach_direct(struct socket *so, int proto,\n                         struct pru_attach_info *ai)\n{\n    struct netmsg_pru_attach msg;\n    netisr_fn_t func = so-&gt;so_proto-&gt;pr_usrreqs-&gt;pru_attach;\n\n    /* Initialize message (no reply port needed) */\n    netmsg_init(&amp;msg.base, so, &amp;netisr_adone_rport, 0, func);\n    msg.nm_proto = proto;\n    msg.nm_ai = ai;\n\n    /* Call handler directly */\n    func((netmsg_t)&amp;msg);\n\n    return msg.base.lmsg.ms_error;\n}\n</code></pre> <p>Direct variants are used when: - The caller is already on the protocol thread - The operation is part of connection setup (e.g., <code>sonewconn()</code>) - Performance is critical and serialization overhead must be avoided</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#per-socket-async-rcvd-messages","title":"Per-Socket Async rcvd Messages","text":"<p>For protocols with <code>PR_ASYNC_RCVD</code>, each socket maintains a dedicated rcvd message (<code>sys/kern/uipc_msg.c:372</code>):</p> <pre><code>void so_pru_rcvd_async(struct socket *so)\n{\n    struct netmsg_pru_rcvd *msg;\n\n    /* Use socket's pre-allocated message */\n    msg = &amp;so-&gt;so_rcvd_msg;\n\n    /* Only send if not already in flight */\n    spin_lock(&amp;so-&gt;so_rcvd_spin);\n    if ((msg-&gt;nm_pru_flags &amp; PRUR_ASYNC) == 0) {\n        msg-&gt;nm_pru_flags |= PRUR_ASYNC;\n        spin_unlock(&amp;so-&gt;so_rcvd_spin);\n\n        netmsg_init(&amp;msg-&gt;base, so, &amp;netisr_apanic_rport,\n                    0, so-&gt;so_proto-&gt;pr_usrreqs-&gt;pru_rcvd);\n        msg-&gt;nm_flags = 0;\n\n        lwkt_sendmsg(so-&gt;so_port, &amp;msg-&gt;base.lmsg);\n    } else {\n        spin_unlock(&amp;so-&gt;so_rcvd_spin);\n    }\n}\n</code></pre> <p>This avoids allocating messages for frequent rcvd notifications in streaming protocols.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#message-initialization","title":"Message Initialization","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#netmsg_init","title":"netmsg_init()","text":"<p>Initializes a network message (<code>sys/kern/uipc_msg.c:65</code>):</p> <pre><code>void netmsg_init(netmsg_base_t msg, struct socket *so,\n                 struct lwkt_port *rport, int flags, netisr_fn_t dispatch)\n{\n    lwkt_initmsg(&amp;msg-&gt;lmsg, rport, flags);\n    msg-&gt;nm_dispatch = dispatch;\n    msg-&gt;nm_so = so;\n}\n</code></pre> <p>Parameters: - <code>msg</code> - Message to initialize - <code>so</code> - Associated socket - <code>rport</code> - Reply port (where completion notification is sent) - <code>flags</code> - LWKT message flags - <code>dispatch</code> - Handler function to call on the protocol thread</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#reply-ports","title":"Reply Ports","text":"<p>Common reply ports:</p> <ul> <li><code>netisr_adone_rport</code> - Async-done reply port (sets ms_error, no wakeup)</li> <li><code>netisr_apanic_rport</code> - Panics if a reply is received (for fire-and-forget)</li> <li><code>curthread-&gt;td_msgport</code> - Current thread's port (for sync operations)</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#control-inputoutput","title":"Control Input/Output","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#kpfctlinput","title":"kpfctlinput()","text":"<p>Broadcasts control input to all protocols (<code>sys/kern/uipc_domain.c:226</code>):</p> <pre><code>void kpfctlinput(int cmd, struct sockaddr *sa)\n{\n    struct domain *dp;\n    struct protosw *pr;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        for (pr = dp-&gt;dom_protosw;\n             pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n            if (pr-&gt;pr_ctlinput)\n                (*pr-&gt;pr_ctlinput)(cmd, sa, NULL, NULL);\n        }\n    }\n}\n</code></pre> <p>Used for network-wide events like interface state changes or ICMP notifications.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#kpfctlinput2","title":"kpfctlinput2()","text":"<p>Extended version with additional context (<code>sys/kern/uipc_domain.c:240</code>):</p> <pre><code>void kpfctlinput2(int cmd, struct sockaddr *sa, void *ctlparam)\n{\n    struct domain *dp;\n    struct protosw *pr;\n\n    SLIST_FOREACH(dp, &amp;domains, dom_next) {\n        if (dp-&gt;dom_family != sa-&gt;sa_family)\n            continue;\n        for (pr = dp-&gt;dom_protosw;\n             pr &lt; dp-&gt;dom_protoswNPROTOSW; pr++) {\n            if (pr-&gt;pr_ctlinput)\n                (*pr-&gt;pr_ctlinput)(cmd, sa, ctlparam, NULL);\n        }\n    }\n}\n</code></pre> <p>Filters by address family for efficiency.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#so_pr_ctloutput","title":"so_pr_ctloutput()","text":"<p>Wrapper for protocol control output (<code>sys/kern/uipc_msg.c:100</code>):</p> <pre><code>int so_pr_ctloutput(struct socket *so, struct sockopt *sopt)\n{\n    return so-&gt;so_proto-&gt;pr_ctloutput(so, sopt);\n}\n</code></pre> <p>Called from <code>sosetopt()</code>/<code>sogetopt()</code> for protocol-specific options.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#local-domain-example","title":"Local Domain Example","text":"<p>The local (Unix) domain (<code>sys/kern/uipc_proto.c</code>) demonstrates domain registration:</p> <pre><code>/* Protocol switch table */\nstruct protosw localsw[] = {\n    {\n        .pr_type = SOCK_STREAM,\n        .pr_domain = &amp;localdomain,\n        .pr_flags = PR_CONNREQUIRED | PR_WANTRCVD | PR_RIGHTS |\n                    PR_SYNC_PORT,\n        .pr_ctloutput = uipc_ctloutput,\n        .pr_usrreqs = &amp;uipc_usrreqs,\n    },\n    {\n        .pr_type = SOCK_DGRAM,\n        .pr_domain = &amp;localdomain,\n        .pr_flags = PR_ATOMIC | PR_ADDR | PR_RIGHTS | PR_SYNC_PORT,\n        .pr_ctloutput = uipc_ctloutput,\n        .pr_usrreqs = &amp;uipc_usrreqs,\n    },\n    {\n        .pr_type = SOCK_SEQPACKET,\n        .pr_domain = &amp;localdomain,\n        .pr_flags = PR_ATOMIC | PR_CONNREQUIRED | PR_WANTRCVD |\n                    PR_RIGHTS | PR_SYNC_PORT,\n        .pr_ctloutput = uipc_ctloutput,\n        .pr_usrreqs = &amp;uipc_usrreqs,\n    },\n};\n\n/* Domain structure */\nstruct domain localdomain = {\n    .dom_family = AF_LOCAL,\n    .dom_name = \"local\",\n    .dom_init = unp_init,\n    .dom_externalize = unp_externalize,\n    .dom_dispose = unp_dispose,\n    .dom_protosw = localsw,\n    .dom_protoswNPROTOSW = &amp;localsw[NELEM(localsw)],\n};\n\n/* Register domain via SYSINIT */\nDOMAIN_SET(local);\n</code></pre> <p>Key observations:</p> <ul> <li><code>PR_SYNC_PORT</code> - All local domain operations use <code>netisr_sync_port</code> for serialization</li> <li><code>PR_RIGHTS</code> - File descriptor passing is supported</li> <li><code>PR_WANTRCVD</code> - Stream and seqpacket protocols need rcvd notifications for flow control</li> <li><code>dom_externalize</code> / <code>dom_dispose</code> - Hooks for FD passing control message handling</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#message-port-assignment","title":"Message Port Assignment","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#socket-creation","title":"Socket Creation","text":"<p>When a socket is created (<code>sys/kern/uipc_socket.c</code>):</p> <pre><code>int socreate(int dom, struct socket **aso, int type, int proto,\n             struct thread *td)\n{\n    struct protosw *prp;\n    struct socket *so;\n\n    /* Find protocol */\n    prp = pffindproto(dom, proto, type);\n    if (prp == NULL)\n        prp = pffindtype(dom, type);\n    if (prp == NULL)\n        return EPROTONOSUPPORT;\n\n    /* Allocate socket */\n    so = soalloc(1, prp);\n\n    /* Assign message port based on protocol flags */\n    if (prp-&gt;pr_flags &amp; PR_SYNC_PORT) {\n        so-&gt;so_port = netisr_sync_port;\n    } else {\n        so-&gt;so_port = netisr_cpuport(0);  /* CPU 0 by default */\n    }\n\n    /* Attach protocol */\n    error = so_pru_attach(so, proto, &amp;ai);\n    ...\n}\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#connection-accept","title":"Connection Accept","text":"<p>For accepted connections (<code>sys/kern/uipc_socket2.c</code>):</p> <pre><code>struct socket *sonewconn_faddr(struct socket *head, int connstatus,\n                               struct sockaddr *faddr, boolean_t keep_ref)\n{\n    struct socket *so;\n    struct protosw *prp = head-&gt;so_proto;\n\n    /* Allocate socket */\n    so = soalloc(1, prp);\n\n    /* Assign port - typically use current CPU */\n    if (prp-&gt;pr_flags &amp; PR_SYNC_PORT) {\n        so-&gt;so_port = netisr_sync_port;\n    } else {\n        so-&gt;so_port = netisr_cpuport(mycpuid);\n    }\n    ...\n}\n</code></pre> <p>Using <code>mycpuid</code> distributes accepted connections across CPUs for better scaling.</p>"},{"location":"sys/kern/ipc/protocol-dispatch/#initialization-timeline","title":"Initialization Timeline","text":"<pre><code>boot\n \u2502\n \u251c\u2500\u25ba SI_SUB_KMEM: Memory allocator ready\n \u2502\n \u251c\u2500\u25ba SI_SUB_PROTO_DOMAIN: Domain registration\n \u2502       \u251c\u2500\u25ba net_add_domain(&amp;localdomain)\n \u2502       \u251c\u2500\u25ba net_add_domain(&amp;inetdomain)\n \u2502       \u2514\u2500\u25ba ... (other domains)\n \u2502\n \u251c\u2500\u25ba SI_SUB_PRE_DRIVERS: Pre-driver init\n \u2502\n \u251c\u2500\u25ba SI_SUB_PROTO_IF: Network interface init\n \u2502\n \u251c\u2500\u25ba SI_SUB_PROTO_END: Domain initialization\n \u2502       \u2514\u2500\u25ba net_init_domains()\n \u2502               \u251c\u2500\u25ba net_init_domain(&amp;localdomain)\n \u2502               \u2502       \u251c\u2500\u25ba pr_usrreqs_init() for each protocol\n \u2502               \u2502       \u251c\u2500\u25ba pr-&gt;pr_init() for each protocol\n \u2502               \u2502       \u2514\u2500\u25ba dom-&gt;dom_init() [unp_init]\n \u2502               \u2514\u2500\u25ba ... (other domains)\n \u2502\n \u2514\u2500\u25ba System operational\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#error-handling","title":"Error Handling","text":""},{"location":"sys/kern/ipc/protocol-dispatch/#default-handlers","title":"Default Handlers","text":"<p>Operations without protocol support return <code>EOPNOTSUPP</code>:</p> <pre><code>static void pr_generic_notsupp(netmsg_t msg)\n{\n    lwkt_replymsg(&amp;msg-&gt;lmsg, EOPNOTSUPP);\n}\n\nstatic struct pr_usrreqs pru_default_notsupp = {\n    .pru_abort = pr_generic_notsupp,\n    .pru_accept = pr_generic_notsupp,\n    .pru_attach = pr_generic_notsupp,\n    /* ... all handlers set to pr_generic_notsupp ... */\n};\n</code></pre>"},{"location":"sys/kern/ipc/protocol-dispatch/#protocol-lookup-failures","title":"Protocol Lookup Failures","text":"<p>Socket creation returns appropriate errors:</p> <ul> <li><code>EPROTONOSUPPORT</code> - Unknown protocol for the domain</li> <li><code>EAFNOSUPPORT</code> - Unknown address family (domain not registered)</li> <li><code>ESOCKTNOSUPPORT</code> - Socket type not supported by protocol</li> </ul>"},{"location":"sys/kern/ipc/protocol-dispatch/#see-also","title":"See Also","text":"<ul> <li>Sockets - Socket layer implementation</li> <li>Unix Domain Sockets - Local domain implementation</li> <li>LWKT Threading - Message passing subsystem</li> <li>Mbufs - Memory buffer management</li> </ul>"},{"location":"sys/kern/ipc/sockets/","title":"Socket Layer","text":"<p>The socket layer provides the kernel interface for network communication in DragonFly BSD. It implements the BSD socket API, managing connection state, buffering, and protocol dispatch through a message-passing architecture optimized for multi-processor systems.</p>"},{"location":"sys/kern/ipc/sockets/#overview","title":"Overview","text":"<p>DragonFly's socket implementation extends the traditional BSD socket model with:</p> <ul> <li>Per-socket message ports for protocol thread routing</li> <li>LWKT tokens for fine-grained synchronization</li> <li>Reference counting with atomic operations</li> <li>Signaling socket buffers with integrated event notification</li> <li>Protocol-specific optimized send/receive paths</li> </ul>"},{"location":"sys/kern/ipc/sockets/#source-files","title":"Source Files","text":"File Description <code>sys/kern/uipc_socket.c</code> Core socket operations (create, bind, listen, accept, connect, send, receive, close) <code>sys/kern/uipc_socket2.c</code> Socket state management, wakeup routines, new connection handling <code>sys/kern/uipc_sockbuf.c</code> Socket buffer (sockbuf) mbuf chain manipulation <code>sys/sys/socketvar.h</code> Socket structure definitions and macros <code>sys/sys/sockbuf.h</code> Generic socket buffer definitions"},{"location":"sys/kern/ipc/sockets/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/sockets/#struct-socket","title":"struct socket","text":"<p>The primary socket structure (<code>sys/sys/socketvar.h:116</code>):</p> <pre><code>struct socket {\n    short   so_type;            /* generic type (SOCK_STREAM, etc.) */\n    short   so_options;         /* from socket call (SO_REUSEADDR, etc.) */\n    short   so_linger;          /* time to linger while closing */\n    short   so_state;           /* internal state flags SS_* */\n    void    *so_pcb;            /* protocol control block */\n    struct  protosw *so_proto;  /* protocol handle */\n    struct  socket *so_head;    /* back pointer to accept socket */\n    lwkt_port_t so_port;        /* message port for protocol thread */\n\n    /* Accept queue management */\n    TAILQ_HEAD(, socket) so_incomp;  /* incomplete connections */\n    TAILQ_HEAD(, socket) so_comp;    /* completed connections */\n    TAILQ_ENTRY(socket) so_list;     /* list of unaccepted connections */\n    short   so_qlen;            /* count of so_comp */\n    short   so_incqlen;         /* count of so_incomp */\n    short   so_qlimit;          /* max queued connections */\n\n    /* Error and signal handling */\n    u_short so_error;           /* error affecting connection */\n    u_short so_rerror;          /* error affecting receiving */\n    struct  sigio *so_sigio;    /* async I/O / SIGURG info */\n    u_long  so_oobmark;         /* chars to out-of-band mark */\n\n    /* Socket buffers */\n    struct signalsockbuf so_rcv;    /* receive buffer */\n    struct signalsockbuf so_snd;    /* send buffer */\n\n    /* Upcall support */\n    void    (*so_upcall)(struct socket *, void *, int);\n    void    *so_upcallarg;\n\n    /* Credentials and reference counting */\n    struct  ucred *so_cred;     /* user credentials */\n    int     so_refs;            /* reference count */\n\n    /* Async receive message handling */\n    struct spinlock so_rcvd_spin;\n    struct netmsg_pru_rcvd so_rcvd_msg;\n\n    uint32_t so_user_cookie;    /* user-specified metadata */\n};\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#struct-signalsockbuf","title":"struct signalsockbuf","text":"<p>The signaling socket buffer wraps the basic <code>struct sockbuf</code> with synchronization and event notification (<code>sys/sys/socketvar.h:72</code>):</p> <pre><code>struct signalsockbuf {\n    struct sockbuf sb;              /* embedded basic sockbuf */\n    struct kqinfo ssb_kq;           /* kqueue/select info */\n    struct notifymsglist ssb_mlist; /* pending predicate messages */\n    uint32_t ssb_flags;             /* flags (atomic operations) */\n    u_int   ssb_timeo;              /* timeout for read/write */\n    long    ssb_lowat;              /* low water mark */\n    u_long  ssb_hiwat;              /* high water mark */\n    u_long  ssb_mbmax;              /* max mbuf chars to use */\n    struct lwkt_token ssb_token;    /* frontend/backend serializer */\n};\n</code></pre> <p>The embedded <code>struct sockbuf</code> (<code>sys/sys/sockbuf.h:48</code>) manages the mbuf chain:</p> <pre><code>struct sockbuf {\n    u_long  sb_cc;              /* actual chars in buffer */\n    u_long  sb_mbcnt;           /* chars of mbufs used */\n    u_long  sb_cc_prealloc;     /* preallocated data count */\n    u_long  sb_mbcnt_prealloc;  /* preallocated mbuf count */\n    u_long  sb_climit;          /* data limit for I/O */\n    struct  mbuf *sb_mb;        /* the mbuf chain */\n    struct  mbuf *sb_lastmbuf;  /* last mbuf in chain */\n    struct  mbuf *sb_lastrecord;/* last record in chain */\n};\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#socket-state-flags-ss_","title":"Socket State Flags (SS_*)","text":"<p>Socket states are tracked via <code>so_state</code> (<code>sys/sys/socketvar.h:186</code>):</p> Flag Value Description <code>SS_NOFDREF</code> 0x0001 No file descriptor reference <code>SS_ISCONNECTED</code> 0x0002 Connected to peer <code>SS_ISCONNECTING</code> 0x0004 Connection in progress <code>SS_ISDISCONNECTING</code> 0x0008 Disconnection in progress <code>SS_CANTSENDMORE</code> 0x0010 Cannot send more data <code>SS_CANTRCVMORE</code> 0x0020 Cannot receive more data <code>SS_RCVATMARK</code> 0x0040 At out-of-band mark <code>SS_ISCLOSING</code> 0x0080 Close in progress <code>SS_ASSERTINPROG</code> 0x0100 Race debugging for sonewconn <code>SS_ASYNC</code> 0x0200 Async I/O notify enabled <code>SS_ISCONFIRMING</code> 0x0400 Deciding on connection request <code>SS_INCOMP</code> 0x0800 Incomplete connection <code>SS_COMP</code> 0x1000 Complete but unaccepted connection <code>SS_ISDISCONNECTED</code> 0x2000 Fully disconnected <code>SS_ACCEPTMECH</code> 0x4000 Allow bind override vs accepted"},{"location":"sys/kern/ipc/sockets/#signaling-sockbuf-flags-ssb_","title":"Signaling Sockbuf Flags (SSB_*)","text":"<p>Socket buffer flags (<code>sys/sys/socketvar.h:90</code>):</p> Flag Value Description <code>SSB_LOCK</code> 0x0001 Lock on data queue <code>SSB_WANT</code> 0x0002 Someone waiting for lock <code>SSB_WAIT</code> 0x0004 Someone waiting for data/space <code>SSB_ASYNC</code> 0x0010 Async I/O, need signals <code>SSB_UPCALL</code> 0x0020 Upcall requested <code>SSB_NOINTR</code> 0x0040 Operations not interruptible <code>SSB_KNOTE</code> 0x0100 Kernel note attached <code>SSB_MEVENT</code> 0x0200 Message event notification needed <code>SSB_STOP</code> 0x0400 Backpressure indicator <code>SSB_AUTOSIZE</code> 0x0800 Automatically size buffer <code>SSB_AUTOLOWAT</code> 0x1000 Automatically scale lowat <code>SSB_WAKEUP</code> 0x2000 Wakeup event race handling <code>SSB_PREALLOC</code> 0x4000 Preallocation supported <code>SSB_STOPSUPP</code> 0x8000 SSB_STOP supported"},{"location":"sys/kern/ipc/sockets/#socket-lifecycle","title":"Socket Lifecycle","text":""},{"location":"sys/kern/ipc/sockets/#socket-creation","title":"Socket Creation","text":"<p><code>socreate()</code> (<code>sys/kern/uipc_socket.c</code>) creates a new socket:</p> <pre><code>socreate(domain, aso, type, proto, td)\n    \u2502\n    \u251c\u2500\u25ba Find protocol via pffindtype() or pffindproto()\n    \u2502\n    \u251c\u2500\u25ba soalloc(1, prp)\n    \u2502       \u251c\u2500\u25ba kmalloc(sizeof(struct socket), M_SOCKET, ...)\n    \u2502       \u251c\u2500\u25ba Initialize TAILQ heads for accept queues\n    \u2502       \u2514\u2500\u25ba Initialize ssb_token for both send/receive buffers\n    \u2502\n    \u251c\u2500\u25ba Set so_proto, hold credentials (crhold)\n    \u2502\n    \u251c\u2500\u25ba Assign message port:\n    \u2502       \u251c\u2500\u25ba PR_SYNC_PORT \u2192 netisr_sync_port\n    \u2502       \u2514\u2500\u25ba Default \u2192 netisr_cpuport(0)\n    \u2502\n    \u2514\u2500\u25ba so_pru_attach() \u2192 Protocol-specific attach\n</code></pre> <p>Socket allocation uses <code>kmalloc()</code> with <code>M_SOCKET</code> type. Each socket gets LWKT tokens for its send and receive buffers.</p>"},{"location":"sys/kern/ipc/sockets/#binding-and-listening","title":"Binding and Listening","text":"<p><code>sobind()</code> binds a socket to a local address:</p> <pre><code>int sobind(struct socket *so, struct sockaddr *nam, struct thread *td)\n{\n    return so_pru_bind(so, nam, td);\n}\n</code></pre> <p><code>solisten()</code> marks a socket as accepting connections:</p> <pre><code>solisten(so, backlog, td)\n    \u2502\n    \u251c\u2500\u25ba Reject if already connected\n    \u2502\n    \u251c\u2500\u25ba so_pru_listen() \u2192 Protocol-specific listen setup\n    \u2502\n    \u2514\u2500\u25ba Set SS_ACCEPTCONN, configure so_qlimit\n</code></pre> <p>The backlog parameter sets <code>so_qlimit</code>, clamped between 0 and <code>somaxconn</code> (default 128).</p>"},{"location":"sys/kern/ipc/sockets/#connection-establishment","title":"Connection Establishment","text":""},{"location":"sys/kern/ipc/sockets/#active-side-client","title":"Active Side (Client)","text":"<p><code>soconnect()</code> initiates a connection:</p> <pre><code>soconnect(so, nam, td, sync)\n    \u2502\n    \u251c\u2500\u25ba Verify not already connecting\n    \u2502\n    \u251c\u2500\u25ba soisconnecting(so)\n    \u2502       \u2514\u2500\u25ba Set SS_ISCONNECTING, clear SS_ISCONNECTED\n    \u2502\n    \u2514\u2500\u25ba so_pru_connect() \u2192 Protocol-specific connect\n</code></pre> <p>The <code>sync</code> parameter controls whether to wait for completion.</p>"},{"location":"sys/kern/ipc/sockets/#passive-side-server","title":"Passive Side (Server)","text":"<p><code>sonewconn()</code> creates a socket for an incoming connection (<code>sys/kern/uipc_socket2.c:347</code>):</p> <pre><code>sonewconn_faddr(head, connstatus, faddr, keep_ref)\n    \u2502\n    \u251c\u2500\u25ba Check queue limits (so_qlen &gt; 3 * so_qlimit / 2)\n    \u2502\n    \u251c\u2500\u25ba soalloc(1, head-&gt;so_proto)\n    \u2502\n    \u251c\u2500\u25ba Assign message port:\n    \u2502       \u251c\u2500\u25ba PR_SYNC_PORT \u2192 netisr_sync_port\n    \u2502       \u2514\u2500\u25ba Default \u2192 netisr_cpuport(mycpuid)\n    \u2502\n    \u251c\u2500\u25ba Inherit options from head socket\n    \u2502\n    \u251c\u2500\u25ba soreserve() \u2192 Reserve buffer space\n    \u2502\n    \u251c\u2500\u25ba so_pru_attach_direct() \u2192 Protocol attach\n    \u2502\n    \u251c\u2500\u25ba If connstatus (already connected):\n    \u2502       \u251c\u2500\u25ba Insert into so_comp queue\n    \u2502       \u251c\u2500\u25ba Set SS_COMP\n    \u2502       \u2514\u2500\u25ba sorwakeup(head) \u2192 Wake acceptors\n    \u2502\n    \u2514\u2500\u25ba Else (connection in progress):\n            \u251c\u2500\u25ba If so_incomp full, abort oldest\n            \u251c\u2500\u25ba Insert into so_incomp queue\n            \u2514\u2500\u25ba Set SS_INCOMP\n</code></pre> <p>When a connection completes, <code>soisconnected()</code> moves it from <code>so_incomp</code> to <code>so_comp</code>:</p> <pre><code>soisconnected(so)\n    \u2502\n    \u251c\u2500\u25ba Get pool token for head socket\n    \u2502\n    \u251c\u2500\u25ba Clear SS_ISCONNECTING, set SS_ISCONNECTED\n    \u2502\n    \u251c\u2500\u25ba If on incomp queue (SS_INCOMP):\n    \u2502       \u251c\u2500\u25ba Check for accept filter\n    \u2502       \u251c\u2500\u25ba TAILQ_REMOVE from so_incomp\n    \u2502       \u251c\u2500\u25ba TAILQ_INSERT_TAIL to so_comp\n    \u2502       \u251c\u2500\u25ba Set SS_COMP, clear SS_INCOMP\n    \u2502       \u2514\u2500\u25ba sorwakeup(head) \u2192 Wake acceptors\n    \u2502\n    \u2514\u2500\u25ba Else: wakeup(&amp;so-&gt;so_timeo)\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#accept","title":"Accept","text":"<p><code>soaccept()</code> extracts a completed connection:</p> <pre><code>int soaccept(struct socket *so, struct sockaddr **nam)\n{\n    int error;\n\n    if ((so-&gt;so_state &amp; SS_NOFDREF) == 0)\n        panic(\"soaccept: !NOFDREF\");\n    soclrstate(so, SS_NOFDREF);  /* File descriptor now references socket */\n    error = so_pru_accept_direct(so, nam);\n    return error;\n}\n</code></pre> <p>The accept happens in <code>kern_accept()</code> which removes the socket from <code>so_comp</code> queue.</p>"},{"location":"sys/kern/ipc/sockets/#data-transfer","title":"Data Transfer","text":""},{"location":"sys/kern/ipc/sockets/#sending-data","title":"Sending Data","text":"<p><code>sosend()</code> is the generic send function (<code>sys/kern/uipc_socket.c</code>):</p> <pre><code>sosend(so, addr, uio, top, control, flags, td)\n    \u2502\n    \u251c\u2500\u25ba Acquire ssb_lock on send buffer\n    \u2502\n    \u251c\u2500\u25ba Loop while data remains:\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u25ba Wait for space if needed (ssb_wait)\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u25ba Check for errors, signals\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u25ba Allocate mbufs for data:\n    \u2502       \u2502       \u251c\u2500\u25ba m_uiomove() for large transfers\n    \u2502       \u2502       \u2514\u2500\u25ba Copy from uio to mbuf chain\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u25ba so_pru_send() \u2192 Protocol send\n    \u2502\n    \u2514\u2500\u25ba Release ssb_lock\n</code></pre> <p>Protocol-optimized variants:</p> <ul> <li><code>sosendtcp()</code> - Optimized for TCP streams</li> <li><code>sosendudp()</code> - Optimized for UDP datagrams</li> </ul>"},{"location":"sys/kern/ipc/sockets/#receiving-data","title":"Receiving Data","text":"<p><code>soreceive()</code> is the generic receive function:</p> <pre><code>soreceive(so, paddr, uio, sio, controlp, flagsp)\n    \u2502\n    \u251c\u2500\u25ba Acquire ssb_lock on receive buffer\n    \u2502\n    \u251c\u2500\u25ba Handle out-of-band data if MSG_OOB\n    \u2502\n    \u251c\u2500\u25ba Wait for data if needed (ssb_wait)\n    \u2502\n    \u251c\u2500\u25ba Extract address (MT_SONAME) if present\n    \u2502\n    \u251c\u2500\u25ba Extract control data (MT_CONTROL) if present\n    \u2502\n    \u251c\u2500\u25ba Copy data to uio:\n    \u2502       \u251c\u2500\u25ba Handle MSG_PEEK (don't remove data)\n    \u2502       \u251c\u2500\u25ba Track OOB mark position\n    \u2502       \u2514\u2500\u25ba sbdrop() to remove consumed data\n    \u2502\n    \u2514\u2500\u25ba Release ssb_lock\n</code></pre> <p><code>sorecvtcp()</code> provides an optimized path for TCP.</p>"},{"location":"sys/kern/ipc/sockets/#socket-shutdown-and-close","title":"Socket Shutdown and Close","text":"<p><code>soshutdown()</code> performs half-close:</p> <pre><code>int soshutdown(struct socket *so, int how)\n{\n    if (how != SHUT_WR)\n        sorflush(so);           /* Flush receive buffer */\n    if (how != SHUT_RD)\n        return so_pru_shutdown(so);\n    return 0;\n}\n</code></pre> <p><code>soclose()</code> fully closes a socket:</p> <pre><code>soclose(so, fflag)\n    \u2502\n    \u251c\u2500\u25ba If listening: Drop all pending connections\n    \u2502       \u251c\u2500\u25ba For each in so_incomp: soabort_async()\n    \u2502       \u2514\u2500\u25ba For each in so_comp: soabort_async()\n    \u2502\n    \u251c\u2500\u25ba If connected and SO_LINGER:\n    \u2502       \u251c\u2500\u25ba sodisconnect()\n    \u2502       \u2514\u2500\u25ba Wait up to so_linger time\n    \u2502\n    \u251c\u2500\u25ba Drop protocol attachment\n    \u2502\n    \u2514\u2500\u25ba sofree(so)\n</code></pre> <p><code>sofree()</code> handles reference counting and final cleanup:</p> <pre><code>sofree(so)\n    \u2502\n    \u251c\u2500\u25ba Decrement so_refs atomically\n    \u2502\n    \u251c\u2500\u25ba If refs == 0 and SS_NOFDREF:\n    \u2502       \u251c\u2500\u25ba Remove from head's queue if applicable\n    \u2502       \u251c\u2500\u25ba ssb_release() for both buffers\n    \u2502       \u251c\u2500\u25ba crfree(so_cred)\n    \u2502       \u2514\u2500\u25ba kfree(so, M_SOCKET)\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#socket-buffers","title":"Socket Buffers","text":""},{"location":"sys/kern/ipc/sockets/#buffer-space-management","title":"Buffer Space Management","text":"<p><code>soreserve()</code> reserves space for both buffers (<code>sys/kern/uipc_socket2.c:664</code>):</p> <pre><code>int soreserve(struct socket *so, u_long sndcc, u_long rcvcc, struct rlimit *rl)\n{\n    if (so-&gt;so_snd.ssb_lowat == 0)\n        atomic_set_int(&amp;so-&gt;so_snd.ssb_flags, SSB_AUTOLOWAT);\n    if (ssb_reserve(&amp;so-&gt;so_snd, sndcc, so, rl) == 0)\n        goto bad;\n    if (ssb_reserve(&amp;so-&gt;so_rcv, rcvcc, so, rl) == 0)\n        goto bad2;\n    /* Set default lowat values */\n    if (so-&gt;so_rcv.ssb_lowat == 0)\n        so-&gt;so_rcv.ssb_lowat = 1;\n    if (so-&gt;so_snd.ssb_lowat == 0)\n        so-&gt;so_snd.ssb_lowat = MCLBYTES;\n    ...\n}\n</code></pre> <p><code>ssb_reserve()</code> allocates buffer space with resource limits:</p> <pre><code>int ssb_reserve(struct signalsockbuf *ssb, u_long cc, struct socket *so,\n                struct rlimit *rl)\n{\n    /* Apply sb_max limit for user sockets */\n    if (rl &amp;&amp; cc &gt; sb_max_adj)\n        cc = sb_max_adj;\n    /* Account against user's resource limits */\n    if (!chgsbsize(so-&gt;so_cred-&gt;cr_uidinfo, &amp;ssb-&gt;ssb_hiwat, cc, ...))\n        return 0;\n    /* Set mbuf limit based on efficiency factor */\n    ssb-&gt;ssb_mbmax = cc * sb_efficiency;  /* default: cc * 8 */\n    ...\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#space-calculation","title":"Space Calculation","text":"<p><code>ssb_space()</code> returns available buffer space (<code>sys/sys/socketvar.h:270</code>):</p> <pre><code>static __inline long ssb_space(struct signalsockbuf *ssb)\n{\n    long bleft, mleft;\n\n    if (ssb-&gt;ssb_flags &amp; SSB_STOP)\n        return 0;  /* Backpressure active */\n    bleft = ssb-&gt;ssb_hiwat - ssb-&gt;ssb_cc;\n    mleft = ssb-&gt;ssb_mbmax - ssb-&gt;ssb_mbcnt;\n    return (bleft &lt; mleft) ? bleft : mleft;\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#appending-data","title":"Appending Data","text":"<p><code>sbappend()</code> appends mbufs to the buffer (<code>sys/kern/uipc_sockbuf.c:83</code>):</p> <pre><code>void sbappend(struct sockbuf *sb, struct mbuf *m)\n{\n    struct mbuf *n;\n\n    if (m) {\n        n = sb-&gt;sb_lastrecord;\n        if (n) {\n            if (n-&gt;m_flags &amp; M_EOR) {\n                sbappendrecord(sb, m);  /* Start new record */\n                return;\n            }\n        }\n        n = sb-&gt;sb_lastmbuf;\n        if (n) {\n            if (n-&gt;m_flags &amp; M_EOR) {\n                sbappendrecord(sb, m);\n                return;\n            }\n        }\n        sbcompress(sb, m, n);  /* Compress into existing */\n    }\n}\n</code></pre> <p><code>sbappendstream()</code> is optimized for stream protocols (TCP):</p> <pre><code>void sbappendstream(struct sockbuf *sb, struct mbuf *m)\n{\n    KKASSERT(m-&gt;m_nextpkt == NULL);\n    sbcompress(sb, m, sb-&gt;sb_lastmbuf);\n}\n</code></pre> <p><code>sbappendrecord()</code> starts a new record:</p> <pre><code>void sbappendrecord(struct sockbuf *sb, struct mbuf *m0)\n{\n    /* Break first mbuf off from chain */\n    firstmbuf = m0;\n    secondmbuf = m0-&gt;m_next;\n    m0-&gt;m_next = NULL;\n\n    /* Insert as new record */\n    if (sb-&gt;sb_mb == NULL)\n        sb-&gt;sb_mb = firstmbuf;\n    else\n        sb-&gt;sb_lastrecord-&gt;m_nextpkt = firstmbuf;\n    sb-&gt;sb_lastrecord = firstmbuf;\n    sb-&gt;sb_lastmbuf = firstmbuf;\n    sballoc(sb, firstmbuf);\n\n    /* Compress rest of chain */\n    sbcompress(sb, secondmbuf, firstmbuf);\n}\n</code></pre> <p><code>sbappendaddr()</code> prepends sender's address (for datagram sockets):</p> <pre><code>int sbappendaddr(struct sockbuf *sb, const struct sockaddr *asa,\n                 struct mbuf *m0, struct mbuf *control)\n{\n    /* Allocate mbuf for address */\n    MGET(m, M_NOWAIT, MT_SONAME);\n    m-&gt;m_len = asa-&gt;sa_len;\n    bcopy(asa, mtod(m, caddr_t), asa-&gt;sa_len);\n\n    /* Chain: address \u2192 control \u2192 data */\n    if (n)  /* control tail */\n        n-&gt;m_next = m0;\n    else\n        control = m0;\n    m-&gt;m_next = control;\n\n    /* Insert as new record */\n    ...\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#compression","title":"Compression","text":"<p><code>sbcompress()</code> coalesces small mbufs (<code>sys/kern/uipc_sockbuf.c:339</code>):</p> <pre><code>void sbcompress(struct sockbuf *sb, struct mbuf *m, struct mbuf *tailm)\n{\n    while (m) {\n        /* Skip empty mbufs unless EOR */\n        if (m-&gt;m_len == 0 &amp;&amp; (eor == 0 || ...)) {\n            /* defer to free_chain */\n            continue;\n        }\n\n        /* Try to coalesce with preceding mbuf */\n        if (tailm &amp;&amp; !(tailm-&gt;m_flags &amp; (M_EOR | M_SOLOCKED)) &amp;&amp;\n            M_WRITABLE(tailm) &amp;&amp;\n            m-&gt;m_len &lt;= MCLBYTES / 4 &amp;&amp;     /* Don't copy too much */\n            m-&gt;m_len &lt;= M_TRAILINGSPACE(tailm) &amp;&amp;\n            tailm-&gt;m_type == m-&gt;m_type) {\n            /* Copy data to tail of existing mbuf */\n            bcopy(mtod(m, caddr_t),\n                  mtod(tailm, caddr_t) + tailm-&gt;m_len,\n                  m-&gt;m_len);\n            tailm-&gt;m_len += m-&gt;m_len;\n            sb-&gt;sb_cc += m-&gt;m_len;\n            /* Move to free chain */\n            continue;\n        }\n\n        /* Insert whole mbuf */\n        if (tailm == NULL) {\n            sb-&gt;sb_mb = m;\n            sb-&gt;sb_lastrecord = m;\n        } else {\n            tailm-&gt;m_next = m;\n        }\n        sb-&gt;sb_lastmbuf = m;\n        sballoc(sb, m);\n        tailm = m;\n        m = m-&gt;m_next;\n        tailm-&gt;m_next = NULL;\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#dropping-data","title":"Dropping Data","text":"<p><code>sbdrop()</code> removes data from the front (<code>sys/kern/uipc_sockbuf.c:472</code>):</p> <pre><code>void sbdrop(struct sockbuf *sb, int len)\n{\n    struct mbuf *m;\n\n    crit_enter();\n    m = sb-&gt;sb_mb;\n    while (m &amp;&amp; len &gt; 0) {\n        if (m-&gt;m_len &gt; len) {\n            m-&gt;m_len -= len;\n            m-&gt;m_data += len;\n            sb-&gt;sb_cc -= len;\n            break;\n        }\n        len -= m-&gt;m_len;\n        m = sbunlinkmbuf(sb, m, &amp;free_chain);\n        if (m == NULL &amp;&amp; len)\n            m = sb-&gt;sb_mb;  /* Move to next record */\n    }\n    /* Remove trailing zero-length mbufs */\n    while (m &amp;&amp; m-&gt;m_len == 0)\n        m = sbunlinkmbuf(sb, m, &amp;free_chain);\n    crit_exit();\n\n    if (free_chain)\n        m_freem(free_chain);\n}\n</code></pre> <p><code>sbdroprecord()</code> removes entire first record:</p> <pre><code>void sbdroprecord(struct sockbuf *sb)\n{\n    struct mbuf *m, *n;\n\n    m = sb-&gt;sb_mb;\n    if (m) {\n        sb-&gt;sb_mb = m-&gt;m_nextpkt;  /* Advance to next record */\n        if (sb-&gt;sb_mb == NULL) {\n            sb-&gt;sb_lastrecord = NULL;\n            sb-&gt;sb_lastmbuf = NULL;\n        }\n        m-&gt;m_nextpkt = NULL;\n        for (n = m; n; n = n-&gt;m_next)\n            sbfree(sb, n);\n        m_freem(m);\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#wakeup-and-notification","title":"Wakeup and Notification","text":""},{"location":"sys/kern/ipc/sockets/#sowakeup","title":"sowakeup()","text":"<p><code>sowakeup()</code> notifies waiters of buffer state changes (<code>sys/kern/uipc_socket2.c:551</code>):</p> <pre><code>void sowakeup(struct socket *so, struct signalsockbuf *ssb)\n{\n    uint32_t flags;\n\n    /* Fast path: if WAKEUP already set and no special features */\n    flags = atomic_fetchadd_int(&amp;ssb-&gt;ssb_flags, 0);\n    if ((flags &amp; SSB_NOTIFY_MASK) == 0) {\n        if (flags &amp; SSB_WAKEUP)\n            return;\n    }\n\n    /* Check conditions and set WAKEUP flag */\n    for (;;) {\n        long space;\n        flags = ssb-&gt;ssb_flags;\n\n        if (ssb-&gt;ssb_flags &amp; SSB_PREALLOC)\n            space = ssb_space_prealloc(ssb);\n        else\n            space = ssb_space(ssb);\n\n        /* Wake if: space available, data ready, or end condition */\n        if ((ssb == &amp;so-&gt;so_snd &amp;&amp; space &gt;= ssb-&gt;ssb_lowat) ||\n            (ssb == &amp;so-&gt;so_rcv &amp;&amp; ssb-&gt;ssb_cc &gt;= ssb-&gt;ssb_lowat) ||\n            (ssb == &amp;so-&gt;so_snd &amp;&amp; (so-&gt;so_state &amp; SS_CANTSENDMORE)) ||\n            (ssb == &amp;so-&gt;so_rcv &amp;&amp; (so-&gt;so_state &amp; SS_CANTRCVMORE))) {\n            if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags,\n                              (flags | SSB_WAKEUP) &amp; ~SSB_WAIT)) {\n                if (flags &amp; SSB_WAIT)\n                    wakeup(&amp;ssb-&gt;ssb_cc);\n                break;\n            }\n        } else {\n            break;\n        }\n    }\n\n    /* Handle async signals and upcalls */\n    if ((so-&gt;so_state &amp; SS_ASYNC) &amp;&amp; so-&gt;so_sigio != NULL)\n        pgsigio(so-&gt;so_sigio, SIGIO, 0);\n    if (ssb-&gt;ssb_flags &amp; SSB_UPCALL)\n        (*so-&gt;so_upcall)(so, so-&gt;so_upcallarg, M_NOWAIT);\n    KNOTE(&amp;ssb-&gt;ssb_kq.ki_note, 0);\n\n    /* Process predicate message notifications */\n    if (ssb-&gt;ssb_flags &amp; SSB_MEVENT) {\n        /* Scan ssb_mlist and reply to satisfied predicates */\n        ...\n    }\n}\n</code></pre> <p>Convenience macros: - <code>sorwakeup(so)</code> \u2192 <code>sowakeup(so, &amp;so-&gt;so_rcv)</code> - <code>sowwakeup(so)</code> \u2192 <code>sowakeup(so, &amp;so-&gt;so_snd)</code></p>"},{"location":"sys/kern/ipc/sockets/#ssb_wait","title":"ssb_wait()","text":"<p><code>ssb_wait()</code> blocks waiting for buffer state change (<code>sys/kern/uipc_socket2.c:103</code>):</p> <pre><code>int ssb_wait(struct signalsockbuf *ssb)\n{\n    uint32_t flags;\n    int pflags, error;\n\n    pflags = (ssb-&gt;ssb_flags &amp; SSB_NOINTR) ? 0 : PCATCH;\n\n    for (;;) {\n        flags = ssb-&gt;ssb_flags;\n        cpu_ccfence();\n\n        /* Check if WAKEUP already set */\n        if (flags &amp; SSB_WAKEUP) {\n            if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags,\n                                  flags &amp; ~SSB_WAKEUP)) {\n                error = 0;\n                break;\n            }\n            continue;\n        }\n\n        /* Set WAIT and sleep */\n        tsleep_interlock(&amp;ssb-&gt;ssb_cc, pflags);\n        if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags, flags | SSB_WAIT)) {\n            error = tsleep(&amp;ssb-&gt;ssb_cc, pflags | PINTERLOCKED,\n                           \"sbwait\", ssb-&gt;ssb_timeo);\n            break;\n        }\n    }\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#connection-state-transitions","title":"Connection State Transitions","text":"<p><code>soisconnecting()</code> - marks connection attempt starting: <pre><code>void soisconnecting(struct socket *so)\n{\n    soclrstate(so, SS_ISCONNECTED | SS_ISDISCONNECTING);\n    sosetstate(so, SS_ISCONNECTING);\n}\n</code></pre></p> <p><code>soisdisconnecting()</code> - marks graceful disconnect starting: <pre><code>void soisdisconnecting(struct socket *so)\n{\n    soclrstate(so, SS_ISCONNECTING);\n    sosetstate(so, SS_ISDISCONNECTING | SS_CANTRCVMORE | SS_CANTSENDMORE);\n    wakeup(&amp;so-&gt;so_timeo);\n    sowwakeup(so);\n    sorwakeup(so);\n}\n</code></pre></p> <p><code>soisdisconnected()</code> - marks socket fully disconnected: <pre><code>void soisdisconnected(struct socket *so)\n{\n    soclrstate(so, SS_ISCONNECTING | SS_ISCONNECTED | SS_ISDISCONNECTING);\n    sosetstate(so, SS_CANTRCVMORE | SS_CANTSENDMORE | SS_ISDISCONNECTED);\n    wakeup(&amp;so-&gt;so_timeo);\n    sbdrop(&amp;so-&gt;so_snd.sb, so-&gt;so_snd.ssb_cc);  /* Discard pending sends */\n    sowwakeup(so);\n    sorwakeup(so);\n}\n</code></pre></p> <p><code>socantsendmore()</code> / <code>socantrcvmore()</code> - half-close notifications: <pre><code>void socantsendmore(struct socket *so)\n{\n    sosetstate(so, SS_CANTSENDMORE);\n    sowwakeup(so);\n}\n\nvoid socantrcvmore(struct socket *so)\n{\n    sosetstate(so, SS_CANTRCVMORE);\n    sorwakeup(so);\n}\n</code></pre></p>"},{"location":"sys/kern/ipc/sockets/#socket-options","title":"Socket Options","text":""},{"location":"sys/kern/ipc/sockets/#sosetopt","title":"sosetopt()","text":"<p><code>sosetopt()</code> sets socket options (<code>sys/kern/uipc_socket.c</code>):</p> <pre><code>int sosetopt(struct socket *so, struct sockopt *sopt)\n{\n    switch (sopt-&gt;sopt_name) {\n    case SO_LINGER:\n        /* Set linger time on close */\n        so-&gt;so_linger = l.l_linger;\n        if (l.l_onoff)\n            so-&gt;so_options |= SO_LINGER;\n        else\n            so-&gt;so_options &amp;= ~SO_LINGER;\n        break;\n\n    case SO_SNDBUF:\n        /* Set send buffer size */\n        ssb_reserve(&amp;so-&gt;so_snd, optval, so, ...);\n        break;\n\n    case SO_RCVBUF:\n        /* Set receive buffer size */\n        ssb_reserve(&amp;so-&gt;so_rcv, optval, so, ...);\n        break;\n\n    case SO_SNDLOWAT:\n    case SO_RCVLOWAT:\n        /* Set low water marks */\n        ssb-&gt;ssb_lowat = (optval &gt; ssb-&gt;ssb_hiwat) ?\n                         ssb-&gt;ssb_hiwat : optval;\n        break;\n\n    case SO_SNDTIMEO:\n    case SO_RCVTIMEO:\n        /* Set timeout values */\n        ssb-&gt;ssb_timeo = val;\n        break;\n\n    /* Boolean options */\n    case SO_DEBUG:\n    case SO_KEEPALIVE:\n    case SO_DONTROUTE:\n    case SO_BROADCAST:\n    case SO_REUSEADDR:\n    case SO_REUSEPORT:\n    case SO_OOBINLINE:\n    case SO_TIMESTAMP:\n    case SO_NOSIGPIPE:\n        if (optval)\n            so-&gt;so_options |= sopt-&gt;sopt_name;\n        else\n            so-&gt;so_options &amp;= ~sopt-&gt;sopt_name;\n        break;\n    }\n\n    /* Forward to protocol if handler exists */\n    if (so-&gt;so_proto-&gt;pr_ctloutput)\n        so_pr_ctloutput(so, sopt);\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#sogetopt","title":"sogetopt()","text":"<p><code>sogetopt()</code> retrieves socket options with similar structure, copying values to user space.</p>"},{"location":"sys/kern/ipc/sockets/#kqueue-integration","title":"kqueue Integration","text":""},{"location":"sys/kern/ipc/sockets/#filter-operations","title":"Filter Operations","text":"<p>Socket kqueue filters (<code>sys/kern/uipc_socket.c:2549</code>):</p> <pre><code>int sokqfilter(struct file *fp, struct knote *kn)\n{\n    struct socket *so = (struct socket *)kn-&gt;kn_fp-&gt;f_data;\n\n    switch (kn-&gt;kn_filter) {\n    case EVFILT_READ:\n        if (so-&gt;so_options &amp; SO_ACCEPTCONN)\n            kn-&gt;kn_fop = &amp;solisten_filtops;  /* Listen socket */\n        else\n            kn-&gt;kn_fop = &amp;soread_filtops;    /* Data socket */\n        ssb = &amp;so-&gt;so_rcv;\n        break;\n    case EVFILT_WRITE:\n        kn-&gt;kn_fop = &amp;sowrite_filtops;\n        ssb = &amp;so-&gt;so_snd;\n        break;\n    case EVFILT_EXCEPT:\n        kn-&gt;kn_fop = &amp;soexcept_filtops;\n        ssb = &amp;so-&gt;so_rcv;\n        break;\n    }\n\n    knote_insert(&amp;ssb-&gt;ssb_kq.ki_note, kn);\n    atomic_set_int(&amp;ssb-&gt;ssb_flags, SSB_KNOTE);\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#read-filter","title":"Read Filter","text":"<p><code>filt_soread()</code> checks read readiness:</p> <pre><code>static int filt_soread(struct knote *kn, long hint)\n{\n    struct socket *so = (struct socket *)kn-&gt;kn_fp-&gt;f_data;\n\n    /* Handle out-of-band data request */\n    if (kn-&gt;kn_sfflags &amp; NOTE_OOB) {\n        if (so-&gt;so_oobmark || (so-&gt;so_state &amp; SS_RCVATMARK)) {\n            kn-&gt;kn_fflags |= NOTE_OOB;\n            return 1;\n        }\n        return 0;\n    }\n\n    kn-&gt;kn_data = so-&gt;so_rcv.ssb_cc;  /* Available data */\n\n    /* Handle EOF conditions */\n    if (so-&gt;so_state &amp; SS_CANTRCVMORE) {\n        if (kn-&gt;kn_data == 0)\n            kn-&gt;kn_flags |= EV_NODATA;\n        kn-&gt;kn_flags |= EV_EOF;\n        kn-&gt;kn_fflags = so-&gt;so_error;\n        if (so-&gt;so_state &amp; SS_CANTSENDMORE)\n            kn-&gt;kn_flags |= EV_HUP;\n        return 1;\n    }\n\n    if (so-&gt;so_error || so-&gt;so_rerror)\n        return 1;\n\n    /* Check low water mark */\n    if (kn-&gt;kn_sfflags &amp; NOTE_LOWAT)\n        return (kn-&gt;kn_data &gt;= kn-&gt;kn_sdata);\n    return (kn-&gt;kn_data &gt;= so-&gt;so_rcv.ssb_lowat) ||\n           !TAILQ_EMPTY(&amp;so-&gt;so_comp);\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#listen-filter","title":"Listen Filter","text":"<p><code>filt_solisten()</code> checks for pending connections:</p> <pre><code>static int filt_solisten(struct knote *kn, long hint)\n{\n    struct socket *so = (struct socket *)kn-&gt;kn_fp-&gt;f_data;\n    int qlen = so-&gt;so_qlen;\n\n    if (soavailconn &gt; 0 &amp;&amp; qlen &gt; soavailconn)\n        qlen = soavailconn;\n    kn-&gt;kn_data = qlen;\n\n    return !TAILQ_EMPTY(&amp;so-&gt;so_comp);\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/sockets/#state-manipulation","title":"State Manipulation","text":"<p>Socket state is modified atomically:</p> <pre><code>static __inline void sosetstate(struct socket *so, int state)\n{\n    atomic_set_int(&amp;so-&gt;so_state, state);\n}\n\nstatic __inline void soclrstate(struct socket *so, int state)\n{\n    atomic_clear_int(&amp;so-&gt;so_state, state);\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#buffer-locking","title":"Buffer Locking","text":"<p><code>_ssb_lock()</code> acquires exclusive buffer access (<code>sys/kern/uipc_socket2.c:148</code>):</p> <pre><code>int _ssb_lock(struct signalsockbuf *ssb)\n{\n    uint32_t flags;\n    int pflags, error;\n\n    pflags = (ssb-&gt;ssb_flags &amp; SSB_NOINTR) ? 0 : PCATCH;\n\n    for (;;) {\n        flags = ssb-&gt;ssb_flags;\n        if (flags &amp; SSB_LOCK) {\n            /* Already locked, wait */\n            tsleep_interlock(&amp;ssb-&gt;ssb_flags, pflags);\n            if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags,\n                                  flags | SSB_WANT)) {\n                error = tsleep(&amp;ssb-&gt;ssb_flags,\n                               pflags | PINTERLOCKED, \"sblock\", 0);\n                if (error)\n                    break;\n            }\n        } else {\n            /* Acquire lock */\n            if (atomic_cmpset_int(&amp;ssb-&gt;ssb_flags, flags,\n                                  flags | SSB_LOCK)) {\n                lwkt_gettoken(&amp;ssb-&gt;ssb_token);\n                error = 0;\n                break;\n            }\n        }\n    }\n    return error;\n}\n</code></pre> <p>The inline <code>ssb_lock()</code> provides the fast path.</p>"},{"location":"sys/kern/ipc/sockets/#reference-counting","title":"Reference Counting","text":"<p>Socket references prevent premature deallocation:</p> <pre><code>static __inline void soreference(struct socket *so)\n{\n    atomic_add_int(&amp;so-&gt;so_refs, 1);\n}\n\nvoid sofree(struct socket *so)\n{\n    /* Atomically decrement and check */\n    if (atomic_fetchadd_int(&amp;so-&gt;so_refs, -1) != 1)\n        return;\n    /* Last reference, actually free if SS_NOFDREF */\n    ...\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#sysctl-parameters","title":"Sysctl Parameters","text":"<p>Key tunable parameters (<code>sys/kern/uipc_socket2.c</code>):</p> Sysctl Default Description <code>kern.ipc.maxsockbuf</code> 512KB Maximum socket buffer size <code>kern.ipc.maxsockets</code> varies Maximum number of sockets <code>kern.ipc.sockbuf_waste_factor</code> 8 Buffer efficiency factor <code>kern.ipc.soaccept_reuse</code> 1 Allow quick local port reuse <code>kern.ipc.somaxconn</code> 128 Maximum listen backlog"},{"location":"sys/kern/ipc/sockets/#accept-filters","title":"Accept Filters","text":"<p>Accept filters allow deferred connection acceptance:</p> <pre><code>struct accept_filter {\n    char    accf_name[16];\n    void    (*accf_callback)(struct socket *so, void *arg, int waitflag);\n    void *  (*accf_create)(struct socket *so, char *arg);\n    void    (*accf_destroy)(struct socket *so);\n    SLIST_ENTRY(accept_filter) accf_next;\n};\n</code></pre> <p>When <code>SO_ACCEPTFILTER</code> is set, new connections go through the filter callback before being moved to <code>so_comp</code>. This allows filtering based on initial data (e.g., HTTP request line).</p>"},{"location":"sys/kern/ipc/sockets/#error-handling","title":"Error Handling","text":""},{"location":"sys/kern/ipc/sockets/#connection-errors","title":"Connection Errors","text":"<p>Errors are stored in <code>so_error</code> and propagated to waiters:</p> <pre><code>/* In protocol code when error occurs */\nso-&gt;so_error = error;\nsorwakeup(so);\nsowwakeup(so);\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#receive-buffer-errors","title":"Receive Buffer Errors","text":"<p>Receive-specific errors use <code>so_rerror</code>:</p> <pre><code>void soroverflow(struct socket *so)\n{\n    if (so-&gt;so_options &amp; SO_RERROR) {\n        so-&gt;so_rerror = ENOBUFS;\n        sorwakeup(so);\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/sockets/#see-also","title":"See Also","text":"<ul> <li>Mbufs - Memory buffer management</li> <li>IPC Overview - Inter-process communication</li> <li>LWKT Threading - Thread subsystem</li> <li>Synchronization - Locking primitives</li> </ul>"},{"location":"sys/kern/ipc/sysv-msg/","title":"System V Message Queues","text":"<p>System V message queues provide inter-process communication through kernel-managed message buffers. DragonFly's implementation derives from FreeBSD and follows the SVID (System V Interface Definition) specification.</p> <p>Source files: - <code>sys/kern/sysv_msg.c</code> - Implementation - <code>sys/sys/msg.h</code> - Public interface</p>"},{"location":"sys/kern/ipc/sysv-msg/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/sysv-msg/#message-queue-descriptor","title":"Message Queue Descriptor","text":"<pre><code>struct msqid_ds {\n    struct  ipc_perm msg_perm;  /* permission bits */\n    struct  msg *msg_first;     /* first message in queue */\n    struct  msg *msg_last;      /* last message in queue */\n    msglen_t msg_cbytes;        /* bytes currently in queue */\n    msgqnum_t msg_qnum;         /* number of messages */\n    msglen_t msg_qbytes;        /* max bytes allowed */\n    pid_t   msg_lspid;          /* last msgsnd() pid */\n    pid_t   msg_lrpid;          /* last msgrcv() pid */\n    time_t  msg_stime;          /* last msgsnd() time */\n    time_t  msg_rtime;          /* last msgrcv() time */\n    time_t  msg_ctime;          /* last msgctl() time */\n};\n</code></pre> <p>Defined in <code>sys/sys/msg.h:68-84</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#internal-message-header","title":"Internal Message Header","text":"<pre><code>struct msg {\n    struct  msg *msg_next;  /* next msg in chain */\n    long    msg_type;       /* message type (&gt;0) or 0 if free */\n    u_short msg_ts;         /* message size in bytes */\n    short   msg_spot;       /* index of first segment in msgpool */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_msg.c:45-52</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#segment-map","title":"Segment Map","text":"<pre><code>struct msgmap {\n    short   next;   /* next segment index, or -1 if available */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_msg.c:104-108</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#memory-layout","title":"Memory Layout","text":"<p>Messages are stored in a segmented buffer pool:</p> <pre><code>msgpool (MSGMAX bytes)\n+--------+--------+--------+--------+-----+\n| seg 0  | seg 1  | seg 2  | seg 3  | ... |\n+--------+--------+--------+--------+-----+\n   ^         ^\n   |         |\nmsgmaps[] tracks segment linkage via indices\n\nmsghdrs[] - preallocated message headers\nmsqids[]  - preallocated queue descriptors\n</code></pre> <p>Each segment is <code>MSGSSZ</code> bytes (default 8, must be power of 2 between 8-1024). Segments are linked via <code>msgmaps[].next</code> forming a free list or per-message chain.</p>"},{"location":"sys/kern/ipc/sysv-msg/#system-limits","title":"System Limits","text":"Parameter Default Description <code>MSGSSZ</code> 8 Segment size (bytes, power of 2) <code>MSGSEG</code> 2048 Total segments (&lt;32767) <code>MSGMAX</code> MSGSSZ*MSGSEG Max message size <code>MSGMNB</code> 2048 Max bytes per queue <code>MSGMNI</code> 40 Max queue identifiers <code>MSGTQL</code> 40 Max messages system-wide <p>Defined in <code>sys/kern/sysv_msg.c:55-70</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#synchronization","title":"Synchronization","text":"<p>A single LWKT token protects all message queue operations:</p> <pre><code>static struct lwkt_token msg_token = LWKT_TOKEN_INITIALIZER(msg_token);\n</code></pre> <p>Defined at <code>sys/kern/sysv_msg.c:119</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#queue-locking","title":"Queue Locking","text":"<p>The <code>MSG_LOCKED</code> flag (value <code>01000</code>) in <code>msg_perm.mode</code> prevents queue reallocation while a process is copying message data to/from userspace:</p> <pre><code>#define MSG_LOCKED  01000\n</code></pre> <p>This lock serializes concurrent senders when resources are scarce, ensuring first-come-first-served semantics.</p>"},{"location":"sys/kern/ipc/sysv-msg/#initialization","title":"Initialization","text":"<p><code>msginit()</code> runs at <code>SI_SUB_SYSV_MSG</code>:</p> <ol> <li>Allocates <code>msgpool</code> (MSGMAX bytes)</li> <li>Allocates <code>msgmaps[]</code> (MSGSEG entries)</li> <li>Allocates <code>msghdrs[]</code> (MSGTQL entries)</li> <li>Allocates <code>msqids[]</code> (MSGMNI entries)</li> <li>Validates <code>msgssz</code> is power of 2 in [8,1024]</li> <li>Links free segment list via <code>free_msgmaps</code></li> <li>Links free header list via <code>free_msghdrs</code></li> <li>Marks all queue slots as available (<code>msg_qbytes = 0</code>)</li> </ol> <p>See <code>sys/kern/sysv_msg.c:121-174</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/sysv-msg/#msgget-create-or-access-queue","title":"msgget - Create or Access Queue","text":"<pre><code>int sys_msgget(struct sysmsg *sysmsg, const struct msgget_args *uap)\n</code></pre> <p>Arguments: <code>key</code>, <code>msgflg</code></p> <p>Operation: 1. Check jail capabilities (<code>PRISON_CAP_SYS_SYSVIPC</code>) 2. If <code>key != IPC_PRIVATE</code>, search for existing queue with matching key 3. If found and <code>IPC_CREAT|IPC_EXCL</code> set, return <code>EEXIST</code> 4. If not found and <code>IPC_CREAT</code> set, allocate new slot 5. Initialize permissions, timestamps, byte limits 6. Return unique msqid: <code>(index &amp; 0xffff) | (seq &lt;&lt; 16)</code></p> <p>The sequence number prevents stale ID reuse after queue deletion.</p> <p>See <code>sys/kern/sysv_msg.c:344-450</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#msgsnd-send-message","title":"msgsnd - Send Message","text":"<pre><code>int sys_msgsnd(struct sysmsg *sysmsg, const struct msgsnd_args *uap)\n</code></pre> <p>Arguments: <code>msqid</code>, <code>msgp</code>, <code>msgsz</code>, <code>msgflg</code></p> <p>Operation: 1. Validate msqid and permissions (<code>IPC_W</code>) 2. Calculate segments needed: <code>howmany(msgsz, msgssz)</code> 3. Wait loop for resources:    - Queue not locked    - Space available (<code>msgsz + msg_cbytes &lt;= msg_qbytes</code>)    - Enough free segments    - Free message header available 4. If <code>IPC_NOWAIT</code> and resources unavailable, return <code>EAGAIN</code> 5. Set <code>MSG_LOCKED</code> to prevent queue reallocation during copy 6. Allocate message header from <code>free_msghdrs</code> 7. Allocate segments from <code>free_msgmaps</code>, linking them 8. <code>copyin()</code> message type (must be &gt; 0) and body segment by segment 9. Append to queue (<code>msg_last-&gt;msg_next = msghdr</code>) 10. Update statistics, clear <code>MSG_LOCKED</code>, <code>wakeup()</code> waiters</p> <p>See <code>sys/kern/sysv_msg.c:455-782</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#msgrcv-receive-message","title":"msgrcv - Receive Message","text":"<pre><code>int sys_msgrcv(struct sysmsg *sysmsg, const struct msgrcv_args *uap)\n</code></pre> <p>Arguments: <code>msqid</code>, <code>msgp</code>, <code>msgsz</code>, <code>msgtyp</code>, <code>msgflg</code></p> <p>Message Selection by <code>msgtyp</code>: - <code>msgtyp == 0</code>: First message (FIFO) - <code>msgtyp &gt; 0</code>: First message with <code>msg_type == msgtyp</code> - <code>msgtyp &lt; 0</code>: First message with <code>msg_type &lt;= |msgtyp|</code> (lowest type first)</p> <p>Operation: 1. Validate msqid and permissions (<code>IPC_R</code>) 2. Search queue for matching message 3. If not found and <code>IPC_NOWAIT</code> set, return <code>ENOMSG</code> 4. If not found, <code>tsleep()</code> on queue address 5. Remove message from queue, update <code>msg_first</code>/<code>msg_last</code> 6. <code>copyout()</code> message type and body segment by segment 7. If <code>msgsz &lt; msg_ts</code> and <code>MSG_NOERROR</code> not set, return <code>E2BIG</code> 8. Free message header and segments via <code>msg_freehdr()</code> 9. <code>wakeup()</code> waiters, return actual bytes copied</p> <p>See <code>sys/kern/sysv_msg.c:787-1070</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#msgctl-control-operations","title":"msgctl - Control Operations","text":"<pre><code>int sys_msgctl(struct sysmsg *sysmsg, const struct msgctl_args *uap)\n</code></pre> <p>Commands:</p> Command Description <code>IPC_STAT</code> Copy <code>msqid_ds</code> to user buffer <code>IPC_SET</code> Update uid, gid, mode, qbytes <code>IPC_RMID</code> Remove queue and free all messages <p>IPC_RMID Operation: 1. Free all message headers via <code>msg_freehdr()</code> 2. Set <code>msg_qbytes = 0</code> to mark slot available 3. <code>wakeup()</code> all blocked processes (they get <code>EIDRM</code>)</p> <p>See <code>sys/kern/sysv_msg.c:202-339</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#segment-allocation","title":"Segment Allocation","text":""},{"location":"sys/kern/ipc/sysv-msg/#allocation-in-msgsnd","title":"Allocation (in msgsnd)","text":"<pre><code>while (segs_needed &gt; 0) {\n    next = free_msgmaps;\n    free_msgmaps = msgmaps[next].next;\n    nfree_msgmaps--;\n    msgmaps[next].next = msghdr-&gt;msg_spot;\n    msghdr-&gt;msg_spot = next;\n    segs_needed--;\n}\n</code></pre> <p>Segments are prepended to the message's chain, so the chain is reversed from allocation order. See <code>sys/kern/sysv_msg.c:657-675</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#deallocation-msg_freehdr","title":"Deallocation (msg_freehdr)","text":"<pre><code>static void msg_freehdr(struct msg *msghdr)\n{\n    while (msghdr-&gt;msg_ts &gt; 0) {\n        next = msgmaps[msghdr-&gt;msg_spot].next;\n        msgmaps[msghdr-&gt;msg_spot].next = free_msgmaps;\n        free_msgmaps = msghdr-&gt;msg_spot;\n        nfree_msgmaps++;\n        msghdr-&gt;msg_spot = next;\n        msghdr-&gt;msg_ts -= msginfo.msgssz;\n    }\n    msghdr-&gt;msg_next = free_msghdrs;\n    free_msghdrs = msghdr;\n}\n</code></pre> <p>See <code>sys/kern/sysv_msg.c:176-197</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#id-encoding","title":"ID Encoding","text":"<p>Message queue IDs encode both index and sequence number:</p> <pre><code>#define MSQID(ix,ds)    ((ix) &amp; 0xffff | (((ds).msg_perm.seq &lt;&lt; 16) &amp; 0xffff0000))\n#define MSQID_IX(id)    ((id) &amp; 0xffff)\n#define MSQID_SEQ(id)   (((id) &gt;&gt; 16) &amp; 0xffff)\n</code></pre> <p>The sequence number increments on each allocation, preventing use of stale IDs after a queue is removed and its slot reused.</p> <p>See <code>sys/kern/sysv_msg.c:96-98</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#jail-support","title":"Jail Support","text":"<p>All system calls check jail capabilities before proceeding:</p> <pre><code>if (pr &amp;&amp; !PRISON_CAP_ISSET(pr-&gt;pr_caps, PRISON_CAP_SYS_SYSVIPC))\n    return (ENOSYS);\n</code></pre>"},{"location":"sys/kern/ipc/sysv-msg/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Type Description <code>kern.ipc.msgmax</code> RD Max chars in a message <code>kern.ipc.msgmni</code> RD Max queue identifiers <code>kern.ipc.msgmnb</code> RD Max chars in queue <code>kern.ipc.msgtql</code> RD Max messages in system <code>kern.ipc.msgssz</code> RD Segment size <code>kern.ipc.msgseg</code> RD Number of segments <code>kern.ipc.msqids</code> RD Queue ID array (raw) <p>Tunable at boot via <code>kern.ipc.msgseg</code>, <code>kern.ipc.msgssz</code>, <code>kern.ipc.msgmni</code>.</p> <p>See <code>sys/kern/sysv_msg.c:1079-1096</code>.</p>"},{"location":"sys/kern/ipc/sysv-msg/#error-handling","title":"Error Handling","text":"Error Condition <code>ENOSYS</code> Jail lacks SYSVIPC capability <code>EINVAL</code> Invalid msqid, deleted queue, or bad parameters <code>EEXIST</code> <code>IPC_CREAT|IPC_EXCL</code> and queue exists <code>ENOENT</code> Queue not found, no <code>IPC_CREAT</code> <code>ENOSPC</code> No free queue slots <code>EAGAIN</code> <code>IPC_NOWAIT</code> and resources unavailable <code>EIDRM</code> Queue deleted while waiting <code>EINTR</code> Signal received while waiting <code>E2BIG</code> Message too large, <code>MSG_NOERROR</code> not set <code>ENOMSG</code> No matching message, <code>IPC_NOWAIT</code> set"},{"location":"sys/kern/ipc/sysv-sem/","title":"System V Semaphores","text":"<p>System V semaphores provide counting semaphores for process synchronization. DragonFly's implementation derives from FreeBSD and follows the SVID specification, supporting atomic operations on semaphore sets.</p> <p>Source files: - <code>sys/kern/sysv_sem.c</code> - Implementation - <code>sys/sys/sem.h</code> - Public interface</p>"},{"location":"sys/kern/ipc/sysv-sem/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/sysv-sem/#semaphore-set-descriptor","title":"Semaphore Set Descriptor","text":"<pre><code>struct semid_ds {\n    struct  ipc_perm sem_perm;  /* permission struct */\n    struct  sem *sem_base;      /* pointer to first semaphore */\n    unsigned short sem_nsems;   /* number of semaphores in set */\n    time_t  sem_otime;          /* last semop() time */\n    time_t  sem_ctime;          /* last change time */\n};\n</code></pre> <p>Defined in <code>sys/sys/sem.h:34-45</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#semaphore-pool-entry","title":"Semaphore Pool Entry","text":"<pre><code>struct semid_pool {\n    struct lock lk;         /* per-set exclusive lock */\n    struct semid_ds ds;     /* the semid_ds descriptor */\n    long gen;               /* generation counter */\n};\n</code></pre> <p>Defined in <code>sys/sys/sem.h:51-55</code>. The <code>gen</code> field detects destroy/recreate races where credentials might match.</p>"},{"location":"sys/kern/ipc/sysv-sem/#individual-semaphore","title":"Individual Semaphore","text":"<pre><code>struct sem {\n    u_short semval;     /* current value */\n    pid_t   sempid;     /* pid of last operation */\n    u_short semncnt;    /* processes waiting for semval &gt; cval */\n    u_short semzcnt;    /* processes waiting for semval == 0 */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_sem.c:39-44</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#semaphore-operation","title":"Semaphore Operation","text":"<pre><code>struct sembuf {\n    unsigned short sem_num;  /* semaphore index in set */\n    short   sem_op;          /* operation value */\n    short   sem_flg;         /* IPC_NOWAIT, SEM_UNDO */\n};\n</code></pre> <p>Defined in <code>sys/sys/sem.h:62-66</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#undo-structure","title":"Undo Structure","text":"<pre><code>struct sem_undo {\n    TAILQ_ENTRY(sem_undo) un_entry;  /* global list linkage */\n    struct  proc *un_proc;            /* owning process */\n    int     un_refs;                  /* reference count */\n    short   un_cnt;                   /* active undo entries */\n    struct undo {\n        short   un_adjval;  /* adjustment value */\n        short   un_num;     /* semaphore number */\n        int     un_id;      /* semaphore set id */\n    } un_ent[1];            /* variable-length array */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_sem.c:49-60</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#system-limits","title":"System Limits","text":"Parameter Default Description <code>SEMMNI</code> 1024 Max semaphore identifiers <code>SEMMNS</code> 32767 Max semaphores system-wide <code>SEMMSL</code> SEMMNS Max semaphores per set <code>SEMOPM</code> 100 Max operations per semop() <code>SEMUME</code> 25 Max undo entries per process <code>SEMVMX</code> 32767 Maximum semaphore value <code>SEMAEM</code> 16384 Max adjust-on-exit value <p>Defined in <code>sys/kern/sysv_sem.c:65-91</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/sysv-sem/#global-lock","title":"Global Lock","text":"<pre><code>static struct lock sema_lk;\n</code></pre> <p>Protects allocation of new semaphore sets and the global <code>semtot</code> counter.</p>"},{"location":"sys/kern/ipc/sysv-sem/#per-set-lock","title":"Per-Set Lock","text":"<pre><code>struct semid_pool {\n    struct lock lk;  /* exclusive lock for this set */\n    ...\n};\n</code></pre> <p>Each semaphore set has its own lock for operations.</p>"},{"location":"sys/kern/ipc/sysv-sem/#per-semaphore-token","title":"Per-Semaphore Token","text":"<p>Individual semaphores use pool tokens for fine-grained locking:</p> <pre><code>lwkt_getpooltoken(semptr);\n/* modify semptr-&gt;semval */\nlwkt_relpooltoken(semptr);\n</code></pre> <p>This allows concurrent operations on different semaphores within the same set.</p>"},{"location":"sys/kern/ipc/sysv-sem/#undo-list-token","title":"Undo List Token","text":"<pre><code>static struct lwkt_token semu_token;\n</code></pre> <p>Protects the global <code>semu_list</code> of undo structures.</p>"},{"location":"sys/kern/ipc/sysv-sem/#initialization","title":"Initialization","text":"<p><code>seminit()</code> runs at <code>SI_SUB_SYSV_SEM</code>:</p> <ol> <li>Allocates <code>sema[]</code> array (SEMMNI entries)</li> <li>Initializes global lock <code>sema_lk</code></li> <li>Initializes per-set locks and marks all slots as unallocated</li> </ol> <p>See <code>sys/kern/sysv_sem.c:164-181</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/sysv-sem/#semget-create-or-access-set","title":"semget - Create or Access Set","text":"<pre><code>int sys_semget(struct sysmsg *sysmsg, const struct semget_args *uap)\n</code></pre> <p>Arguments: <code>key</code>, <code>nsems</code>, <code>semflg</code></p> <p>Operation: 1. Check jail capabilities 2. If <code>key != IPC_PRIVATE</code>, search for existing set with matching key 3. Validate permissions and nsems count 4. If not found and <code>IPC_CREAT</code>, allocate new set:    - Acquire <code>sema_lk</code> exclusive    - Check system-wide semaphore limit (<code>semtot + nsems &lt;= semmns</code>)    - Find free slot, initialize descriptor    - Allocate <code>sem_base</code> array    - Set <code>SEM_ALLOC</code> flag, increment <code>semtot</code> 5. Return unique semid with sequence number</p> <p>See <code>sys/kern/sysv_sem.c:573-722</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#semop-perform-operations","title":"semop - Perform Operations","text":"<pre><code>int sys_semop(struct sysmsg *sysmsg, const struct semop_args *uap)\n</code></pre> <p>Arguments: <code>semid</code>, <code>sops</code>, <code>nsops</code></p> <p>Atomicity: All operations in a semop() call succeed or fail together.</p> <p>Operation: 1. Copy <code>sops[]</code> array from userspace (max <code>MAX_SOPS</code> = 5) 2. Acquire set lock shared 3. For each operation:    - <code>sem_op &lt; 0</code>: Decrement if <code>semval + sem_op &gt;= 0</code>    - <code>sem_op == 0</code>: Wait until <code>semval == 0</code>    - <code>sem_op &gt; 0</code>: Increment unconditionally 4. If any operation blocks:    - Increment <code>semncnt</code> or <code>semzcnt</code>    - Rollback all completed operations    - If <code>IPC_NOWAIT</code>, return <code>EAGAIN</code>    - Release set lock, <code>tsleep()</code> on semaphore address    - On wakeup, reacquire lock and retry from beginning 5. On success, record undo adjustments if <code>SEM_UNDO</code> set 6. Update <code>sempid</code> for each touched semaphore</p> <p>Rollback: If blocking occurs, previously applied operations are undone to maintain atomicity. See <code>sys/kern/sysv_sem.c:883-892</code>.</p> <p>See <code>sys/kern/sysv_sem.c:727-1050</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#semctl-control-operations","title":"semctl - Control Operations","text":"<pre><code>int sys___semctl(struct sysmsg *sysmsg, const struct __semctl_args *uap)\n</code></pre> <p>Arguments: <code>semid</code>, <code>semnum</code>, <code>cmd</code>, <code>arg</code></p> <p>Commands:</p> Command Description <code>IPC_STAT</code> Copy semid_ds to user buffer <code>IPC_SET</code> Update uid, gid, mode <code>IPC_RMID</code> Remove semaphore set <code>SEM_STAT</code> Like IPC_STAT but semid is array index <code>GETVAL</code> Get single semaphore value <code>SETVAL</code> Set single semaphore value <code>GETALL</code> Get all semaphore values <code>SETALL</code> Set all semaphore values <code>GETPID</code> Get last operation pid <code>GETNCNT</code> Get semncnt (waiters for increment) <code>GETZCNT</code> Get semzcnt (waiters for zero) <p>IPC_RMID Operation: 1. Decrement global <code>semtot</code> by <code>sem_nsems</code> 2. Free <code>sem_base</code> array 3. Clear <code>SEM_ALLOC</code> flag 4. Call <code>semundo_clear()</code> to purge undo entries</p> <p>See <code>sys/kern/sysv_sem.c:346-568</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#sem_undo-mechanism","title":"SEM_UNDO Mechanism","text":"<p>When <code>SEM_UNDO</code> flag is set on an operation, the kernel records an adjustment that will be applied when the process exits.</p>"},{"location":"sys/kern/ipc/sysv-sem/#recording-adjustments","title":"Recording Adjustments","text":"<p><code>semundo_adjust()</code> maintains per-process undo entries:</p> <pre><code>static int semundo_adjust(struct proc *p, int semid, int semnum, int adjval)\n</code></pre> <ul> <li>Allocates <code>sem_undo</code> structure on first use</li> <li>Stores negative of the operation value</li> <li>Entries are compacted when adjustment becomes zero</li> </ul> <p>See <code>sys/kern/sysv_sem.c:218-269</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#process-exit","title":"Process Exit","text":"<p><code>semexit()</code> is called when a process exits:</p> <ol> <li>Iterate through undo entries in reverse order</li> <li>For each entry, apply the recorded adjustment</li> <li>Wake up any waiters on affected semaphores</li> <li>Remove undo structure from global list</li> </ol> <p>See <code>sys/kern/sysv_sem.c:1058-1163</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#clearing-undos-on-set-removal","title":"Clearing Undos on Set Removal","text":"<p><code>semundo_clear()</code> removes undo entries for a deleted semaphore set:</p> <pre><code>static void semundo_clear(int semid, int semnum)\n</code></pre> <p>Called by <code>IPC_RMID</code> with <code>semnum = -1</code> to clear all entries for the set.</p> <p>See <code>sys/kern/sysv_sem.c:274-339</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#wakeup-optimization","title":"Wakeup Optimization","text":"<p>The implementation uses delayed wakeups for efficiency:</p> <pre><code>wakeup_start_delayed();\n/* ... operations that may cause wakeups ... */\nwakeup_end_delayed();\n</code></pre> <p>This batches wakeup signals to reduce context switch overhead.</p>"},{"location":"sys/kern/ipc/sysv-sem/#generation-counter","title":"Generation Counter","text":"<p>Each semaphore set has a generation counter:</p> <pre><code>struct semid_pool {\n    ...\n    long gen;\n};\n</code></pre> <p>Incremented on allocation, used to detect races where a set is destroyed and recreated while a process sleeps. The sleeping process compares the generation before and after sleep.</p>"},{"location":"sys/kern/ipc/sysv-sem/#jail-support","title":"Jail Support","text":"<p>All system calls check jail capabilities:</p> <pre><code>if (pr &amp;&amp; !PRISON_CAP_ISSET(pr-&gt;pr_caps, PRISON_CAP_SYS_SYSVIPC))\n    return (ENOSYS);\n</code></pre>"},{"location":"sys/kern/ipc/sysv-sem/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Type Description <code>kern.ipc.semmap</code> RW Entries in semaphore map <code>kern.ipc.semmni</code> RD Max semaphore identifiers <code>kern.ipc.semmns</code> RD Max semaphores in system <code>kern.ipc.semmnu</code> RD Undo structures in system <code>kern.ipc.semmsl</code> RW Max semaphores per id <code>kern.ipc.semopm</code> RD Max operations per semop <code>kern.ipc.semume</code> RD Max undo entries per process <code>kern.ipc.semusz</code> RD Size of undo structure <code>kern.ipc.semvmx</code> RW Semaphore maximum value <code>kern.ipc.semaem</code> RW Adjust on exit max value <p>All parameters tunable at boot via loader.</p> <p>See <code>sys/kern/sysv_sem.c:119-149</code>.</p>"},{"location":"sys/kern/ipc/sysv-sem/#error-handling","title":"Error Handling","text":"Error Condition <code>ENOSYS</code> Jail lacks SYSVIPC capability <code>EINVAL</code> Invalid semid, semnum, or nsems <code>EEXIST</code> <code>IPC_CREAT|IPC_EXCL</code> and set exists <code>ENOENT</code> Set not found, no <code>IPC_CREAT</code> <code>ENOSPC</code> No free slots or semaphore limit reached <code>EAGAIN</code> <code>IPC_NOWAIT</code> and would block <code>EIDRM</code> Set deleted while waiting <code>EINTR</code> Signal received while waiting <code>EFBIG</code> sem_num &gt;= sem_nsems <code>E2BIG</code> Too many operations (nsops &gt; MAX_SOPS)"},{"location":"sys/kern/ipc/sysv-shm/","title":"System V Shared Memory","text":"<p>System V shared memory allows processes to share memory regions directly. DragonFly's implementation derives from FreeBSD and uses VM objects backed by either physical memory or the swap pager.</p> <p>Source files: - <code>sys/kern/sysv_shm.c</code> - Implementation - <code>sys/sys/shm.h</code> - Public interface</p>"},{"location":"sys/kern/ipc/sysv-shm/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/sysv-shm/#shared-memory-descriptor","title":"Shared Memory Descriptor","text":"<pre><code>struct shmid_ds {\n    struct ipc_perm shm_perm;   /* permission structure */\n    size_t    shm_segsz;        /* segment size in bytes */\n    pid_t     shm_lpid;         /* last shmat/shmdt pid */\n    pid_t     shm_cpid;         /* creator pid */\n    shmatt_t  shm_nattch;       /* current attach count */\n    time_t    shm_atime;        /* last shmat() time */\n    time_t    shm_dtime;        /* last shmdt() time */\n    time_t    shm_ctime;        /* last shmctl() time */\n    void     *shm_internal;     /* kernel-internal handle */\n};\n</code></pre> <p>Defined in <code>sys/sys/shm.h:74-84</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#internal-handle","title":"Internal Handle","text":"<pre><code>struct shm_handle {\n    vm_object_t shm_object;  /* backing VM object */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_shm.c:71-74</code>. The <code>shm_internal</code> field of <code>shmid_ds</code> points to this structure.</p>"},{"location":"sys/kern/ipc/sysv-shm/#per-process-mapping-state","title":"Per-Process Mapping State","text":"<pre><code>struct shmmap_state {\n    vm_offset_t va;     /* virtual address of mapping */\n    int shmid;          /* attached segment id, or -1 */\n    int reserved;       /* reservation flag for races */\n};\n</code></pre> <p>Defined in <code>sys/kern/sysv_shm.c:76-80</code>. Each process has an array of these (size <code>shmseg</code>) stored in <code>p-&gt;p_vmspace-&gt;vm_shm</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#segment-state-flags","title":"Segment State Flags","text":"Flag Value Description <code>SHMSEG_FREE</code> 0x0200 Slot is available <code>SHMSEG_REMOVED</code> 0x0400 Marked for removal <code>SHMSEG_ALLOCATED</code> 0x0800 Segment is in use <code>SHMSEG_WANTED</code> 0x1000 Someone waiting for allocation <p>Defined in <code>sys/kern/sysv_shm.c:62-65</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#system-limits","title":"System Limits","text":"Parameter Default Description <code>SHMMIN</code> 1 Minimum segment size <code>SHMMNI</code> 512 Max segment identifiers <code>SHMSEG</code> 1024 Max segments per process <code>shmmax</code> 2/3 RAM Max segment size (auto-computed) <code>shmall</code> 2/3 RAM pages Max total pages (auto-computed) <p>If <code>shmall</code> is not set via tunable, it defaults to 2/3 of physical pages. <code>shmmax</code> is computed as <code>shmall * PAGE_SIZE</code>.</p> <p>Defined in <code>sys/kern/sysv_shm.c:92-108</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#configuration-options","title":"Configuration Options","text":""},{"location":"sys/kern/ipc/sysv-shm/#shm_use_phys","title":"shm_use_phys","text":"<pre><code>static int shm_use_phys = 1;\n</code></pre> <p>When enabled, uses <code>phys_pager_alloc()</code> instead of <code>swap_pager_alloc()</code>. Physical backing provides better performance for large segments by allowing pmap optimizations. Pages are effectively wired.</p> <p>When set to 2 or higher, pages are pre-allocated at segment creation time, improving database warm-up times by enabling concurrent page faults on already-existing pages.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shm_allow_removed","title":"shm_allow_removed","text":"<pre><code>static int shm_allow_removed = 1;\n</code></pre> <p>When enabled, allows <code>shmat()</code> to attach to segments marked for removal (<code>IPC_RMID</code>) as long as they still have references. Used by Chrome and other applications to ensure cleanup after unexpected termination.</p>"},{"location":"sys/kern/ipc/sysv-shm/#synchronization","title":"Synchronization","text":"<p>A single LWKT token protects all shared memory operations:</p> <pre><code>static struct lwkt_token shm_token = LWKT_TOKEN_INITIALIZER(shm_token);\n</code></pre> <p>The <code>reserved</code> field in <code>shmmap_state</code> prevents races when the token is released during blocking operations in <code>shmat()</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#initialization","title":"Initialization","text":"<p><code>shminit()</code> runs at <code>SI_SUB_SYSV_SHM</code>:</p> <ol> <li>If <code>shmall == 0</code>, set to 2/3 of <code>v_page_count</code></li> <li>Compute <code>shmmax = shmall * PAGE_SIZE</code></li> <li>Allocate <code>shmsegs[]</code> array (SHMMNI entries)</li> <li>Mark all slots as <code>SHMSEG_FREE</code></li> </ol> <p>See <code>sys/kern/sysv_shm.c:704-727</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#system-calls","title":"System Calls","text":""},{"location":"sys/kern/ipc/sysv-shm/#shmget-create-or-access-segment","title":"shmget - Create or Access Segment","text":"<pre><code>int sys_shmget(struct sysmsg *sysmsg, const struct shmget_args *uap)\n</code></pre> <p>Arguments: <code>key</code>, <code>size</code>, <code>shmflg</code></p> <p>Operation: 1. If <code>key != IPC_PRIVATE</code>, search for existing segment 2. If found, call <code>shmget_existing()</code>:    - If <code>SHMSEG_REMOVED</code> set, sleep and retry    - Check <code>IPC_CREAT|IPC_EXCL</code> conflict    - Validate permissions and size 3. If not found and <code>IPC_CREAT</code>, call <code>shmget_allocate_segment()</code>:    - Validate size against limits    - Check system-wide page commitment    - Find free slot (may call <code>shmrealloc()</code> to expand)    - Mark slot <code>ALLOCATED | REMOVED</code> during allocation    - Allocate <code>shm_handle</code> and backing VM object    - Choose <code>phys_pager</code> or <code>swap_pager</code> based on <code>shm_use_phys</code>    - Optionally pre-fault pages if <code>shm_use_phys &gt; 1</code>    - Wake waiters if <code>SHMSEG_WANTED</code> was set</p> <p>See <code>sys/kern/sysv_shm.c:610-644</code>, <code>464-605</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shmat-attach-segment","title":"shmat - Attach Segment","text":"<pre><code>int sys_shmat(struct sysmsg *sysmsg, const struct shmat_args *uap)\n</code></pre> <p>Arguments: <code>shmid</code>, <code>shmaddr</code>, <code>shmflg</code></p> <p>Operation: 1. Allocate per-process <code>shmmap_state[]</code> array if needed 2. Find segment by shmid (respects <code>shm_allow_removed</code>) 3. Check permissions (<code>IPC_R</code> or <code>IPC_R|IPC_W</code>) 4. Find free slot in per-process array, mark <code>reserved = 1</code> 5. Calculate attach address:    - If <code>shmaddr</code> given with <code>SHM_RND</code>, round down to <code>SHMLBA</code>    - If <code>shmaddr</code> given without <code>SHM_RND</code>, must be <code>SHMLBA</code>-aligned    - Otherwise, hint near end of data segment 6. For large segments aligned to <code>SEG_SIZE</code>, use <code>SEG_SIZE</code> alignment 7. Call <code>vm_map_find()</code> to map the VM object 8. Set <code>VM_INHERIT_SHARE</code> so mappings persist across fork 9. Update <code>shmmap_state</code>, increment <code>shm_nattch</code></p> <p>See <code>sys/kern/sysv_shm.c:260-395</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shmdt-detach-segment","title":"shmdt - Detach Segment","text":"<pre><code>int sys_shmdt(struct sysmsg *sysmsg, const struct shmdt_args *uap)\n</code></pre> <p>Arguments: <code>shmaddr</code></p> <p>Operation: 1. Find mapping in per-process array by address 2. Call <code>shm_delete_mapping()</code>:    - <code>vm_map_remove()</code> the mapping    - Clear the <code>shmmap_state</code> entry    - Decrement <code>shm_nattch</code>    - If <code>shm_nattch == 0</code> and <code>SHMSEG_REMOVED</code>, deallocate segment</p> <p>See <code>sys/kern/sysv_shm.c:222-255</code>, <code>196-217</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shmctl-control-operations","title":"shmctl - Control Operations","text":"<pre><code>int sys_shmctl(struct sysmsg *sysmsg, const struct shmctl_args *uap)\n</code></pre> <p>Commands:</p> Command Description <code>IPC_STAT</code> Copy <code>shmid_ds</code> to user buffer <code>IPC_SET</code> Update uid, gid, mode <code>IPC_RMID</code> Mark segment for removal <p>IPC_RMID Operation: 1. Set <code>shm_perm.key = IPC_PRIVATE</code> (prevents new lookups) 2. Set <code>SHMSEG_REMOVED</code> flag 3. If <code>shm_nattch == 0</code>, deallocate immediately 4. Otherwise, wait for all detaches</p> <p>See <code>sys/kern/sysv_shm.c:400-462</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#segment-deallocation","title":"Segment Deallocation","text":"<pre><code>static void shm_deallocate_segment(struct shmid_ds *shmseg)\n</code></pre> <ol> <li>Get <code>shm_handle</code> from <code>shm_internal</code></li> <li>Release VM object reference (<code>vm_object_deallocate()</code>)</li> <li>Free <code>shm_handle</code></li> <li>Decrease <code>shm_committed</code> by segment pages</li> <li>Decrement <code>shm_nused</code></li> <li>Mark slot as <code>SHMSEG_FREE</code></li> </ol> <p>See <code>sys/kern/sysv_shm.c:180-194</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#fork-and-exit-handling","title":"Fork and Exit Handling","text":""},{"location":"sys/kern/ipc/sysv-shm/#shmfork","title":"shmfork","text":"<p>Called when a process forks:</p> <pre><code>void shmfork(struct proc *p1, struct proc *p2)\n</code></pre> <ol> <li>Allocate new <code>shmmap_state[]</code> for child</li> <li>Copy parent's mappings</li> <li>Increment <code>shm_nattch</code> for each attached segment</li> </ol> <p>The <code>VM_INHERIT_SHARE</code> flag ensures the actual mappings are shared.</p> <p>See <code>sys/kern/sysv_shm.c:646-663</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#shmexit","title":"shmexit","text":"<p>Called when a process exits or execs:</p> <pre><code>void shmexit(struct vmspace *vm)\n</code></pre> <ol> <li>Detach all attached segments via <code>shm_delete_mapping()</code></li> <li>Free the <code>shmmap_state[]</code> array</li> </ol> <p>See <code>sys/kern/sysv_shm.c:665-681</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#dynamic-array-expansion","title":"Dynamic Array Expansion","text":"<pre><code>static void shmrealloc(void)\n</code></pre> <p>If <code>shmalloced &lt; shmmni</code>, reallocates <code>shmsegs[]</code> to full size. Called during allocation when no free slots exist.</p> <p>See <code>sys/kern/sysv_shm.c:683-702</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#vm-object-backing","title":"VM Object Backing","text":"<p>Two pager types are supported:</p>"},{"location":"sys/kern/ipc/sysv-shm/#physical-pager-shm_use_phys-1","title":"Physical Pager (shm_use_phys = 1)","text":"<pre><code>shm_handle-&gt;shm_object = phys_pager_alloc(NULL, size, VM_PROT_DEFAULT, 0);\n</code></pre> <ul> <li>Pages are wired in physical memory</li> <li>Better pmap optimization for large segments</li> <li>No swap backing</li> </ul>"},{"location":"sys/kern/ipc/sysv-shm/#swap-pager-shm_use_phys-0","title":"Swap Pager (shm_use_phys = 0)","text":"<pre><code>shm_handle-&gt;shm_object = swap_pager_alloc(NULL, size, VM_PROT_DEFAULT, 0);\n</code></pre> <ul> <li>Pages can be swapped out</li> <li>More flexible memory usage</li> </ul> <p>Both set <code>OBJ_NOSPLIT</code> to prevent the object from being split.</p>"},{"location":"sys/kern/ipc/sysv-shm/#jail-support","title":"Jail Support","text":"<p>All system calls check jail capabilities:</p> <pre><code>if (pr &amp;&amp; !PRISON_CAP_ISSET(pr-&gt;pr_caps, PRISON_CAP_SYS_SYSVIPC))\n    return (ENOSYS);\n</code></pre>"},{"location":"sys/kern/ipc/sysv-shm/#sysctl-interface","title":"Sysctl Interface","text":"Sysctl Type Description <code>kern.ipc.shmmax</code> RW Max segment size <code>kern.ipc.shmmin</code> RW Min segment size <code>kern.ipc.shmmni</code> RD Max identifiers <code>kern.ipc.shmseg</code> RW Max segments per process <code>kern.ipc.shmall</code> RW Max total pages <code>kern.ipc.shm_use_phys</code> RW Use physical pager <code>kern.ipc.shm_allow_removed</code> RW Allow attach to removed <p>Boot tunables: <code>kern.ipc.shmmin</code>, <code>kern.ipc.shmmni</code>, <code>kern.ipc.shmseg</code>, <code>kern.ipc.shmmaxpgs</code>, <code>kern.ipc.shm_use_phys</code>.</p> <p>See <code>sys/kern/sysv_shm.c:126-146</code>.</p>"},{"location":"sys/kern/ipc/sysv-shm/#error-handling","title":"Error Handling","text":"Error Condition <code>ENOSYS</code> Jail lacks SYSVIPC capability <code>EINVAL</code> Invalid shmid, size, or address <code>EEXIST</code> <code>IPC_CREAT|IPC_EXCL</code> and segment exists <code>ENOENT</code> Segment not found, no <code>IPC_CREAT</code> <code>ENOSPC</code> No free slots <code>ENOMEM</code> Page commitment exceeded or mapping failed <code>EMFILE</code> Per-process segment limit reached <code>EACCES</code> Permission denied"},{"location":"sys/kern/ipc/unix-sockets/","title":"Unix Domain Sockets","text":"<p>Unix domain sockets provide local inter-process communication using the familiar socket API. DragonFly BSD implements Unix domain sockets with LWKT token-based synchronization and reference counting for safe concurrent access across multiple processors.</p>"},{"location":"sys/kern/ipc/unix-sockets/#overview","title":"Overview","text":"<p>Unix domain sockets (also called local sockets) enable efficient communication between processes on the same machine without network protocol overhead. Key features include:</p> <ul> <li>Filesystem binding - Sockets can be bound to pathnames in the filesystem</li> <li>File descriptor passing - Transfer open file descriptors between processes</li> <li>Credential passing - Automatic sender credential transmission</li> <li>Three socket types - SOCK_STREAM, SOCK_DGRAM, and SOCK_SEQPACKET</li> <li>Direct mbuf transfer - Zero-copy data delivery to peer's receive buffer</li> <li>Garbage collection - Automatic cleanup of in-flight file descriptors</li> </ul>"},{"location":"sys/kern/ipc/unix-sockets/#source-files","title":"Source Files","text":"File Description <code>sys/kern/uipc_usrreq.c</code> Unix domain socket protocol implementation <code>sys/sys/unpcb.h</code> Unix domain protocol control block definitions <code>sys/sys/un.h</code> Unix domain socket address structure"},{"location":"sys/kern/ipc/unix-sockets/#data-structures","title":"Data Structures","text":""},{"location":"sys/kern/ipc/unix-sockets/#struct-unpcb","title":"struct unpcb","text":"<p>The Unix domain protocol control block (<code>sys/sys/unpcb.h:68</code>):</p> <pre><code>struct unpcb {\n    struct socket   *unp_socket;    /* pointer back to socket */\n    struct unpcb    *unp_conn;      /* control block of connected socket */\n    int             unp_flags;      /* flags */\n    int             unp_refcnt;     /* reference count */\n    struct unp_head unp_refs;       /* referencing socket linked list (DGRAM) */\n    LIST_ENTRY(unpcb) unp_reflink;  /* link in unp_refs list */\n    struct sockaddr_un *unp_addr;   /* bound address of socket */\n    struct xucred   unp_peercred;   /* peer credentials, if applicable */\n    int             unp_msgcount;   /* # of cmsgs this unp are in */\n    int             unp_gcflags;    /* flags reserved for unp GC to use */\n    struct file     *unp_fp;        /* corresponding fp if unp is in cmsg */\n    long            unp_unused01;\n    struct vnode    *unp_vnode;     /* if associated with file */\n    struct vnode    *unp_rvnode;    /* root vp for creating process (jail) */\n    TAILQ_ENTRY(unpcb) unp_link;    /* glue on list of all PCBs */\n    unp_gen_t       unp_gencnt;     /* generation count of this instance */\n};\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#struct-sockaddr_un","title":"struct sockaddr_un","text":"<p>The Unix domain socket address (<code>sys/sys/un.h</code>):</p> <pre><code>struct sockaddr_un {\n    uint8_t  sun_len;           /* total sockaddr length */\n    sa_family_t sun_family;     /* AF_LOCAL / AF_UNIX */\n    char     sun_path[104];     /* path name (null-terminated) */\n};\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#global-socket-lists","title":"Global Socket Lists","text":"<p>Unix domain sockets are organized by type (<code>sys/kern/uipc_usrreq.c:124</code>):</p> <pre><code>struct unp_global_head {\n    struct unpcb_qhead  list;   /* TAILQ of unpcbs */\n    int                 count;  /* number in list */\n};\n\nstatic struct unp_global_head unp_stream_head;   /* SOCK_STREAM sockets */\nstatic struct unp_global_head unp_dgram_head;    /* SOCK_DGRAM sockets */\nstatic struct unp_global_head unp_seqpkt_head;   /* SOCK_SEQPACKET sockets */\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#unp-flags","title":"UNP Flags","text":"<p>Protocol control block flags (<code>sys/sys/unpcb.h:102</code>):</p> Flag Value Description <code>UNP_HAVEPC</code> 0x001 <code>unp_peercred</code> contains connected peer credentials <code>UNP_HAVEPCCACHED</code> 0x002 <code>unp_peercred</code> cached from listen() <code>UNP_DETACHED</code> 0x004 Socket detached from global list <code>UNP_CONNECTING</code> 0x008 Connection in progress <code>UNP_DROPPED</code> 0x010 Socket has been dropped <code>UNP_MARKER</code> 0x020 Marker for list traversal"},{"location":"sys/kern/ipc/unix-sockets/#synchronization","title":"Synchronization","text":""},{"location":"sys/kern/ipc/unix-sockets/#token-hierarchy","title":"Token Hierarchy","text":"<p>Unix domain sockets use a multi-level token hierarchy (<code>sys/kern/uipc_usrreq.c:185</code>):</p> <pre><code>unp_token (global)\n    \u2514\u2500\u2500 Per-unpcb pool token (lwkt_token_pool_lookup(unp))\n            \u2514\u2500\u2500 unp_rights_token (for file descriptor passing)\n</code></pre> <p>Rules:</p> <ol> <li>Any change to <code>unp_conn</code> requires both <code>unp_token</code> and the per-unpcb pool token</li> <li>Access to <code>so_pcb</code> to obtain unp requires the pool token</li> <li>File descriptor tracking (<code>unp_rights</code>) requires <code>unp_rights_token</code></li> </ol>"},{"location":"sys/kern/ipc/unix-sockets/#reference-counting","title":"Reference Counting","text":"<pre><code>static __inline void\nunp_reference(struct unpcb *unp)\n{\n    KKASSERT(unp-&gt;unp_refcnt &gt; 0);\n    atomic_add_int(&amp;unp-&gt;unp_refcnt, 1);\n}\n\nstatic __inline void\nunp_free(struct unpcb *unp)\n{\n    KKASSERT(unp-&gt;unp_refcnt &gt; 0);\n    if (atomic_fetchadd_int(&amp;unp-&gt;unp_refcnt, -1) == 1)\n        unp_detach(unp);\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#socket-token-acquisition","title":"Socket Token Acquisition","text":"<p><code>unp_getsocktoken()</code> safely acquires the pool token (<code>sys/kern/uipc_usrreq.c:215</code>):</p> <pre><code>static __inline struct unpcb *\nunp_getsocktoken(struct socket *so)\n{\n    struct unpcb *unp;\n\n    /* The unp pointer is invalid until verified by re-checking\n     * so_pcb AFTER obtaining the token. */\n    while ((unp = so-&gt;so_pcb) != NULL) {\n        lwkt_getpooltoken(unp);\n        if (unp == so-&gt;so_pcb)\n            break;\n        lwkt_relpooltoken(unp);\n    }\n    return unp;\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#protocol-operations","title":"Protocol Operations","text":""},{"location":"sys/kern/ipc/unix-sockets/#uipc_usrreqs","title":"uipc_usrreqs","text":"<p>The protocol switch operations (<code>sys/kern/uipc_usrreq.c:905</code>):</p> <pre><code>struct pr_usrreqs uipc_usrreqs = {\n    .pru_abort = uipc_abort,\n    .pru_accept = uipc_accept,\n    .pru_attach = uipc_attach,\n    .pru_bind = uipc_bind,\n    .pru_connect = uipc_connect,\n    .pru_connect2 = uipc_connect2,\n    .pru_control = pr_generic_notsupp,\n    .pru_detach = uipc_detach,\n    .pru_disconnect = uipc_disconnect,\n    .pru_listen = uipc_listen,\n    .pru_peeraddr = uipc_peeraddr,\n    .pru_rcvd = uipc_rcvd,\n    .pru_rcvoob = pr_generic_notsupp,\n    .pru_send = uipc_send,\n    .pru_sense = uipc_sense,\n    .pru_shutdown = uipc_shutdown,\n    .pru_sockaddr = uipc_sockaddr,\n    .pru_sosend = sosend,\n    .pru_soreceive = soreceive\n};\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#socket-lifecycle","title":"Socket Lifecycle","text":""},{"location":"sys/kern/ipc/unix-sockets/#attach","title":"Attach","text":"<p><code>unp_attach()</code> creates the protocol control block (<code>sys/kern/uipc_usrreq.c:1025</code>):</p> <pre><code>unp_attach(so, ai)\n    \u2502\n    \u251c\u2500\u25ba Reserve buffer space based on socket type:\n    \u2502       \u251c\u2500\u25ba SOCK_STREAM: unpst_sendspace/recvspace (65536/65536)\n    \u2502       \u251c\u2500\u25ba SOCK_DGRAM: unpdg_sendspace/recvspace (65536/65536)\n    \u2502       \u2514\u2500\u25ba SOCK_SEQPACKET: unpsp_sendspace/recvspace (65536/65536)\n    \u2502\n    \u251c\u2500\u25ba For SOCK_STREAM: Set SSB_STOPSUPP on both buffers\n    \u2502\n    \u251c\u2500\u25ba Allocate unpcb (kmalloc M_UNPCB)\n    \u2502\n    \u251c\u2500\u25ba Initialize:\n    \u2502       \u251c\u2500\u25ba unp_refcnt = 1\n    \u2502       \u251c\u2500\u25ba unp_gencnt = ++unp_gencnt\n    \u2502       \u251c\u2500\u25ba LIST_INIT(&amp;unp-&gt;unp_refs)\n    \u2502       \u251c\u2500\u25ba unp_socket = so\n    \u2502       \u251c\u2500\u25ba unp_rvnode = ai-&gt;fd_rdir (jail root)\n    \u2502       \u2514\u2500\u25ba so-&gt;so_pcb = unp\n    \u2502\n    \u2514\u2500\u25ba Add to global type-specific list\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#bind","title":"Bind","text":"<p><code>unp_bind()</code> binds a socket to a filesystem path (<code>sys/kern/uipc_usrreq.c:1124</code>):</p> <pre><code>unp_bind(unp, nam, td)\n    \u2502\n    \u251c\u2500\u25ba Check unp_vnode == NULL (not already bound)\n    \u2502\n    \u251c\u2500\u25ba Extract and null-terminate path from sockaddr_un\n    \u2502\n    \u251c\u2500\u25ba nlookup_init() with NLC_LOCKVP | NLC_CREATE | NLC_REFDVP\n    \u2502\n    \u251c\u2500\u25ba nlookup() \u2192 Find parent directory\n    \u2502\n    \u251c\u2500\u25ba Check path doesn't exist (EADDRINUSE)\n    \u2502\n    \u251c\u2500\u25ba VOP_NCREATE() \u2192 Create VSOCK vnode\n    \u2502       \u251c\u2500\u25ba vattr.va_type = VSOCK\n    \u2502       \u2514\u2500\u25ba vattr.va_mode = ACCESSPERMS &amp; ~cmask\n    \u2502\n    \u2514\u2500\u25ba Link vnode to socket:\n            \u251c\u2500\u25ba vp-&gt;v_socket = unp-&gt;unp_socket\n            \u251c\u2500\u25ba unp-&gt;unp_vnode = vp\n            \u2514\u2500\u25ba unp-&gt;unp_addr = dup_sockaddr(nam)\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#listen","title":"Listen","text":"<p><code>unp_listen()</code> prepares for incoming connections (<code>sys/kern/uipc_usrreq.c:2322</code>):</p> <pre><code>static int\nunp_listen(struct unpcb *unp, struct thread *td)\n{\n    struct proc *p = td-&gt;td_proc;\n\n    KKASSERT(p);\n    cru2x(p-&gt;p_ucred, &amp;unp-&gt;unp_peercred);  /* Cache server credentials */\n    unp_setflags(unp, UNP_HAVEPCCACHED);\n    return (0);\n}\n</code></pre> <p>The cached credentials are later copied to connecting clients.</p>"},{"location":"sys/kern/ipc/unix-sockets/#connect","title":"Connect","text":"<p><code>unp_connect()</code> establishes a connection (<code>sys/kern/uipc_usrreq.c:1177</code>):</p> <pre><code>unp_connect(so, nam, td)\n    \u2502\n    \u251c\u2500\u25ba Get socket token, check attached\n    \u2502\n    \u251c\u2500\u25ba Check not already connecting or connected\n    \u2502\n    \u251c\u2500\u25ba Set UNP_CONNECTING flag\n    \u2502\n    \u251c\u2500\u25ba unp_find_lockref() \u2192 Find and lock target unpcb\n    \u2502       \u251c\u2500\u25ba nlookup() the path\n    \u2502       \u251c\u2500\u25ba Check vnode type == VSOCK\n    \u2502       \u251c\u2500\u25ba VOP_EACCESS() for VWRITE permission\n    \u2502       \u2514\u2500\u25ba Lock and reference target unpcb\n    \u2502\n    \u251c\u2500\u25ba For connection-oriented (STREAM/SEQPACKET):\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u25ba Check target has SO_ACCEPTCONN and UNP_HAVEPCCACHED\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u25ba sonewconn_faddr() \u2192 Create new socket for connection\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u25ba Copy server's bound address to new socket\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u25ba Exchange credentials:\n    \u2502       \u2502       \u251c\u2500\u25ba Client creds \u2192 new socket's unp_peercred\n    \u2502       \u2502       \u2514\u2500\u25ba Server's cached creds \u2192 client's unp_peercred\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u25ba unp_connect_pair(unp, unp3)\n    \u2502\n    \u2514\u2500\u25ba For connectionless (DGRAM):\n            \u2514\u2500\u25ba unp_connect_pair(unp, unp2)\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#connect-pair","title":"Connect Pair","text":"<p><code>unp_connect_pair()</code> links two sockets (<code>sys/kern/uipc_usrreq.c:2480</code>):</p> <pre><code>static int\nunp_connect_pair(struct unpcb *unp, struct unpcb *unp2)\n{\n    unp-&gt;unp_conn = unp2;\n\n    switch (so-&gt;so_type) {\n    case SOCK_DGRAM:\n        /* DGRAM: one-way reference, unp2 keeps list of referrers */\n        LIST_INSERT_HEAD(&amp;unp2-&gt;unp_refs, unp, unp_reflink);\n        soisconnected(so);\n        break;\n\n    case SOCK_STREAM:\n    case SOCK_SEQPACKET:\n        /* STREAM/SEQPACKET: bidirectional connection */\n        unp2-&gt;unp_conn = unp;\n        soisconnected(so);\n        soisconnected(so2);\n        break;\n    }\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#data-transfer","title":"Data Transfer","text":""},{"location":"sys/kern/ipc/unix-sockets/#send","title":"Send","text":"<p><code>uipc_send()</code> transmits data (<code>sys/kern/uipc_usrreq.c:603</code>):</p>"},{"location":"sys/kern/ipc/unix-sockets/#datagram-send","title":"Datagram Send","text":"<pre><code>uipc_send() [SOCK_DGRAM]\n    \u2502\n    \u251c\u2500\u25ba If address provided:\n    \u2502       \u251c\u2500\u25ba Check not already connected\n    \u2502       \u2514\u2500\u25ba unp_find_lockref() \u2192 Find destination\n    \u2502\n    \u251c\u2500\u25ba If SO_PASSCRED on receiver:\n    \u2502       \u2514\u2500\u25ba Add SCM_CREDS control message\n    \u2502\n    \u251c\u2500\u25ba Get receiver's ssb_token\n    \u2502\n    \u251c\u2500\u25ba ssb_appendaddr() \u2192 Append with source address\n    \u2502\n    \u2514\u2500\u25ba sorwakeup(so2)\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#streamseqpacket-send","title":"Stream/Seqpacket Send","text":"<pre><code>uipc_send() [SOCK_STREAM/SOCK_SEQPACKET]\n    \u2502\n    \u251c\u2500\u25ba Connect if not connected and address provided\n    \u2502\n    \u251c\u2500\u25ba Check SS_CANTSENDMORE\n    \u2502\n    \u251c\u2500\u25ba Get peer's so_rcv.ssb_token\n    \u2502\n    \u251c\u2500\u25ba Transfer data directly to peer's receive buffer:\n    \u2502       \u251c\u2500\u25ba With control: ssb_appendcontrol()\n    \u2502       \u251c\u2500\u25ba SEQPACKET: sbappendrecord() (preserve boundaries)\n    \u2502       \u2514\u2500\u25ba STREAM: sbappend() (byte stream)\n    \u2502\n    \u251c\u2500\u25ba Apply backpressure if needed:\n    \u2502       \u2502   if (so2-&gt;so_rcv.ssb_cc &gt;= so-&gt;so_snd.ssb_hiwat ||\n    \u2502       \u2502       so2-&gt;so_rcv.ssb_mbcnt &gt;= so-&gt;so_snd.ssb_mbmax)\n    \u2502       \u2514\u2500\u25ba atomic_set_int(&amp;so-&gt;so_snd.ssb_flags, SSB_STOP)\n    \u2502\n    \u2514\u2500\u25ba sorwakeup(so2)\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#receive-notification","title":"Receive Notification","text":"<p><code>uipc_rcvd()</code> handles flow control after receive (<code>sys/kern/uipc_usrreq.c:539</code>):</p> <pre><code>case SOCK_STREAM:\ncase SOCK_SEQPACKET:\n    if (unp-&gt;unp_conn == NULL)\n        break;\n    unp2 = unp-&gt;unp_conn;\n    so2 = unp2-&gt;unp_socket;\n\n    unp_reference(unp2);\n    lwkt_gettoken(&amp;so2-&gt;so_rcv.ssb_token);\n\n    /* Clear backpressure if buffer space available */\n    if (so-&gt;so_rcv.ssb_cc &lt; so2-&gt;so_snd.ssb_hiwat &amp;&amp;\n        so-&gt;so_rcv.ssb_mbcnt &lt; so2-&gt;so_snd.ssb_mbmax) {\n        atomic_clear_int(&amp;so2-&gt;so_snd.ssb_flags, SSB_STOP);\n        sowwakeup(so2);  /* Wake sender */\n    }\n\n    lwkt_reltoken(&amp;so2-&gt;so_rcv.ssb_token);\n    unp_free(unp2);\n    break;\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#file-descriptor-passing","title":"File Descriptor Passing","text":"<p>Unix domain sockets can transfer file descriptors between processes using ancillary data (SCM_RIGHTS).</p>"},{"location":"sys/kern/ipc/unix-sockets/#internalize-send-side","title":"Internalize (Send Side)","text":"<p><code>unp_internalize()</code> converts user FDs to kernel file pointers (<code>sys/kern/uipc_usrreq.c:1701</code>):</p> <pre><code>unp_internalize(control, td)\n    \u2502\n    \u251c\u2500\u25ba Validate control message (SCM_RIGHTS or SCM_CREDS)\n    \u2502\n    \u251c\u2500\u25ba For SCM_CREDS: Fill in sender credentials and return\n    \u2502\n    \u251c\u2500\u25ba Calculate number of FDs: (cmsg_len - CMSG_LEN(0)) / sizeof(int)\n    \u2502\n    \u251c\u2500\u25ba Expand mbuf if needed for larger struct file pointers\n    \u2502\n    \u251c\u2500\u25ba Lock unp_rights_token and fd_spin\n    \u2502\n    \u251c\u2500\u25ba Validate all FDs:\n    \u2502       \u251c\u2500\u25ba Check fd &lt; fd_nfiles\n    \u2502       \u251c\u2500\u25ba Check fd_files[fd].fp != NULL\n    \u2502       \u2514\u2500\u25ba Reject kqueues (EOPNOTSUPP)\n    \u2502\n    \u251c\u2500\u25ba Convert FDs to file pointers (reverse order):\n    \u2502       for (i = oldfds-1; i &gt;= 0; i--) {\n    \u2502           fp = fdescp-&gt;fd_files[fdp[i]].fp;\n    \u2502           rp[i] = fp;\n    \u2502           fhold(fp);\n    \u2502           unp_add_right(fp);  /* Track in-flight FDs */\n    \u2502       }\n    \u2502\n    \u2514\u2500\u25ba Update cmsg_len for pointer size\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#externalize-receive-side","title":"Externalize (Receive Side)","text":"<p><code>unp_externalize()</code> converts kernel file pointers to user FDs (<code>sys/kern/uipc_usrreq.c:1542</code>):</p> <pre><code>unp_externalize(rights, flags)\n    \u2502\n    \u251c\u2500\u25ba Calculate number of file pointers\n    \u2502\n    \u251c\u2500\u25ba Check fdavail() for enough FD slots\n    \u2502\n    \u251c\u2500\u25ba Hold revoke_token (shared) to catch revoked files\n    \u2502\n    \u251c\u2500\u25ba For each file pointer:\n    \u2502       \u251c\u2500\u25ba fdalloc() \u2192 Allocate new FD\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u25ba unp_fp_externalize():\n    \u2502               \u251c\u2500\u25ba Handle FREVOKED files specially\n    \u2502               \u251c\u2500\u25ba Apply MSG_CMSG_CLOEXEC \u2192 UF_EXCLOSE\n    \u2502               \u251c\u2500\u25ba Apply MSG_CMSG_CLOFORK \u2192 UF_FOCLOSE\n    \u2502               \u251c\u2500\u25ba fsetfd() \u2192 Install in process FD table\n    \u2502               \u251c\u2500\u25ba unp_del_right(fp) \u2192 Remove from in-flight count\n    \u2502               \u2514\u2500\u25ba fdrop(fp)\n    \u2502\n    \u2514\u2500\u25ba Adjust cmsg_len for int size\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#in-flight-tracking","title":"In-Flight Tracking","text":"<pre><code>static __inline void\nunp_add_right(struct file *fp)\n{\n    struct unpcb *unp;\n\n    ASSERT_LWKT_TOKEN_HELD(&amp;unp_rights_token);\n\n    unp = unp_fp2unpcb(fp);\n    if (unp != NULL) {\n        unp-&gt;unp_fp = fp;\n        unp-&gt;unp_msgcount++;\n    }\n    fp-&gt;f_msgcount++;\n    unp_rights++;  /* Global in-flight counter */\n}\n\nstatic __inline void\nunp_del_right(struct file *fp)\n{\n    struct unpcb *unp;\n\n    unp = unp_fp2unpcb(fp);\n    if (unp != NULL) {\n        unp-&gt;unp_msgcount--;\n        if (unp-&gt;unp_msgcount == 0)\n            unp-&gt;unp_fp = NULL;\n    }\n    fp-&gt;f_msgcount--;\n    unp_rights--;\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#credential-passing","title":"Credential Passing","text":""},{"location":"sys/kern/ipc/unix-sockets/#scm_creds","title":"SCM_CREDS","text":"<p>Sender credentials are passed via <code>unp_internalize()</code> when control message type is SCM_CREDS:</p> <pre><code>if (cm-&gt;cmsg_type == SCM_CREDS) {\n    cmcred = (struct cmsgcred *)CMSG_DATA(cm);\n    cmcred-&gt;cmcred_pid = p-&gt;p_pid;\n    cmcred-&gt;cmcred_uid = p-&gt;p_ucred-&gt;cr_ruid;\n    cmcred-&gt;cmcred_gid = p-&gt;p_ucred-&gt;cr_rgid;\n    cmcred-&gt;cmcred_euid = p-&gt;p_ucred-&gt;cr_uid;\n    cmcred-&gt;cmcred_ngroups = MIN(p-&gt;p_ucred-&gt;cr_ngroups, CMGROUP_MAX);\n    for (i = 0; i &lt; cmcred-&gt;cmcred_ngroups; i++)\n        cmcred-&gt;cmcred_groups[i] = p-&gt;p_ucred-&gt;cr_groups[i];\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#so_passcred","title":"SO_PASSCRED","text":"<p>When the receiving socket has <code>SO_PASSCRED</code> set, credentials are automatically included even if the sender didn't provide them:</p> <pre><code>if (so2-&gt;so_options &amp; SO_PASSCRED) {\n    /* Check if SCM_CREDS already present */\n    mp = &amp;control;\n    while ((ncon = *mp) != NULL) {\n        cm = mtod(ncon, struct cmsghdr *);\n        if (cm-&gt;cmsg_type == SCM_CREDS &amp;&amp; cm-&gt;cmsg_level == SOL_SOCKET)\n            break;\n        mp = &amp;ncon-&gt;m_next;\n    }\n    if (ncon == NULL) {\n        /* Create and internalize credentials */\n        ncon = sbcreatecontrol(&amp;cred, sizeof(cred), SCM_CREDS, SOL_SOCKET);\n        unp_internalize(ncon, msg-&gt;send.nm_td);\n        *mp = ncon;\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#local_peercred","title":"LOCAL_PEERCRED","text":"<p>Connected stream/seqpacket sockets can retrieve peer credentials:</p> <pre><code>case LOCAL_PEERCRED:\n    if (unp-&gt;unp_flags &amp; UNP_HAVEPC)\n        soopt_from_kbuf(sopt, &amp;unp-&gt;unp_peercred, sizeof(unp-&gt;unp_peercred));\n    else {\n        if (so-&gt;so_type == SOCK_STREAM || so-&gt;so_type == SOCK_SEQPACKET)\n            error = ENOTCONN;\n        else\n            error = EINVAL;\n    }\n    break;\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#garbage-collection","title":"Garbage Collection","text":""},{"location":"sys/kern/ipc/unix-sockets/#problem-statement","title":"Problem Statement","text":"<p>File descriptors passed over Unix domain sockets can form unreachable cycles:</p> <ol> <li>Socket A holds a reference to socket B in its receive buffer</li> <li>Socket B holds a reference to socket A in its receive buffer</li> <li>Both sockets are closed by their owning processes</li> <li>The file descriptors in flight keep each other alive indefinitely</li> </ol>"},{"location":"sys/kern/ipc/unix-sockets/#gc-algorithm","title":"GC Algorithm","text":"<p>The garbage collector runs when <code>unp_rights &gt; 0</code> and a socket is detached (<code>sys/kern/uipc_usrreq.c:2144</code>):</p> <pre><code>unp_gc()\n    \u2502\n    \u251c\u2500\u25ba Clear all gcflags from previous runs\n    \u2502\n    \u251c\u2500\u25ba Mark phase (iterate until no new marks):\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u25ba For each unpcb:\n    \u2502       \u2502       \u2502\n    \u2502       \u2502       \u251c\u2500\u25ba If already has UNPGC_REF: skip\n    \u2502       \u2502       \u2502\n    \u2502       \u2502       \u251c\u2500\u25ba If potentially in cycle (all refs from messages):\n    \u2502       \u2502       \u2502       \u2514\u2500\u25ba Mark UNPGC_DEAD, increment unp_unreachable\n    \u2502       \u2502       \u2502\n    \u2502       \u2502       \u2514\u2500\u25ba Otherwise: scan receive buffer\n    \u2502       \u2502               \u2514\u2500\u25ba Mark referenced sockets with UNPGC_REF\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u25ba Repeat until unp_marked == 0\n    \u2502\n    \u2514\u2500\u25ba Sweep phase:\n            \u251c\u2500\u25ba Collect sockets marked UNPGC_DEAD\n            \u251c\u2500\u25ba sorflush() each to release rights\n            \u2514\u2500\u25ba fdrop() to release extra reference\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#gc-flags","title":"GC Flags","text":"Flag Value Description <code>UNPGC_REF</code> 0x1 Socket has external reference <code>UNPGC_DEAD</code> 0x2 Socket might be in unreachable cycle <code>UNPGC_SCANNED</code> 0x4 Socket's receive buffer has been scanned"},{"location":"sys/kern/ipc/unix-sockets/#deferred-discard","title":"Deferred Discard","text":"<p>To avoid deep recursion when discarding Unix domain sockets, disposal is deferred to a taskqueue:</p> <pre><code>static void\nunp_discard(struct file *fp, void *data __unused)\n{\n    unp_del_right(fp);\n    if (unp_fp2unpcb(fp) != NULL) {\n        /* This is a Unix socket - defer to avoid recursion */\n        struct unp_defdiscard *d;\n\n        d = kmalloc(sizeof(*d), M_UNPCB, M_WAITOK);\n        d-&gt;fp = fp;\n\n        spin_lock(&amp;unp_defdiscard_spin);\n        SLIST_INSERT_HEAD(&amp;unp_defdiscard_head, d, next);\n        spin_unlock(&amp;unp_defdiscard_spin);\n\n        taskqueue_enqueue(unp_taskqueue, &amp;unp_defdiscard_task);\n    } else {\n        fdrop(fp);\n    }\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#disconnect-and-cleanup","title":"Disconnect and Cleanup","text":""},{"location":"sys/kern/ipc/unix-sockets/#disconnect","title":"Disconnect","text":"<p><code>unp_disconnect()</code> breaks a connection (<code>sys/kern/uipc_usrreq.c:1353</code>):</p> <pre><code>static void\nunp_disconnect(struct unpcb *unp, int error)\n{\n    struct socket *so = unp-&gt;unp_socket;\n    struct unpcb *unp2;\n\n    if (error)\n        so-&gt;so_error = error;\n\n    /* Get peer's token */\n    while ((unp2 = unp-&gt;unp_conn) != NULL) {\n        lwkt_getpooltoken(unp2);\n        if (unp2 == unp-&gt;unp_conn)\n            break;\n        lwkt_relpooltoken(unp2);\n    }\n    if (unp2 == NULL)\n        return;\n\n    unp-&gt;unp_conn = NULL;\n\n    switch (so-&gt;so_type) {\n    case SOCK_DGRAM:\n        LIST_REMOVE(unp, unp_reflink);\n        soclrstate(so, SS_ISCONNECTED);\n        break;\n\n    case SOCK_STREAM:\n    case SOCK_SEQPACKET:\n        unp_reference(unp2);\n        unp2-&gt;unp_conn = NULL;\n        soisdisconnected(so);\n        soisdisconnected(unp2-&gt;unp_socket);\n        unp_free(unp2);\n        break;\n    }\n\n    lwkt_relpooltoken(unp2);\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#drop","title":"Drop","text":"<p><code>unp_drop()</code> removes a socket from the system (<code>sys/kern/uipc_usrreq.c:2521</code>):</p> <pre><code>unp_drop(unp, error)\n    \u2502\n    \u251c\u2500\u25ba Mark UNP_DETACHED\n    \u2502\n    \u251c\u2500\u25ba Remove from global list (stream/dgram/seqpkt)\n    \u2502\n    \u251c\u2500\u25ba unp_disconnect() \u2192 Break connection\n    \u2502\n    \u251c\u2500\u25ba For each socket referencing us (DGRAM):\n    \u2502       \u2514\u2500\u25ba unp_disconnect(unp2, ECONNRESET)\n    \u2502\n    \u251c\u2500\u25ba Mark UNP_DROPPED\n    \u2502\n    \u2514\u2500\u25ba unp_free() \u2192 Try to free\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#detach","title":"Detach","text":"<p><code>unp_detach()</code> performs final cleanup (<code>sys/kern/uipc_usrreq.c:1087</code>):</p> <pre><code>static void\nunp_detach(struct unpcb *unp)\n{\n    struct socket *so;\n\n    lwkt_gettoken(&amp;unp_token);\n    lwkt_getpooltoken(unp);\n\n    so = unp-&gt;unp_socket;\n\n    unp-&gt;unp_gencnt = ++unp_gencnt;\n    if (unp-&gt;unp_vnode) {\n        unp-&gt;unp_vnode-&gt;v_socket = NULL;\n        vrele(unp-&gt;unp_vnode);\n        unp-&gt;unp_vnode = NULL;\n    }\n    soisdisconnected(so);\n    so-&gt;so_pcb = NULL;\n    unp-&gt;unp_socket = NULL;\n\n    lwkt_relpooltoken(unp);\n    lwkt_reltoken(&amp;unp_token);\n\n    sofree(so);\n\n    if (unp-&gt;unp_addr)\n        kfree(unp-&gt;unp_addr, M_SONAME);\n    kfree(unp, M_UNPCB);\n\n    /* Trigger GC if file descriptors still in flight */\n    if (unp_rights)\n        taskqueue_enqueue(unp_taskqueue, &amp;unp_gc_task);\n}\n</code></pre>"},{"location":"sys/kern/ipc/unix-sockets/#sysctl-parameters","title":"Sysctl Parameters","text":"<p>Buffer size tunables (<code>sys/kern/uipc_usrreq.c:1005</code>):</p> Sysctl Default Description <code>net.local.stream.sendspace</code> 65536 Stream socket send buffer <code>net.local.stream.recvspace</code> 65536 Stream socket receive buffer <code>net.local.dgram.maxdgram</code> 65536 Maximum datagram size <code>net.local.dgram.recvspace</code> 65536 Datagram socket receive buffer <code>net.local.seqpacket.maxseqpacket</code> 65536 Maximum seqpacket size <code>net.local.seqpacket.recvspace</code> 65536 Seqpacket receive buffer <code>net.local.inflight</code> (read-only) File descriptors currently in flight <p>PCB list access for netstat:</p> Sysctl Description <code>net.local.stream.pcblist</code> List of active stream sockets <code>net.local.dgram.pcblist</code> List of active datagram sockets <code>net.local.seqpacket.pcblist</code> List of active seqpacket sockets"},{"location":"sys/kern/ipc/unix-sockets/#jail-support","title":"Jail Support","text":"<p>Unix domain sockets respect jail boundaries via <code>prison_unpcb()</code> (<code>sys/kern/uipc_usrreq.c:1416</code>):</p> <pre><code>static int\nprison_unpcb(struct thread *td, struct unpcb *unp)\n{\n    struct proc *p;\n\n    if (td == NULL)\n        return (0);\n    if ((p = td-&gt;td_proc) == NULL)\n        return (0);\n    if (!p-&gt;p_ucred-&gt;cr_prison)\n        return (0);\n    /* Check if unp's root matches jail root */\n    if (p-&gt;p_fd-&gt;fd_rdir == unp-&gt;unp_rvnode)\n        return (0);\n    return (1);  /* Reject - different jail */\n}\n</code></pre> <p>The <code>unp_rvnode</code> field stores the root vnode of the process that created the socket, allowing cross-jail access to be filtered.</p>"},{"location":"sys/kern/ipc/unix-sockets/#error-handling","title":"Error Handling","text":"<p>Common error codes returned by Unix domain socket operations:</p> Error Condition <code>EINVAL</code> Socket not attached, already bound, invalid address <code>EADDRINUSE</code> Bind path already exists <code>ECONNREFUSED</code> Target not listening, not attached, or connection rejected <code>EPROTOTYPE</code> Socket type mismatch <code>EISCONN</code> Already connected <code>ENOTCONN</code> Not connected (for connected operations) <code>ENOTSOCK</code> Path is not a socket <code>ENOBUFS</code> Buffer space exhausted <code>EPIPE</code> Cannot send (SS_CANTSENDMORE) <code>EOPNOTSUPP</code> Out-of-band data not supported <code>EMSGSIZE</code> Not enough FD slots for rights <code>E2BIG</code> Too many file descriptors in control message <code>EBADF</code> Invalid file descriptor in SCM_RIGHTS"},{"location":"sys/kern/ipc/unix-sockets/#see-also","title":"See Also","text":"<ul> <li>Socket Layer - Generic socket infrastructure</li> <li>Mbufs - Memory buffer management</li> <li>IPC Overview - Inter-process communication</li> <li>Synchronization - LWKT tokens and locking</li> </ul>"},{"location":"sys/kern/vfs/","title":"Virtual Filesystem (VFS)","text":""},{"location":"sys/kern/vfs/#overview","title":"Overview","text":"<p>The Virtual Filesystem (VFS) layer is DragonFly BSD's abstraction layer that provides a uniform interface between the kernel and concrete filesystem implementations. It allows the kernel to support multiple filesystem types (UFS, HAMMER, HAMMER2, NFS, tmpfs, devfs, etc.) through a common API.</p> <p>Architecture: - Vnodes - In-memory representation of files, directories, devices - VFS operations - Filesystem-level operations (mount, statfs, sync) - Vnode operations (VOPs) - File/directory-level operations (open, read, write, lookup) - Name cache - High-performance path component caching - Buffer cache - Disk block caching and I/O management</p> <p>Key source files (Phase 6a - Initialization and Core): - <code>vfs_init.c</code> (504 lines) \u2014 VFS subsystem initialization - <code>vfs_conf.c</code> (713 lines) \u2014 Filesystem type registration, root mounting - <code>vfs_subr.c</code> (2,650 lines) \u2014 Vnode lifecycle, buffer management utilities - <code>vfs_vfsops.c</code> (321 lines) \u2014 VFS operation wrappers (mount, unmount, sync, etc.) - <code>vfs_vnops.c</code> (1,352 lines) \u2014 High-level vnode operations (vn_open, vn_close, vn_rdwr) - <code>vfs_vopops.c</code> (2,227 lines) \u2014 Vnode operation dispatch layer - <code>vfs_default.c</code> (1,684 lines) \u2014 Default vnode operation implementations</p>"},{"location":"sys/kern/vfs/#vfs-initialization-vfs_initc","title":"VFS Initialization (vfs_init.c)","text":""},{"location":"sys/kern/vfs/#initialization-flow","title":"Initialization Flow","text":"<p>Called during kernel bootstrap via <code>SYSINIT(vfs, SI_SUB_VFS, SI_ORDER_FIRST, vfsinit, NULL)</code>:</p> <pre><code>vfsinit()\n\u251c\u2500 TAILQ_INIT(&amp;vnodeopv_list)           // Initialize vop vector list\n\u251c\u2500 namei_oc = objcache_create_simple()  // Create namei path buffer cache\n\u251c\u2500 vfs_subr_init()                      // Initialize vnode subsystem\n\u251c\u2500 vfs_mount_init()                     // Initialize mount structures\n\u251c\u2500 vfs_lock_init()                      // Initialize vnode locking\n\u251c\u2500 nchinit()                            // Initialize name cache\n\u2514\u2500 vattr_null(&amp;va_null)                 // Initialize null vattr template\n</code></pre>"},{"location":"sys/kern/vfs/#vnode-operations-vector-management","title":"Vnode Operations Vector Management","text":"<p>Key functions: - <code>vfs_add_vnodeops()</code> - Add/register vnode operations vector - <code>vfs_rm_vnodeops()</code> - Remove vnode operations vector - <code>vfs_calc_vnodeops()</code> - Fill in NULL entries with defaults</p> <p>Each filesystem provides a <code>struct vop_ops</code> with function pointers for file operations. The VFS layer ensures NULL entries are replaced with default implementations.</p>"},{"location":"sys/kern/vfs/#filesystem-registration-vfsconf","title":"Filesystem Registration (vfsconf)","text":"<p>Data structures:</p> <pre><code>struct vfsconf {\n    struct vfsops *vfc_vfsops;       // Filesystem operations\n    char          vfc_name[MFSNAMELEN]; // Filesystem type name (e.g., \"ufs\")\n    int           vfc_typenum;       // Unique type number\n    int           vfc_refcount;      // Active mount count\n    ...\n};\n</code></pre> <p>Global registry: - <code>vfsconf_list</code> - STAILQ of all registered filesystem types - <code>vfsconf_maxtypenum</code> - Highest assigned type number</p> <p>Key functions (vfs_init.c): - <code>vfs_register(struct vfsconf *)</code> - Register a filesystem type   - Assigns unique <code>vfc_typenum</code>   - Fills in default vfsops entries (vfs_root, vfs_statfs, vfs_sync, etc.)   - Calls filesystem's <code>vfs_init()</code> method   - Registers sysctl nodes under <code>vfs.&lt;fsname&gt;</code> - <code>vfs_unregister(struct vfsconf *)</code> - Unregister (checks refcount) - <code>vfsconf_find_by_name(const char *)</code> - Lookup filesystem by name - <code>vfsconf_find_by_typenum(int)</code> - Lookup by type number</p> <p>Module integration: <code>vfs_modevent()</code> handles MOD_LOAD/MOD_UNLOAD for filesystem kernel modules.</p>"},{"location":"sys/kern/vfs/#root-filesystem-mounting-vfs_confc","title":"Root Filesystem Mounting (vfs_conf.c)","text":""},{"location":"sys/kern/vfs/#boot-sequence","title":"Boot Sequence","text":"<p><code>SYSINIT(mountroot, SI_SUB_MOUNT_ROOT, SI_ORDER_SECOND, vfs_mountroot, NULL)</code> orchestrates root filesystem mounting:</p> <pre><code>vfs_mountroot()\n\u251c\u2500 sync_devs()                    // Wait for disk device probing\n\u251c\u2500 tsleep(..., hz * wakedelay)    // Default 2s delay (vfs.root.wakedelay)\n\u251c\u2500 Try boot-time root specifications:\n\u2502  \u251c\u2500 RB_CDROM flag \u2192 try cdrom_rootdevnames[] array\n\u2502  \u251c\u2500 ROOTDEVNAME (compile-time)\n\u2502  \u251c\u2500 kgetenv(\"vfs.root.mountfrom\") from loader\n\u2502  \u251c\u2500 rootdevnames[0], rootdevnames[1] (machine-dependent legacy)\n\u2502  \u2514\u2500 RB_ASKNAME \u2192 vfs_mountroot_ask() (interactive prompt)\n\u2514\u2500 Panic if all methods fail\n</code></pre> <p>Root mount string format: <code>&lt;fstype&gt;:&lt;device&gt;</code> (e.g., <code>\"hammer2:da0s1a\"</code>, <code>\"ufs:da0s1a\"</code>)</p>"},{"location":"sys/kern/vfs/#vfs_mountroot_try","title":"vfs_mountroot_try()","text":"<p>Attempts to mount a specified root:</p> <ol> <li>Parse <code>&lt;fstype&gt;:&lt;devname&gt;</code> string</li> <li>Call <code>vfs_rootmountalloc()</code> to allocate <code>struct mount</code></li> <li>Set <code>mp-&gt;mnt_flag |= MNT_ROOTFS</code></li> <li>Call <code>VFS_MOUNT(mp, NULL, NULL, cred)</code></li> <li>On success:</li> <li>Insert into <code>mountlist</code> (first position)</li> <li>Call <code>inittodr()</code> to sync system clock from fs timestamp</li> <li>Get root vnode via <code>VFS_ROOT()</code></li> <li>Setup <code>proc0</code>'s fd_cdir, fd_rdir, fd_ncdir, fd_nrdir</li> <li>Call <code>vfs_cache_setroot()</code> for global rootnch</li> <li>Allocate syncer vnode via <code>vfs_allocate_syncvnode()</code></li> <li>Call <code>VFS_START(mp, 0)</code></li> </ol>"},{"location":"sys/kern/vfs/#interactive-root-prompt","title":"Interactive Root Prompt","text":"<p><code>vfs_mountroot_ask()</code> - when RB_ASKNAME boot flag set:</p> <pre><code>mountroot&gt; ?           # List available disk devices\nmountroot&gt; ufs:da0s1a  # Try mount\nmountroot&gt; panic       # Panic the kernel\nmountroot&gt; abort       # Give up\n</code></pre> <p>Uses <code>devfs_scan_callback()</code> to enumerate disk devices.</p>"},{"location":"sys/kern/vfs/#devfs-mounting","title":"devfs Mounting","text":"<p><code>vfs_mountroot_devfs()</code> - Mounts <code>/dev</code> (or <code>&lt;init_chroot&gt;/dev</code>):</p> <ol> <li>nlookup <code>/dev</code> path</li> <li>Allocate mount structure</li> <li>Call <code>VFS_MOUNT(mp, \"/dev\", NULL, cred)</code></li> <li>Mark ncp with <code>NCF_ISMOUNTPT</code></li> <li>Insert into mountlist</li> </ol>"},{"location":"sys/kern/vfs/#vnode-lifecycle-vfs_subrc","title":"Vnode Lifecycle (vfs_subr.c)","text":""},{"location":"sys/kern/vfs/#vnode-subsystem-initialization","title":"Vnode Subsystem Initialization","text":"<p>vfs_subr_init() (called from vfsinit):</p> <p>Calculates <code>maxvnodes</code> based on available RAM: - Base formula: <code>maxvnodes = freemem / (80 * (sizeof(struct vm_object) + sizeof(struct vnode)))</code> - Non-linear scaling for systems &gt; 1GB and &gt; 8GB - Minimum: max(MINVNODES=2000, maxproc * 8) - Maximum: MAXVNODES=4000000 - Bounded by kernel VA space (KvaSize)</p> <p>Global state: - <code>numvnodes</code> - Current vnode count (sysctl <code>debug.numvnodes</code>) - <code>maxvnodes</code> - Maximum vnodes (sysctl <code>kern.maxvnodes</code>) - <code>spechash_token</code> - Token protecting device vnode hash</p>"},{"location":"sys/kern/vfs/#vnode-buffer-tree-management","title":"Vnode Buffer Tree Management","text":"<p>Vnodes maintain red-black trees for buffer management (defined in vfs_subr.c:133):</p> <pre><code>struct vnode {\n    struct buf_rb_tree v_rbclean_tree;  // Clean buffers\n    struct buf_rb_tree v_rbdirty_tree;  // Dirty buffers\n    struct buf_rb_hash v_rbhash_tree;   // All buffers (hash by b_loffset)\n    ...\n};\n</code></pre> <p>Buffer-vnode association: - <code>bgetvp(struct vnode *, struct buf *)</code> - Associate buffer with vnode (vfs_subr.c:964)   - Inserts into <code>v_rbhash_tree</code>   - Diagnostics check for overlapping buffers - <code>brelvp(struct buf *)</code> - Disassociate buffer from vnode</p>"},{"location":"sys/kern/vfs/#buffer-invalidation","title":"Buffer Invalidation","text":"<p>vinvalbuf() (vfs_subr.c:313) - Flush and invalidate all buffers for a vnode:</p> <pre><code>vinvalbuf(struct vnode *vp, int flags, int slpflag, int slptimeo)\n</code></pre> <p>Flags: - <code>V_SAVE</code> - Call <code>VOP_FSYNC()</code> before invalidating</p> <p>Algorithm: 1. If V_SAVE: wait for write I/O, then <code>VOP_FSYNC()</code> 2. Loop:    - Scan <code>v_rbclean_tree</code> and <code>v_rbdirty_tree</code> with <code>vinvalbuf_bp()</code>    - Wait for all I/O completion (<code>bio_track_wait()</code>)    - Wait for VM paging I/O 3. Remove VM pages via <code>vm_object_page_remove()</code> 4. Panic if any buffers remain</p> <p>Used during: - Vnode reclamation - Truncation/unmount operations</p>"},{"location":"sys/kern/vfs/#buffer-truncation","title":"Buffer Truncation","text":"<p>vtruncbuf() (vfs_subr.c:475) - Truncate file buffers to new length:</p> <pre><code>vtruncbuf(struct vnode *vp, off_t length, int blksize)\n</code></pre> <ol> <li>Round <code>length</code> up to next block boundary</li> <li>Scan clean/dirty trees with <code>vtruncbuf_bp_trunc_cmp()</code></li> <li>Destroy buffers with <code>b_loffset &gt;= truncloffset</code></li> <li>For non-zero truncation: fsync remaining metadata buffers</li> <li>Call <code>vnode_pager_setsize()</code> to truncate VM backing store</li> <li>Wait for I/O completion</li> </ol>"},{"location":"sys/kern/vfs/#filesystem-sync-vfsync","title":"Filesystem Sync (vfsync)","text":"<p>vfsync() (vfs_subr.c:680) - Sync dirty buffers for a vnode:</p> <pre><code>vfsync(struct vnode *vp, int waitfor, int passes,\n       int (*checkdef)(struct buf *),\n       int (*waitoutput)(struct vnode *, struct thread *))\n</code></pre> <p>Wait modes: - <code>MNT_LAZY</code> - Lazy flush (limit to 1MB data), used by syncer - <code>MNT_NOWAIT</code> - Asynchronous flush (one data pass, one metadata pass) - <code>MNT_WAIT</code> - Synchronous (multiple passes until clean)</p> <p>Algorithm for MNT_WAIT: 1. Data-only pass (fast, no waiting) 2. Wait for I/O 3. Full pass (data + metadata) 4. Additional passes (up to <code>passes</code> count) until no dirty buffers remain 5. On final pass: set <code>info.synchronous = 1</code> to force blocking writes</p> <p>Lazy mode (<code>MNT_LAZY</code>): - Scan from <code>vp-&gt;v_lazyw</code> offset - Stop after flushing 1MB (<code>info.lazylimit</code>) - Updates <code>v_lazyw</code> to track progress - Reschedules vnode for syncer if incomplete</p> <p>Used by: - <code>VOP_FSYNC()</code> implementations - Periodic sync daemon</p>"},{"location":"sys/kern/vfs/#timestamp-precision","title":"Timestamp Precision","text":"<p>vfs_timestamp() (vfs_subr.c:238) - Get current timestamp with configurable precision:</p> <p>Sysctl <code>vfs.timestamp_precision</code>: - 0 = Seconds only - 1 = Microseconds (tick precision, default if hz &gt;= 100) - 2 = Microseconds (tick precision) - 3 = Nanoseconds (tick precision) - 4 = Microseconds (maximum precision, default if hz &lt; 100) - 5 = Nanoseconds (maximum precision)</p>"},{"location":"sys/kern/vfs/#vfs-operations-vfs_vfsopsc","title":"VFS Operations (vfs_vfsops.c)","text":""},{"location":"sys/kern/vfs/#mpsafe-wrapper-layer","title":"MPSAFE Wrapper Layer","text":"<p><code>vfs_vfsops.c</code> provides MPSAFE wrappers for all <code>struct vfsops</code> methods. Each wrapper:</p> <ol> <li>Acquires mount's MP lock (<code>VFS_MPLOCK(mp)</code>)</li> <li>Calls filesystem's method via function pointer</li> <li>Releases MP lock (<code>VFS_MPUNLOCK()</code>)</li> </ol> <p>Key wrappers:</p> <pre><code>int vfs_mount(struct mount *mp, char *path, caddr_t data, struct ucred *cred)\nint vfs_start(struct mount *mp, int flags)\nint vfs_unmount(struct mount *mp, int mntflags)\nint vfs_root(struct mount *mp, struct vnode **vpp)\nint vfs_sync(struct mount *mp, int waitfor)\nint vfs_statfs(struct mount *mp, struct statfs *sbp, struct ucred *cred)\nint vfs_statvfs(struct mount *mp, struct statvfs *sbp, struct ucred *cred)\nint vfs_vget(struct mount *mp, struct vnode *dvp, ino_t ino, struct vnode **vpp)\nint vfs_fhtovp(struct mount *mp, struct vnode *rootvp, struct fid *fhp, struct vnode **vpp)\nint vfs_vptofh(struct vnode *vp, struct fid *fhp)\nint vfs_checkexp(struct mount *mp, struct sockaddr *nam, int *extflagsp, struct ucred **credanonp)\nint vfs_extattrctl(struct mount *mp, int cmd, struct vnode *vp, int attrnamespace, \n                   const char *attrname, struct ucred *cred)\n</code></pre> <p>Quota integration: - <code>vfs_start()</code> calls <code>VFS_ACINIT()</code> on successful start - <code>vfs_unmount()</code> calls <code>VFS_ACDONE()</code> before unmounting</p> <p>Mount point locking strategy: - Most operations use exclusive MP lock - Some operations conditionally use MP lock via <code>VFS_MPLOCK_FLAG()</code> - Filesystems can opt-in to MPSAFE via <code>MNTK_*_MPSAFE</code> flags</p>"},{"location":"sys/kern/vfs/#high-level-vnode-operations-vfs_vnopsc","title":"High-Level Vnode Operations (vfs_vnops.c)","text":""},{"location":"sys/kern/vfs/#vn_open-opencreate-files","title":"vn_open() - Open/Create Files","text":"<p>Signature:</p> <pre><code>int vn_open(struct nlookupdata *nd, struct file **fpp, int fmode, int cmode)\n</code></pre> <p>Purpose: Unified entry point for opening and creating files/directories.</p> <p>Flow:</p> <pre><code>vn_open()\n\u251c\u2500 Setup nd-&gt;nl_flags (NLC_OPEN, NLC_APPEND, NLC_READ, NLC_WRITE, etc.)\n\u251c\u2500 if (fmode &amp; O_CREAT):\n\u2502  \u251c\u2500 Set NLC_CREATE, NLC_REFDVP, call nlookup()\n\u2502  \u2514\u2500 bwillinode(1)  // Reserve inode space\n\u251c\u2500 else:\n\u2502  \u2514\u2500 nlookup()  // Normal lookup\n\u251c\u2500 Check filesystem modification stall (ncp_writechk())\n\u251c\u2500 if (O_CREAT and ncp-&gt;nc_vp == NULL):\n\u2502  \u2514\u2500 VOP_NCREATE(&amp;nl_nch, nl_dvp, &amp;vp, cred, vap)  // Create new file\n\u251c\u2500 else:\n\u2502  \u2514\u2500 cache_vget(&amp;nl_nch, cred, LK_EXCLUSIVE/LK_SHARED, &amp;vp)  // Get existing\n\u251c\u2500 Validate vnode type (reject VLNK, VSOCK; check O_DIRECTORY)\n\u251c\u2500 Check write permission (vn_writechk()) if FWRITE|O_TRUNC\n\u251c\u2500 if (O_TRUNC):\n\u2502  \u251c\u2500 VOP_SETATTR_FP(vp, vap-&gt;va_size=0, cred, fp)\n\u2502  \u2514\u2500 VFS_ACCOUNT()  // Quota adjustment\n\u251c\u2500 Setup VNSWAPCACHE flags based on NCF_UF_CACHE/NCF_SF_NOCACHE\n\u251c\u2500 if (fp):\n\u2502  \u2514\u2500 fp-&gt;f_nchandle = nd-&gt;nl_nch  // Store namecache handle\n\u251c\u2500 VOP_OPEN(vp, fmode, cred, fpp)  // Call filesystem's open method\n\u2514\u2500 Return vnode in nd-&gt;nl_open_vp (if fp == NULL) or fp-&gt;f_data (if fp != NULL)\n</code></pre> <p>Key features: - Shared locking optimization: Uses LK_SHARED for read-only opens (when appropriate) - ESTALE handling: Re-resolves namecache on ESTALE errors - Quota integration: Checks/accounts for size changes on O_TRUNC - Swapcache control: Propagates NCF_UF_CACHE flags to VNSWAPCACHE vnode flag</p> <p>Error cases: - <code>EACCES</code> - Permission denied - <code>EEXIST</code> - O_CREAT | O_EXCL and file exists - <code>EISDIR</code> - Attempting to write/truncate a directory - <code>ENOTDIR</code> - O_DIRECTORY on non-directory - <code>EMLINK</code> - Opened a symlink (shouldn't happen with NLC_FOLLOW) - <code>ETXTBSY</code> - File is executing, cannot write - <code>EROFS</code> - Read-only filesystem - <code>ESTALE</code> - NFS stale file handle (triggers retry)</p>"},{"location":"sys/kern/vfs/#vn_close-close-files","title":"vn_close() - Close Files","text":"<pre><code>int vn_close(struct vnode *vp, int flags, struct file *fp)\n</code></pre> <ol> <li>Lock vnode (LK_SHARED | LK_RETRY | LK_FAILRECLAIM)</li> <li>Call <code>VOP_CLOSE(vp, flags, fp)</code></li> <li>Unlock vnode</li> </ol> <p>Flags: - <code>FREAD</code>, <code>FWRITE</code> - Indicating how file was opened - <code>FNONBLOCK</code> - Non-blocking close</p>"},{"location":"sys/kern/vfs/#vn_rdwr-kernel-file-io","title":"vn_rdwr() - Kernel File I/O","text":"<pre><code>int vn_rdwr(enum uio_rw rw, struct vnode *vp, caddr_t base, int len,\n            off_t offset, enum uio_seg segflg, int ioflags,\n            struct ucred *cred, int *aresid)\n</code></pre> <p>Purpose: Synchronous read/write from kernel context.</p> <p>Used by: - Executable loading (imgact_elf.c) - Core dumps (kern_sig.c) - Swap pager - Kernel module loading</p> <p>Steps: 1. Setup struct uio with I/O parameters 2. If UIO_SYSSPACE and vnode has VM object: use <code>vn_cache_strategy()</code> 3. Else: call <code>VOP_READ()</code> or <code>VOP_WRITE()</code> 4. Return residual count in <code>*aresid</code></p>"},{"location":"sys/kern/vfs/#vn_writechk-write-permission-check","title":"vn_writechk() - Write Permission Check","text":"<pre><code>int vn_writechk(struct vnode *vp)\n</code></pre> <p>Checks: - <code>VTEXT</code> flag - File is executing (returns ETXTBSY) - <code>MNT_RDONLY</code> - Filesystem is read-only (returns EROFS)</p> <p>Called after vnode is locked.</p>"},{"location":"sys/kern/vfs/#ncp_writechk-namecache-write-check","title":"ncp_writechk() - Namecache Write Check","text":"<pre><code>int ncp_writechk(struct nchandle *nch)\n</code></pre> <p>Checks: - <code>MNT_RDONLY</code> - Associated mount is read-only (returns EROFS) - Calls <code>VFS_MODIFYING()</code> if filesystem has special modifying callback</p> <p>Called BEFORE vnodes are locked (allows filesystem to stall modifications).</p>"},{"location":"sys/kern/vfs/#file-descriptor-operations","title":"File Descriptor Operations","text":"<p>vnode_fileops structure (vfs_vnops.c:77) - Provides file operations for vnodes:</p> <pre><code>struct fileops vnode_fileops = {\n    .fo_read     = vn_read,\n    .fo_write    = vn_write,\n    .fo_ioctl    = vn_ioctl,\n    .fo_kqfilter = vn_kqfilter,\n    .fo_stat     = vn_statfile,\n    .fo_close    = vn_closefile,\n    .fo_shutdown = nofo_shutdown,\n    .fo_seek     = vn_seek\n};\n</code></pre> <p>These functions bridge between file descriptor operations (<code>read(2)</code>, <code>write(2)</code>, etc.) and vnode operations (<code>VOP_READ()</code>, <code>VOP_WRITE()</code>).</p> <p>vn_read(): - Validates file is open for reading - For VREG: updates <code>f_offset</code> optimistically - Calls <code>VOP_READ(vp, uio, ioflag, cred, fp)</code> - Handles <code>f_offset</code> races</p> <p>vn_write(): - Validates file is open for writing - Handles <code>IO_APPEND</code> flag - Quota checks for regular files - Calls <code>VOP_WRITE(vp, uio, ioflag, cred, fp)</code> - Quota accounting on success</p> <p>vn_ioctl(): - Validates vnode type (reject directories) - Calls <code>VOP_IOCTL(vp, cmd, data, fflag, cred, msg)</code></p> <p>vn_statfile(): - Calls <code>VOP_GETATTR(vp, &amp;vattr)</code> - Converts <code>struct vattr</code> to <code>struct stat</code> - Fills in st_dev, st_ino, st_mode, st_size, timestamps, etc.</p> <p>vn_seek(): - Validates seek offset (no negative offsets) - For VREG: allows seeks beyond EOF - For VDIR: offset must be &lt;= current size</p>"},{"location":"sys/kern/vfs/#vnode-operation-dispatch-vfs_vopopsc","title":"Vnode Operation Dispatch (vfs_vopops.c)","text":""},{"location":"sys/kern/vfs/#purpose","title":"Purpose","text":"<p><code>vfs_vopops.c</code> provides MPSAFE dispatch wrappers for all vnode operations. Similar to <code>vfs_vfsops.c</code> but for per-vnode operations rather than per-mount operations.</p>"},{"location":"sys/kern/vfs/#wrapper-pattern","title":"Wrapper Pattern","text":"<p>Each VOP wrapper:</p> <ol> <li>Initializes <code>struct vop_*_args</code> with operation parameters</li> <li>Sets <code>a_head.a_desc</code> (operation descriptor)</li> <li>Sets <code>a_head.a_ops</code> (vnode's ops vector)</li> <li>Acquires mount's MP lock (<code>VFS_MPLOCK(vp-&gt;v_mount)</code>)</li> <li>Calls operation via <code>DO_OPS(ops, error, &amp;ap, vop_field)</code></li> <li>Releases MP lock (<code>VFS_MPUNLOCK()</code>)</li> <li>Returns error</li> </ol> <p>Example - VOP_OPEN():</p> <pre><code>int vop_open(struct vop_ops *ops, struct vnode *vp, int mode,\n             struct ucred *cred, struct file **fpp)\n{\n    struct vop_open_args ap;\n    VFS_MPLOCK_DECLARE;\n    int error;\n\n    // Decrement VAGE0/VAGE1 flags (aging mechanism)\n    if (vp-&gt;v_flag &amp; VAGE0) {\n        vclrflags(vp, VAGE0);\n    } else if (vp-&gt;v_flag &amp; VAGE1) {\n        vclrflags(vp, VAGE1);\n        vsetflags(vp, VAGE0);\n    }\n\n    ap.a_head.a_desc = &amp;vop_open_desc;\n    ap.a_head.a_ops = ops;\n    ap.a_vp = vp;\n    ap.a_fpp = fpp;\n    ap.a_mode = mode;\n    ap.a_cred = cred;\n\n    VFS_MPLOCK(vp-&gt;v_mount);\n    DO_OPS(ops, error, &amp;ap, vop_open);\n    VFS_MPUNLOCK();\n\n    return(error);\n}\n</code></pre>"},{"location":"sys/kern/vfs/#key-vnode-operations","title":"Key Vnode Operations","text":"<p>Namespace operations (new API): - <code>vop_nresolve()</code> - Resolve namecache entry to vnode - <code>vop_nlookupdotdot()</code> - Lookup parent directory (..) - <code>vop_ncreate()</code> - Create file via namecache - <code>vop_nmkdir()</code> - Create directory via namecache - <code>vop_nmknod()</code> - Create device node via namecache - <code>vop_nlink()</code> - Create hard link via namecache - <code>vop_nsymlink()</code> - Create symbolic link via namecache - <code>vop_nwhiteout()</code> - Create/delete whiteout entry - <code>vop_nremove()</code> - Remove file via namecache - <code>vop_nrmdir()</code> - Remove directory via namecache - <code>vop_nrename()</code> - Rename file/directory via namecache</p> <p>File operations: - <code>vop_open()</code> - Open file - <code>vop_close()</code> - Close file - <code>vop_read()</code> - Read data - <code>vop_write()</code> - Write data (with quota accounting) - <code>vop_ioctl()</code> - I/O control operations - <code>vop_poll()</code> - Poll for events - <code>vop_kqfilter()</code> - Register kqueue filter - <code>vop_fsync()</code> - Sync dirty data/metadata - <code>vop_fdatasync()</code> - Sync data only</p> <p>Metadata operations: - <code>vop_getattr()</code> - Get vnode attributes - <code>vop_getattr_lite()</code> - Get lightweight attributes - <code>vop_setattr()</code> - Set vnode attributes - <code>vop_access()</code> - Check access permissions</p> <p>I/O operations: - <code>vop_bmap()</code> - Map logical block to physical - <code>vop_strategy()</code> - Perform I/O strategy - <code>vop_getpages()</code> - Get VM pages - <code>vop_putpages()</code> - Flush VM pages</p> <p>Directory operations: - <code>vop_readdir()</code> - Read directory entries - <code>vop_readlink()</code> - Read symbolic link target</p> <p>Lifecycle operations: - <code>vop_inactive()</code> - Vnode is no longer referenced - <code>vop_reclaim()</code> - Reclaim vnode resources</p> <p>Special operations: - <code>vop_mmap()</code> - Memory-map file - <code>vop_advlock()</code> - Advisory locking - <code>vop_balloc()</code> - Allocate blocks - <code>vop_freeblks()</code> - Free blocks (sparse files/truncation) - <code>vop_pathconf()</code> - Get filesystem path configuration - <code>vop_markatime()</code> - Mark access time (deferred atime updates) - <code>vop_allocate()</code> - Preallocate space (fallocate)</p> <p>Extended attributes: - <code>vop_getacl()</code> - Get ACL - <code>vop_setacl()</code> - Set ACL - <code>vop_aclcheck()</code> - Check ACL validity - <code>vop_getextattr()</code> - Get extended attribute - <code>vop_setextattr()</code> - Set extended attribute</p>"},{"location":"sys/kern/vfs/#mpsafe-optimization","title":"MPSAFE Optimization","text":"<p>Some operations support conditional locking for better concurrency:</p> <p>VFS_MPLOCK_FLAG() variants: - <code>MNTK_GA_MPSAFE</code> - Getattr is MP-safe - <code>MNTK_RD_MPSAFE</code> - Read is MP-safe - <code>MNTK_WR_MPSAFE</code> - Write is MP-safe - <code>MNTK_ST_MPSAFE</code> - Start is MP-safe</p> <p>If the mount has the appropriate flag set, the wrapper skips MP lock acquisition.</p>"},{"location":"sys/kern/vfs/#quota-integration","title":"Quota Integration","text":"<p>vop_write() wrapper includes comprehensive quota handling:</p> <ol> <li>Before write: <code>VOP_GETATTR()</code> to get current size and ownership</li> <li>Calculate potential new size (accounting for IO_APPEND)</li> <li>Check quota: <code>vq_write_ok(mp, uid, gid, delta)</code></li> <li>Perform write via filesystem's method</li> <li>On success: <code>VFS_ACCOUNT(mp, uid, gid, actual_delta)</code></li> </ol>"},{"location":"sys/kern/vfs/#default-vnode-operations-vfs_defaultc","title":"Default Vnode Operations (vfs_default.c)","text":""},{"location":"sys/kern/vfs/#default-operations-table","title":"Default Operations Table","text":"<p><code>default_vnode_vops</code> provides fallback implementations when filesystems don't implement specific operations:</p> <p>Common defaults: - <code>.vop_default = vop_eopnotsupp</code> - Return EOPNOTSUPP for unimplemented ops - <code>.vop_advlock = vop_einval</code> - Advisory locking not supported - <code>.vop_fsync = vop_null</code> - Successful no-op (for filesystems with no dirty buffers) - <code>.vop_fdatasync = vop_stdfdatasync</code> - Calls vop_fsync - <code>.vop_open = vop_stdopen</code> - Standard open logic - <code>.vop_close = vop_stdclose</code> - Standard close logic - <code>.vop_mmap = vop_einval</code> - Memory-mapping not supported by default - <code>.vop_readlink = vop_einval</code> - Not a symlink - <code>.vop_markatime = vop_stdmarkatime</code> - Standard atime marking</p> <p>Compatibility wrappers (old namespace API \u2192 new): - <code>.vop_nresolve = vop_compat_nresolve</code> - <code>.vop_ncreate = vop_compat_ncreate</code> - <code>.vop_nmkdir = vop_compat_nmkdir</code> - <code>.vop_nremove = vop_compat_nremove</code> - <code>.vop_nrename = vop_compat_nrename</code></p> <p>These wrappers translate new-style namecache operations (VOPs taking <code>struct nchandle *</code>) to old-style operations (VOPs taking <code>struct componentname *</code>), allowing legacy filesystems to work with modern code.</p>"},{"location":"sys/kern/vfs/#standard-error-returns","title":"Standard Error Returns","text":"<pre><code>int vop_eopnotsupp(struct vop_generic_args *ap) { return EOPNOTSUPP; }\nint vop_ebadf(struct vop_generic_args *ap)      { return EBADF; }\nint vop_enotty(struct vop_generic_args *ap)     { return ENOTTY; }\nint vop_einval(struct vop_generic_args *ap)     { return EINVAL; }\nint vop_null(struct vop_generic_args *ap)       { return 0; }\n</code></pre>"},{"location":"sys/kern/vfs/#standard-implementations","title":"Standard Implementations","text":"<p>vop_stdopen(): - For VCHR (character device): calls <code>spec_open()</code> - For VFIFO: calls <code>fifo_open()</code> - Otherwise: returns 0</p> <p>vop_stdclose(): - For VCHR: calls <code>spec_close()</code> - For VFIFO: calls <code>fifo_close()</code> - Otherwise: returns 0</p> <p>vop_stdgetattr_lite(): - Calls <code>VOP_GETATTR()</code> and extracts lightweight fields - Used for stat-like operations that don't need full vattr</p> <p>vop_stdmarkatime(): - Sets <code>VN_ATIME</code> flag on vnode - Actual atime update deferred until vnode is written back</p> <p>vop_stdpathconf(): - Returns standard POSIX path configuration values - <code>_PC_LINK_MAX</code>, <code>_PC_NAME_MAX</code>, <code>_PC_PATH_MAX</code>, etc.</p> <p>vop_stdallocate(): - Default fallocate(2) implementation - Simply extends file size via <code>VOP_SETATTR()</code> (non-sparse)</p>"},{"location":"sys/kern/vfs/#compatibility-layer-old-new-namespace-api","title":"Compatibility Layer (Old \u2192 New Namespace API)","text":"<p>vop_compat_nresolve(): Translates <code>VOP_NRESOLVE(nch, dvp, cred)</code> to: 1. Extract componentname from nch 2. Call <code>VOP_OLD_LOOKUP(dvp, &amp;vp, cnp)</code> 3. Cache result in nch</p> <p>vop_compat_ncreate(): Translates <code>VOP_NCREATE(nch, dvp, &amp;vp, cred, vap)</code> to: 1. Lock parent directory exclusively 2. Call <code>VOP_OLD_CREATE(dvp, &amp;vp, cnp, vap)</code> 3. Cache new vnode in nch</p> <p>vop_compat_nremove(): Translates <code>VOP_NREMOVE(nch, dvp, cred)</code> to: 1. Extract componentname 2. Lock parent and target 3. Call <code>VOP_OLD_REMOVE(dvp, vp, cnp)</code></p> <p>Similar wrappers exist for nmkdir, nmknod, nlink, nsymlink, nrmdir, nrename, nwhiteout.</p> <p>Purpose: Allows old filesystems (written for the componentname API) to work transparently with the modern namecache-centric API.</p>"},{"location":"sys/kern/vfs/#summary-phase-6a","title":"Summary: Phase 6a","text":"<p>Files analyzed (7 files, 9,451 lines): 1. vfs_init.c (504 lines) - VFS/vfsconf initialization, vnode ops registration 2. vfs_conf.c (713 lines) - Root filesystem mounting, interactive prompt 3. vfs_subr.c (2,650 lines) - Vnode lifecycle, buffer management, sync operations 4. vfs_vfsops.c (321 lines) - MPSAFE VFS operation wrappers 5. vfs_vnops.c (1,352 lines) - High-level vnode operations (vn_open, vn_close, vn_rdwr) 6. vfs_vopops.c (2,227 lines) - Vnode operation dispatch layer 7. vfs_default.c (1,684 lines) - Default VOP implementations, compatibility layer</p> <p>Key Concepts: - Vnode - In-memory file/directory representation - VFS operations - Filesystem-level (mount, statfs, sync) - Vnode operations - File-level (open, read, write, lookup) - vfsconf - Filesystem type registry - MPSAFE wrappers - Per-mount/vnode operation locking - Namespace API - Modern namecache-centric operations (VOP_NRESOLVE, VOP_NCREATE) - Compatibility layer - Old componentname API \u2192 new namecache API</p> <p>Next Phase 6 Steps: - 6b: Name lookup and caching (vfs_cache.c, vfs_nlookup.c) - 6c: Mounting and syscalls (vfs_mount.c, vfs_syscalls.c) - 6d: Buffer cache and I/O (vfs_bio.c, vfs_cluster.c) - 6e: Extensions (locking, journaling, quota, AIO)</p>"},{"location":"sys/kern/vfs/buffer-cache/","title":"VFS Buffer Cache and I/O","text":""},{"location":"sys/kern/vfs/buffer-cache/#overview","title":"Overview","text":"<p>The VFS buffer cache is a critical subsystem that mediates between the filesystem layer and the underlying storage devices while integrating tightly with the VM system. It provides caching for filesystem metadata and file data, manages asynchronous and synchronous I/O, implements read-ahead and write-behind optimizations, and coordinates between buffer-based and VM-page-based I/O.</p> <p>Key source files: - <code>sys/kern/vfs_bio.c</code> (4,659 lines) - Buffer cache management - <code>sys/kern/vfs_cluster.c</code> (1,814 lines) - Cluster I/O optimization - <code>sys/kern/vfs_vm.c</code> (503 lines) - VM integration - <code>sys/sys/buf.h</code> - Buffer structure definitions - <code>sys/sys/bio.h</code> - BIO layer structures</p> <p>Core responsibilities: - Cache filesystem blocks in memory - Manage dirty data and write-behind - Implement read-ahead for sequential access - Cluster I/O operations for performance - Integrate with VM system for unified caching - Provide async/sync I/O primitives</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-structure","title":"Buffer Structure","text":""},{"location":"sys/kern/vfs/buffer-cache/#struct-buf","title":"struct buf","text":"<p>The <code>struct buf</code> (sys/sys/buf.h:153) is the central data structure representing a cached filesystem block:</p> <pre><code>struct buf {\n    /* Tree linkages */\n    RB_ENTRY(buf) b_rbnode;         /* RB node in vnode clean/dirty tree */\n    RB_ENTRY(buf) b_rbhash;         /* RB node in vnode hash tree */\n    TAILQ_ENTRY(buf) b_freelist;    /* Free list position if not active */\n    struct buf *b_cluster_next;     /* Next buffer (cluster code) */\n\n    /* Vnode association */\n    struct vnode *b_vp;             /* Vnode for this buffer */\n\n    /* BIO translation layers */\n    struct bio b_bio_array[NBUF_BIO]; /* Typically 6 layers */\n\n    /* State and control */\n    u_int32_t b_flags;              /* B_* flags */\n    unsigned int b_qindex;          /* Buffer queue index */\n    unsigned int b_qcpu;            /* Buffer queue CPU */\n    unsigned char b_act_count;      /* Activity count (like vm_page) */\n    unsigned char b_swindex;        /* Swap index */\n    cpumask_t b_cpumask;            /* KVABIO API CPU mask */\n    struct lock b_lock;             /* Buffer lock */\n    buf_cmd_t b_cmd;                /* I/O command */\n\n    /* Size fields */\n    int b_bufsize;                  /* Allocated buffer size (filesystem block) */\n    int b_runningbufspace;          /* When I/O is running, pipelining */\n    int b_bcount;                   /* Valid bytes in buffer */\n    int b_resid;                    /* Remaining I/O */\n    int b_error;                    /* Error return */\n\n    /* Data pointers */\n    caddr_t b_data;                 /* Data pointer (KVA) */\n    caddr_t b_kvabase;              /* Base KVA for buffer */\n    int b_kvasize;                  /* Size of KVA for buffer */\n\n    /* Dirty tracking */\n    int b_dirtyoff;                 /* Offset in buffer of dirty region */\n    int b_dirtyend;                 /* Offset of end of dirty region */\n\n    /* Reference counting */\n    int b_refs;                     /* FINDBLK_REF/bqhold()/bqdrop() */\n\n    /* Page list management */\n    struct xio b_xio;               /* Data buffer page list management */\n\n    /* Filesystem dependencies */\n    struct bio_ops *b_ops;          /* Bio_ops used w/ b_dep */\n    union {\n        struct workhead b_dep;      /* List of filesystem dependencies */\n        void *b_priv;               /* Filesystem private data */\n    };\n};\n</code></pre> <p>Key field groups:</p> <ol> <li>Indexing: b_rbnode, b_rbhash organize buffers by (vnode, offset)</li> <li>Bio layers: b_bio_array[] provides I/O address translation</li> <li>State: b_flags, b_cmd, b_error track buffer state</li> <li>Sizing: b_bufsize (allocation), b_bcount (valid data), b_resid (remaining)</li> <li>Data: b_data points to KVA, b_xio manages VM pages</li> <li>Dirty: b_dirtyoff/b_dirtyend track partial dirty ranges</li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#bio-layer-fields","title":"BIO Layer Fields","text":"<p>b_bio1 (b_bio_array[0]) - Logical layer: - Contains logical offset (b_loffset = b_bio1.bio_offset) - Used with primary vnode (bp-&gt;b_vp) - Operations: <code>vn_strategy(bp-&gt;b_vp, &amp;bp-&gt;b_bio1)</code></p> <p>b_bio2 (b_bio_array[1]) - Physical layer: - Contains device-relative offset (translated from logical) - Used with device vnode in filesystems - Set by VOP_BMAP() call</p> <p>Additional layers: Allocated from object cache for device stacking (RAID, encryption, etc.)</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-flags-b_flags","title":"Buffer Flags (b_flags)","text":"<p>Buffer state flags (sys/sys/buf.h:304):</p> <p>Cache state: - <code>B_CACHE</code> (0x00000020) - Buffer found in cache, data valid - <code>B_INVAL</code> (0x00002000) - Buffer does not contain valid info - <code>B_DELWRI</code> (0x00000080) - Delayed write (dirty, needs flush) - <code>B_DIRTY</code> (0x00200000) - Needs writing later</p> <p>I/O state: - <code>B_ERROR</code> (0x00000800) - I/O error occurred - <code>B_EINTR</code> (0x00000400) - I/O was interrupted - <code>B_IOISSUED</code> (0x00001000) - I/O has been issued (vfs can clear)</p> <p>VM integration: - <code>B_VMIO</code> (0x20000000) - Buffer tied to VM object - <code>B_PAGING</code> (0x04000000) - Volatile paging I/O, bypass VMIO - <code>B_RAM</code> (0x10000000) - Read-ahead mark</p> <p>Clustering: - <code>B_CLUSTER</code> (0x40000000) - Part of a cluster operation - <code>B_CLUSTEROK</code> (0x00020000) - May be clustered with adjacent buffers</p> <p>Locking: - <code>B_LOCKED</code> (0x00004000) - Locked in core (not reusable) - <code>B_KVABIO</code> (0x00010000) - Lockholder uses KVABIO API</p> <p>Lifecycle: - <code>B_AGE</code> (0x00000001) - Reuse more quickly - <code>B_RELBUF</code> (0x00400000) - Release VMIO buffer - <code>B_NOCACHE</code> (0x00008000) - Destroy buffer AND backing store</p> <p>Special: - <code>B_HEAVY</code> (0x00100000) - Heavy-weight buffer (needs special handling) - <code>B_BNOCLIP</code> (0x00000100) - EOF clipping not allowed - <code>B_NOTMETA</code> (0x00000004) - Not metadata (affects VM page handling) - <code>B_MARKER</code> (0x00040000) - Special marker buffer in queue - <code>B_HASHED</code> (0x00000040) - Indexed via v_rbhash_tree</p> <p>Tree linkage: - <code>B_VNCLEAN</code> (0x01000000) - On vnode clean list - <code>B_VNDIRTY</code> (0x02000000) - On vnode dirty list</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-commands-buf_cmd_t","title":"Buffer Commands (buf_cmd_t)","text":"<p>I/O operation types (sys/sys/buf.h:87):</p> <pre><code>typedef enum buf_cmd {\n    BUF_CMD_DONE = 0,      /* I/O completed */\n    BUF_CMD_READ,          /* Read operation */\n    BUF_CMD_WRITE,         /* Write operation */\n    BUF_CMD_FREEBLKS,      /* Free blocks */\n    BUF_CMD_FORMAT,        /* Format operation */\n    BUF_CMD_FLUSH,         /* Cache flush */\n    BUF_CMD_SEEK,          /* Seek operation */\n} buf_cmd_t;\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-queues","title":"Buffer Queues","text":""},{"location":"sys/kern/vfs/buffer-cache/#per-cpu-queue-structure","title":"Per-CPU Queue Structure","text":"<p>Buffers are organized into per-CPU queues to reduce lock contention (vfs_bio.c:72):</p> <pre><code>enum bufq_type {\n    BQUEUE_NONE,        /* not on any queue */\n    BQUEUE_LOCKED,      /* locked buffers */\n    BQUEUE_CLEAN,       /* non-B_DELWRI buffers */\n    BQUEUE_DIRTY,       /* B_DELWRI buffers */\n    BQUEUE_DIRTY_HW,    /* B_DELWRI buffers - heavy weight */\n    BQUEUE_EMPTY,       /* empty buffer headers */\n\n    BUFFER_QUEUES       /* number of buffer queues */\n};\n\nstruct bufpcpu {\n    struct spinlock spin;\n    struct bqueues bufqueues[BUFFER_QUEUES];\n} __cachealign;\n\nstruct bufpcpu bufpcpu[MAXCPU];\n</code></pre> <p>Queue semantics:</p> <ul> <li>BQUEUE_NONE: Buffer is actively in use (locked, doing I/O)</li> <li>BQUEUE_LOCKED: Buffer explicitly locked with B_LOCKED flag</li> <li>BQUEUE_CLEAN: Clean cached buffers eligible for reuse</li> <li>BQUEUE_DIRTY: Dirty buffers awaiting flush</li> <li>BQUEUE_DIRTY_HW: Heavy-weight dirty buffers (special flushing)</li> <li>BQUEUE_EMPTY: Empty buffer headers (no data allocated)</li> </ul>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-lifecycle-through-queues","title":"Buffer Lifecycle Through Queues","text":"<ol> <li>Allocation (getnewbuf):</li> <li>Pull from BQUEUE_EMPTY or BQUEUE_CLEAN</li> <li> <p>State: BQUEUE_NONE (in use)</p> </li> <li> <p>Active use:</p> </li> <li>Buffer locked via BUF_LOCK()</li> <li>State: BQUEUE_NONE</li> <li> <p>I/O operations performed</p> </li> <li> <p>Release (brelse/bqrelse):</p> </li> <li>Unlock buffer</li> <li> <p>Move to appropriate queue:</p> <ul> <li>B_DELWRI \u2192 BQUEUE_DIRTY or BQUEUE_DIRTY_HW</li> <li>B_LOCKED \u2192 BQUEUE_LOCKED</li> <li>Clean \u2192 BQUEUE_CLEAN</li> </ul> </li> <li> <p>Reuse (getnewbuf):</p> </li> <li>Scan BQUEUE_CLEAN for victims</li> <li> <p>Invalidate and reallocate</p> </li> <li> <p>Flush (buf_daemon):</p> </li> <li>Scan BQUEUE_DIRTY/BQUEUE_DIRTY_HW</li> <li>Write dirty buffers asynchronously</li> <li>Move to BQUEUE_CLEAN after write completes</li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-cache-tuning-parameters","title":"Buffer Cache Tuning Parameters","text":""},{"location":"sys/kern/vfs/buffer-cache/#sysctl-tunables","title":"Sysctl Tunables","text":"<p>Space management (vfs_bio.c:162-210):</p> <pre><code>/* Operational control */\nlong maxbufspace;          /* Hard limit on buffer space */\nlong hibufspace;           /* Soft limit (high watermark) */\nlong lobufspace;           /* Low watermark */\nlong bufspace;             /* Current buffer space used */\n\nlong lodirtybufspace;      /* Trigger buf_daemon activation */\nlong hidirtybufspace;      /* High watermark for dirty buffers */\nlong dirtybufspace;        /* Current dirty buffer space */\nlong dirtybufcount;        /* Number of dirty buffers */\nlong dirtybufspacehw;      /* Dirty space (heavy-weight) */\nlong dirtybufcounthw;      /* Dirty count (heavy-weight) */\n\nlong lorunningspace;       /* Minimum space for active I/O */\nlong hirunningspace;       /* Maximum space for active I/O */\nlong runningbufspace;      /* Currently running I/O space */\nlong runningbufcount;      /* Currently running I/O count */\n\nu_int flushperqueue;       /* Buffers to flush per queue (default: 1024) */\nlong bufcache_bw;          /* Buffer\u2192VM transfer bandwidth (200 MB/s) */\n</code></pre> <p>Watermark behavior:</p> <ul> <li>lobufspace \u2192 hibufspace: Normal operation, allocate freely</li> <li>&gt; hibufspace: Trigger aggressive buffer reclamation</li> <li>lodirtybufspace \u2192 hidirtybufspace: Normal dirty buffer accumulation</li> <li>&gt; hidirtybufspace: Wake buf_daemon to flush aggressively</li> <li>lorunningspace \u2192 hirunningspace: Control I/O pipeline depth</li> </ul>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-daemon-threads","title":"Buffer Daemon Threads","text":"<p>Two kernel threads manage buffer flushing (vfs_bio.c:155-156):</p> <p>bufdaemon_td - Standard buffer daemon: - Flushes BQUEUE_DIRTY when dirtybufspace &gt; lodirtybufspace - Triggered via bd_request atomic flag - Writes dirty buffers asynchronously - Moves flushed buffers to BQUEUE_CLEAN</p> <p>bufdaemonhw_td - Heavy-weight buffer daemon: - Flushes BQUEUE_DIRTY_HW - Separate thread prevents deadlock (heavy buffers may need more buffers to flush) - Triggered via bd_request_hw atomic flag</p> <p>bd_signal() (vfs_bio.c:4522): Wakes buffer daemons based on dirty space:</p> <pre><code>static void bd_signal(long totalspace)\n{\n    if (totalspace &gt; 0 &amp;&amp;\n        runningbufspace + dirtykvaspace &gt;= lodirtybufspace) {\n        atomic_set_int(&amp;bd_request, 1);\n        wakeup(&amp;bd_request);\n\n        if (dirtybufspacehw &gt; lodirtybufspace / 2) {\n            atomic_set_int(&amp;bd_request_hw, 1);\n            wakeup(&amp;bd_request_hw);\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-allocation","title":"Buffer Allocation","text":""},{"location":"sys/kern/vfs/buffer-cache/#getnewbuf-core-allocation","title":"getnewbuf() - Core Allocation","text":"<p>getnewbuf() (vfs_bio.c:1885)</p> <p>Allocates a buffer for use, reusing existing buffers when necessary:</p> <pre><code>struct buf *getnewbuf(int blkflags, int slptimeo, int size, int maxsize)\n</code></pre> <p>Allocation strategy:</p> <ol> <li> <p>Check space limits:    <pre><code>while (bufspace + maxsize &gt; hibufspace)\n    bufspacewakeup();  /* wait for space */\n</code></pre></p> </li> <li> <p>Try BQUEUE_EMPTY (fast path):</p> </li> <li>Pull empty buffer header</li> <li> <p>No data backing store yet</p> </li> <li> <p>Scan BQUEUE_CLEAN (reuse path):</p> </li> <li>Look for buffers with B_AGE (prefer aged buffers)</li> <li>Check lock availability (LK_NOWAIT)</li> <li> <p>Validate buffer can be reused:</p> <ul> <li>Not locked (B_LOCKED clear)</li> <li>Not in I/O (B_IOISSUED clear)</li> <li>Not referenced (b_refs == 0)</li> </ul> </li> <li> <p>Flush and wait (pressure path):</p> </li> <li>If no buffers available, flush BQUEUE_DIRTY</li> <li>Wait for buffers to become available</li> <li> <p>Retry allocation</p> </li> <li> <p>Initialize buffer:    <pre><code>bp-&gt;b_flags = B_CACHE;  /* Start cached */\nbp-&gt;b_cmd = BUF_CMD_DONE;\nbp-&gt;b_qindex = BQUEUE_NONE;\nbp-&gt;b_error = 0;\n</code></pre></p> </li> </ol> <p>Heavy-weight handling:</p> <p>Heavy-weight buffers (B_HEAVY) have restrictions: - May need additional buffers to complete write - Prevented from being allocated when dirty space is high - Separate daemon thread (bufdaemonhw_td) for flushing</p>"},{"location":"sys/kern/vfs/buffer-cache/#getblk-cached-block-access","title":"getblk() - Cached Block Access","text":"<p>getblk() (vfs_bio.c:2729)</p> <p>Main entry point for accessing cached filesystem blocks:</p> <pre><code>struct buf *getblk(struct vnode *vp, off_t loffset, int size, \n                   int blkflags, int slptimeo)\n</code></pre> <p>Lookup and allocation workflow:</p> <ol> <li> <p>Search vnode's buffer tree:    <pre><code>bp = findblk(vp, loffset, FINDBLK_NBLOCK);\nif (bp) {\n    /* Found in cache */\n    if (BUF_LOCK(bp, LK_EXCLUSIVE | LK_NOWAIT)) {\n        /* Lock contention, retry or wait */\n    }\n    return bp;  /* Cache hit */\n}\n</code></pre></p> </li> <li> <p>Allocate new buffer:    <pre><code>bp = getnewbuf(blkflags, slptimeo, size, maxsize);\n</code></pre></p> </li> <li> <p>Insert into vnode's tree:    <pre><code>lwkt_gettoken(&amp;vp-&gt;v_token);\n/* Re-check for race (someone else inserted) */\nbp2 = findblk(vp, loffset, 0);\nif (bp2) {\n    /* Lost race, use bp2 */\n    brelse(bp);\n    bp = bp2;\n} else {\n    /* Won race, insert bp */\n    bp-&gt;b_vp = vp;\n    bp-&gt;b_loffset = loffset;\n    buf_rb_tree_RB_INSERT(&amp;vp-&gt;v_rbclean_tree, bp);\n    buf_rb_hash_RB_INSERT(&amp;vp-&gt;v_rbhash_tree, bp);\n    bp-&gt;b_flags |= B_HASHED | B_VNCLEAN;\n}\nlwkt_reltoken(&amp;vp-&gt;v_token);\n</code></pre></p> </li> <li> <p>Handle size changes:</p> </li> <li> <p>If buffer exists but size doesn't match:</p> <ul> <li>GETBLK_SZMATCH: Return NULL</li> <li>Otherwise: Reallocate buffer with new size</li> </ul> </li> <li> <p>Initialize for use:    <pre><code>if ((bp-&gt;b_flags &amp; B_CACHE) == 0) {\n    /* Not valid, caller must issue I/O */\n    bp-&gt;b_flags &amp;= ~(B_ERROR | B_INVAL);\n    /* Don't set BUF_CMD_READ here, caller does it */\n}\n</code></pre></p> </li> </ol> <p>Flags: - <code>GETBLK_PCATCH</code>: Allow signals (can return NULL) - <code>GETBLK_BHEAVY</code>: Mark as heavy-weight buffer - <code>GETBLK_SZMATCH</code>: Fail if size doesn't match - <code>GETBLK_NOWAIT</code>: Non-blocking lock - <code>GETBLK_KVABIO</code>: Request KVABIO buffer</p> <p>Return states: - Buffer locked and ready for use - B_CACHE set if data valid - B_CACHE clear if I/O needed</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-io-operations","title":"Buffer I/O Operations","text":""},{"location":"sys/kern/vfs/buffer-cache/#bread-synchronous-read","title":"bread() - Synchronous Read","text":"<p>bread() (vfs_bio.c:857)</p> <p>Reads a single block synchronously:</p> <pre><code>int bread(struct vnode *vp, off_t loffset, int size, struct buf **bpp)\n</code></pre> <p>Workflow:</p> <pre><code>/* Get buffer (from cache or allocate) */\nbp = getblk(vp, loffset, size, 0, 0);\n\n/* If not in cache, issue I/O */\nif ((bp-&gt;b_flags &amp; B_CACHE) == 0) {\n    bp-&gt;b_flags &amp;= ~(B_ERROR | B_EINTR | B_INVAL);\n    bp-&gt;b_cmd = BUF_CMD_READ;\n    bp-&gt;b_bio1.bio_done = biodone_sync;\n    bp-&gt;b_bio1.bio_flags |= BIO_SYNC;\n    vfs_busy_pages(vp, bp);\n    vn_strategy(vp, &amp;bp-&gt;b_bio1);\n    error = biowait(&amp;bp-&gt;b_bio1, \"biord\");\n}\n\n*bpp = bp;  /* Return locked buffer */\nreturn error;\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#breadn-read-with-read-ahead","title":"breadn() - Read with Read-ahead","text":"<p>breadn() (vfs_bio.c:892)</p> <p>Reads a block with optional read-ahead:</p> <pre><code>int breadn(struct vnode *vp, off_t loffset, int size, int bflags,\n           off_t *raoffset, int *rabsize, int cnt, struct buf **bpp)\n</code></pre> <p>Read-ahead strategy:</p> <ol> <li> <p>Issue primary read (synchronous):    <pre><code>bp = getblk(vp, loffset, size, 0, 0);\nif ((bp-&gt;b_flags &amp; B_CACHE) == 0) {\n    /* Issue sync I/O */\n    vn_strategy(vp, &amp;bp-&gt;b_bio1);\n    readwait = 1;\n}\n</code></pre></p> </li> <li> <p>Issue read-ahead requests (asynchronous):    <pre><code>for (i = 0; i &lt; cnt; i++) {\n    if (inmem(vp, raoffset[i]))\n        continue;  /* Already cached */\n\n    rabp = getblk(vp, raoffset[i], rabsize[i], 0, 0);\n    if ((rabp-&gt;b_flags &amp; B_CACHE) == 0) {\n        rabp-&gt;b_cmd = BUF_CMD_READ;\n        BUF_KERNPROC(rabp);  /* Async, owned by kernel */\n        vn_strategy(vp, &amp;rabp-&gt;b_bio1);\n    } else {\n        brelse(rabp);  /* Already cached */\n    }\n}\n</code></pre></p> </li> <li> <p>Wait for primary read:    <pre><code>if (readwait)\n    error = biowait(&amp;bp-&gt;b_bio1, \"biord\");\n</code></pre></p> </li> </ol> <p>Read-ahead benefits: - Overlap disk I/O with CPU processing - Exploit sequential access patterns - Improve throughput for streaming reads</p>"},{"location":"sys/kern/vfs/buffer-cache/#bwrite-synchronous-write","title":"bwrite() - Synchronous Write","text":"<p>bwrite() (vfs_bio.c:963)</p> <p>Writes a buffer synchronously:</p> <pre><code>int bwrite(struct buf *bp)\n</code></pre> <p>Workflow:</p> <pre><code>if (bp-&gt;b_flags &amp; B_INVAL) {\n    brelse(bp);\n    return 0;\n}\n\n/* Clear errors, mark cached */\nbp-&gt;b_flags &amp;= ~(B_ERROR | B_EINTR);\nbp-&gt;b_flags |= B_CACHE;\nbp-&gt;b_cmd = BUF_CMD_WRITE;\nbp-&gt;b_bio1.bio_done = biodone_sync;\nbp-&gt;b_bio1.bio_flags |= BIO_SYNC;\n\nvfs_busy_pages(bp-&gt;b_vp, bp);\nbsetrunningbufspace(bp, bp-&gt;b_bufsize);  /* Account running space */\nvn_strategy(bp-&gt;b_vp, &amp;bp-&gt;b_bio1);\n\nerror = biowait(&amp;bp-&gt;b_bio1, \"biows\");\nbrelse(bp);\nreturn error;\n</code></pre> <p>Key points: - Waits for I/O completion - Sets B_CACHE (data valid after write) - Tracks running I/O space - Always releases buffer after completion</p>"},{"location":"sys/kern/vfs/buffer-cache/#bawrite-asynchronous-write","title":"bawrite() - Asynchronous Write","text":"<p>bawrite() (vfs_bio.c:1014)</p> <p>Writes a buffer asynchronously:</p> <pre><code>void bawrite(struct buf *bp)\n</code></pre> <p>Differences from bwrite(): - Does NOT wait for completion (no biowait) - Uses default biodone (not biodone_sync) - Marks buffer as kernel-owned (BUF_KERNPROC) - Returns immediately</p> <pre><code>bp-&gt;b_flags &amp;= ~(B_ERROR | B_EINTR);\nbp-&gt;b_flags |= B_CACHE;\nbp-&gt;b_cmd = BUF_CMD_WRITE;\nbp-&gt;b_bio1.bio_done = NULL;  /* Use default */\n\nvfs_busy_pages(bp-&gt;b_vp, bp);\nbsetrunningbufspace(bp, bp-&gt;b_bufsize);\nBUF_KERNPROC(bp);  /* Transfer to kernel */\nvn_strategy(bp-&gt;b_vp, &amp;bp-&gt;b_bio1);\n/* Returns immediately, I/O in progress */\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#bdwrite-delayed-write","title":"bdwrite() - Delayed Write","text":"<p>bdwrite() (vfs_bio.c:1060)</p> <p>Marks a buffer dirty for later writing:</p> <pre><code>void bdwrite(struct buf *bp)\n</code></pre> <p>Delayed write behavior:</p> <pre><code>bdirty(bp);  /* Mark B_DELWRI, move to dirty tree */\nbp-&gt;b_flags |= B_CACHE;\n\n/* Pre-map physical block to avoid deadlock during sync */\nif (bp-&gt;b_bio2.bio_offset == NOOFFSET) {\n    VOP_BMAP(bp-&gt;b_vp, bp-&gt;b_loffset, \n             &amp;bp-&gt;b_bio2.bio_offset, NULL, NULL, \n             BUF_CMD_WRITE);\n}\n\n/* Mark pages clean (earmarked for buffer flush) */\nvfs_clean_pages(bp);\nbqrelse(bp);  /* Release to dirty queue */\n</code></pre> <p>Why delay writes? - Batch multiple writes together - Allow cancellation (if file deleted) - Enable write clustering - Avoid synchronous delays</p> <p>VOP_BMAP call importance: - Pre-translates logical\u2192physical block mapping - Avoids needing buffers during sync (prevents deadlock) - Memory for indirect blocks may not be available during sync</p>"},{"location":"sys/kern/vfs/buffer-cache/#bdirty-mark-buffer-dirty","title":"bdirty() - Mark Buffer Dirty","text":"<p>bdirty() (vfs_bio.c:1163)</p> <p>Core function to mark a buffer dirty:</p> <pre><code>void bdirty(struct buf *bp)\n{\n    KASSERT(bp-&gt;b_qindex == BQUEUE_NONE, ...);\n\n    bp-&gt;b_flags &amp;= ~(B_RELBUF | B_NOCACHE);\n\n    if ((bp-&gt;b_flags &amp; B_DELWRI) == 0) {\n        lwkt_gettoken(&amp;bp-&gt;b_vp-&gt;v_token);\n        bp-&gt;b_flags |= B_DELWRI;\n        reassignbuf(bp);  /* Move from clean\u2192dirty tree */\n        lwkt_reltoken(&amp;bp-&gt;b_vp-&gt;v_token);\n\n        /* Update global counters */\n        atomic_add_long(&amp;dirtybufcount, 1);\n        atomic_add_long(&amp;dirtykvaspace, bp-&gt;b_kvasize);\n        atomic_add_long(&amp;dirtybufspace, bp-&gt;b_bufsize);\n        if (bp-&gt;b_flags &amp; B_HEAVY) {\n            atomic_add_long(&amp;dirtybufcounthw, 1);\n            atomic_add_long(&amp;dirtybufspacehw, bp-&gt;b_bufsize);\n        }\n\n        bd_heatup();  /* Signal buffer daemon */\n    }\n}\n</code></pre> <p>reassignbuf(): Moves buffer between vnode trees: - From: <code>vp-&gt;v_rbclean_tree</code>, <code>B_VNCLEAN</code> - To: <code>vp-&gt;v_rbdirty_tree</code>, <code>B_VNDIRTY</code></p>"},{"location":"sys/kern/vfs/buffer-cache/#buwrite-fake-write-tmpfs","title":"buwrite() - Fake Write (tmpfs)","text":"<p>buwrite() (vfs_bio.c:1127)</p> <p>Used by tmpfs to mark pages dirty without writing to disk:</p> <pre><code>void buwrite(struct buf *bp)\n{\n    /* Only for VMIO buffers */\n    if ((bp-&gt;b_flags &amp; B_VMIO) == 0 || (bp-&gt;b_flags &amp; B_DELWRI)) {\n        bdwrite(bp);\n        return;\n    }\n\n    /* Mark VM pages as needing commit */\n    for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n        m = bp-&gt;b_xio.xio_pages[i];\n        vm_page_need_commit(m);\n    }\n\n    bqrelse(bp);  /* Release without marking buffer dirty */\n}\n</code></pre> <p>Use case: - tmpfs stores data in VM pages, not disk - Pages need marking dirty for VM system - Buffer itself doesn't need writing</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-release","title":"Buffer Release","text":""},{"location":"sys/kern/vfs/buffer-cache/#brelse-standard-release","title":"brelse() - Standard Release","text":"<p>brelse() (vfs_bio.c:1268)</p> <p>Releases a buffer back to the cache:</p> <pre><code>void brelse(struct buf *bp)\n</code></pre> <p>Release workflow:</p> <ol> <li> <p>Clear transient flags:    <pre><code>bp-&gt;b_flags &amp;= ~(B_IOISSUED | B_EINTR | B_NOTMETA | B_KVABIO);\n</code></pre></p> </li> <li> <p>Handle B_NOCACHE (destroy request):    <pre><code>if (bp-&gt;b_flags &amp; B_NOCACHE) {\n    bp-&gt;b_flags |= B_INVAL;\n}\n</code></pre></p> </li> <li> <p>Handle B_INVAL (invalidate):    <pre><code>if (bp-&gt;b_flags &amp; B_INVAL) {\n    if (bp-&gt;b_flags &amp; (B_DELWRI | B_VNDIRTY))\n        bundirty(bp);  /* Remove from dirty tree */\n    if (bp-&gt;b_flags &amp; B_HASHED)\n        buf_rb_hash_RB_REMOVE(&amp;vp-&gt;v_rbhash_tree, bp);\n    if (bp-&gt;b_flags &amp; (B_VNCLEAN | B_VNDIRTY))\n        buf_rb_tree_RB_REMOVE(..., bp);\n    bp-&gt;b_vp = NULL;\n    bp-&gt;b_flags &amp;= ~(B_HASHED | B_VNCLEAN | B_VNDIRTY);\n}\n</code></pre></p> </li> <li> <p>Determine destination queue:    <pre><code>if (bp-&gt;b_flags &amp; B_LOCKED)\n    qindex = BQUEUE_LOCKED;\nelse if (bp-&gt;b_flags &amp; B_DELWRI)\n    qindex = (bp-&gt;b_flags &amp; B_HEAVY) ? \n             BQUEUE_DIRTY_HW : BQUEUE_DIRTY;\nelse if (bp-&gt;b_vp)\n    qindex = BQUEUE_CLEAN;\nelse\n    qindex = BQUEUE_EMPTY;\n</code></pre></p> </li> <li> <p>Insert into queue:    <pre><code>spin_lock(&amp;bufqspin);\nTAILQ_INSERT_TAIL(&amp;bufqueues[qindex], bp, b_freelist);\nspin_unlock(&amp;bufqspin);\nbp-&gt;b_qindex = qindex;\n</code></pre></p> </li> <li> <p>Unlock buffer:    <pre><code>BUF_UNLOCK(bp);\n</code></pre></p> </li> <li> <p>Wake waiters:    <pre><code>bufcountwakeup();  /* Wake anyone waiting for buffers */\nbufspacewakeup();  /* Wake anyone waiting for space */\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#bqrelse-quick-release","title":"bqrelse() - Quick Release","text":"<p>bqrelse() (vfs_bio.c:1564)</p> <p>Optimized release for buffers expected to be reused:</p> <pre><code>void bqrelse(struct buf *bp)\n</code></pre> <p>Difference from brelse(): - Doesn't set B_AGE flag - Leaves buffer in favorable position for reuse - Used for metadata that's likely to be accessed again soon</p>"},{"location":"sys/kern/vfs/buffer-cache/#vm-integration-vmio","title":"VM Integration (VMIO)","text":""},{"location":"sys/kern/vfs/buffer-cache/#vmio-buffers","title":"VMIO Buffers","text":"<p>Buffers can be backed by VM pages instead of pure KVA (B_VMIO flag):</p> <p>Benefits: - Unified buffer cache and page cache - Pages shared between mmap() and read()/write() - Better memory utilization - Supports direct I/O to user pages</p> <p>b_xio structure (sys/sys/xio.h): Manages the list of VM pages backing the buffer:</p> <pre><code>struct xio {\n    int xio_npages;                    /* Number of pages */\n    int xio_flags;                     /* Flags */\n    vm_page_t xio_pages[XIO_INTERNAL_PAGES];  /* Page array */\n    struct vm_page *xio_internal_pages;        /* Internal storage */\n};\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_vmio_alloc-allocate-vm-pages","title":"vfs_vmio_alloc() - Allocate VM Pages","text":"<p>vfs_vmio_alloc() (vfs_bio.c:2247)</p> <p>Allocates VM pages to back a buffer:</p> <pre><code>static int vfs_vmio_alloc(struct buf *bp, off_t loffset, \n                          int size, int bsize)\n</code></pre> <p>Allocation workflow:</p> <ol> <li> <p>Calculate page range:    <pre><code>vm_object_t obj = vp-&gt;v_object;\noff_t pgoff = loffset &amp; PAGE_MASK;\nsize_t npages = btoc(round_page(size + pgoff));\n</code></pre></p> </li> <li> <p>Allocate/lookup pages:    <pre><code>for (i = 0; i &lt; npages; i++) {\n    vm_pindex_t pg = btop(loffset) + i;\n    m = bio_page_alloc(bp, obj, pg, ...);\n    bp-&gt;b_xio.xio_pages[i] = m;\n}\n</code></pre></p> </li> <li> <p>Set buffer properties:    <pre><code>bp-&gt;b_flags |= B_VMIO;\nbp-&gt;b_data = (caddr_t)(pgoff + (vm_offset_t)m);\nbp-&gt;b_xio.xio_npages = npages;\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_bio_clrbuf-clear-buffer","title":"vfs_bio_clrbuf() - Clear Buffer","text":"<p>vfs_bio_clrbuf() (vfs_bio.c:729)</p> <p>Zeros a buffer and validates all pages:</p> <pre><code>void vfs_bio_clrbuf(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_VMIO) {\n        /* Zero VM pages */\n        for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n            m = bp-&gt;b_xio.xio_pages[i];\n            if (m-&gt;valid != VM_PAGE_BITS_ALL) {\n                pmap_zero_page(m-&gt;phys_addr);\n                m-&gt;valid = VM_PAGE_BITS_ALL;\n                m-&gt;dirty = 0;\n            }\n        }\n        bp-&gt;b_resid = 0;\n    } else {\n        /* Zero KVA */\n        clrbuf(bp);\n    }\n    bp-&gt;b_flags |= B_CACHE;\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_busy_pages-vfs_unbusy_pages","title":"vfs_busy_pages() / vfs_unbusy_pages()","text":"<p>vfs_busy_pages() (vfs_bio.c:4051)</p> <p>Prepares VM pages for I/O:</p> <pre><code>void vfs_busy_pages(struct vnode *vp, struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_VMIO) {\n        for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n            m = bp-&gt;b_xio.xio_pages[i];\n\n            if (bp-&gt;b_cmd == BUF_CMD_READ) {\n                m-&gt;flags &amp;= ~PG_ZERO;\n                vm_page_io_start(m);\n            } else {\n                vm_page_protect(m, VM_PROT_READ);\n                vm_page_io_start(m);\n            }\n        }\n    }\n}\n</code></pre> <p>vfs_unbusy_pages() (vfs_bio.c:4096)</p> <p>Completes I/O on VM pages:</p> <pre><code>void vfs_unbusy_pages(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_VMIO) {\n        for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n            m = bp-&gt;b_xio.xio_pages[i];\n            vm_page_io_finish(m);\n\n            if (bp-&gt;b_cmd == BUF_CMD_READ &amp;&amp; !error) {\n                /* Mark page valid after successful read */\n                m-&gt;valid = VM_PAGE_BITS_ALL;\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_vmio_release-release-vm-pages","title":"vfs_vmio_release() - Release VM Pages","text":"<p>vfs_vmio_release() (vfs_bio.c:1233)</p> <p>Releases VM pages when buffer destroyed:</p> <pre><code>static void vfs_vmio_release(struct buf *bp)\n{\n    for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n        m = bp-&gt;b_xio.xio_pages[i];\n        bp-&gt;b_xio.xio_pages[i] = NULL;\n\n        vm_page_busy_wait(m, FALSE, \"vmiorl\");\n\n        /* Free page if appropriate */\n        if (bp-&gt;b_flags &amp; (B_NOCACHE | B_DIRECT)) {\n            vm_page_try_to_free(m);\n        } else {\n            vm_page_try_to_cache(m);\n        }\n        vm_page_wakeup(m);\n    }\n    bp-&gt;b_xio.xio_npages = 0;\n    bp-&gt;b_flags &amp;= ~B_VMIO;\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#cluster-io","title":"Cluster I/O","text":""},{"location":"sys/kern/vfs/buffer-cache/#overview_1","title":"Overview","text":"<p>Cluster I/O optimization groups contiguous filesystem blocks into single I/O operations for improved throughput. Implemented in <code>sys/kern/vfs_cluster.c</code>.</p> <p>Benefits: - Reduces per-I/O overhead - Better utilizes disk bandwidth - Exploits spatial locality - Amortizes seek time over multiple blocks</p>"},{"location":"sys/kern/vfs/buffer-cache/#cluster-cache","title":"Cluster Cache","text":"<p>cluster_cache_t structure (vfs_cluster.c:70):</p> <pre><code>typedef struct cluster_cache {\n    off_t cc_loffset;          /* Logical offset (cluster start) */\n    off_t cc_lastloffset;      /* Last offset in cluster */\n    int cc_flags;              /* Flags */\n    struct vnode *cc_vp;       /* Vnode */\n    int cc_refs;               /* Reference count */\n} cluster_cache_t;\n\n#define CLUSTER_CACHE_SIZE 16\ncluster_cache_t cluster_array[CLUSTER_CACHE_SIZE];\n</code></pre> <p>Per-vnode cluster state: - Tracks sequential access patterns - Maintains read-ahead context - Cached in global array indexed by vnode</p> <p>cluster_getcache() / cluster_putcache(): Manage cluster cache entries</p>"},{"location":"sys/kern/vfs/buffer-cache/#cluster_read-clustered-read","title":"cluster_read() - Clustered Read","text":"<p>cluster_read() (vfs_cluster.c:224)</p> <p>Main entry point for clustered read operations:</p> <pre><code>int cluster_read(struct vnode *vp, off_t filesize, off_t loffset,\n                 int blksize, int totalbytes, int seqcount, \n                 struct buf **bpp)\n</code></pre> <p>Parameters: - <code>filesize</code>: Total file size - <code>loffset</code>: Logical offset to read - <code>blksize</code>: Filesystem block size - <code>totalbytes</code>: Total read size - <code>seqcount</code>: Sequential access count (for read-ahead) - <code>bpp</code>: Returns buffer pointer</p> <p>Read clustering workflow:</p> <ol> <li> <p>Check for existing buffer:    <pre><code>bp = getblk(vp, loffset, blksize, 0, 0);\nif (bp-&gt;b_flags &amp; B_CACHE) {\n    *bpp = bp;\n    return 0;  /* Cache hit */\n}\n</code></pre></p> </li> <li> <p>Determine cluster size:    <pre><code>/* Use seqcount to scale read-ahead */\nmaxra = seqcount * blksize;\nmaxra = min(maxra, MAXPHYS);  /* Cap at MAXPHYS (128KB) */\n</code></pre></p> </li> <li> <p>Build read-ahead list:    <pre><code>for (i = 1; i &lt; maxblocks; i++) {\n    if (inmem(vp, loffset + i * blksize))\n        break;  /* Stop at cached block */\n    raoffset[rablks] = loffset + i * blksize;\n    rabsize[rablks] = blksize;\n    rablks++;\n}\n</code></pre></p> </li> <li> <p>Issue clustered I/O:    <pre><code>error = cluster_rbuild(vp, filesize, bp, \n                       loffset, raoffset, rabsize, rablks);\n</code></pre></p> </li> <li> <p>Update cluster cache:    <pre><code>cc = cluster_getcache(NULL, vp, loffset);\ncc-&gt;cc_lastloffset = loffset + rablks * blksize;\ncluster_putcache(cc);\n</code></pre></p> </li> </ol>"},{"location":"sys/kern/vfs/buffer-cache/#cluster_rbuild-build-read-cluster","title":"cluster_rbuild() - Build Read Cluster","text":"<p>cluster_rbuild() (vfs_cluster.c:893)</p> <p>Constructs a clustered read I/O operation:</p> <pre><code>static int cluster_rbuild(struct vnode *vp, off_t filesize, \n                          struct buf *bp, off_t loffset,\n                          off_t *raoffset, int *rabsize, int rablks)\n</code></pre> <p>Clustering strategy:</p> <ol> <li> <p>Allocate read-ahead buffers:    <pre><code>for (i = 0; i &lt; rablks; i++) {\n    rabp = getblk(vp, raoffset[i], rabsize[i], 0, 0);\n    if (rabp-&gt;b_flags &amp; B_CACHE) {\n        brelse(rabp);\n        continue;  /* Skip cached */\n    }\n    rabp-&gt;b_flags |= B_RAM;  /* Mark read-ahead */\n    rabp-&gt;b_cmd = BUF_CMD_READ;\n    /* Link into cluster chain */\n    cluster_append(&amp;bp-&gt;b_bio1, rabp);\n}\n</code></pre></p> </li> <li> <p>Issue parent I/O:    <pre><code>bp-&gt;b_cmd = BUF_CMD_READ;\nbp-&gt;b_bio1.bio_done = cluster_callback;\nbp-&gt;b_flags |= B_CLUSTER;\nvfs_busy_pages(vp, bp);\nvn_strategy(vp, &amp;bp-&gt;b_bio1);\n</code></pre></p> </li> <li> <p>Cluster callback (cluster_callback):</p> </li> <li>Completes parent bio</li> <li>Iterates child bios (read-ahead buffers)</li> <li>Calls biodone() on each child</li> <li>Releases buffers</li> </ol> <p>Chain structure: - Parent buffer has bio chain (bio-&gt;bio_caller_info.cluster_head) - Child buffers linked via cluster_next - All complete when parent I/O finishes</p>"},{"location":"sys/kern/vfs/buffer-cache/#cluster_write-clustered-write","title":"cluster_write() - Clustered Write","text":"<p>cluster_write() (vfs_cluster.c:1244)</p> <p>Attempts to cluster a write operation with adjacent dirty buffers:</p> <pre><code>void cluster_write(struct buf *bp, off_t filesize, int blksize, int seqcount)\n</code></pre> <p>Write clustering workflow:</p> <ol> <li> <p>Check if clustering allowed:    <pre><code>if ((bp-&gt;b_flags &amp; B_CLUSTEROK) == 0)\n    goto out;  /* Filesystem doesn't allow clustering */\nif (bp-&gt;b_flags &amp; B_LOCKED)\n    goto out;  /* Locked buffer */\n</code></pre></p> </li> <li> <p>Scan for adjacent dirty buffers:    <pre><code>/* Scan backwards */\nfor (i = 1; i &lt;= maxback; i++) {\n    tbp = findblk(vp, loffset - i * blksize, FINDBLK_TEST);\n    if (!tbp || !(tbp-&gt;b_flags &amp; B_DELWRI))\n        break;\n    /* Collect buffer */\n}\n\n/* Scan forwards */\nfor (i = 1; i &lt;= maxahead; i++) {\n    tbp = findblk(vp, loffset + i * blksize, FINDBLK_TEST);\n    if (!tbp || !(tbp-&gt;b_flags &amp; B_DELWRI))\n        break;\n    /* Collect buffer */\n}\n</code></pre></p> </li> <li> <p>Build cluster:    <pre><code>cluster_wbuild(vp, bpp, numblks, start_loffset, blksize);\n</code></pre></p> </li> <li> <p>Issue clustered write:    <pre><code>if (nblocks == 1) {\n    /* Single block, issue normally */\n    bawrite(bp);\n} else {\n    /* Multi-block cluster */\n    for each buffer in cluster:\n        cluster_append(&amp;parent-&gt;b_bio1, child);\n    vn_strategy(vp, &amp;parent-&gt;b_bio1);\n}\n</code></pre></p> </li> </ol> <p>Write clustering benefits: - Reduces write overhead - Better disk utilization - Elevator seeking optimization - Improved metadata write performance</p>"},{"location":"sys/kern/vfs/buffer-cache/#cluster_awrite-asynchronous-cluster-write","title":"cluster_awrite() - Asynchronous Cluster Write","text":"<p>cluster_awrite() (vfs_cluster.c:1418)</p> <p>Called during buffer flushing to attempt clustering:</p> <pre><code>void cluster_awrite(struct buf *bp)\n{\n    /* If already doing I/O, skip */\n    if (bp-&gt;b_flags &amp; B_IOISSUED)\n        return;\n\n    /* Try to build cluster */\n    cluster_write(bp, filesize, blksize, seqcount);\n}\n</code></pre> <p>Called by buf_daemon when flushing BQUEUE_DIRTY.</p>"},{"location":"sys/kern/vfs/buffer-cache/#sequential-access-detection","title":"Sequential Access Detection","text":"<p>sequential_heuristic() (vfs_vnops.c):</p> <p>Filesystem code tracks sequential access via VOP_READ/VOP_WRITE:</p> <pre><code>int seqcount = (bp-&gt;b_flags &amp; B_SEQMASK) &gt;&gt; B_SEQSHIFT;\nif (loffset == last_loffset + blksize)\n    seqcount = min(seqcount + 1, B_SEQMAX);  /* 127 max */\nelse\n    seqcount = 0;  /* Reset on non-sequential */\n</code></pre> <p>seqcount usage: - 0: Random access, minimal read-ahead - &gt;0: Sequential, scale read-ahead proportionally - Max (127): Aggressive read-ahead (127 * blksize)</p>"},{"location":"sys/kern/vfs/buffer-cache/#bio-operations","title":"BIO Operations","text":""},{"location":"sys/kern/vfs/buffer-cache/#bio-layer-overview","title":"BIO Layer Overview","text":"<p>The BIO (Block I/O) layer provides a flexible mechanism for I/O request transformation and stacking.</p> <p>struct bio (sys/sys/bio.h):</p> <pre><code>struct bio {\n    struct bio *bio_next;          /* Next bio in chain */\n    struct bio *bio_prev;          /* Previous bio */\n    off_t bio_offset;              /* Logical offset (or block number) */\n    struct buf *bio_buf;           /* Associated buffer */\n    bio_track_t *bio_track;        /* I/O tracking */\n    void (*bio_done)(struct bio *); /* Completion callback */\n    void *bio_caller_info1;        /* Caller private data */\n    union {\n        void *cluster_head;        /* Cluster I/O head */\n        void *cluster_parent;      /* Cluster parent */\n    } bio_caller_info;\n    u_int bio_flags;               /* BIO flags */\n    ...\n};\n</code></pre> <p>BIO flags: - <code>BIO_SYNC</code> - Synchronous I/O - <code>BIO_WANT</code> - Want notification on completion - <code>BIO_DONE</code> - I/O completed</p>"},{"location":"sys/kern/vfs/buffer-cache/#bio-callbacks","title":"Bio Callbacks","text":"<p>biodone() (vfs_bio.c:4201)</p> <p>Default I/O completion handler:</p> <pre><code>void biodone(struct bio *bio)\n{\n    struct buf *bp = bio-&gt;bio_buf;\n\n    /* Call bio-specific done function if present */\n    if (bio-&gt;bio_done) {\n        bio-&gt;bio_done(bio);\n        return;\n    }\n\n    /* Default completion */\n    bufdone(bp);  /* Complete buffer I/O */\n}\n</code></pre> <p>biodone_sync() (vfs_bio.c:4226)</p> <p>Synchronous I/O completion:</p> <pre><code>void biodone_sync(struct bio *bio)\n{\n    bio-&gt;bio_flags |= BIO_DONE;\n    wakeup(bio);  /* Wake biowait() */\n}\n</code></pre> <p>biowait() (vfs_bio.c:4243)</p> <p>Wait for synchronous I/O completion:</p> <pre><code>int biowait(struct bio *bio, const char *wmesg)\n{\n    while ((bio-&gt;bio_flags &amp; BIO_DONE) == 0)\n        tsleep(bio, 0, wmesg, 0);\n\n    if (bio-&gt;bio_flags &amp; BIO_ERROR)\n        return bio-&gt;bio_buf-&gt;b_error;\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#bio-stacking","title":"Bio Stacking","text":"<p>Device drivers and filesystems can push additional BIO layers:</p> <pre><code>/* Filesystem layer (logical offset) */\nvn_strategy(vp, &amp;bp-&gt;b_bio1);\n    \u2193\n/* Filesystem translates bio1 \u2192 bio2 */\nVOP_STRATEGY(devvp, &amp;bp-&gt;b_bio2);\n    \u2193\n/* Device driver handles bio2 (physical offset) */\ndev_dstrategy(...);\n</code></pre> <p>Example: HAMMER filesystem 1. bio1: File logical offset 2. bio2: HAMMER volume offset 3. bio3: Device physical offset</p>"},{"location":"sys/kern/vfs/buffer-cache/#buffer-flushing","title":"Buffer Flushing","text":""},{"location":"sys/kern/vfs/buffer-cache/#buf_daemon-main-flush-thread","title":"buf_daemon() - Main Flush Thread","text":"<p>buf_daemon() (vfs_bio.c:4566)</p> <p>Kernel thread that flushes dirty buffers:</p> <pre><code>static void buf_daemon(void)\n{\n    for (;;) {\n        /* Sleep until work needed */\n        tsleep(&amp;bd_request, 0, \"psleep\", hz);\n\n        /* Check if flushing needed */\n        if (runningbufspace + dirtykvaspace &lt; lodirtybufspace)\n            continue;\n\n        /* Flush dirty buffers */\n        flushbufqueues(NULL, BQUEUE_DIRTY);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#flushbufqueues-queue-flusher","title":"flushbufqueues() - Queue Flusher","text":"<p>flushbufqueues() (vfs_bio.c:4330)</p> <p>Scans a buffer queue and flushes dirty buffers:</p> <pre><code>static int flushbufqueues(struct buf *marker, bufq_type_t q)\n{\n    int flushed = 0;\n\n    /* Scan per-CPU queues */\n    for (cpu = 0; cpu &lt; ncpus; cpu++) {\n        TAILQ_FOREACH(bp, &amp;bufpcpu[cpu].bufqueues[q], b_freelist) {\n            if (bp-&gt;b_flags &amp; B_MARKER)\n                continue;\n            if (bp-&gt;b_flags &amp; B_DELWRI) {\n                /* Remove from queue */\n                bremfree(bp);\n\n                /* Try to lock */\n                if (BUF_LOCK(bp, LK_EXCLUSIVE | LK_NOWAIT))\n                    continue;  /* Skip if locked */\n\n                /* Cluster and write */\n                cluster_awrite(bp);\n                flushed++;\n\n                if (flushed &gt;= flushperqueue)\n                    break;  /* Flushed enough */\n            }\n        }\n    }\n\n    return flushed;\n}\n</code></pre> <p>Flush triggers: - <code>dirtybufspace &gt; hidirtybufspace</code> - High watermark exceeded - System sync operation (sync(2) system call) - Vnode reclamation (vnode has dirty buffers) - Filesystem-specific sync (VFS_SYNC)</p>"},{"location":"sys/kern/vfs/buffer-cache/#vfs_sync-filesystem-sync","title":"VFS_SYNC - Filesystem Sync","text":"<p>VFS_SYNC() vnode operation:</p> <p>Called to sync a filesystem:</p> <pre><code>int VFS_SYNC(struct mount *mp, int waitfor)\n</code></pre> <p>Typical implementation:</p> <pre><code>static int myfs_sync(struct mount *mp, int waitfor)\n{\n    /* Sync inodes */\n    myfs_sync_inodes(mp, waitfor);\n\n    /* Scan dirty vnodes */\n    sync_info.waitfor = waitfor;\n    vmntvnodescan(mp, VMSC_GETVP, NULL, myfs_sync_callback, &amp;sync_info);\n\n    /* Write superblock */\n    myfs_write_superblock(mp);\n\n    return 0;\n}\n</code></pre> <p>waitfor values: - <code>MNT_WAIT</code>: Wait for all I/O to complete - <code>MNT_NOWAIT</code>: Initiate I/O but don't wait - <code>MNT_LAZY</code>: Lazy sync (metadata only)</p>"},{"location":"sys/kern/vfs/buffer-cache/#page-cleaning","title":"Page Cleaning","text":""},{"location":"sys/kern/vfs/buffer-cache/#vfs_clean_pages-mark-pages-clean","title":"vfs_clean_pages() - Mark Pages Clean","text":"<p>vfs_clean_pages() (vfs_bio.c:3980)</p> <p>Marks VM pages clean after buffer written:</p> <pre><code>static void vfs_clean_pages(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_VMIO) {\n        for (i = 0; i &lt; bp-&gt;b_xio.xio_npages; i++) {\n            m = bp-&gt;b_xio.xio_pages[i];\n            vfs_clean_one_page(bp, i, m);\n        }\n    }\n}\n</code></pre> <p>vfs_clean_one_page() (vfs_bio.c:4002):</p> <pre><code>static void vfs_clean_one_page(struct buf *bp, int pageno, vm_page_t m)\n{\n    int soff, eoff;\n\n    /* Calculate page-relative dirty range */\n    soff = max(bp-&gt;b_dirtyoff - pageno * PAGE_SIZE, 0);\n    eoff = min(bp-&gt;b_dirtyend - pageno * PAGE_SIZE, PAGE_SIZE);\n\n    if (eoff &gt; soff) {\n        /* Mark page range clean */\n        vm_page_set_valid(m, soff, eoff - soff);\n        vm_page_clear_dirty(m, soff, eoff - soff);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#page-validity","title":"Page Validity","text":"<p>VM pages have validity bits tracking which parts contain valid data:</p> <p>vm_page-&gt;valid bitmask: - One bit per 512-byte sector (DEV_BSIZE) - <code>VM_PAGE_BITS_ALL</code> (0xFF): Entire page valid - Partial validity supported</p> <p>vm_page_set_valid(): Sets validity bits for byte range</p> <p>vm_page_clear_dirty(): Clears dirty bits for byte range</p>"},{"location":"sys/kern/vfs/buffer-cache/#kvabio-api","title":"KVABIO API","text":""},{"location":"sys/kern/vfs/buffer-cache/#overview_2","title":"Overview","text":"<p>KVABIO (Kernel Virtual Address BIO) allows efficient buffer access across CPUs without explicit synchronization.</p> <p>Problem: Regular buffers (b_data) may be accessed via different CPUs: - Data written on CPU0 - Buffer passed to CPU1 - CPU1 reads stale data from cache</p> <p>Solution: KVABIO API provides: - Explicit synchronization primitives - Per-CPU data mapping tracking - Automatic cache coherency</p>"},{"location":"sys/kern/vfs/buffer-cache/#kvabio-functions","title":"KVABIO Functions","text":"<p>bkvasync() (vfs_bio.c):</p> <p>Synchronizes buffer data for current CPU:</p> <pre><code>void bkvasync(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_KVABIO) {\n        /* Ensure data visible to current CPU */\n        cpu_lfence();  /* Load fence */\n    }\n}\n</code></pre> <p>bkvasync_all() (vfs_bio.c):</p> <p>Synchronizes buffer data for all CPUs:</p> <pre><code>void bkvasync_all(struct buf *bp)\n{\n    if (bp-&gt;b_flags &amp; B_KVABIO) {\n        /* Flush to all CPUs */\n        cpu_sfence();  /* Store fence */\n        /* IPI to other CPUs if needed */\n    }\n}\n</code></pre> <p>Usage: - Call bkvasync() before reading bp-&gt;b_data - Call bkvasync_all() after writing bp-&gt;b_data - Only needed for B_KVABIO buffers</p>"},{"location":"sys/kern/vfs/buffer-cache/#performance-considerations","title":"Performance Considerations","text":""},{"location":"sys/kern/vfs/buffer-cache/#buffer-cache-sizing","title":"Buffer Cache Sizing","text":"<p>Optimal buffer cache size: - Default: ~10% of physical RAM - Minimum (lobufspace): 4 MB - Maximum (maxbufspace): Computed from available RAM - Adjust via sysctl: <code>vfs.maxbufspace</code>, <code>vfs.hibufspace</code></p> <p>Trade-offs: - Larger cache: Better hit rate, less I/O - Smaller cache: More RAM for VM page cache - Balance based on workload</p>"},{"location":"sys/kern/vfs/buffer-cache/#dirty-buffer-limits","title":"Dirty Buffer Limits","text":"<p>Configure dirty limits: - <code>vfs.lodirtybufspace</code>: When to start flushing (default: ~5% RAM) - <code>vfs.hidirtybufspace</code>: Aggressive flushing (default: ~10% RAM) - <code>vfs.dirtybufspace</code>: Current dirty space (read-only)</p> <p>Why limit dirty buffers? - Prevent memory exhaustion - Bound data loss on crash - Ensure write progress - Avoid sync stalls</p>"},{"location":"sys/kern/vfs/buffer-cache/#read-ahead-tuning","title":"Read-ahead Tuning","text":"<p>Sequential detection: - Tracked via seqcount (0-127) - Scales read-ahead: seqcount * blksize - Maximum read-ahead: MAXPHYS (128KB typically)</p> <p>Read-ahead benefits: - Hides disk latency - Improves streaming performance - Minimal overhead for random access</p> <p>Disable read-ahead: - For random workloads - Flash storage with fast random access - Set <code>vfs.read_max</code> lower</p>"},{"location":"sys/kern/vfs/buffer-cache/#write-clustering","title":"Write Clustering","text":"<p>Enable clustering: - Filesystem sets B_CLUSTEROK on buffers - bdwrite() for delayed writes - buf_daemon clusters during flush</p> <p>Benefits: - Reduces write overhead - Larger I/O sizes - Better disk scheduling</p> <p>When not to cluster: - Synchronous writes (bwrite) - Small files (&lt;128KB) - Random write patterns</p>"},{"location":"sys/kern/vfs/buffer-cache/#error-handling","title":"Error Handling","text":""},{"location":"sys/kern/vfs/buffer-cache/#io-errors","title":"I/O Errors","text":"<p>Error propagation:</p> <pre><code>/* I/O completes with error */\nbiodone() {\n    bio-&gt;bio_flags |= BIO_ERROR;\n    bp-&gt;b_error = EIO;  /* Or specific error */\n    bp-&gt;b_flags |= B_ERROR;\n    bufdone(bp);\n}\n\n/* Sync I/O: Error returned */\nerror = biowait(bio, \"biord\");\nif (error)\n    return error;\n\n/* Async I/O: Error logged, buffer marked invalid */\nbiodone() {\n    if (bp-&gt;b_flags &amp; B_ERROR) {\n        bp-&gt;b_flags |= B_INVAL;  /* Invalidate */\n        /* Error may be logged by filesystem */\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#retry-strategies","title":"Retry Strategies","text":"<p>Filesystem-level retry: - VFS layers don't retry automatically - Filesystem must detect error and retry - Example: NFS retries on EIO</p> <p>User-level retry: - read(2)/write(2) return -1 with errno - Application decides retry policy</p>"},{"location":"sys/kern/vfs/buffer-cache/#debugging","title":"Debugging","text":""},{"location":"sys/kern/vfs/buffer-cache/#buffer-state-inspection","title":"Buffer State Inspection","text":"<p>DDB commands (when kernel debugger active):</p> <pre><code>db&gt; show buffer &lt;addr&gt;       # Display buffer state\ndb&gt; show allbufs             # List all buffers\ndb&gt; show lockedbufs          # List locked buffers\ndb&gt; show dirtybufs           # List dirty buffers\n</code></pre> <p>Sysctl inspection:</p> <pre><code># Buffer cache statistics\nsysctl vfs.nbuf              # Total buffers\nsysctl vfs.bufspace          # Current space used\nsysctl vfs.dirtybufspace     # Dirty buffer space\nsysctl vfs.dirtybufcount     # Dirty buffer count\nsysctl vfs.runningbufspace   # Running I/O space\nsysctl vfs.runningbufcount   # Running I/O count\n\n# Tuning parameters\nsysctl vfs.maxbufspace       # Max buffer space\nsysctl vfs.hibufspace        # High watermark\nsysctl vfs.lodirtybufspace   # Dirty low watermark\nsysctl vfs.hidirtybufspace   # Dirty high watermark\n</code></pre>"},{"location":"sys/kern/vfs/buffer-cache/#common-issues","title":"Common Issues","text":"<p>Issue: System hangs during sync - Cause: Deadlock in buffer allocation - Debug: Check runningbufspace vs hirunningspace - Solution: Increase vfs.hirunningspace</p> <p>Issue: Poor write performance - Cause: Not clustering writes - Debug: Check if B_CLUSTEROK set on buffers - Solution: Enable write clustering in filesystem</p> <p>Issue: Excessive read-ahead - Cause: High seqcount on random workload - Debug: Monitor vfs.lowmempgallocs - Solution: Reduce MAXPHYS or tune read-ahead</p> <p>Issue: Buffer cache thrashing - Cause: Working set larger than cache - Debug: Monitor vfs.getnewbufcalls - Solution: Increase vfs.maxbufspace</p>"},{"location":"sys/kern/vfs/buffer-cache/#summary","title":"Summary","text":"<p>The VFS buffer cache is a sophisticated subsystem providing:</p> <ol> <li>Caching: Filesystem block caching with LRU eviction</li> <li>I/O Management: Sync/async I/O primitives (bread, bwrite, bdwrite)</li> <li>VM Integration: Unified buffer/page cache via VMIO</li> <li>Clustering: Read-ahead and write clustering for performance</li> <li>Dirty Tracking: Write-behind with configurable watermarks</li> <li>Multi-threading: Per-CPU queues and dedicated flush threads</li> <li>Flexibility: BIO layer enables device stacking and transformation</li> </ol> <p>The buffer cache sits at a critical junction between filesystems, the VM system, and device drivers, providing high-performance cached I/O while maintaining data consistency and integrity.</p> <p>Key design principles: - Lock-free fast paths: Atomic operations and per-CPU structures - Unified caching: VMIO integrates buffer and page caches - Asynchronous I/O: Pipeline writes, overlap operations - Adaptive behavior: Sequential detection, watermark-based flushing - Layered I/O: BIO translation enables complex storage stacks</p>"},{"location":"sys/kern/vfs/journaling/","title":"VFS Journaling System","text":""},{"location":"sys/kern/vfs/journaling/#overview","title":"Overview","text":"<p>The DragonFly BSD VFS journaling system provides a flexible infrastructure for recording filesystem operations to a journal stream. This enables features like:</p> <ul> <li>Transaction logging - Record all filesystem changes</li> <li>Replication - Stream changes to remote systems</li> <li>Crash recovery - Replay operations after system failures</li> <li>Auditing - Track all filesystem modifications</li> <li>Two-way acknowledgement - Full-duplex journaling with commit confirmation</li> </ul> <p>The journaling layer sits between VOP wrappers and the underlying filesystem, transparently intercepting and recording operations before passing them through to the actual filesystem implementation.</p> <p>Key components: - Memory FIFO - Circular buffer for batching journal records - Worker threads - Asynchronous write-out to journal targets - Stream records - Structured format for journal data - Transaction IDs - Sequencing and acknowledgement - Subrecords - Nested transaction structure</p> <p>Key files: - <code>sys/kern/vfs_journal.c</code> - Core journaling infrastructure and FIFO management - <code>sys/kern/vfs_jops.c</code> - Journal VOP implementations - <code>sys/sys/journal.h</code> - Journaling data structures and protocol definitions</p>"},{"location":"sys/kern/vfs/journaling/#architecture","title":"Architecture","text":""},{"location":"sys/kern/vfs/journaling/#layered-design","title":"Layered Design","text":"<pre><code>User/Kernel VFS calls\n        \u2193\nVOP Wrappers (vfs_vopops.c)\n        \u2193\nJournal Layer (vfs_jops.c) \u2190 If journaling enabled\n        \u2193\nUnderlying Filesystem\n        \u2193\nDisk/Storage\n</code></pre> <p>When journaling is enabled for a mount point: 1. Mount point's <code>mnt_vn_journal_ops</code> is set to <code>journal_vnode_vops</code> 2. VOP operations are intercepted by journal functions 3. Journal records operation details to memory FIFO 4. Worker thread writes FIFO contents to journal target 5. Operation is passed to underlying filesystem</p>"},{"location":"sys/kern/vfs/journaling/#memory-fifo-structure","title":"Memory FIFO Structure","text":"<p>The memory FIFO is a circular buffer that batches journal records before writing to the target:</p> <pre><code>struct journal_fifo {\n    char *membase;      // Base of circular buffer\n    size_t size;        // Total buffer size (power of 2)\n    size_t mask;        // Size - 1 (for wrapping)\n    int64_t windex;     // Write index (monotonically increasing)\n    int64_t rindex;     // Read index (what's been written out)\n    int64_t xindex;     // Acknowledgement index (what's been committed)\n};\n</code></pre> <p>Index relationships: - <code>windex &gt;= rindex &gt;= xindex</code> (always) - Available write space: <code>size - (windex - xindex)</code> - Unwritten data: <code>windex - rindex</code> - Unacknowledged data: <code>rindex - xindex</code></p> <p>Key properties: - Indices never decrease (monotonically increasing) - Wrapping uses mask: <code>physicaloffset = index &amp; mask</code> - 16-byte alignment for all records - Incomplete records block worker thread progress</p>"},{"location":"sys/kern/vfs/journaling/#journal-records","title":"Journal Records","text":""},{"location":"sys/kern/vfs/journaling/#stream-record-structure","title":"Stream Record Structure","text":"<p>Stream records are the fundamental unit of journaling:</p> <pre><code>struct journal_rawrecbeg {\n    u_int16_t begmagic;     // JREC_BEGMAGIC (0x1234) or INCOMPLETE\n    u_int16_t streamid;     // Stream ID + control bits\n    int32_t recsize;        // Total record size (includes header/trailer)\n    int64_t transid;        // Sequence/transaction ID\n    // ... payload data ...\n};\n\nstruct journal_rawrecend {\n    u_int16_t endmagic;     // JREC_ENDMAGIC (0xCDEF)\n    u_int16_t check;        // Checksum (0 = disabled)\n    int32_t recsize;        // Same as rawrecbeg-&gt;recsize\n};\n</code></pre> <p>Record layout: <pre><code>+-------------------+\n| rawrecbeg (16B)   | Header\n+-------------------+\n| Payload data      | Variable size\n| (subrecords)      |\n+-------------------+\n| rawrecend (8B)    | Trailer\n+-------------------+\nTotal: 16-byte aligned\n</code></pre></p> <p>Magic numbers: - <code>JREC_BEGMAGIC (0x1234)</code> - Valid record ready to write - <code>JREC_INCOMPLETEMAGIC (0xFFFF)</code> - Reserved but not yet committed - <code>JREC_ENDMAGIC (0xCDEF)</code> - End marker for reverse scanning</p>"},{"location":"sys/kern/vfs/journaling/#stream-control-bits","title":"Stream Control Bits","text":"<p>The <code>streamid</code> field combines control bits and stream identifier:</p> <pre><code>#define JREC_STREAMCTL_BEGIN    0x8000  // Start of logical stream\n#define JREC_STREAMCTL_END      0x4000  // End of logical stream\n#define JREC_STREAMCTL_ABORTED  0x2000  // Stream was aborted\n#define JREC_STREAMID_MASK      0x1FFF  // Actual stream ID (bits 0-12)\n</code></pre> <p>Stream lifecycle: - Single record: <code>BEGIN | END</code> set (complete transaction in one record) - Multi-record: First has <code>BEGIN</code>, intermediate have neither, last has <code>END</code> - Aborted: Last record has <code>ABORTED | END</code></p>"},{"location":"sys/kern/vfs/journaling/#special-stream-ids","title":"Special Stream IDs","text":"<pre><code>#define JREC_STREAMID_PAD       0x0001  // Padding (FIFO wrap-around)\n#define JREC_STREAMID_SYNCPT    0x0000  // Synchronization point\n#define JREC_STREAMID_DISCONT   0x0002  // Discontinuity marker\n#define JREC_STREAMID_ACK       0x0004  // Acknowledgement record\n#define JREC_STREAMID_RESTART   0x0005  // Journal restart marker\n\n// Filesystem operation streams: 0x0100 - 0x1FFF\n</code></pre>"},{"location":"sys/kern/vfs/journaling/#subrecords","title":"Subrecords","text":"<p>Within a stream record, operations are broken down into subrecords:</p> <pre><code>struct journal_subrecord {\n    u_int16_t rectype;      // Control bits + type\n    int16_t reserved;       // Future use\n    int32_t recsize;        // Subrecord size\n    // ... type-specific data ...\n};\n</code></pre> <p>Subrecord control bits: <pre><code>#define JMASK_NESTED    0x8000  // Contains nested subrecords\n#define JMASK_LAST      0x4000  // Last subrecord in group\n</code></pre></p> <p>Common subrecord types: - <code>JTYPE_SETATTR</code> - Attribute changes (nested) - <code>JTYPE_WRITE</code> - File write operation (nested) - <code>JTYPE_CREATE</code> - File creation (nested) - <code>JTYPE_REMOVE</code> - File removal (nested) - <code>JTYPE_RENAME</code> - File rename (nested) - <code>JTYPE_UNDO</code> - Undo information (nested) - <code>JLEAF_FILEDATA</code> - File content data (leaf) - <code>JLEAF_PATH1/2/3/4</code> - Pathname components (leaf) - <code>JLEAF_UID/GID</code> - User/group IDs (leaf)</p>"},{"location":"sys/kern/vfs/journaling/#fifo-management","title":"FIFO Management","text":""},{"location":"sys/kern/vfs/journaling/#reservation-process","title":"Reservation Process","text":"<p>Located at <code>sys/kern/vfs_journal.c:496</code>.</p> <p>Function: <code>journal_reserve()</code></p> <p>The reservation process ensures thread-safe allocation of FIFO space:</p> <ol> <li> <p>Calculate required space: <pre><code>total_bytes = header_size + payload_size + trailer_size;\naligned_bytes = (total_bytes + 15) &amp; ~15;  // 16-byte align\n</code></pre></p> </li> <li> <p>Check for wrap-around: <pre><code>availtoend = fifo_size - (windex &amp; fifo_mask);\nif (bytes &gt; availtoend) {\n    req = bytes + availtoend;  // Need pad record at end\n}\n</code></pre></p> </li> <li> <p>Wait for space if needed: <pre><code>avail = fifo_size - (windex - xindex);\nif (avail &lt; req) {\n    jo-&gt;flags |= MC_JOURNAL_WWAIT;\n    tsleep(&amp;jo-&gt;fifo.windex, 0, \"jwrite\", 0);\n}\n</code></pre></p> </li> <li> <p>Create pad record if wrapping:</p> </li> <li>Pad record fills dead space at end of FIFO</li> <li>Has valid transaction ID for sequencing</li> <li> <p>Worker thread skips pad records</p> </li> <li> <p>Reserve space: <pre><code>rawp-&gt;begmagic = JREC_INCOMPLETEMAGIC;  // Blocks worker thread\nrawp-&gt;recsize = bytes;\nrawp-&gt;streamid = streamid | JREC_STREAMCTL_BEGIN;\nrawp-&gt;transid = jo-&gt;transid;\nwindex += aligned_bytes;\n</code></pre></p> </li> <li> <p>Return pointer to payload area: <pre><code>return (rawp + 1);  // Skip header\n</code></pre></p> </li> </ol> <p>Key insight: The incomplete magic prevents the worker thread from writing past this record until it's committed, allowing the caller to populate the record at leisure.</p>"},{"location":"sys/kern/vfs/journaling/#extension-and-truncation","title":"Extension and Truncation","text":"<p>Located at <code>sys/kern/vfs_journal.c:610</code>.</p> <p>Function: <code>journal_extend()</code></p> <p>Streams can be extended after initial reservation:</p> <p>Case 1: Simple extension (no size class change) <pre><code>if (new_aligned_size == old_aligned_size) {\n    rawp-&gt;recsize += bytes;  // Just update size\n    return (payload + truncbytes);\n}\n</code></pre></p> <p>Case 2: FIFO still at our record (can adjust windex) <pre><code>if (windex is still at end of our record) {\n    windex += (new_size - old_size);\n    rawp-&gt;recsize += bytes;\n    return (payload + truncbytes);\n}\n</code></pre></p> <p>Case 3: Must create new stream record <pre><code>// Commit current record (marked END)\njournal_commit(jo, rawpp, truncbytes, 0);\n\n// Create new continuing record (no BEGIN mark)\nrptr = journal_reserve(jo, rawpp, streamid, bytes);\nrawp-&gt;streamid &amp;= ~JREC_STREAMCTL_BEGIN;\n</code></pre></p> <p>This creates a multi-record stream where records share the same stream ID but only the first has <code>BEGIN</code> and only the last has <code>END</code>.</p>"},{"location":"sys/kern/vfs/journaling/#commit-process","title":"Commit Process","text":"<p>Located at <code>sys/kern/vfs_journal.c:712</code>.</p> <p>Function: <code>journal_commit()</code></p> <p>Committing a record makes it visible to the worker thread:</p> <ol> <li> <p>Truncate if requested: <pre><code>if (bytes &gt;= 0) {\n    new_recsize = bytes + header_size + trailer_size;\n    new_aligned = (new_recsize + 15) &amp; ~15;\n}\n</code></pre></p> </li> <li> <p>Handle freed space:</p> </li> <li>If windex still at our record: Back-index windex</li> <li> <p>Otherwise: Create pad record in dead space</p> </li> <li> <p>Fill in trailer: <pre><code>rendp = (char *)rawp + aligned_size - sizeof(*rendp);\nrendp-&gt;endmagic = JREC_ENDMAGIC;\nrendp-&gt;recsize = rawp-&gt;recsize;\nrendp-&gt;check = 0;  // Checksum (currently disabled)\n</code></pre></p> </li> <li> <p>Mark stream end if closeout: <pre><code>if (closeout)\n    rawp-&gt;streamid |= JREC_STREAMCTL_END;\n</code></pre></p> </li> <li> <p>Commit with memory barrier: <pre><code>cpu_sfence();  // Ensure trailer written before magic\nrawp-&gt;begmagic = JREC_BEGMAGIC;  // Makes record visible\n</code></pre></p> </li> <li> <p>Wake worker if needed:</p> </li> <li>If FIFO more than half full</li> <li>If threads waiting for space (<code>MC_JOURNAL_WWAIT</code>)</li> </ol>"},{"location":"sys/kern/vfs/journaling/#abort-process","title":"Abort Process","text":"<p>Located at <code>sys/kern/vfs_journal.c:678</code>.</p> <p>Function: <code>journal_abort()</code></p> <p>Aborts can optimize away uncommitted records:</p> <p>Case 1: Can reverse windex (record at end of FIFO) <pre><code>if (is_begin &amp;&amp; windex == end_of_our_record) {\n    windex -= aligned_size;  // Completely remove record\n    *rawpp = NULL;\n}\n</code></pre></p> <p>Case 2: Must mark as aborted <pre><code>else {\n    rawp-&gt;streamid |= JREC_STREAMCTL_ABORTED;\n    journal_commit(jo, rawpp, 0, 1);  // Commit with 0 payload\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#worker-threads","title":"Worker Threads","text":""},{"location":"sys/kern/vfs/journaling/#write-worker-thread","title":"Write Worker Thread","text":"<p>Located at <code>sys/kern/vfs_journal.c:165</code>.</p> <p>Function: <code>journal_wthread()</code></p> <p>The write worker drains the FIFO to the journal target:</p> <p>Main loop: <pre><code>for (;;) {\n    // Calculate writable bytes\n    bytes = windex - rindex;\n\n    // Sleep if nothing to write\n    if (bytes == 0) {\n        if (stop_requested) break;\n        tsleep(&amp;jo-&gt;fifo, 0, \"jfifo\", hz);\n        continue;\n    }\n\n    // Block on incomplete records\n    rawp = membase + (rindex &amp; mask);\n    if (rawp-&gt;begmagic == JREC_INCOMPLETEMAGIC) {\n        tsleep(&amp;jo-&gt;fifo, 0, \"jpad\", hz);\n        continue;\n    }\n\n    // Skip pad records\n    if (rawp-&gt;streamid == JREC_STREAMID_PAD) {\n        rindex += aligned_recsize;\n        xindex += aligned_recsize;  // (if not full-duplex)\n        continue;\n    }\n\n    // Calculate contiguous writable region\n    res = 0;\n    avail = fifo_size - (rindex &amp; mask);  // To end of buffer\n    while (res &lt; bytes &amp;&amp; rawp-&gt;begmagic == JREC_BEGMAGIC) {\n        res += aligned_recsize;\n        if (res &gt;= avail) break;  // Hit end of buffer\n        rawp = next_record(rawp);\n    }\n\n    // Write to target\n    rindex += bytes;  // Advance BEFORE write (for ack racing)\n    error = fp_write(fp, membase + old_rindex, bytes, &amp;written);\n\n    // Advance acknowledgement index (if not full-duplex)\n    if (!full_duplex) {\n        xindex += bytes;\n        wakeup_waiters();\n    }\n}\n</code></pre></p> <p>Key aspects: - Never writes incomplete records (blocks until committed) - Writes contiguous regions up to buffer wrap - Advances rindex before writing (allows acks to race) - Handles full-duplex vs simplex differently</p>"},{"location":"sys/kern/vfs/journaling/#read-worker-thread-full-duplex","title":"Read Worker Thread (Full-Duplex)","text":"<p>Located at <code>sys/kern/vfs_journal.c:301</code>.</p> <p>Function: <code>journal_rthread()</code></p> <p>For two-way journaling streams, reads acknowledgements from target:</p> <p>Main loop: <pre><code>for (;;) {\n    if (stop_requested) break;\n\n    // Read acknowledgement record\n    if (transid == 0) {\n        error = fp_read(fp, &amp;ack, sizeof(ack), &amp;count, 1, UIO_SYSSPACE);\n        if (error || count != sizeof(ack)) break;\n\n        // Validate magic numbers\n        if (ack.rbeg.begmagic != JREC_BEGMAGIC) break;\n        if (ack.rend.endmagic != JREC_ENDMAGIC) break;\n\n        transid = ack.rbeg.transid;\n    }\n\n    // Check for unacknowledged data\n    bytes = rindex - xindex;\n    if (bytes == 0) {\n        // Unsent data acknowledged - protocol error\n        kprintf(\"warning: unsent data acknowledged\\n\");\n        transid = 0;\n        continue;\n    }\n\n    // Get record at xindex\n    rawp = membase + (xindex &amp; mask);\n\n    // Advance xindex for all records up to transid\n    if (rawp-&gt;transid &lt; transid) {\n        xindex += aligned_recsize;\n        total_acked += aligned_recsize;\n        wakeup_waiters();\n        continue;\n    }\n\n    // Found matching transid\n    if (rawp-&gt;transid == transid) {\n        xindex += aligned_recsize;\n        total_acked += aligned_recsize;\n        wakeup_waiters();\n        transid = 0;\n        continue;\n    }\n\n    // Unsent data acknowledged - protocol error\n    transid = 0;\n}\n</code></pre></p> <p>Key aspects: - Target can acknowledge multiple records at once - Target sends back transaction IDs that are committed - Advances xindex to free up FIFO space - Wakes up threads waiting for space</p>"},{"location":"sys/kern/vfs/journaling/#journal-vop-operations","title":"Journal VOP Operations","text":"<p>The file <code>sys/kern/vfs_jops.c</code> implements journal-aware VOP operations that intercept filesystem operations to record them before passing through to the underlying filesystem.</p>"},{"location":"sys/kern/vfs/journaling/#operation-interception","title":"Operation Interception","text":"<p>When journaling is enabled:</p> <pre><code>struct vop_ops journal_vnode_vops = {\n    .vop_default =      vop_journal_operate_ap,\n    .vop_setattr =      journal_setattr,\n    .vop_write =        journal_write,\n    .vop_fsync =        journal_fsync,\n    .vop_ncreate =      journal_ncreate,\n    .vop_nremove =      journal_nremove,\n    .vop_nrename =      journal_nrename,\n    // ... etc\n};\n</code></pre> <p>Each intercepted operation follows this pattern:</p> <ol> <li>Start journal record</li> <li>Record UNDO information (if journaling is reversible)</li> <li>Call underlying VOP</li> <li>Record REDO information (operation details)</li> <li>Commit journal record</li> <li>Return result</li> </ol>"},{"location":"sys/kern/vfs/journaling/#example-journal_write","title":"Example: journal_write()","text":"<p>Located at <code>sys/kern/vfs_jops.c:400</code> (approximate).</p> <p>Simplified flow: <pre><code>static int\njournal_write(struct vop_write_args *ap)\n{\n    struct mount *mp = ap-&gt;a_vp-&gt;v_mount;\n    struct journal *jo;\n    struct jrecord jrec;\n    int error;\n\n    // For each journal on this mount\n    TAILQ_FOREACH(jo, &amp;mp-&gt;mnt_jlist, jentry) {\n        // Initialize journal record\n        jrecord_init(jo, &amp;jrec, JTYPE_WRITE);\n\n        // Record UNDO data (old file contents) if reversible\n        if (jo-&gt;flags &amp; MC_JOURNAL_WANT_REVERSIBLE) {\n            jrecord_undo_file(&amp;jrec, ap-&gt;a_vp, JRUNDO_FILEDATA,\n                            ap-&gt;a_uio-&gt;uio_offset, \n                            ap-&gt;a_uio-&gt;uio_resid);\n        }\n\n        // Record write operation details\n        jrecord_write(&amp;jrec, ap-&gt;a_vp, ap-&gt;a_uio, ap-&gt;a_ioflag);\n    }\n\n    // Call underlying filesystem VOP\n    error = vop_write_ap(ap);\n\n    // Commit all journal records\n    TAILQ_FOREACH(jo, &amp;mp-&gt;mnt_jlist, jentry) {\n        jrecord_done(&amp;jrec, error);\n    }\n\n    return error;\n}\n</code></pre></p> <p>Key steps: 1. Loop through all journals on mount point (can have multiple) 2. Create <code>JTYPE_WRITE</code> stream record 3. Record UNDO if reversible (old file data) 4. Record REDO (write parameters + new data) 5. Perform actual write via underlying VOP 6. Commit journal records with result</p>"},{"location":"sys/kern/vfs/journaling/#undo-recording","title":"UNDO Recording","text":"<p>Located at <code>sys/kern/vfs_jops.c:600</code> (approximate).</p> <p>Function: <code>jrecord_undo_file()</code></p> <p>For reversible journals, UNDO information allows replaying backwards:</p> <pre><code>static void\njrecord_undo_file(struct jrecord *jrec, struct vnode *vp,\n                  int jrflags, off_t off, off_t bytes)\n{\n    struct vattr vat;\n    struct uio uio;\n\n    // Start UNDO subrecord\n    jrecord_push(jrec, JTYPE_UNDO);\n\n    // Record file attributes\n    if (jrflags &amp; JRUNDO_VATTR) {\n        VOP_GETATTR(vp, &amp;vat);\n        jrecord_write_vattr(jrec, &amp;vat);\n    }\n\n    // Record file data\n    if (jrflags &amp; JRUNDO_FILEDATA) {\n        // Read old file contents\n        uio.uio_offset = off;\n        uio.uio_resid = bytes;\n        VOP_READ(vp, &amp;uio, IO_NODELOCKED, cred);\n\n        // Write to journal\n        jrecord_write_uio(jrec, &amp;uio);\n    }\n\n    // End UNDO subrecord\n    jrecord_pop(jrec);\n}\n</code></pre> <p>UNDO flags: - <code>JRUNDO_SIZE</code> - File size - <code>JRUNDO_UID/GID</code> - Ownership - <code>JRUNDO_MODES</code> - Permissions - <code>JRUNDO_MTIME/ATIME/CTIME</code> - Timestamps - <code>JRUNDO_FILEDATA</code> - File contents - <code>JRUNDO_NLINK</code> - Link count - <code>JRUNDO_VATTR</code> - All vattr fields</p>"},{"location":"sys/kern/vfs/journaling/#redo-recording","title":"REDO Recording","text":"<p>REDO information describes the operation being performed:</p> <p>For JTYPE_WRITE: <pre><code>jrecord_push(jrec, JTYPE_WRITE);\njrecord_leaf(jrec, JLEAF_PATH1, pathname, pathlen);\njrecord_leaf(jrec, JLEAF_FILEDATA, data, datalen);\njrecord_leaf(jrec, JLEAF_OFFSET, &amp;offset, sizeof(offset));\njrecord_pop(jrec);\n</code></pre></p> <p>For JTYPE_RENAME: <pre><code>jrecord_push(jrec, JTYPE_RENAME);\njrecord_push(jrec, JTYPE_UNDO);\n    jrecord_leaf(jrec, JLEAF_PATH1, oldpath, oldlen);\njrecord_pop(jrec);\njrecord_leaf(jrec, JLEAF_PATH1, oldpath, oldlen);\njrecord_leaf(jrec, JLEAF_PATH2, newpath, newlen);\njrecord_pop(jrec);\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#journal-management","title":"Journal Management","text":""},{"location":"sys/kern/vfs/journaling/#installing-a-journal","title":"Installing a Journal","text":"<p>Located at <code>sys/kern/vfs_jops.c:250</code> (approximate).</p> <p>Function: <code>journal_install_vfs_journal()</code></p> <p>Journals are installed via <code>mountctl()</code> system call:</p> <pre><code>static int\njournal_install_vfs_journal(struct mount *mp, struct file *fp,\n                           const struct mountctl_install_journal *info)\n{\n    struct journal *jo;\n\n    // Check for duplicate journal ID\n    TAILQ_FOREACH(jo, &amp;mp-&gt;mnt_jlist, jentry) {\n        if (strcmp(jo-&gt;id, info-&gt;id) == 0)\n            return EALREADY;\n    }\n\n    // Allocate journal structure\n    jo = kmalloc(sizeof(*jo), M_JOURNAL, M_WAITOK | M_ZERO);\n\n    // Initialize fields\n    strlcpy(jo-&gt;id, info-&gt;id, sizeof(jo-&gt;id));\n    jo-&gt;fp = fp;  // File/socket to write journal to\n    jo-&gt;flags = info-&gt;flags;\n\n    // Allocate memory FIFO\n    jo-&gt;fifo.size = info-&gt;fifo_size;\n    jo-&gt;fifo.mask = jo-&gt;fifo.size - 1;\n    jo-&gt;fifo.membase = kmalloc(jo-&gt;fifo.size, M_JFIFO, M_WAITOK);\n\n    // Initialize indices\n    jo-&gt;fifo.windex = 0;\n    jo-&gt;fifo.rindex = 0;\n    jo-&gt;fifo.xindex = 0;\n    jo-&gt;transid = 1;\n\n    // Create worker threads\n    journal_create_threads(jo);\n\n    // Add to mount point's journal list\n    TAILQ_INSERT_TAIL(&amp;mp-&gt;mnt_jlist, jo, jentry);\n\n    return 0;\n}\n</code></pre> <p>Installation flags: - <code>MC_JOURNAL_WANT_AUDIT</code> - Audit trail mode - <code>MC_JOURNAL_WANT_REVERSIBLE</code> - Record UNDO information - <code>MC_JOURNAL_WANT_FULLDUPLEX</code> - Two-way acknowledgement</p>"},{"location":"sys/kern/vfs/journaling/#journal-lifecycle","title":"Journal Lifecycle","text":"<p>Attach: <code>journal_attach(mp)</code> - Switches mount point's vnops to <code>journal_vnode_vops</code> - Sets <code>mp-&gt;mnt_vn_journal_ops</code></p> <p>Install: <code>journal_install_vfs_journal()</code> - Creates journal structure - Allocates FIFO - Starts worker threads - Adds to <code>mp-&gt;mnt_jlist</code></p> <p>Operate: Normal VFS operations are intercepted and journaled</p> <p>Detach: <code>journal_detach(mp)</code> - Stops worker threads - Flushes FIFO - Frees resources - Restores normal vnops</p>"},{"location":"sys/kern/vfs/journaling/#transaction-structure","title":"Transaction Structure","text":""},{"location":"sys/kern/vfs/journaling/#nested-subrecords","title":"Nested Subrecords","text":"<p>Journal records use nested subrecord structure:</p> <pre><code>Stream Record (JTYPE_RENAME)\n\u251c\u2500 JTYPE_UNDO (nested)\n\u2502  \u251c\u2500 JLEAF_PATH1 (old source path) [leaf]\n\u2502  \u2514\u2500 JLEAF_PATH2 (old dest path if exists) [leaf, LAST]\n\u251c\u2500 JLEAF_PATH1 (source path) [leaf]\n\u2514\u2500 JLEAF_PATH2 (destination path) [leaf, LAST]\n</code></pre> <p>Subrecord traversal: <pre><code>void traverse_subrecords(char *data, int size) {\n    struct journal_subrecord *sub = (void *)data;\n\n    while ((char *)sub &lt; data + size) {\n        if (sub-&gt;rectype &amp; JMASK_NESTED) {\n            // Recurse into nested subrecord\n            traverse_subrecords(sub + 1, sub-&gt;recsize - 8);\n        } else {\n            // Process leaf subrecord\n            process_leaf(sub);\n        }\n\n        // Check for last subrecord\n        if (sub-&gt;rectype &amp; JMASK_LAST)\n            break;\n\n        // Advance to next subrecord\n        sub = (char *)sub + sub-&gt;recsize;\n    }\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#jrecord-api","title":"jrecord API","text":"<p>High-level API for building journal records:</p> <p>Initialize: <pre><code>void jrecord_init(struct journal *jo, struct jrecord *jrec, \n                  int16_t streamid);\n</code></pre></p> <p>Push/pop nested subrecords: <pre><code>void jrecord_push(struct jrecord *jrec, int16_t rectype);\nvoid jrecord_pop(struct jrecord *jrec);\n</code></pre></p> <p>Write leaf data: <pre><code>void jrecord_leaf(struct jrecord *jrec, int16_t rectype, \n                  void *data, int bytes);\nvoid jrecord_data(struct jrecord *jrec, void *buf, int bytes, int dtype);\n</code></pre></p> <p>Commit: <pre><code>void jrecord_done(struct jrecord *jrec, int error);\n</code></pre></p> <p>Example usage: <pre><code>struct jrecord jrec;\n\njrecord_init(jo, &amp;jrec, JTYPE_WRITE);\n\njrecord_push(&amp;jrec, JTYPE_UNDO);\n    jrecord_leaf(&amp;jrec, JLEAF_FILEDATA, oldbuf, oldsize);\njrecord_pop(&amp;jrec);\n\njrecord_leaf(&amp;jrec, JLEAF_PATH1, path, pathlen);\njrecord_leaf(&amp;jrec, JLEAF_FILEDATA, newbuf, newsize);\n\njrecord_done(&amp;jrec, error);\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#synchronization-and-locking","title":"Synchronization and Locking","text":""},{"location":"sys/kern/vfs/journaling/#fifo-concurrency","title":"FIFO Concurrency","text":"<p>The memory FIFO supports concurrent operations:</p> <p>Multiple writers:  - Each thread reserves its own space via <code>journal_reserve()</code> - Incomplete magic blocks worker thread from writing past incomplete records - Records can be completed out-of-order - Worker thread writes in reservation order</p> <p>Single writer thread: - One worker thread per journal - Reads from FIFO via rindex - Blocks on incomplete records</p> <p>Acknowledgement: - Single reader thread (full-duplex only) - Advances xindex based on acknowledgements - Frees up FIFO space</p> <p>Wait conditions: - Writers wait on <code>&amp;jo-&gt;fifo.windex</code> when FIFO full (<code>MC_JOURNAL_WWAIT</code>) - Worker wakes writers when space available - Worker waits on <code>&amp;jo-&gt;fifo</code> when nothing to write</p>"},{"location":"sys/kern/vfs/journaling/#memory-barriers","title":"Memory Barriers","text":"<p>Critical use of memory barriers for correctness:</p> <p>Reserve: <pre><code>// Initialize record header and trailer\nrawp-&gt;begmagic = JREC_INCOMPLETEMAGIC;\nrawp-&gt;recsize = bytes;\n// ... fill in fields ...\n\ncpu_sfence();  // Ensure writes complete before advancing windex\njo-&gt;fifo.windex += aligned_bytes;\n</code></pre></p> <p>Commit: <pre><code>// Fill in trailer\nrendp-&gt;endmagic = JREC_ENDMAGIC;\nrendp-&gt;recsize = rawp-&gt;recsize;\n\ncpu_sfence();  // Ensure trailer written before magic\nrawp-&gt;begmagic = JREC_BEGMAGIC;  // Make visible to worker\n</code></pre></p> <p>Pad record: <pre><code>rawp-&gt;streamid = JREC_STREAMID_PAD;\nrawp-&gt;recsize = recsize;\nrendp-&gt;endmagic = JREC_ENDMAGIC;\n\ncpu_sfence();  // Ensure complete before making visible\nrawp-&gt;begmagic = JREC_BEGMAGIC;\n</code></pre></p> <p>These barriers prevent: - Worker thread seeing incomplete records - Reordered writes corrupting record structure - CPU/compiler optimization breaking protocol</p>"},{"location":"sys/kern/vfs/journaling/#journal-targets","title":"Journal Targets","text":""},{"location":"sys/kern/vfs/journaling/#filesocket-support","title":"File/Socket Support","text":"<p>Journals can write to:</p> <p>Regular files: <pre><code>fp_write(jo-&gt;fp, buf, size, &amp;written, UIO_SYSSPACE);\n</code></pre></p> <p>Sockets (network journaling): - TCP sockets for remote replication - Can span network boundaries - Two-way acknowledgement over socket</p> <p>Special devices: - Raw disk partitions for fast local journaling - Block devices</p>"},{"location":"sys/kern/vfs/journaling/#full-duplex-journaling","title":"Full-Duplex Journaling","text":"<p>For two-way acknowledgement:</p> <p>Setup: <pre><code>info-&gt;flags |= MC_JOURNAL_WANT_FULLDUPLEX;\njournal_install_vfs_journal(mp, fp, info);\n</code></pre></p> <p>Operation: - Write worker sends journal records - Read worker receives acknowledgements - Target must implement ack protocol - xindex only advances on ack receipt</p> <p>Acknowledgement record format: <pre><code>struct journal_ackrecord {\n    struct journal_rawrecbeg rbeg;\n    int32_t filler0;\n    int32_t filler1;\n    struct journal_rawrecend rend;\n};\n</code></pre></p> <p>Target sends back transaction ID when committed to stable storage.</p>"},{"location":"sys/kern/vfs/journaling/#restart-and-resync","title":"Restart and Resync","text":"<p>Journals support interruption and restart:</p> <p>Restart marker: <pre><code>JREC_STREAMID_RESTART  // Marks journal restart after interruption\n</code></pre></p> <p>Resync operation: - Target can request resync to transaction ID - Journal fast-forwards xindex - Allows recovery after link interruption</p> <p>Use cases: - Network failure recovery - Target system restart - Catching up after outage</p>"},{"location":"sys/kern/vfs/journaling/#performance-considerations","title":"Performance Considerations","text":""},{"location":"sys/kern/vfs/journaling/#fifo-sizing","title":"FIFO Sizing","text":"<p>FIFO size affects performance and stall behavior:</p> <p>Too small: - Frequent stalls waiting for space - <code>fifostalls</code> counter increments - Threads block in <code>journal_reserve()</code></p> <p>Too large: - Memory overhead - Longer recovery window on restart - Delayed error detection</p> <p>Recommended: - 1-4 MB for local journaling - 8-32 MB for network journaling - Power of 2 for efficient masking</p>"},{"location":"sys/kern/vfs/journaling/#batching-efficiency","title":"Batching Efficiency","text":"<p>Worker thread batching reduces overhead:</p> <p>Wakeup policy: <pre><code>if (fifo &gt; 50% full || waiters_present)\n    wakeup(&amp;jo-&gt;fifo);\n</code></pre></p> <p>Benefits: - Amortizes thread switch overhead - Better CPU cache utilization - Reduces syscall/write overhead - Batches related operations</p> <p>Heartbeat: - Worker wakes periodically (HZ) - Flushes small amounts of data - Prevents indefinite delay</p>"},{"location":"sys/kern/vfs/journaling/#zero-copy-optimization","title":"Zero-Copy Optimization","text":"<p>Journal records are built directly in FIFO:</p> <ol> <li>Reserve space in FIFO</li> <li>Write data directly to reserved space</li> <li>Commit when complete</li> <li>Worker writes directly from FIFO to target</li> </ol> <p>No intermediate buffering or copying required.</p>"},{"location":"sys/kern/vfs/journaling/#error-handling","title":"Error Handling","text":""},{"location":"sys/kern/vfs/journaling/#filesystem-operation-errors","title":"Filesystem Operation Errors","text":"<p>If underlying VOP fails:</p> <pre><code>error = vop_write_ap(ap);\njrecord_done(&amp;jrec, error);  // Records error in journal\n</code></pre> <p>Journal records operation and result, even on failure.</p>"},{"location":"sys/kern/vfs/journaling/#journal-write-errors","title":"Journal Write Errors","text":"<p>If worker thread encounters write error:</p> <pre><code>error = fp_write(jo-&gt;fp, buf, bytes, &amp;res);\nif (error) {\n    kprintf(\"journal_thread(%s) write, error %d\\n\", jo-&gt;id, error);\n    // XXX: Error policy TBD\n    // Options: pause, abort, mark journal failed\n}\n</code></pre> <p>Current behavior: Log error and continue Future: Configurable error policies</p>"},{"location":"sys/kern/vfs/journaling/#fifo-overflow","title":"FIFO Overflow","text":"<p>When FIFO fills up:</p> <pre><code>avail = fifo_size - (windex - xindex);\nif (avail &lt; required) {\n    jo-&gt;flags |= MC_JOURNAL_WWAIT;\n    ++jo-&gt;fifostalls;\n    tsleep(&amp;jo-&gt;fifo.windex, 0, \"jwrite\", 0);\n}\n</code></pre> <p>Blocking behavior: - Thread sleeps until space available - Worker thread wakes waiters - <code>fifostalls</code> tracks frequency</p> <p>Implications: - Filesystem operations block - System slows to journal speed - Prevents memory exhaustion</p>"},{"location":"sys/kern/vfs/journaling/#use-cases","title":"Use Cases","text":""},{"location":"sys/kern/vfs/journaling/#replication","title":"Replication","text":"<p>Stream filesystem changes to remote system:</p> <ol> <li>Install journal with socket to remote host</li> <li>All filesystem operations recorded</li> <li>Remote system applies changes</li> <li>Full-duplex acks ensure durability</li> </ol> <p>Benefits: - Near real-time replication - Crash recovery via replay - Bandwidth efficient (operation-level)</p>"},{"location":"sys/kern/vfs/journaling/#auditing","title":"Auditing","text":"<p>Record all filesystem access:</p> <pre><code>info-&gt;flags = MC_JOURNAL_WANT_AUDIT;\n</code></pre> <p>Captures: - All file creates/deletes - All writes and modifications - User credentials - Timestamps - Process information</p> <p>Use: Security auditing, compliance</p>"},{"location":"sys/kern/vfs/journaling/#reversible-journaling","title":"Reversible Journaling","text":"<p>Enable undo capability:</p> <pre><code>info-&gt;flags = MC_JOURNAL_WANT_REVERSIBLE;\n</code></pre> <p>Records: - UNDO information (old data) - REDO information (new data) - Complete state for rollback</p> <p>Use: Snapshot-like functionality, experimental</p>"},{"location":"sys/kern/vfs/journaling/#local-fast-journal","title":"Local Fast Journal","text":"<p>For crash recovery:</p> <ol> <li>Journal to fast SSD/NVMe</li> <li>Async write to slower main storage</li> <li>Replay journal on crash</li> <li>Discard journal when committed</li> </ol> <p>Benefits: - Fast write acknowledgement - Large write coalescing - Crash consistency</p>"},{"location":"sys/kern/vfs/journaling/#debugging","title":"Debugging","text":""},{"location":"sys/kern/vfs/journaling/#statistics","title":"Statistics","text":"<p>Each journal maintains statistics:</p> <pre><code>struct journal {\n    int64_t total_acked;    // Total bytes acknowledged\n    int fifostalls;         // FIFO full stall count\n    // ...\n};\n</code></pre> <p>Access via mountctl: <pre><code>MOUNTCTL_STATUS_VFS_JOURNAL\n</code></pre></p>"},{"location":"sys/kern/vfs/journaling/#tracing","title":"Tracing","text":"<p>Enable debugging output:</p> <pre><code>#if 1\nkprintf(\"ackskip %08llx/%08llx\\n\", rawp-&gt;transid, transid);\n#endif\n</code></pre> <p>Traces: - Acknowledgement processing - Record sequencing - Error conditions</p>"},{"location":"sys/kern/vfs/journaling/#common-issues","title":"Common Issues","text":"<p>FIFO stalls: - <code>fifostalls</code> counter high - Increase FIFO size - Check target write performance</p> <p>Incomplete record hangs: - Thread crashed while populating record - Incomplete magic left in FIFO - Worker thread blocked forever - Solution: Timeout + recovery logic (TBD)</p> <p>Acknowledgement protocol errors: - \"warning: unsent data acknowledged\" - Target acknowledging wrong transid - Check target implementation</p>"},{"location":"sys/kern/vfs/journaling/#limitations-and-future-work","title":"Limitations and Future Work","text":""},{"location":"sys/kern/vfs/journaling/#current-limitations","title":"Current Limitations","text":"<ol> <li>MPLOCK dependency:</li> <li>Worker threads still use MPLOCK</li> <li> <p>Not fully SMP-optimized</p> </li> <li> <p>Error handling:</p> </li> <li>Limited error policies</li> <li> <p>No automatic journal failure handling</p> </li> <li> <p>Checksum disabled:</p> </li> <li><code>check</code> field in records unused</li> <li> <p>No data integrity verification</p> </li> <li> <p>No encryption:</p> </li> <li>All data in clear text</li> <li>Security via transport layer only</li> </ol>"},{"location":"sys/kern/vfs/journaling/#planned-enhancements","title":"Planned Enhancements","text":"<p>From <code>vfs_journal.c:35-60</code> comments:</p> <ol> <li>Two-way acknowledgement:</li> <li>Transaction ID acknowledgement (partially implemented)</li> <li>Explicit and implicit ack schemes</li> <li>Resynchronization support</li> <li> <p>Restart after interruption</p> </li> <li> <p>Swap space spooling:</p> </li> <li>Use swap to absorb long interruptions</li> <li>Prevent slow links from blocking local ops</li> <li> <p>Larger buffer capacity</p> </li> <li> <p>Per-CPU FIFOs:</p> </li> <li>Remove locking requirements</li> <li>Better SMP scalability</li> <li> <p>Reduce contention</p> </li> <li> <p>Filesystem integration:</p> </li> <li>Allow filesystems to use journal layer directly</li> <li>Avoid rolling their own journaling</li> <li>Leverage kernel infrastructure</li> </ol>"},{"location":"sys/kern/vfs/journaling/#summary","title":"Summary","text":"<p>The VFS journaling system provides a sophisticated infrastructure for recording filesystem operations. Key aspects:</p> <p>Architecture: - Interception layer between VOP wrappers and filesystem - Memory FIFO batches records before writing - Asynchronous worker threads for write-out - Optional two-way acknowledgement</p> <p>Record structure: - Stream records with headers/trailers - Nested subrecord hierarchy - Transaction IDs for sequencing - Extensible operation types</p> <p>Concurrency: - Lock-free reservation with incomplete magic - Multiple writers, single worker thread - Memory barriers for correctness - Wait/wakeup for flow control</p> <p>Features: - Multiple journals per mount point - Full-duplex acknowledgement - Reversible journals (UNDO) - Network and local targets</p> <p>Use cases: - Replication to remote systems - Security auditing trails - Crash recovery journals - Experimental undo functionality</p> <p>The journaling system is mature but retains areas for optimization, particularly in SMP scalability and error handling sophistication.</p> <p>Related documentation: - VFS Operations - VOP dispatch mechanism - Buffer Cache - Block I/O infrastructure - Mounting - Mount point management</p>"},{"location":"sys/kern/vfs/mounting/","title":"VFS Mounting and Unmounting","text":""},{"location":"sys/kern/vfs/mounting/#overview","title":"Overview","text":"<p>The VFS mounting subsystem manages filesystem mount points throughout their lifecycle, from initial allocation through mounting, operation, and eventual unmounting. This documentation covers the mount point management infrastructure in <code>vfs_mount.c</code> and the mount/unmount system calls in <code>vfs_syscalls.c</code>.</p> <p>Key source files: - <code>sys/kern/vfs_mount.c</code> (1,249 lines) - Mount point lifecycle and management - <code>sys/kern/vfs_syscalls.c</code> (5,512+ lines) - VFS system calls including mount/unmount - <code>sys/sys/mount.h</code> - Mount structure and flag definitions</p>"},{"location":"sys/kern/vfs/mounting/#mount-structure","title":"Mount Structure","text":""},{"location":"sys/kern/vfs/mounting/#struct-mount","title":"struct mount","text":"<p>The <code>struct mount</code> (defined in sys/sys/mount.h:216) is the central data structure representing a mounted filesystem instance:</p> <pre><code>struct mount {\n    TAILQ_ENTRY(mount) mnt_list;        /* mount list linkage */\n    struct vfsops      *mnt_op;         /* filesystem operations */\n    struct vfsconf     *mnt_vfc;        /* filesystem configuration */\n    u_int              mnt_namecache_gen; /* negative cache invalidation */\n    u_int              mnt_pbuf_count;  /* pbuf usage limit */\n    struct vnode       *mnt_syncer;     /* syncer vnode */\n    struct syncer_ctx  *mnt_syncer_ctx; /* syncer process context */\n    struct vnodelst    mnt_nvnodelist;  /* list of vnodes on this mount */\n    TAILQ_HEAD(,vmntvnodescan_info) mnt_vnodescan_list;\n    struct lock        mnt_lock;        /* mount structure lock */\n    int                mnt_flag;        /* user-visible flags */\n    int                mnt_kern_flag;   /* kernel-only flags */\n    int                mnt_maxsymlinklen; /* max short symlink size */\n    struct statfs      mnt_stat;        /* filesystem statistics */\n    struct statvfs     mnt_vstat;       /* extended statistics */\n    qaddr_t            mnt_data;        /* filesystem-private data */\n    time_t             mnt_time;        /* last write time */\n    u_int              mnt_iosize_max;  /* max IO request size */\n    struct vnodelst    mnt_reservedvnlist; /* reserved/dirty vnode list */\n    int                mnt_nvnodelistsize; /* vnode list size */\n\n    /* VFS operations vectors (stacked) */\n    struct vop_ops     *mnt_vn_use_ops;      /* current ops */\n    struct vop_ops     *mnt_vn_coherency_ops; /* cache coherency */\n    struct vop_ops     *mnt_vn_journal_ops;   /* journaling */\n    struct vop_ops     *mnt_vn_norm_ops;      /* normal ops */\n    struct vop_ops     *mnt_vn_spec_ops;      /* special files */\n    struct vop_ops     *mnt_vn_fifo_ops;      /* FIFOs */\n\n    /* Namecache integration */\n    struct nchandle    mnt_ncmountpt;   /* mount point (root of fs) */\n    struct nchandle    mnt_ncmounton;   /* mounted on (directory) */\n\n    /* Reference counting */\n    struct ucred       *mnt_cred;       /* credentials */\n    int                mnt_refs;        /* nchandle references */\n    int                mnt_hold;        /* prevent premature free */\n    struct lwkt_token  mnt_token;       /* token lock if !MPSAFE */\n\n    /* Journaling support */\n    struct journallst  mnt_jlist;       /* active journals */\n    u_int8_t           *mnt_jbitmap;    /* streamid bitmap */\n    int16_t            mnt_streamid;    /* last streamid */\n\n    /* Buffer I/O operations */\n    struct bio_ops     *mnt_bioops;     /* BIO ops (HAMMER, softupd) */\n    struct lock        mnt_renlock;     /* rename directory lock */\n\n    /* Quota accounting */\n    struct vfs_acct    mnt_acct;        /* space accounting */\n    RB_ENTRY(mount)    mnt_node;        /* mounttree RB-tree node */\n};\n</code></pre> <p>Key nchandle fields: - <code>mnt_ncmountpt</code>: Points to the root of the mounted filesystem (created during mount) - <code>mnt_ncmounton</code>: Points to the directory where the filesystem is mounted</p>"},{"location":"sys/kern/vfs/mounting/#mount-flags-mnt_flag","title":"Mount Flags (mnt_flag)","text":"<p>User-visible flags in <code>mnt_flag</code> (sys/sys/mount.h:274):</p> <p>Access control: - <code>MNT_RDONLY</code> (0x00000001) - Read-only filesystem - <code>MNT_NOSUID</code> (0x00000008) - Ignore setuid/setgid bits - <code>MNT_NOEXEC</code> (0x00000004) - Disallow program execution - <code>MNT_NODEV</code> (0x00000010) - Ignore device files</p> <p>Performance: - <code>MNT_SYNCHRONOUS</code> (0x00000002) - Synchronous writes - <code>MNT_ASYNC</code> (0x00000040) - Asynchronous writes - <code>MNT_NOATIME</code> (0x10000000) - Don't update access times - <code>MNT_NOCLUSTERR</code> (0x40000000) - Disable cluster read - <code>MNT_NOCLUSTERW</code> (0x80000000) - Disable cluster write</p> <p>Special behavior: - <code>MNT_NOSYMFOLLOW</code> (0x00400000) - Don't follow symlinks - <code>MNT_SUIDDIR</code> (0x00100000) - Special SUID directory handling - <code>MNT_TRIM</code> (0x01000000) - Enable online FS trimming - <code>MNT_AUTOMOUNTED</code> (0x00000020) - Mounted by automountd(8)</p> <p>NFS export: - <code>MNT_EXPORTED</code> (0x00000100) - Filesystem is exported - <code>MNT_DEFEXPORTED</code> (0x00000200) - Exported to the world - <code>MNT_EXRDONLY</code> (0x00000080) - Exported read-only</p> <p>Internal: - <code>MNT_LOCAL</code> (0x00001000) - Stored locally - <code>MNT_QUOTA</code> (0x00002000) - Quotas enabled - <code>MNT_ROOTFS</code> (0x00004000) - Root filesystem - <code>MNT_USER</code> (0x00008000) - Mounted by non-root user - <code>MNT_IGNORE</code> (0x00800000) - Hide from df output</p> <p>Command flags (transient): - <code>MNT_UPDATE</code> (0x00010000) - Update existing mount - <code>MNT_RELOAD</code> (0x00040000) - Reload filesystem data - <code>MNT_FORCE</code> (0x00080000) - Force unmount/readonly change</p>"},{"location":"sys/kern/vfs/mounting/#kernel-flags-mnt_kern_flag","title":"Kernel Flags (mnt_kern_flag)","text":"<p>Kernel-only flags in <code>mnt_kern_flag</code> (sys/sys/mount.h:348):</p> <p>Unmounting: - <code>MNTK_UNMOUNT</code> (0x01000000) - Unmount in progress - <code>MNTK_UNMOUNTF</code> (0x00000001) - Forced unmount in progress - <code>MNTK_MWAIT</code> (0x02000000) - Waiting for unmount to finish - <code>MNTK_QUICKHALT</code> (0x00008000) - Quick unmount on system halt</p> <p>MPSAFE operation flags: - <code>MNTK_ALL_MPSAFE</code> - All VFS operations are MPSAFE - <code>MNTK_MPSAFE</code> (0x00010000) - VFS operations don't need mnt_token - <code>MNTK_RD_MPSAFE</code> (0x00020000) - vop_read is MPSAFE - <code>MNTK_WR_MPSAFE</code> (0x00040000) - vop_write is MPSAFE - <code>MNTK_GA_MPSAFE</code> (0x00080000) - vop_getattr is MPSAFE - <code>MNTK_IN_MPSAFE</code> (0x00100000) - vop_inactive is MPSAFE - <code>MNTK_SG_MPSAFE</code> (0x00200000) - vop_strategy is MPSAFE - <code>MNTK_ST_MPSAFE</code> (0x80000000) - vfs_start is MPSAFE</p> <p>Other: - <code>MNTK_WANTRDWR</code> (0x04000000) - Upgrade to read/write requested - <code>MNTK_NOSTKMNT</code> (0x10000000) - No stacked mounts allowed - <code>MNTK_NCALIASED</code> (0x00800000) - Namecache is aliased - <code>MNTK_NOMSYNC</code> (0x20000000) - Used by tmpfs - <code>MNTK_THR_SYNC</code> (0x40000000) - FS sync thread requested</p>"},{"location":"sys/kern/vfs/mounting/#mount-point-lifecycle","title":"Mount Point Lifecycle","text":""},{"location":"sys/kern/vfs/mounting/#initialization","title":"Initialization","text":"<p>vfs_mount_init() (vfs_mount.c:154)</p> <p>Called from <code>vfsinit()</code> during system initialization:</p> <pre><code>void vfs_mount_init(void)\n{\n    lwkt_token_init(&amp;mountlist_token, \"mntlist\");\n    lwkt_token_init(&amp;mntid_token, \"mntid\");\n    TAILQ_INIT(&amp;mountscan_list);\n    mount_init(&amp;dummymount, NULL);\n    dummymount.mnt_flag |= MNT_RDONLY;\n    dummymount.mnt_kern_flag |= MNTK_ALL_MPSAFE;\n}\n</code></pre> <p>Creates a dummy mount used for vnodes that have no filesystem (e.g., early boot devices).</p> <p>mount_init() (vfs_mount.c:373)</p> <p>Initializes a mount structure:</p> <pre><code>void mount_init(struct mount *mp, struct vfsops *ops)\n{\n    lockinit(&amp;mp-&gt;mnt_lock, \"vfslock\", hz*5, 0);\n    lockinit(&amp;mp-&gt;mnt_renlock, \"renamlk\", hz*5, 0);\n    lwkt_token_init(&amp;mp-&gt;mnt_token, \"permnt\");\n\n    TAILQ_INIT(&amp;mp-&gt;mnt_vnodescan_list);\n    TAILQ_INIT(&amp;mp-&gt;mnt_nvnodelist);\n    TAILQ_INIT(&amp;mp-&gt;mnt_reservedvnlist);\n    TAILQ_INIT(&amp;mp-&gt;mnt_jlist);\n\n    mp-&gt;mnt_nvnodelistsize = 0;\n    mp-&gt;mnt_flag = 0;\n    mp-&gt;mnt_hold = 1;  /* hold for umount last drop */\n    mp-&gt;mnt_iosize_max = MAXPHYS;\n    mp-&gt;mnt_op = ops;\n\n    if (ops == NULL || (ops-&gt;vfs_flags &amp; VFSOPSF_NOSYNCERTHR) == 0)\n        vn_syncer_thr_create(mp);  /* create syncer thread */\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#reference-counting","title":"Reference Counting","text":"<p>Mount structures use two reference counters:</p> <p>mnt_refs - nchandle references: - Incremented when nchandles reference this mount - Managed automatically by the namecache system - Must reach 1 (only mnt_ncmountpt reference) before unmount</p> <p>mnt_hold - Hold count: - Prevents premature kfree of the mount structure - Initialized to 1 in mount_init() - Used when mount might be accessed without holding mountlist_token</p> <p>mount_hold() / mount_drop() (vfs_mount.c:393, 399):</p> <pre><code>void mount_hold(struct mount *mp)\n{\n    atomic_add_int(&amp;mp-&gt;mnt_hold, 1);\n}\n\nvoid mount_drop(struct mount *mp)\n{\n    if (atomic_fetchadd_int(&amp;mp-&gt;mnt_hold, -1) == 1) {\n        KKASSERT(mp-&gt;mnt_refs == 0);\n        kfree(mp, M_MOUNT);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#filesystem-id-fsid-management","title":"Filesystem ID (FSID) Management","text":"<p>vfs_getnewfsid() (vfs_mount.c:441)</p> <p>Generates a unique filesystem identifier based on the mount path:</p> <pre><code>void vfs_getnewfsid(struct mount *mp)\n{\n    fsid_t tfsid;\n    int mtype;\n    char *retbuf, *freebuf;\n\n    mtype = mp-&gt;mnt_vfc-&gt;vfc_typenum;\n    tfsid.val[1] = mtype;\n\n    /* Hash the mount point path to create unique FSID */\n    error = cache_fullpath(NULL, &amp;mp-&gt;mnt_ncmounton, NULL,\n                          &amp;retbuf, &amp;freebuf, 0);\n    if (error) {\n        tfsid.val[0] = makeudev(255, 0);\n    } else {\n        tfsid.val[0] = makeudev(255,\n                               iscsi_crc32(retbuf, strlen(retbuf)) &amp;\n                               ~makeudev(255, 0));\n        kfree(freebuf, M_TEMP);\n    }\n\n    mp-&gt;mnt_stat.f_fsid.val[0] = tfsid.val[0];\n    mp-&gt;mnt_stat.f_fsid.val[1] = tfsid.val[1];\n}\n</code></pre> <p>The FSID will be adjusted automatically during <code>mountlist_insert()</code> if collisions occur.</p>"},{"location":"sys/kern/vfs/mounting/#mount-lists-and-trees","title":"Mount Lists and Trees","text":""},{"location":"sys/kern/vfs/mounting/#global-mount-data-structures","title":"Global Mount Data Structures","text":"<p>mountlist (vfs_mount.c:143) - Ordered list of all mounts: <pre><code>struct mntlist mountlist = TAILQ_HEAD_INITIALIZER(mountlist);\n</code></pre></p> <p>mounttree (vfs_mount.c:144) - Red-black tree indexed by FSID: <pre><code>struct mount_rb_tree mounttree = RB_INITIALIZER(dev_tree_mounttree);\n</code></pre></p> <p>mountlist_token (vfs_mount.c:146) - Protects both structures</p>"},{"location":"sys/kern/vfs/mounting/#mount-list-operations","title":"Mount List Operations","text":"<p>mountlist_insert() (vfs_mount.c:590)</p> <p>Adds a mount to the global mount list and tree:</p> <pre><code>void mountlist_insert(struct mount *mp, int how)\n{\n    int lim = 0x01000000;\n\n    lwkt_gettoken(&amp;mountlist_token);\n\n    /* Add to ordered list */\n    if (how == MNTINS_FIRST)\n        TAILQ_INSERT_HEAD(&amp;mountlist, mp, mnt_list);\n    else\n        TAILQ_INSERT_TAIL(&amp;mountlist, mp, mnt_list);\n\n    /* Add to RB-tree, adjusting FSID on collision */\n    while (mount_rb_tree_RB_INSERT(&amp;mounttree, mp)) {\n        int32_t val = mp-&gt;mnt_stat.f_fsid.val[0];\n        val = ((val &amp; 0xFFFF0000) &gt;&gt; 8) | (val &amp; 0x000000FF);\n        ++val;\n        val = ((val &lt;&lt; 8) &amp; 0xFFFF0000) | (val &amp; 0x000000FF);\n        mp-&gt;mnt_stat.f_fsid.val[0] = val;\n\n        if (--lim == 0) {\n            lim = 0x01000000;\n            mp-&gt;mnt_stat.f_fsid.val[1] += 0x0100;\n            kprintf(\"mountlist_insert: fsid collision, \"\n                   \"too many mounts\\n\");\n        }\n    }\n\n    lwkt_reltoken(&amp;mountlist_token);\n}\n</code></pre> <p>mountlist_remove() (vfs_mount.c:663)</p> <p>Removes a mount from both structures:</p> <pre><code>void mountlist_remove(struct mount *mp)\n{\n    struct mountscan_info *msi;\n\n    lwkt_gettoken(&amp;mountlist_token);\n\n    /* Adjust any active scans past this mount */\n    TAILQ_FOREACH(msi, &amp;mountscan_list, msi_entry) {\n        if (msi-&gt;msi_node == mp) {\n            if (msi-&gt;msi_how &amp; MNTSCAN_FORWARD)\n                msi-&gt;msi_node = TAILQ_NEXT(mp, mnt_list);\n            else\n                msi-&gt;msi_node = TAILQ_PREV(mp, mntlist, mnt_list);\n        }\n    }\n\n    TAILQ_REMOVE(&amp;mountlist, mp, mnt_list);\n    mount_rb_tree_RB_REMOVE(&amp;mounttree, mp);\n\n    lwkt_reltoken(&amp;mountlist_token);\n}\n</code></pre> <p>vfs_getvfs() (vfs_mount.c:414)</p> <p>Looks up a mount by FSID:</p> <pre><code>struct mount *vfs_getvfs(fsid_t *fsid)\n{\n    struct mount *mp;\n\n    lwkt_gettoken_shared(&amp;mountlist_token);\n    mp = mount_rb_tree_RB_LOOKUP_FSID(&amp;mounttree, fsid);\n    if (mp)\n        mount_hold(mp);  /* caller must mount_drop() */\n    lwkt_reltoken(&amp;mountlist_token);\n\n    return (mp);\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#scanning-the-mount-list","title":"Scanning the Mount List","text":"<p>mountlist_scan() (vfs_mount.c:736)</p> <p>Safely iterates over all mounts with a callback:</p> <pre><code>int mountlist_scan(int (*callback)(struct mount *, void *),\n                   void *data, int how)\n</code></pre> <p>Scan flags: - <code>MNTSCAN_FORWARD</code> - Forward iteration - <code>MNTSCAN_REVERSE</code> - Reverse iteration - <code>MNTSCAN_NOBUSY</code> - Don't call vfs_busy() before callback - <code>MNTSCAN_NOUNLOCK</code> - Keep mountlist_token held during callback</p> <p>The scanner: 1. Registers scan state with mountscan_list 2. Iterates mount list calling callback for each mount 3. Calls vfs_busy() unless MNTSCAN_NOBUSY is set 4. Unlocks mountlist_token during callback (unless MNTSCAN_NOUNLOCK) 5. Handles mount removal during iteration 6. Aggregates callback return values</p> <p>Example usage (sys_sync): <pre><code>int sys_sync(struct sysmsg *sysmsg, const struct sync_args *uap)\n{\n    mountlist_scan(sync_callback, NULL, MNTSCAN_FORWARD);\n    return (0);\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#mount-busy-protocol","title":"Mount Busy Protocol","text":""},{"location":"sys/kern/vfs/mounting/#vfs_busy-vfs_unbusy","title":"vfs_busy() / vfs_unbusy()","text":"<p>The busy protocol prevents a mount from being unmounted while operations are in progress.</p> <p>vfs_busy() (vfs_mount.c:271)</p> <p>Acquires a shared lock on the mount, incrementing mnt_refs:</p> <pre><code>int vfs_busy(struct mount *mp, int flags)\n{\n    atomic_add_int(&amp;mp-&gt;mnt_refs, 1);\n    lwkt_gettoken(&amp;mp-&gt;mnt_token);\n\n    /* Check if unmount is in progress */\n    if (mp-&gt;mnt_kern_flag &amp; MNTK_UNMOUNT) {\n        if (flags &amp; LK_NOWAIT) {\n            lwkt_reltoken(&amp;mp-&gt;mnt_token);\n            atomic_add_int(&amp;mp-&gt;mnt_refs, -1);\n            return (ENOENT);\n        }\n        /* Wait for unmount to complete */\n        mp-&gt;mnt_kern_flag |= MNTK_MWAIT;\n        tsleep((caddr_t)mp, 0, \"vfs_busy\", 0);\n        lwkt_reltoken(&amp;mp-&gt;mnt_token);\n        atomic_add_int(&amp;mp-&gt;mnt_refs, -1);\n        return (ENOENT);\n    }\n\n    /* Acquire shared lock */\n    if (lockmgr(&amp;mp-&gt;mnt_lock, LK_SHARED))\n        panic(\"vfs_busy: unexpected lock failure\");\n\n    lwkt_reltoken(&amp;mp-&gt;mnt_token);\n    return (0);\n}\n</code></pre> <p>vfs_unbusy() (vfs_mount.c:317)</p> <p>Releases the busy lock:</p> <pre><code>void vfs_unbusy(struct mount *mp)\n{\n    mount_hold(mp);  /* prevent race with final unmount */\n    atomic_add_int(&amp;mp-&gt;mnt_refs, -1);\n    lockmgr(&amp;mp-&gt;mnt_lock, LK_RELEASE);\n    mount_drop(mp);\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#vnode-mount-integration","title":"Vnode-Mount Integration","text":""},{"location":"sys/kern/vfs/mounting/#associating-vnodes-with-mounts","title":"Associating Vnodes with Mounts","text":"<p>insmntque() (vfs_mount.c:835)</p> <p>Moves a vnode to a mount's vnode list:</p> <pre><code>void insmntque(struct vnode *vp, struct mount *mp)\n{\n    struct mount *omp;\n\n    /* Remove from old mount if present */\n    if ((omp = vp-&gt;v_mount) != NULL) {\n        lwkt_gettoken(&amp;omp-&gt;mnt_token);\n        vremovevnodemnt(vp);\n        omp-&gt;mnt_nvnodelistsize--;\n        lwkt_reltoken(&amp;omp-&gt;mnt_token);\n    }\n\n    if (mp == NULL) {\n        vp-&gt;v_mount = NULL;\n        return;\n    }\n\n    /* Insert into new mount's vnode list */\n    lwkt_gettoken(&amp;mp-&gt;mnt_token);\n    vp-&gt;v_mount = mp;\n\n    /* Insert before syncer vnode if present, else at tail */\n    if (mp-&gt;mnt_syncer) {\n        TAILQ_INSERT_BEFORE(mp-&gt;mnt_syncer, vp, v_nmntvnodes);\n    } else {\n        TAILQ_INSERT_TAIL(&amp;mp-&gt;mnt_nvnodelist, vp, v_nmntvnodes);\n    }\n\n    mp-&gt;mnt_nvnodelistsize++;\n    lwkt_reltoken(&amp;mp-&gt;mnt_token);\n}\n</code></pre> <p>getnewvnode() (vfs_mount.c:194)</p> <p>Allocates a new vnode and associates it with a mount:</p> <pre><code>int getnewvnode(enum vtagtype tag, struct mount *mp,\n                struct vnode **vpp, int lktimeout, int lkflags)\n{\n    struct vnode *vp;\n\n    KKASSERT(mp != NULL);\n\n    vp = allocvnode(lktimeout, lkflags);\n    vp-&gt;v_tag = tag;\n    vp-&gt;v_data = NULL;\n\n    /* Assign mount's normal operations vector */\n    vp-&gt;v_ops = &amp;mp-&gt;mnt_vn_use_ops;\n    vp-&gt;v_pbuf_count = nswbuf_kva / NSWBUF_SPLIT;\n\n    /* Make vnode visible on mount */\n    insmntque(vp, mp);\n\n    *vpp = vp;  /* VX locked &amp; refd */\n    return (0);\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#scanning-mount-vnodes","title":"Scanning Mount Vnodes","text":"<p>vmntvnodescan() (vfs_mount.c:894)</p> <p>Scans vnodes on a mount point with fast and slow callbacks:</p> <pre><code>int vmntvnodescan(struct mount *mp, int flags,\n                  int (*fastfunc)(...),\n                  int (*slowfunc)(...),\n                  void *data)\n</code></pre> <p>Flags: - <code>VMSC_GETVP</code> - Lock vnode with vget() before slowfunc - <code>VMSC_GETVX</code> - Lock vnode with vx_get() before slowfunc - <code>VMSC_NOWAIT</code> - Use LK_NOWAIT when locking - <code>VMSC_ONEPASS</code> - Stop after one pass through list</p> <p>Callback semantics: - <code>fastfunc()</code>: Called with only mnt_token held, vnode not locked   - Return &lt; 0: Skip slowfunc, continue   - Return 0: Call slowfunc   - Return &gt; 0: Terminate scan</p> <ul> <li><code>slowfunc()</code>: Called with vnode locked</li> <li>Return 0: Continue</li> <li>Return != 0: Terminate scan</li> </ul> <p>Used by vflush(), filesystem sync operations, and vnode reclamation.</p>"},{"location":"sys/kern/vfs/mounting/#mounting-a-filesystem","title":"Mounting a Filesystem","text":""},{"location":"sys/kern/vfs/mounting/#the-sys_mount-system-call","title":"The sys_mount() System Call","text":"<p>sys_mount() (vfs_syscalls.c:118)</p> <p>Main entry point for the mount(2) system call:</p> <pre><code>int sys_mount(struct sysmsg *sysmsg, const struct mount_args *uap)\n{\n    /* uap-&gt;type:  filesystem type name */\n    /* uap-&gt;path:  mount point path */\n    /* uap-&gt;flags: mount flags */\n    /* uap-&gt;data:  filesystem-specific data */\n}\n</code></pre> <p>Mount workflow:</p> <ol> <li>Permission and type checks (lines 136-156):</li> <li>Deny user mounts inside jails</li> <li>Copy in filesystem type name</li> <li>Check capabilities (get_fscap)</li> <li> <p>Enforce MNT_NOSUID|MNT_NODEV for non-root</p> </li> <li> <p>Path lookup (lines 174-207):</p> </li> <li>Use nlookup() to resolve mount point path</li> <li>Extract nchandle and vnode</li> <li> <p>Check if already mounted (cache_findmount)</p> </li> <li> <p>Update vs. new mount (lines 223-279):</p> </li> <li>For MNT_UPDATE: verify VROOT|VPFSROOT, check ownership</li> <li> <p>For new mount: check ownership, validate directory</p> </li> <li> <p>Find or load VFS (lines 309-339):</p> </li> <li>Look up vfsconf by name</li> <li> <p>Auto-load kernel module if not found (root only)</p> </li> <li> <p>Allocate mount structure (lines 350-361):</p> </li> <li>Allocate and initialize struct mount</li> <li>Call mount_init()</li> <li> <p>Set initial flags from vfsconf</p> </li> <li> <p>Call VFS_MOUNT (lines 395-412):</p> </li> <li>For update: call with MNT_UPDATE flag</li> <li> <p>For new: call to initialize filesystem</p> </li> <li> <p>Finalize mount (lines 426-451):</p> </li> <li>Create mnt_ncmountpt if needed</li> <li>Mark directory as mount point (NCF_ISMOUNTPT)</li> <li>Insert into mountlist</li> <li>Update process directories (checkdirs)</li> <li>Allocate syncer vnode</li> <li> <p>Call VFS_START()</p> </li> <li> <p>Error handling (lines 452-470):</p> </li> <li>On failure: stop syncer, free mount structure</li> <li>Clean up nchandles and vnode</li> </ol>"},{"location":"sys/kern/vfs/mounting/#mount-point-namecache-integration","title":"Mount Point Namecache Integration","text":"<p>Two nchandles connect a mount to the namecache:</p> <p>mnt_ncmounton - The directory being mounted on: - Set to the mount point directory - Used to navigate \"up\" from the filesystem</p> <p>mnt_ncmountpt - The root of the mounted filesystem: - Created with cache_allocroot() during mount - Marked with NCF_ISMOUNTPT flag - Used to navigate \"down\" into the filesystem</p> <p>checkdirs() (vfs_syscalls.c:494)</p> <p>After mounting, updates process current/root directories:</p> <pre><code>static void checkdirs(struct nchandle *old_nch,\n                      struct nchandle *new_nch)\n{\n    struct vnode *olddp = old_nch-&gt;ncp-&gt;nc_vp;\n    struct vnode *newdp;\n    struct mount *mp = new_nch-&gt;mount;\n\n    /* Skip if no processes reference the old vnode */\n    if (olddp == NULL || VREFCNT(olddp) == 1)\n        return;\n\n    /* Resolve new mount's root vnode */\n    VFS_ROOT(mp, &amp;newdp);\n    cache_setvp(new_nch, newdp);\n\n    /* Update rootvnode if mounting over root */\n    if (rootvnode == olddp) {\n        vref(newdp);\n        vfs_cache_setroot(newdp, cache_hold(new_nch));\n    }\n\n    /* Scan all processes, updating fd_ncdir/fd_nrdir */\n    allproc_scan(checkdirs_callback, &amp;info, 0);\n\n    vput(newdp);\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#filesystem-specific-mount-data","title":"Filesystem-Specific Mount Data","text":"<p>VFS calls VFS_MOUNT() to let the filesystem: 1. Parse filesystem-specific mount options from uap-&gt;data 2. Read filesystem superblock/metadata 3. Initialize mp-&gt;mnt_data with private data 4. Set mp-&gt;mnt_stat fields (f_bsize, f_blocks, etc.) 5. Optionally create root vnode</p> <p>Example (from a typical VFS_MOUNT): <pre><code>static int myfs_mount(struct mount *mp, char *path,\n                      caddr_t data, struct ucred *cred)\n{\n    struct myfs_mount *mmp;\n    struct vnode *devvp;\n    int error;\n\n    /* Parse mount options */\n    error = myfs_parse_opts(data, &amp;opts);\n\n    /* Open device */\n    error = nlookup_init(&amp;nd, opts.fspec, ...);\n    devvp = nd.nl_nch.ncp-&gt;nc_vp;\n\n    /* Read superblock */\n    error = myfs_read_super(devvp, &amp;sb);\n\n    /* Allocate private mount data */\n    mmp = kmalloc(sizeof(*mmp), M_MYFS, M_WAITOK | M_ZERO);\n    mmp-&gt;devvp = devvp;\n    mp-&gt;mnt_data = (qaddr_t)mmp;\n\n    /* Fill in mount stats */\n    mp-&gt;mnt_stat.f_bsize = sb.s_blocksize;\n    mp-&gt;mnt_stat.f_blocks = sb.s_blocks;\n    mp-&gt;mnt_stat.f_bfree = sb.s_free_blocks;\n\n    /* Set max symlink length */\n    mp-&gt;mnt_maxsymlinklen = sb.s_symlink_max;\n\n    vfs_getnewfsid(mp);  /* generate unique FSID */\n    return (0);\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#unmounting-a-filesystem","title":"Unmounting a Filesystem","text":""},{"location":"sys/kern/vfs/mounting/#the-sys_unmount-system-call","title":"The sys_unmount() System Call","text":"<p>sys_unmount() (vfs_syscalls.c:610)</p> <p>Entry point for umount(2):</p> <pre><code>int sys_unmount(struct sysmsg *sysmsg,\n                const struct unmount_args *uap)\n{\n    /* uap-&gt;path:  mount point path */\n    /* uap-&gt;flags: MNT_FORCE, etc. */\n}\n</code></pre> <p>Unmount workflow:</p> <ol> <li>Permission checks (lines 625-657):</li> <li>Deny user unmounts in jails</li> <li>Resolve path with nlookup()</li> <li>Check filesystem type capabilities</li> <li> <p>Verify ownership or root privilege</p> </li> <li> <p>Validation (lines 660-682):</p> </li> <li>Reject unmounting root filesystem</li> <li>Verify unmounting at mount point root</li> <li> <p>Check jail ownership</p> </li> <li> <p>Call dounmount() (lines 690-694):</p> </li> <li>Hold mount to prevent races</li> <li>Release nlookup resources</li> <li>Perform actual unmount</li> </ol>"},{"location":"sys/kern/vfs/mounting/#the-dounmount-function","title":"The dounmount() Function","text":"<p>dounmount() (vfs_syscalls.c:793)</p> <p>The core unmount implementation:</p> <pre><code>int dounmount(struct mount *mp, int flags, int halting)\n</code></pre> <p>Unmount phases:</p> <p>1. Interlock and lock (lines 806-842): <pre><code>/* Check for quickhalt (devfs, tmpfs, procfs on shutdown) */\nif (halting &amp;&amp; (mp-&gt;mnt_kern_flag &amp; MNTK_QUICKHALT))\n    quickhalt = 1;\n\n/* Set MNTK_UNMOUNT flag atomically */\nmountlist_interlock(dounmount_interlock, mp);\n\n/* Set MNTK_UNMOUNTF if forced */\nif (flags &amp; MNT_FORCE)\n    mp-&gt;mnt_kern_flag |= MNTK_UNMOUNTF;\n\n/* Acquire exclusive mount lock */\nlflags = LK_EXCLUSIVE | ((flags &amp; MNT_FORCE) ? 0 : LK_TIMELOCK);\nerror = lockmgr(&amp;mp-&gt;mnt_lock, lflags);\n</code></pre></p> <p>2. Sync and stop syncer (lines 847-871): <pre><code>/* Sync dirty data */\nvfs_msync(mp, MNT_WAIT);\nmp-&gt;mnt_flag &amp;= ~MNT_ASYNC;\n\n/* Stop syncer vnode */\nif ((vp = mp-&gt;mnt_syncer) != NULL) {\n    mp-&gt;mnt_syncer = NULL;\n    vrele(vp);\n}\n\n/* Final sync (unless quickhalt) */\nif (quickhalt == 0) {\n    if ((mp-&gt;mnt_flag &amp; MNT_RDONLY) == 0)\n        VFS_SYNC(mp, MNT_WAIT);\n}\n</code></pre></p> <p>3. Wait for references to drain (lines 880-955): <pre><code>for (retry = 0; retry &lt; UMOUNTF_RETRIES; ++retry) {\n    /* Invalidate namecache under mount point */\n    if ((mp-&gt;mnt_kern_flag &amp; MNTK_NCALIASED) == 0) {\n        cache_inval(&amp;mp-&gt;mnt_ncmountpt,\n                   CINV_DESTROY | CINV_CHILDREN);\n    }\n\n    /* Clear per-CPU caches */\n    cache_unmounting(mp);\n    if (mp-&gt;mnt_refs != 1)\n        cache_clearmntcache(mp);\n\n    /* Check if ready to unmount */\n    ncp = mp-&gt;mnt_ncmountpt.ncp;\n    if (mp-&gt;mnt_refs == 1 &amp;&amp;\n        (ncp == NULL || (ncp-&gt;nc_refs == 1 &amp;&amp;\n                        TAILQ_FIRST(&amp;ncp-&gt;nc_list) == NULL))) {\n        break;  /* Success! */\n    }\n\n    /* Force unmount: kill processes using the mount */\n    if (flags &amp; MNT_FORCE) {\n        switch(retry) {\n        case 3:  info.sig = SIGINT;  break;\n        case 7:  info.sig = SIGKILL; break;\n        default: info.sig = 0;       break;\n        }\n        allproc_scan(&amp;unmount_allproc_cb, &amp;info, 0);\n    }\n\n    /* Sleep and retry */\n    tsleep(&amp;dummy, 0, \"mntbsy\", hz / 4 + 1);\n}\n</code></pre></p> <p>The retry loop: - Invalidates the namecache tree under the mount - Clears per-CPU namecache entries - Checks if mnt_refs dropped to 1 (only mnt_ncmountpt left) - For forced unmount: sends SIGINT (retry 3) then SIGKILL (retry 7) to processes - Retries up to 50 times (12.5 seconds)</p> <p>4. Call VFS_UNMOUNT (lines 990-1007): <pre><code>if (error == 0 &amp;&amp; quickhalt == 0) {\n    if (mp-&gt;mnt_flag &amp; MNT_RDONLY) {\n        error = VFS_UNMOUNT(mp, flags);\n    } else {\n        error = VFS_SYNC(mp, MNT_WAIT);\n        if (error == 0 || error == EOPNOTSUPP ||\n            (flags &amp; MNT_FORCE)) {\n            error = VFS_UNMOUNT(mp, flags);\n        }\n    }\n}\n</code></pre></p> <p>5. Handle errors or finalize (lines 1010-1120):</p> <p>On error: <pre><code>if (error) {\n    /* Re-allocate syncer if needed */\n    if (mp-&gt;mnt_syncer == NULL &amp;&amp; hadsyncer)\n        vfs_allocate_syncvnode(mp);\n\n    /* Clear unmount flags */\n    mp-&gt;mnt_kern_flag &amp;= ~(MNTK_UNMOUNT | MNTK_UNMOUNTF);\n    mp-&gt;mnt_flag |= async_flag;\n\n    /* Release lock and wakeup waiters */\n    lockmgr(&amp;mp-&gt;mnt_lock, LK_RELEASE);\n    if (mp-&gt;mnt_kern_flag &amp; MNTK_MWAIT)\n        wakeup(mp);\n\n    goto out;\n}\n</code></pre></p> <p>On success: <pre><code>/* Remove journals */\njournal_remove_all_journals(mp, ...);\n\n/* Remove from mountlist */\nmountlist_remove(mp);\n\n/* Remove vnode ops (unless quickhalt) */\nif (quickhalt == 0) {\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_coherency_ops);\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_journal_ops);\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_norm_ops);\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_spec_ops);\n    vfs_rm_vnodeops(mp, NULL, &amp;mp-&gt;mnt_vn_fifo_ops);\n}\n\n/* Drop nchandle references */\nif (mp-&gt;mnt_ncmountpt.ncp != NULL) {\n    nch = mp-&gt;mnt_ncmountpt;\n    cache_zero(&amp;mp-&gt;mnt_ncmountpt);\n    cache_clrmountpt(&amp;nch);\n    cache_drop(&amp;nch);\n}\nif (mp-&gt;mnt_ncmounton.ncp != NULL) {\n    nch = mp-&gt;mnt_ncmounton;\n    cache_zero(&amp;mp-&gt;mnt_ncmounton);\n    cache_clrmountpt(&amp;nch);\n    cache_drop(&amp;nch);\n}\n\n/* Release credentials */\nif (mp-&gt;mnt_cred) {\n    crfree(mp-&gt;mnt_cred);\n    mp-&gt;mnt_cred = NULL;\n}\n\n/* Decrement vfsconf refcount */\nmp-&gt;mnt_vfc-&gt;vfc_refcount--;\n\n/* Verify no vnodes remain (unless quickhalt) */\nif (quickhalt == 0 &amp;&amp; !TAILQ_EMPTY(&amp;mp-&gt;mnt_nvnodelist))\n    panic(\"unmount: dangling vnode\");\n\n/* Release lock */\nlockmgr(&amp;mp-&gt;mnt_lock, LK_RELEASE);\n\n/* Free mount structure if freeok */\nif (freeok) {\n    /* Wait for mnt_refs to drop to 0 */\n    while (mp-&gt;mnt_refs &gt; 0) {\n        cache_clearmntcache(mp);\n        tsleep(&amp;mp-&gt;mnt_refs, 0, \"umntrwait\", hz / 10 + 1);\n    }\n    mount_drop(mp);  /* Final free */\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#forced-unmount","title":"Forced Unmount","text":"<p>When <code>MNT_FORCE</code> is specified:</p> <ol> <li>MNTK_UNMOUNTF flag alerts filesystem of forced unmount</li> <li>Process termination: Processes using the mount are killed:</li> <li>Retry 3: Send SIGINT</li> <li>Retry 7: Send SIGKILL</li> <li>Checks: p_textnch, fd_ncdir, fd_nrdir, fd_njdir, open files</li> <li>Ignore busy errors: Unmount proceeds even if refs remain</li> <li>vflush FORCECLOSE: Vnodes are forcibly reclaimed</li> </ol> <p>unmount_allproc_cb() (vfs_syscalls.c:761): <pre><code>static int unmount_allproc_cb(struct proc *p, void *arg)\n{\n    struct unmount_allproc_info *info = arg;\n    struct mount *mp = info-&gt;mp;\n\n    /* Drop text reference */\n    if (p-&gt;p_textnch.mount == mp)\n        cache_drop(&amp;p-&gt;p_textnch);\n\n    /* Signal if using mount */\n    if (info-&gt;sig &amp;&amp; process_uses_mount(p, mp)) {\n        p-&gt;p_flags |= P_MUSTKILL;\n        ksignal(p, info-&gt;sig);\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#vflush-flushing-mount-vnodes","title":"vflush() - Flushing Mount Vnodes","text":"<p>vflush() (vfs_mount.c:1070)</p> <p>Removes vnodes from a mount during unmount:</p> <pre><code>int vflush(struct mount *mp, int rootrefs, int flags)\n{\n    /* flags:\n     *   FORCECLOSE - forcibly close active vnodes\n     *   WRITECLOSE - close vnodes open for writing\n     *   SKIPSYSTEM - skip VSYSTEM vnodes\n     */\n}\n</code></pre> <p>Uses vmntvnodescan() to iterate vnodes:</p> <p>vflush_scan() (vfs_mount.c:1124): <pre><code>static int vflush_scan(struct mount *mp, struct vnode *vp,\n                       void *data)\n{\n    struct vflush_info *info = data;\n    int flags = info-&gt;flags;\n\n    /* Mark for finalization */\n    atomic_set_int(&amp;vp-&gt;v_refcnt, VREF_FINALIZE);\n\n    /* Skip VSYSTEM vnodes if requested */\n    if ((flags &amp; SKIPSYSTEM) &amp;&amp; (vp-&gt;v_flag &amp; VSYSTEM))\n        return (0);\n\n    /* Don't force-close VCHR/VBLK */\n    if (vp-&gt;v_type == VCHR || vp-&gt;v_type == VBLK)\n        flags &amp;= ~(WRITECLOSE | FORCECLOSE);\n\n    /* WRITECLOSE: only flush writable regular files */\n    if ((flags &amp; WRITECLOSE) &amp;&amp; ...) {\n        return (0);\n    }\n\n    /* If only holder, reclaim vnode */\n    if (VREFCNT(vp) &lt;= 1) {\n        vgone_vxlocked(vp);\n        return (0);\n    }\n\n    /* FORCECLOSE: forcibly destroy vnode */\n    if (flags &amp; FORCECLOSE) {\n        vhold(vp);\n        vgone_vxlocked(vp);\n        if (vp-&gt;v_mount == NULL)\n            insmntque(vp, &amp;dummymount);  /* orphan vnode */\n        vdrop(vp);\n        return (0);\n    }\n\n    /* Vnode is busy */\n    ++info-&gt;busy;\n    return (0);\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/mounting/#bio-operations-integration","title":"Bio Operations Integration","text":""},{"location":"sys/kern/vfs/mounting/#struct-bio_ops","title":"struct bio_ops","text":"<p>Mount points can register bio_ops for buffer I/O interception (used by HAMMER, soft updates):</p> <pre><code>struct bio_ops {\n    TAILQ_ENTRY(bio_ops) entry;\n    void (*io_start)(struct buf *);         /* I/O initiated */\n    void (*io_complete)(struct buf *);      /* I/O completed */\n    void (*io_deallocate)(struct buf *);    /* buffer freed */\n    int  (*io_fsync)(struct vnode *);       /* fsync vnode */\n    int  (*io_sync)(struct mount *);        /* sync filesystem */\n    void (*io_movedeps)(struct buf *, struct buf *);  /* move deps */\n    int  (*io_countdeps)(struct buf *, int); /* count deps */\n    int  (*io_checkread)(struct buf *);     /* check read */\n    int  (*io_checkwrite)(struct buf *);    /* check write */\n};\n</code></pre> <p>add_bio_ops() / rem_bio_ops() (vfs_mount.c:1199, 1205):</p> <pre><code>void add_bio_ops(struct bio_ops *ops)\n{\n    TAILQ_INSERT_TAIL(&amp;bio_ops_list, ops, entry);\n}\n\nvoid rem_bio_ops(struct bio_ops *ops)\n{\n    TAILQ_REMOVE(&amp;bio_ops_list, ops, entry);\n}\n</code></pre> <p>bio_ops_sync() (vfs_mount.c:1218):</p> <p>Called during sync operations:</p> <pre><code>void bio_ops_sync(struct mount *mp)\n{\n    struct bio_ops *ops;\n\n    if (mp) {\n        /* Sync specific mount */\n        if ((ops = mp-&gt;mnt_bioops) != NULL)\n            ops-&gt;io_sync(mp);\n    } else {\n        /* Sync all registered bio_ops */\n        TAILQ_FOREACH(ops, &amp;bio_ops_list, entry) {\n            ops-&gt;io_sync(NULL);\n        }\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/mounting/#root-filesystem-mounting","title":"Root Filesystem Mounting","text":""},{"location":"sys/kern/vfs/mounting/#vfs_rootmountalloc","title":"vfs_rootmountalloc()","text":"<p>vfs_rootmountalloc() (vfs_mount.c:332)</p> <p>Allocates a mount structure for the root filesystem:</p> <pre><code>int vfs_rootmountalloc(char *fstypename, char *devname,\n                       struct mount **mpp)\n{\n    struct vfsconf *vfsp;\n    struct mount *mp;\n\n    /* Find filesystem type */\n    vfsp = vfsconf_find_by_name(fstypename);\n    if (vfsp == NULL)\n        return (ENODEV);\n\n    /* Allocate and initialize mount */\n    mp = kmalloc(sizeof(struct mount), M_MOUNT,\n                 M_WAITOK | M_ZERO);\n    mount_init(mp, vfsp-&gt;vfc_vfsops);\n    lockinit(&amp;mp-&gt;mnt_lock, \"vfslock\", VLKTIMEOUT, 0);\n    lockinit(&amp;mp-&gt;mnt_renlock, \"renamlk\", VLKTIMEOUT, 0);\n\n    /* Mark as busy */\n    vfs_busy(mp, 0);\n\n    /* Set initial state */\n    mp-&gt;mnt_vfc = vfsp;\n    mp-&gt;mnt_pbuf_count = nswbuf_kva / NSWBUF_SPLIT;\n    vfsp-&gt;vfc_refcount++;\n    mp-&gt;mnt_stat.f_type = vfsp-&gt;vfc_typenum;\n    mp-&gt;mnt_flag |= MNT_RDONLY;\n    mp-&gt;mnt_flag |= vfsp-&gt;vfc_flags &amp; MNT_VISFLAGMASK;\n\n    /* Set names */\n    strncpy(mp-&gt;mnt_stat.f_fstypename, vfsp-&gt;vfc_name,\n            MFSNAMELEN);\n    copystr(devname, mp-&gt;mnt_stat.f_mntfromname,\n            MNAMELEN - 1, 0);\n\n    /* Pre-set MPSAFE flags */\n    if (vfsp-&gt;vfc_flags &amp; VFCF_MPSAFE)\n        mp-&gt;mnt_kern_flag |= MNTK_ALL_MPSAFE;\n\n    *mpp = mp;\n    return (0);\n}\n</code></pre> <p>Called early in kernel initialization before the full VFS is available.</p>"},{"location":"sys/kern/vfs/mounting/#summary","title":"Summary","text":""},{"location":"sys/kern/vfs/mounting/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Mount lifecycle: init \u2192 busy \u2192 insert \u2192 operate \u2192 unmount \u2192 free</li> <li>Two reference counters: mnt_refs (nchandle) and mnt_hold (kfree prevention)</li> <li>Two nchandles: mnt_ncmounton (mounting on) and mnt_ncmountpt (root of fs)</li> <li>Busy protocol: vfs_busy/unbusy prevents unmount during operations</li> <li>Mount lists: mountlist (ordered) and mounttree (RB-tree by FSID)</li> <li>Safe iteration: mountlist_scan() and vmntvnodescan() handle concurrent modifications</li> <li>Forced unmount: SIGINT \u2192 SIGKILL for processes, FORCECLOSE for vnodes</li> <li>FSID generation: Based on mount path hash, auto-adjusted on collision</li> <li>Syncer integration: Each mount has a syncer vnode for dirty data tracking</li> <li>Bio ops: Extensible buffer I/O hooks for soft updates, journaling</li> </ol>"},{"location":"sys/kern/vfs/mounting/#important-invariants","title":"Important Invariants","text":"<ul> <li>Mount on mountlist \u21d4 visible to lookups</li> <li>MNTK_UNMOUNT set \u21d4 vfs_busy() will fail</li> <li>mnt_refs == 1 at unmount \u21d4 only mnt_ncmountpt reference remains</li> <li>mnt_lock exclusive \u21d4 unmounting or updating</li> <li>mnt_lock shared \u21d4 normal operations (via vfs_busy)</li> </ul>"},{"location":"sys/kern/vfs/mounting/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Forgetting vfs_unbusy(): Prevents unmounting</li> <li>Not checking MNTK_UNMOUNT: Can race with unmount</li> <li>Holding mnt_token too long: Blocks all mount operations</li> <li>Not using mount_hold/drop: Can cause use-after-free</li> <li>Modifying mount without mnt_token: Race conditions</li> </ol>"},{"location":"sys/kern/vfs/mounting/#related-documentation","title":"Related Documentation","text":"<ul> <li>VFS Name Lookup and Caching - Namecache integration</li> <li>VFS Initialization - System initialization</li> <li>VFS Vnodes - Vnode lifecycle</li> </ul>"},{"location":"sys/kern/vfs/mounting/#source-code-locations","title":"Source Code Locations","text":"<ul> <li><code>sys/kern/vfs_mount.c</code> - Mount point management</li> <li><code>sys/kern/vfs_syscalls.c:118</code> - sys_mount()</li> <li><code>sys/kern/vfs_syscalls.c:610</code> - sys_unmount()</li> <li><code>sys/kern/vfs_syscalls.c:793</code> - dounmount()</li> <li><code>sys/sys/mount.h:216</code> - struct mount definition</li> <li><code>sys/sys/mount.h:274</code> - Mount flags (MNT_*)</li> <li><code>sys/sys/mount.h:348</code> - Kernel flags (MNTK_*)</li> </ul>"},{"location":"sys/kern/vfs/namecache/","title":"VFS Name Lookup and Caching","text":""},{"location":"sys/kern/vfs/namecache/#overview","title":"Overview","text":"<p>The DragonFly BSD namecache subsystem provides a high-performance layer between pathname lookup operations and the underlying filesystem. It consists of three main components:</p> <ol> <li>Namecache (<code>vfs_cache.c</code>) - The core caching infrastructure that maintains mappings between directory entries and vnodes</li> <li>New Lookup API (<code>vfs_nlookup.c</code>) - Modern path resolution based on namecache records instead of vnode locking</li> <li>Legacy Lookup (<code>vfs_lookup.c</code>) - Traditional BSD <code>namei()</code> compatibility for old code</li> </ol> <p>The namecache is fundamental to DragonFly's VFS design and all filesystem operations must interact with it, even filesystems that don't want to cache.</p>"},{"location":"sys/kern/vfs/namecache/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/kern/vfs/namecache/#struct-namecache-sysnamecacheh125","title":"<code>struct namecache</code> (<code>sys/namecache.h:125</code>)","text":"<p>The core namecache entry representing a single pathname component:</p> <pre><code>struct namecache {\n    TAILQ_ENTRY(namecache) nc_hash;      /* hash chain (nc_parent,name) */\n    TAILQ_ENTRY(namecache) nc_entry;     /* scan via nc_parent-&gt;nc_list */\n    TAILQ_ENTRY(namecache) nc_vnode;     /* scan via vnode-&gt;v_namecache */\n    struct namecache_list  nc_list;      /* list of children */\n    struct nchash_head    *nc_head;\n    struct namecache      *nc_parent;    /* namecache entry for parent */\n    struct vnode          *nc_vp;        /* vnode representing name or NULL */\n    u_short               nc_flag;\n    u_char                nc_nlen;       /* name length, 255 max */\n    u_char                nc_unused;\n    char                  *nc_name;      /* separately allocated segment name */\n    int                   nc_error;\n    int                   nc_timeout;    /* compared against ticks, or 0 */\n    int                   nc_negcpu;     /* which ncneg list are we on? */\n    struct {\n        u_int             nc_namecache_gen; /* mount generation (autoclear) */\n        u_int             nc_generation;    /* see notes below */\n        int               nc_refs;          /* ref count prevents deletion */\n    } __cachealign;\n    struct {\n        struct lock       nc_lock;\n    } __cachealign;\n};\n</code></pre> <p>Key fields:</p> <ul> <li><code>nc_parent</code> - Points to parent directory's namecache entry, forming a tree from leaf to root</li> <li><code>nc_vp</code> - The vnode this entry represents; NULL for negative cache entries (non-existent files)</li> <li><code>nc_name</code> - The filename component (allocated separately)</li> <li><code>nc_list</code> - Children of this directory entry</li> <li><code>nc_refs</code> - Reference count (naturally 1, +1 if resolved, +1 for each child)</li> <li><code>nc_generation</code> - Incremented by 2 when entry changes, allowing lock-free detection of modifications</li> <li><code>nc_flag</code> - See flags below</li> </ul> <p>Important flags (<code>NCF_*</code>):</p> <ul> <li><code>NCF_UNRESOLVED</code> (0x0004) - Entry not yet resolved or invalidated</li> <li><code>NCF_WHITEOUT</code> (0x0002) - Negative entry is a whiteout (for union mounts)</li> <li><code>NCF_DESTROYED</code> (0x0400) - Name association considered destroyed</li> <li><code>NCF_ISMOUNTPT</code> (0x0008) - Someone may have mounted here</li> <li><code>NCF_ISSYMLINK</code> (0x0100) - Entry is a symlink</li> <li><code>NCF_ISDIR</code> (0x0200) - Entry is a directory</li> </ul>"},{"location":"sys/kern/vfs/namecache/#struct-nchandle-sysnamecacheh154","title":"<code>struct nchandle</code> (<code>sys/namecache.h:154</code>)","text":"<p>A handle to a namecache entry with associated mount point:</p> <pre><code>struct nchandle {\n    struct namecache *ncp;    /* ncp in underlying filesystem */\n    struct mount     *mount;  /* mount pt (possible overlay) */\n};\n</code></pre> <p>The mount reference allows topologies to be replicated across mount overlays (nullfs, unionfs, etc.). This is DragonFly's key innovation for handling stacked filesystems.</p>"},{"location":"sys/kern/vfs/namecache/#struct-nlookupdata-sysnlookuph68","title":"<code>struct nlookupdata</code> (<code>sys/nlookup.h:68</code>)","text":"<p>Encapsulates all state for a path lookup operation:</p> <pre><code>struct nlookupdata {\n    struct nchandle  nl_nch;       /* result */\n    struct nchandle *nl_basench;   /* start-point directory */\n    struct nchandle  nl_rootnch;   /* root directory */\n    struct nchandle  nl_jailnch;   /* jail directory */\n\n    char            *nl_path;      /* path buffer */\n    struct thread   *nl_td;        /* thread requesting the nlookup */\n    struct ucred    *nl_cred;      /* credentials for nlookup */\n    struct vnode    *nl_dvp;       /* NLC_REFDVP */\n\n    int              nl_flags;     /* operations flags */\n    int              nl_loopcnt;   /* symlinks encountered */\n    int              nl_dir_error; /* error assoc w/intermediate dir */\n    int              nl_elmno;     /* iteration# to help caches */\n\n    struct vnode    *nl_open_vp;\n    int              nl_vp_fmode;\n};\n</code></pre> <p>Key flags (<code>NLC_*</code>):</p> <ul> <li><code>NLC_FOLLOW</code> (0x00000001) - Follow leaf symlink</li> <li><code>NLC_CREATE</code> (0x00000080) - Do create checks</li> <li><code>NLC_DELETE</code> (0x00000100) - Do delete checks</li> <li><code>NLC_RENAME_SRC</code> (0x00002000) - Do rename checks (source)</li> <li><code>NLC_RENAME_DST</code> (0x00000200) - Do rename checks (target)</li> <li><code>NLC_OPEN</code> (0x00000400) - Do open checks</li> <li><code>NLC_SHAREDLOCK</code> (0x00004000) - Allow shared ncp &amp; vp lock</li> <li><code>NLC_REFDVP</code> (0x00040000) - Set ref'd/unlocked nl_dvp</li> <li><code>NLC_READ</code> (0x00400000) - Require read access</li> <li><code>NLC_WRITE</code> (0x00800000) - Require write access</li> <li><code>NLC_EXEC</code> (0x01000000) - Require execute access</li> </ul>"},{"location":"sys/kern/vfs/namecache/#struct-nlcomponent-sysnlookuph55","title":"<code>struct nlcomponent</code> (<code>sys/nlookup.h:55</code>)","text":"<p>Represents a single path component during lookup:</p> <pre><code>struct nlcomponent {\n    char *nlc_nameptr;\n    int   nlc_namelen;\n};\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#architecture","title":"Architecture","text":""},{"location":"sys/kern/vfs/namecache/#namecache-topology","title":"Namecache Topology","text":"<p>The DragonFly namecache maintains a complete path from any active vnode to the root (except for NFS server and removed files). This is a key difference from traditional BSD systems:</p> <pre><code>Root (\"/\")\n  \u2514\u2500 bin/\n      \u2514\u2500 ls (vnode)\n  \u2514\u2500 usr/\n      \u2514\u2500 local/\n          \u2514\u2500 bin/\n              \u2514\u2500 bash (vnode)\n</code></pre> <p>Each <code>namecache</code> entry points to its <code>nc_parent</code>, and parents maintain a list of children in <code>nc_list</code>. This bidirectional tree enables:</p> <ol> <li>Efficient forward lookups (parent \u2192 child)</li> <li>Efficient reverse path reconstruction (child \u2192 root for <code>getcwd()</code>)</li> <li>Efficient subtree invalidation (e.g., when unmounting)</li> </ol>"},{"location":"sys/kern/vfs/namecache/#positive-vs-negative-caching","title":"Positive vs Negative Caching","text":"<p>Positive entries: - <code>nc_vp</code> != NULL - Represent files/directories that exist - May be unresolved (NCF_UNRESOLVED) if not yet looked up</p> <p>Negative entries: - <code>nc_vp</code> == NULL - Represent failed lookups (file doesn't exist) - Stored in per-CPU negative lists for quick reclamation - May be whiteouts (NCF_WHITEOUT) for union filesystems</p> <p>Negative caching is crucial for performance, avoiding expensive filesystem lookups for common cases like: - PATH searches in shell (<code>/bin/foo</code>, <code>/usr/bin/foo</code>, etc.) - Probing for config files (<code>.bashrc</code>, <code>.config/app</code>, etc.)</p>"},{"location":"sys/kern/vfs/namecache/#hash-table-organization","title":"Hash Table Organization","text":"<p>The namecache uses a hash table indexed by <code>(nc_parent, name)</code>:</p> <pre><code>#define NCHHASH(hash)  (&amp;nchashtbl[(hash) &amp; nchash])\n</code></pre> <p>Each hash bucket has its own spinlock. The implementation uses an update counter (<code>nc_generation</code>) to allow lock-free lookups in common cases - the code can detect if an entry changed during access and retry with locks if needed.</p>"},{"location":"sys/kern/vfs/namecache/#reference-counting","title":"Reference Counting","text":"<p>Namecache entries use <code>nc_refs</code> for lifecycle management:</p> <ul> <li>Base ref: 1 (entry exists)</li> <li>Resolved ref: +1 if entry is resolved (positive or negative)</li> <li>Child refs: +1 for each child in <code>nc_list</code></li> <li>Lookup refs: +1 while held by threads or mountcache</li> </ul> <p>On the 1\u21920 transition, the entry must be destroyed immediately. The entry cannot be on any list at this point.</p> <p>Reference management functions:</p> <ul> <li><code>cache_hold()</code> / <code>cache_get()</code> - Acquire reference</li> <li><code>cache_put()</code> - Release reference (drop lock + drop ref)</li> <li><code>cache_drop()</code> - Release reference (no lock, just drop ref)</li> <li><code>_cache_drop()</code> - Internal version that handles 1\u21920 transition</li> </ul>"},{"location":"sys/kern/vfs/namecache/#locking-strategy","title":"Locking Strategy","text":"<p>DragonFly uses child-to-parent lock ordering:</p> <ol> <li>Lock child first, then parent</li> <li>Allows forward scans (parent \u2192 child) to hold parent unlocked</li> <li>Deletions propagate bottom-up naturally</li> </ol> <p>Lock types:</p> <ul> <li>Exclusive locks - Required for modifications, last element of path (unless NLC_SHAREDLOCK)</li> <li>Shared locks - Allowed for intermediate path components, read-only operations</li> </ul> <p>The <code>nc_generation</code> field enables optimistic lock-free access:</p> <pre><code>static __inline void\n_cache_ncp_gen_enter(struct namecache *ncp)\n{\n    ncp-&gt;nc_generation += 2;  /* Odd = in-progress */\n    cpu_sfence();\n}\n\nstatic __inline void\n_cache_ncp_gen_exit(struct namecache *ncp)\n{\n    cpu_sfence();\n    ncp-&gt;nc_generation += 2;  /* Even = stable */\n    cpu_sfence();\n}\n</code></pre> <p>If <code>(nc_generation &amp; 1)</code> is set, modification is in progress. Readers check generation before and after access, retrying if it changed.</p>"},{"location":"sys/kern/vfs/namecache/#core-namecache-operations","title":"Core Namecache Operations","text":""},{"location":"sys/kern/vfs/namecache/#path-lookup-cache_nlookup-vfs_cachec3228","title":"Path Lookup: <code>cache_nlookup()</code> (<code>vfs_cache.c:3228</code>)","text":"<p>Looks up a single path component in the cache:</p> <pre><code>struct nchandle cache_nlookup(struct nchandle *par_nch, \n                              struct nlcomponent *nlc);\n</code></pre> <p>Algorithm:</p> <ol> <li>Hash lookup - Compute <code>hash(parent, name)</code> and search bucket</li> <li>Lock-free scan - Check each entry's generation before/after reading</li> <li>Match found - Return referenced nchandle</li> <li>Miss - Call <code>cache_nlookup_create()</code> to create unresolved entry</li> <li>Resolve - Call <code>cache_resolve()</code> to resolve via VOP_NRESOLVE()</li> </ol> <p>Variants:</p> <ul> <li><code>cache_nlookup_nonblock()</code> - Returns immediately if lock unavailable</li> <li><code>cache_nlookup_nonlocked()</code> - Optimistic lock-free lookup</li> <li><code>cache_nlookup_maybe_shared()</code> - Allows shared locks if <code>!excl</code></li> </ul>"},{"location":"sys/kern/vfs/namecache/#resolution-cache_resolve-vfs_cachec4273","title":"Resolution: <code>cache_resolve()</code> (<code>vfs_cache.c:4273</code>)","text":"<p>Resolves an unresolved namecache entry by calling into the filesystem:</p> <pre><code>int cache_resolve(struct nchandle *nch, u_int *genp, struct ucred *cred);\n</code></pre> <p>Steps:</p> <ol> <li>Check if already resolved (fast path)</li> <li>Check generation counter for races</li> <li>Call <code>VOP_NRESOLVE(dvp, ncp, cred)</code> on parent directory</li> <li>Filesystem fills in <code>ncp-&gt;nc_vp</code> or leaves NULL for negative entry</li> <li>Clear <code>NCF_UNRESOLVED</code> flag</li> </ol> <p>This is the critical bridge between namecache and filesystem-specific code.</p>"},{"location":"sys/kern/vfs/namecache/#invalidation-cache_inval-vfs_cachec1692","title":"Invalidation: <code>cache_inval()</code> (<code>vfs_cache.c:1692</code>)","text":"<p>Invalidates a namecache entry:</p> <pre><code>int cache_inval(struct nchandle *nch, int flags);\n</code></pre> <p>Flags:</p> <ul> <li><code>CINV_DESTROY</code> (0x0001) - Mark destroyed so lookups ignore it</li> <li><code>CINV_CHILDREN</code> (0x0004) - Recursively invalidate all children</li> </ul> <p>Use cases:</p> <ul> <li>File/directory deletion</li> <li>Filesystem unmount</li> <li>Stale NFS entries</li> </ul>"},{"location":"sys/kern/vfs/namecache/#reference-management","title":"Reference Management","text":"<p><code>cache_get()</code> (<code>vfs_cache.c:1293</code>)</p> <p>Acquires a reference and returns a handle:</p> <pre><code>void cache_get(struct nchandle *nch, struct nchandle *target);\n</code></pre> <p><code>cache_put()</code> (<code>vfs_cache.c:1322</code>)</p> <p>Drops lock and reference:</p> <pre><code>void cache_put(struct nchandle *nch);\n</code></pre> <p><code>cache_drop()</code> (<code>vfs_cache.c:1090</code>)</p> <p>Drops reference without affecting lock:</p> <pre><code>void cache_drop(struct nchandle *nch);\n</code></pre> <p><code>cache_lock()</code> (<code>vfs_cache.c:1111</code>)</p> <p>Acquires exclusive lock on namecache entry:</p> <pre><code>void cache_lock(struct nchandle *nch);\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#mount-point-caching","title":"Mount Point Caching","text":"<p>DragonFly caches mount point references to reduce atomic operations on <code>mnt_refs</code>:</p> <p>Per-CPU mount cache (<code>pcpu_mntcache</code>):</p> <pre><code>struct mntcache_elm {\n    struct namecache *ncp;\n    struct mount     *mp;\n    int              ticks;\n    int              unused01;\n};\n</code></pre> <ul> <li>32 entries per CPU, 8-way set associative</li> <li>LRU replacement based on <code>ticks</code></li> <li>Avoids cache-line ping-ponging in multi-socket systems</li> </ul> <p>Functions:</p> <ul> <li><code>_cache_mntref()</code> - Cache a mount ref</li> <li><code>_cache_mntrel()</code> - Release a cached mount ref</li> <li><code>cache_findmount()</code> - Find mount point for a namecache entry</li> </ul>"},{"location":"sys/kern/vfs/namecache/#new-lookup-api-nlookup","title":"New Lookup API (nlookup)","text":"<p>The nlookup API is DragonFly's modern path resolution interface, replacing the traditional <code>namei()</code>.</p>"},{"location":"sys/kern/vfs/namecache/#key-advantages-over-namei","title":"Key Advantages Over namei()","text":"<ol> <li>Namecache-centric - Operations work on namecache records, not vnodes</li> <li>Better parallelism - Lock granularity is per-entry, not per-vnode</li> <li>Cleaner semantics - Locked/unlocked state is explicit</li> <li>Overlay-aware - Native support for nullfs, unionfs via <code>nchandle</code></li> <li>Negative caching - First-class support for non-existent entries</li> </ol>"},{"location":"sys/kern/vfs/namecache/#usage-pattern","title":"Usage Pattern","text":"<pre><code>struct nlookupdata nd;\nint error;\n\n/* Initialize lookup */\nerror = nlookup_init(&amp;nd, path, UIO_USERSPACE, NLC_FOLLOW);\nif (error == 0) {\n    /* Perform lookup */\n    error = nlookup(&amp;nd);\n    if (error == 0) {\n        /* Use nd.nl_nch result */\n        struct nchandle *nch = &amp;nd.nl_nch;\n        struct vnode *vp;\n\n        error = cache_vget(nch, cred, LK_EXCLUSIVE, &amp;vp);\n        if (error == 0) {\n            /* ... use vp ... */\n            vput(vp);\n        }\n    }\n    /* Cleanup */\n    nlookup_done(&amp;nd);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#core-functions","title":"Core Functions","text":""},{"location":"sys/kern/vfs/namecache/#nlookup_init-vfs_nlookupc116","title":"<code>nlookup_init()</code> (<code>vfs_nlookup.c:116</code>)","text":"<p>Initialize a lookup operation:</p> <pre><code>int nlookup_init(struct nlookupdata *nd, const char *path, \n                 enum uio_seg seg, int flags);\n</code></pre> <p>Setup:</p> <ol> <li>Allocate path buffer from <code>namei_oc</code> objcache</li> <li>Copy path from userspace/kernelspace</li> <li>Initialize <code>nl_nch</code> to current working directory (or root)</li> <li>Copy root directory to <code>nl_rootnch</code></li> <li>Copy jail directory to <code>nl_jailnch</code> (if jailed)</li> <li>Set credentials from current thread</li> </ol>"},{"location":"sys/kern/vfs/namecache/#nlookup-vfs_nlookupc530","title":"<code>nlookup()</code> (<code>vfs_nlookup.c:530</code>)","text":"<p>Perform the actual path lookup:</p> <pre><code>int nlookup(struct nlookupdata *nd);\n</code></pre> <p>Main loop algorithm:</p> <pre><code>for each path component:\n    1. Skip leading '/' characters\n    2. Check for root directory replacement\n    3. Check execute permission on current directory (naccess)\n    4. Extract next component (up to 255 chars)\n    5. Handle special cases:\n       - \".\" = current directory (no-op)\n       - \"..\" = parent directory (traverse mounts)\n       - regular name = cache_nlookup()\n    6. If unresolved, call cache_resolve()\n    7. Handle symlinks (if NLC_FOLLOW and nc_flag &amp; NCF_ISSYMLINK)\n    8. Handle mount point crossings\n    9. Perform access checks based on nl_flags\n    10. Update nl_nch to point to new entry\n</code></pre> <p>Symlink handling:</p> <ul> <li>Detect via <code>NCF_ISSYMLINK</code> flag</li> <li>Read symlink contents via <code>VOP_READLINK()</code></li> <li>Restart lookup from symlink target</li> <li>Limit to <code>MAXSYMLINKS</code> (typically 32) to prevent loops</li> </ul> <p>Mount point traversal:</p> <pre><code>/* Cross into mounted filesystem */\nif (nch.ncp-&gt;nc_flag &amp; NCF_ISMOUNTPT) {\n    mp = cache_findmount(&amp;nch);\n    if (mp) {\n        /* Replace with mount point's root */\n        cache_dropmount(mp);\n    }\n}\n</code></pre> <p>Generation tracking:</p> <p>The code carefully tracks <code>nc_generation</code> throughout:</p> <pre><code>nl_gen = nd-&gt;nl_nch.ncp-&gt;nc_generation &amp; ~3;\n...\nif (gen_changed || (nl_gen &amp; 1)) {\n    /* Retry lookup */\n    goto nlookup_start;\n}\n</code></pre> <p>This allows detection of concurrent modifications and automatic retry.</p>"},{"location":"sys/kern/vfs/namecache/#nlookup_done-vfs_nlookupc289","title":"<code>nlookup_done()</code> (<code>vfs_nlookup.c:289</code>)","text":"<p>Cleanup after lookup:</p> <pre><code>void nlookup_done(struct nlookupdata *nd);\n</code></pre> <ul> <li>Release all nchandles</li> <li>Free path buffer</li> <li>Drop credential reference</li> <li>Close <code>nl_open_vp</code> if set</li> </ul>"},{"location":"sys/kern/vfs/namecache/#access-checking-naccess-vfs_nlookupc1531","title":"Access Checking: <code>naccess()</code> (<code>vfs_nlookup.c:1531</code>)","text":"<p>Check permissions during path traversal:</p> <pre><code>static int naccess(struct nlookupdata *nd, struct nchandle *nch,\n                   u_int *genp, int vmode, struct ucred *cred, \n                   int *stickyp, int nchislocked);\n</code></pre> <p>Optimizations:</p> <ol> <li>Cached permissions - Check <code>NCF_WXOK</code> flag for world-searchable dirs</li> <li>Lock-free - Avoids locking if cached perms are sufficient</li> <li>Generation tracking - Detect races with <code>nc_generation</code></li> </ol>"},{"location":"sys/kern/vfs/namecache/#legacy-lookup-api-namei","title":"Legacy Lookup API (namei)","text":""},{"location":"sys/kern/vfs/namecache/#relookup-vfs_lookupc75","title":"<code>relookup()</code> (<code>vfs_lookup.c:75</code>)","text":"<p>Old API function used only by legacy <code>*_rename()</code> code:</p> <pre><code>int relookup(struct vnode *dvp, struct vnode **vpp, \n             struct componentname *cnp);\n</code></pre> <p>This is a compatibility shim. New code should use nlookup exclusively.</p> <p>Key differences from nlookup:</p> <ul> <li>Works with vnodes directly (requires vnode locks)</li> <li>Uses <code>componentname</code> instead of <code>nlcomponent</code></li> <li>Less efficient due to vnode-based locking</li> <li>Does not support overlay mounts as cleanly</li> </ul>"},{"location":"sys/kern/vfs/namecache/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"sys/kern/vfs/namecache/#lock-free-lookups","title":"Lock-Free Lookups","text":"<p>The core innovation is optimistic lock-free access:</p> <pre><code>/* Fast path - no locks */\ndo {\n    gen_before = ncp-&gt;nc_generation;\n    cpu_lfence();\n    /* ... read fields ... */\n    cpu_lfence();\n    gen_after = ncp-&gt;nc_generation;\n} while (gen_before != gen_after || (gen_after &amp; 1));\n</code></pre> <p>If generation matches and is even (not in-progress), the read is consistent.</p>"},{"location":"sys/kern/vfs/namecache/#per-cpu-negative-lists","title":"Per-CPU Negative Lists","text":"<p>Negative entries are stored in per-CPU lists (<code>pcpu_ncache[cpu].neg_list</code>):</p> <pre><code>struct pcpu_ncache {\n    struct spinlock       umount_spin;\n    struct spinlock       neg_spin;\n    struct namecache_list neg_list;\n    long                  neg_count;\n    long                  vfscache_negs;\n    long                  vfscache_count;\n    /* ... statistics ... */\n} __cachealign;\n</code></pre> <p>Benefits:</p> <ul> <li>No inter-CPU contention on negative entry allocation/free</li> <li>Cache-line alignment prevents false sharing</li> <li>Quick reclamation when memory pressure hits</li> </ul>"},{"location":"sys/kern/vfs/namecache/#mount-reference-caching","title":"Mount Reference Caching","text":"<p>The per-CPU mount cache (<code>pcpu_mntcache</code>) is critical for performance:</p> <ul> <li>Problem: Atomic ops on <code>mp-&gt;mnt_refs</code> cause cache-line bouncing</li> <li>Solution: Cache mount refs per-CPU, only updating global ref periodically</li> <li>Result: 10-100x reduction in cache misses on multi-socket systems</li> </ul>"},{"location":"sys/kern/vfs/namecache/#namecache-size-limits","title":"Namecache Size Limits","text":"<p>DragonFly dynamically balances cache sizes:</p> <pre><code>__read_mostly static int ncnegfactor = 16;   /* ratio of negative entries */\n__read_mostly static int ncposfactor = 16;   /* ratio of unres+leaf entries */\n</code></pre> <p>Functions like <code>_cache_cleanneg()</code> and <code>_cache_cleanpos()</code> trim caches when:</p> <ol> <li>Memory pressure increases</li> <li>Ratios exceed configured factors</li> <li>Mount/unmount operations occur</li> </ol>"},{"location":"sys/kern/vfs/namecache/#filesystem-integration","title":"Filesystem Integration","text":""},{"location":"sys/kern/vfs/namecache/#required-vop-operations","title":"Required VOP Operations","text":"<p>Filesystems must implement these to integrate with namecache:</p>"},{"location":"sys/kern/vfs/namecache/#vop_nresolve","title":"<code>VOP_NRESOLVE()</code>","text":"<p>Resolve an unresolved namecache entry:</p> <pre><code>int VOP_NRESOLVE(struct vnode *dvp, struct namecache *ncp, \n                 struct ucred *cred);\n</code></pre> <p>Responsibilities:</p> <ol> <li>Search directory <code>dvp</code> for name <code>ncp-&gt;nc_name</code></li> <li>If found: Call <code>cache_setvp(nch, vp)</code> to set <code>ncp-&gt;nc_vp</code></li> <li>If not found: Leave <code>ncp-&gt;nc_vp</code> as NULL (negative entry)</li> <li>Return 0 on success, error otherwise</li> </ol>"},{"location":"sys/kern/vfs/namecache/#vop_ncreate","title":"<code>VOP_NCREATE()</code>","text":"<p>Create file via namecache:</p> <pre><code>int VOP_NCREATE(struct vnode *dvp, struct vnode **vpp,\n                struct nchandle *nch, struct vattr *vap,\n                struct ucred *cred);\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#vop_nremove","title":"<code>VOP_NREMOVE()</code>","text":"<p>Remove file via namecache:</p> <pre><code>int VOP_NREMOVE(struct vnode *dvp, struct nchandle *nch,\n                struct ucred *cred);\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#vop_nrename","title":"<code>VOP_NRENAME()</code>","text":"<p>Rename via namecache:</p> <pre><code>int VOP_NRENAME(struct nchandle *fnch, struct nchandle *tnch,\n                struct vnode *fdvp, struct vnode *tdvp,\n                struct ucred *cred);\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#invalidation-requirements","title":"Invalidation Requirements","text":"<p>Filesystems must invalidate namecache entries when:</p> <ol> <li>File/directory deleted - <code>cache_inval(nch, CINV_DESTROY)</code></li> <li>Directory modified - <code>cache_inval_vp(dvp, CINV_CHILDREN)</code></li> <li>Vnode recycled - <code>cache_inval_vp(vp, CINV_DESTROY)</code></li> <li>Mount/unmount - <code>cache_purgevfs(mp)</code></li> </ol> <p>Example (tmpfs):</p> <pre><code>/* After unlinking a file */\nerror = tmpfs_remove_dirent(dvp, node, nch);\nif (error == 0) {\n    cache_inval(nch, CINV_DESTROY);\n    cache_inval_vp(vp, CINV_DESTROY);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#special-cases","title":"Special Cases","text":""},{"location":"sys/kern/vfs/namecache/#mount-point-handling","title":"Mount Point Handling","text":"<p>When a lookup encounters a mount point:</p> <ol> <li>Detect via <code>NCF_ISMOUNTPT</code> flag</li> <li>Call <code>cache_findmount(nch)</code> to get mounted filesystem</li> <li>Replace <code>nch</code> with mount point's root <code>mp-&gt;mnt_ncmountpt</code></li> <li>Continue lookup in new filesystem</li> </ol> <p>Reverse traversal (`..'):</p> <pre><code>while (nctmp.ncp == nctmp.mount-&gt;mnt_ncmountpt.ncp) {\n    /* Traverse to mounted-on directory */\n    nctmp = nctmp.mount-&gt;mnt_ncmounton;\n}\nnctmp.ncp = nctmp.ncp-&gt;nc_parent;\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#jail-and-chroot","title":"Jail and Chroot","text":"<p>The lookup code respects process jails and chroot:</p> <ul> <li><code>nl_rootnch</code> - Process's root (may be chrooted)</li> <li><code>nl_jailnch</code> - Jail's root (if jailed)</li> </ul> <p>Root clamping:</p> <pre><code>if (nd-&gt;nl_nch.mount == nd-&gt;nl_rootnch.mount &amp;&amp;\n    nd-&gt;nl_nch.ncp == nd-&gt;nl_rootnch.ncp) {\n    /* At root, \"..\" returns root */\n    cache_copy(&amp;nd-&gt;nl_rootnch, &amp;nch);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#whiteouts-union-mounts","title":"Whiteouts (Union Mounts)","text":"<p>Whiteouts represent explicitly deleted entries in union filesystems:</p> <ul> <li>Negative entry with <code>NCF_WHITEOUT</code> flag set</li> <li>Prevents lower layers from showing through</li> <li>Created by <code>VOP_NWHITEOUT()</code></li> </ul>"},{"location":"sys/kern/vfs/namecache/#common-operations","title":"Common Operations","text":""},{"location":"sys/kern/vfs/namecache/#get-vnode-from-nchandle","title":"Get vnode from nchandle","text":"<pre><code>struct vnode *vp;\nerror = cache_vget(nch, cred, LK_EXCLUSIVE, &amp;vp);\nif (error == 0) {\n    /* ... use vp ... */\n    vput(vp);  /* Release lock + ref */\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#get-nchandle-from-vnode","title":"Get nchandle from vnode","text":"<pre><code>struct nchandle nch;\nerror = cache_fromdvp(vp, cred, 1, &amp;nch);\nif (error == 0) {\n    /* ... use nch ... */\n    cache_drop(&amp;nch);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#full-path-from-nchandle","title":"Full path from nchandle","text":"<pre><code>char *freebuf;\nchar *fullpath;\nerror = cache_fullpath(p, nch, NULL, &amp;fullpath, &amp;freebuf, 0);\nif (error == 0) {\n    kprintf(\"Path: %s\\n\", fullpath);\n    kfree(freebuf, M_TEMP);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#check-if-path-is-open","title":"Check if path is open","text":"<pre><code>if (cache_isopen(nch)) {\n    /* Entry has open file descriptors */\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#statistics-and-debugging","title":"Statistics and Debugging","text":""},{"location":"sys/kern/vfs/namecache/#sysctl-variables","title":"Sysctl Variables","text":"<pre><code>vfs.cache.numneg       - Number of negative entries\nvfs.cache.numcache     - Total namecache entries\nvfs.cache.numleafs     - Leaf entries (no children)\nvfs.cache.numunres     - Unresolved leaf entries\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#per-cpu-statistics-struct-nchstats","title":"Per-CPU Statistics (<code>struct nchstats</code>)","text":"<p>Exported via <code>vfs.cache.nchstats</code> sysctl:</p> <ul> <li><code>ncs_goodhits</code> - Successful cache hits</li> <li><code>ncs_neghits</code> - Negative cache hits</li> <li><code>ncs_badhits</code> - Hits that required locking</li> <li><code>ncs_miss</code> - Cache misses requiring VOP_NRESOLVE</li> <li><code>ncs_longhits</code> - Long name hits (&gt; 32 chars)</li> </ul>"},{"location":"sys/kern/vfs/namecache/#debug-variables","title":"Debug Variables","text":"<pre><code>debug.ncvp_debug       - Namecache debug level (0-3)\ndebug.ncnegflush       - Batch flush negative entries\ndebug.ncposflush       - Batch flush positive entries\ndebug.nclockwarn       - Warn on locked entries in ticks\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#example-creating-a-file","title":"Example: Creating a File","text":"<pre><code>struct nlookupdata nd;\nstruct vnode *vp;\nstruct vattr vat;\nint error;\n\n/* Lookup parent directory and target name */\nerror = nlookup_init(&amp;nd, \"/tmp/newfile\", UIO_SYSSPACE, NLC_FOLLOW);\nif (error == 0) {\n    nd.nl_flags |= NLC_CREATE;\n    error = nlookup(&amp;nd);\n    if (error == 0) {\n        /* Setup attributes */\n        VATTR_NULL(&amp;vat);\n        vat.va_type = VREG;\n        vat.va_mode = 0644;\n\n        /* Get parent directory vnode */\n        error = cache_vget(&amp;nd.nl_nch, cred, LK_EXCLUSIVE, &amp;vp);\n        if (error == 0) {\n            /* Create the file */\n            error = VOP_NCREATE(vp, &amp;vp, &amp;nd.nl_nch, &amp;vat, cred);\n            if (error == 0) {\n                /* ... file created, vp is new file ... */\n                vput(vp);\n            } else {\n                vput(vp);\n            }\n        }\n    }\n    nlookup_done(&amp;nd);\n}\n</code></pre>"},{"location":"sys/kern/vfs/namecache/#summary","title":"Summary","text":"<p>The DragonFly namecache and nlookup system provides:</p> <ol> <li>High-performance caching with lock-free reads</li> <li>Negative caching to avoid redundant lookups</li> <li>Clean API separating pathname lookup from vnode operations</li> <li>Native overlay support via nchandle abstraction</li> <li>Fine-grained locking for better SMP scalability</li> <li>Complete path maintenance from leaf to root</li> </ol> <p>This design is a significant improvement over traditional BSD namecache, enabling better performance on modern multi-core systems while simplifying filesystem implementation.</p>"},{"location":"sys/kern/vfs/namecache/#related-documentation","title":"Related Documentation","text":"<ul> <li>VFS Core - VFS subsystem overview</li> <li>VFS Mounting - Mount point management (Phase 6c)</li> <li>Process File Descriptors - <code>fd_ncdir</code>, <code>fd_nrdir</code> usage</li> </ul>"},{"location":"sys/kern/vfs/namecache/#source-files","title":"Source Files","text":"<ul> <li><code>sys/kern/vfs_cache.c</code> (~5,000 lines) - Namecache implementation</li> <li><code>sys/kern/vfs_nlookup.c</code> (~2,300 lines) - New lookup API</li> <li><code>sys/kern/vfs_lookup.c</code> (~160 lines) - Legacy lookup API</li> <li><code>sys/sys/namecache.h</code> - Namecache structures and API</li> <li><code>sys/sys/nlookup.h</code> - nlookup structures and flags</li> </ul>"},{"location":"sys/kern/vfs/vfs-extensions/","title":"VFS Extensions and Helpers","text":""},{"location":"sys/kern/vfs/vfs-extensions/#overview","title":"Overview","text":"<p>This document covers several VFS subsystems that extend the core VFS functionality:</p> <ul> <li>VFS Helper Functions (<code>vfs_helper.c</code>) - UNIX permission checking and attribute modification</li> <li>Filesystem Synchronization (<code>vfs_sync.c</code>) - Per-mount syncer daemon and dirty vnode management</li> <li>Synthetic Filesystem (<code>vfs_synth.c</code>) - Early boot devfs access for root device lookup</li> <li>VFS Quota System (<code>vfs_quota.c</code>) - Per-user/group space accounting and limits</li> <li>Asynchronous I/O (<code>vfs_aio.c</code>) - POSIX AIO stubs (not implemented)</li> </ul>"},{"location":"sys/kern/vfs/vfs-extensions/#vfs-helper-functions","title":"VFS Helper Functions","text":"<p>The <code>vfs_helper.c</code> file provides filesystem-agnostic implementations of common UNIX operations. These helpers allow filesystems to share code for permission checking, ownership changes, and file attribute modification.</p> <p>Key files: - <code>sys/kern/vfs_helper.c</code> - Helper function implementations</p>"},{"location":"sys/kern/vfs/vfs-extensions/#permission-checking-vop_helper_access","title":"Permission Checking (vop_helper_access)","text":"<p>Standard UNIX permission semantics for <code>VOP_ACCESS</code>:</p> <pre><code>int vop_helper_access(struct vop_access_args *ap, uid_t ino_uid, gid_t ino_gid,\n                      mode_t ino_mode, u_int32_t ino_flags)\n</code></pre> <p>Permission check order: 1. Check read-only filesystem for write attempts 2. Check immutable flag (<code>IMMUTABLE</code>) for write attempts 3. Allow root (uid 0) unconditional access 4. Check owner permissions if <code>proc_uid == ino_uid</code> 5. Check group permissions if <code>proc_gid == ino_gid</code> or user is in supplementary group 6. Check \"other\" permissions</p> <p>AT_EACCESS support: <pre><code>if (ap-&gt;a_flags &amp; AT_EACCESS) {\n    proc_uid = cred-&gt;cr_uid;   /* effective uid */\n    proc_gid = cred-&gt;cr_gid;   /* effective gid */\n} else {\n    proc_uid = cred-&gt;cr_ruid;  /* real uid */\n    proc_gid = cred-&gt;cr_rgid;  /* real gid */\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-extensions/#file-attribute-modification","title":"File Attribute Modification","text":"<p><code>vop_helper_setattr_flags()</code> - Modify file flags (chflags):</p> <pre><code>int vop_helper_setattr_flags(u_int32_t *ino_flags, u_int32_t vaflags,\n                             uid_t uid, struct ucred *cred)\n</code></pre> <ul> <li>Non-owner requires <code>SYSCAP_NOVFS_SYSFLAGS</code> capability</li> <li>Root can set system flags (<code>SF_*</code>) unless securelevel &gt; 0</li> <li>Regular users can only modify user flags (<code>UF_SETTABLE</code>)</li> <li>Jail restrictions apply via <code>PRISON_CAP_VFS_CHFLAGS</code></li> </ul> <p><code>vop_helper_chmod()</code> - Change file mode:</p> <pre><code>int vop_helper_chmod(struct vnode *vp, mode_t new_mode, struct ucred *cred,\n                     uid_t cur_uid, gid_t cur_gid, mode_t *cur_modep)\n</code></pre> <ul> <li>Non-owner requires <code>SYSCAP_NOVFS_CHMOD</code> capability</li> <li>Non-root users cannot set sticky bit on non-directories</li> <li>Non-root users cannot set SGID if not in file's group</li> </ul> <p><code>vop_helper_chown()</code> - Change file ownership:</p> <pre><code>int vop_helper_chown(struct vnode *vp, uid_t new_uid, gid_t new_gid,\n                     struct ucred *cred,\n                     uid_t *cur_uidp, gid_t *cur_gidp, mode_t *cur_modep)\n</code></pre> <ul> <li>Non-owner requires <code>SYSCAP_NOVFS_CHOWN</code> capability</li> <li>Changing owner or group clears SUID/SGID bits (unless root)</li> <li>Validates group membership for non-privileged users</li> </ul> <p><code>vop_helper_create_uid()</code> - Determine new file ownership:</p> <pre><code>uid_t vop_helper_create_uid(struct mount *mp, mode_t dmode, uid_t duid,\n                            struct ucred *cred, mode_t *modep)\n</code></pre> <ul> <li>Supports <code>SUIDDIR</code> mount option (files inherit directory owner)</li> <li>Otherwise returns creator's uid</li> </ul>"},{"location":"sys/kern/vfs/vfs-extensions/#vm-read-shortcut","title":"VM Read Shortcut","text":"<p><code>vop_helper_read_shortcut()</code> - Bypass VFS for cached reads:</p> <p>When <code>LWBUF_IS_OPTIMAL</code> is defined, this function attempts to read directly from VM pages without going through the buffer cache:</p> <pre><code>int vop_helper_read_shortcut(struct vop_read_args *ap)\n{\n    // Check prerequisites\n    if (vp-&gt;v_object == NULL || uio-&gt;uio_segflg == UIO_NOCOPY)\n        return 0;  // Fall back to normal path\n\n    vm_object_hold_shared(obj);\n\n    while (uio-&gt;uio_resid &amp;&amp; error == 0) {\n        // Look up page in VM object\n        m = vm_page_lookup_sbusy_try(obj, OFF_TO_IDX(uio-&gt;uio_offset), ...);\n        if (m == NULL || (m-&gt;valid &amp; VM_PAGE_BITS_ALL) != VM_PAGE_BITS_ALL)\n            break;  // Fall back to normal path\n\n        // Copy directly from page\n        lwb = lwbuf_alloc(m, &amp;lwb_cache);\n        error = uiomove_nofault((char *)lwbuf_kva(lwb) + offset, n, uio);\n        lwbuf_free(lwb);\n        vm_page_sbusy_drop(m);\n    }\n\n    vm_object_drop(obj);\n    return error;\n}\n</code></pre> <p>Controlled via sysctl <code>vm.read_shortcut_enable</code>.</p>"},{"location":"sys/kern/vfs/vfs-extensions/#filesystem-synchronization","title":"Filesystem Synchronization","text":"<p>The <code>vfs_sync.c</code> file implements per-filesystem syncer daemons that periodically flush dirty data to disk.</p> <p>Key files: - <code>sys/kern/vfs_sync.c</code> - Syncer daemon and worklist management</p>"},{"location":"sys/kern/vfs/vfs-extensions/#architecture","title":"Architecture","text":"<p>Each mounted filesystem with <code>MNTK_THR_SYNC</code> gets its own syncer thread:</p> <pre><code>flowchart TB\n    SYNCER[\"syncer_thread(per mount)\"]\n\n    W0[\"workitem[0](sync now)\"]\n    W1[\"workitem[1](sync +1s)\"]\n    WN[\"workitem[N](sync +Ns)\"]\n\n    SYNCER --&gt; W0\n    SYNCER --&gt; W1\n    SYNCER --&gt; WN\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#syncer-context-structure","title":"Syncer Context Structure","text":"<pre><code>struct syncer_ctx {\n    struct mount        *sc_mp;\n    struct lwkt_token   sc_token;\n    struct thread       *sc_thread;\n    int                 sc_flags;\n    struct synclist     *syncer_workitem_pending;  /* Hash table */\n    long                syncer_mask;               /* Hash mask */\n    int                 syncer_delayno;            /* Current slot */\n    int                 syncer_forced;             /* Force sync mode */\n    int                 syncer_rushjob;            /* Rush sync mode */\n    int                 syncer_trigger;            /* Trigger full sync */\n    long                syncer_count;              /* Vnodes on lists */\n};\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#delay-parameters","title":"Delay Parameters","text":"<p>Tunable via sysctl:</p> Parameter Default Sysctl Description <code>syncdelay</code> 30s <code>kern.syncdelay</code> Max delay for data sync <code>filedelay</code> 30s <code>kern.filedelay</code> File data sync delay <code>dirdelay</code> 29s <code>kern.dirdelay</code> Directory sync delay <code>metadelay</code> 28s <code>kern.metadelay</code> Metadata sync delay <code>retrydelay</code> 1s <code>kern.retrydelay</code> Retry delay after failure"},{"location":"sys/kern/vfs/vfs-extensions/#worklist-management","title":"Worklist Management","text":"<p>Adding vnodes to syncer worklist:</p> <pre><code>void vn_syncer_add(struct vnode *vp, int delay)\n{\n    ctx = vp-&gt;v_mount-&gt;mnt_syncer_ctx;\n    lwkt_gettoken(&amp;ctx-&gt;sc_token);\n\n    if (vp-&gt;v_flag &amp; VONWORKLST) {\n        LIST_REMOVE(vp, v_synclist);\n        --ctx-&gt;syncer_count;\n    }\n\n    slot = (ctx-&gt;syncer_delayno + delay) &amp; ctx-&gt;syncer_mask;\n    LIST_INSERT_HEAD(&amp;ctx-&gt;syncer_workitem_pending[slot], vp, v_synclist);\n    vsetflags(vp, VONWORKLST);\n    ++ctx-&gt;syncer_count;\n\n    lwkt_reltoken(&amp;ctx-&gt;sc_token);\n}\n</code></pre> <p>Removing vnodes from worklist:</p> <pre><code>void vn_syncer_remove(struct vnode *vp, int force)\n</code></pre> <p>Called when vnode is no longer dirty or during forced unmount.</p>"},{"location":"sys/kern/vfs/vfs-extensions/#dirty-vnode-tracking","title":"Dirty Vnode Tracking","text":"<p><code>vsetisdirty(vp)</code> - Mark vnode as having dirty inode data:</p> <pre><code>void vsetisdirty(struct vnode *vp)\n{\n    if ((vp-&gt;v_flag &amp; VISDIRTY) == 0) {\n        vsetflags(vp, VISDIRTY);\n        if ((vp-&gt;v_flag &amp; VONWORKLST) == 0)\n            vn_syncer_add(vp, syncdelay);\n    }\n}\n</code></pre> <p><code>vsetobjdirty(vp)</code> - Mark vnode as having dirty VM object:</p> <pre><code>void vsetobjdirty(struct vnode *vp)\n{\n    if ((vp-&gt;v_flag &amp; VOBJDIRTY) == 0) {\n        vsetflags(vp, VOBJDIRTY);\n        if ((vp-&gt;v_flag &amp; VONWORKLST) == 0)\n            vn_syncer_add(vp, syncdelay);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#syncer-thread-operation","title":"Syncer Thread Operation","text":"<p>The syncer thread runs a continuous loop:</p> <pre><code>static void syncer_thread(void *_ctx)\n{\n    for (;;) {\n        // 1. Handle triggered full sync\n        if (ctx-&gt;syncer_trigger) {\n            VOP_FSYNC(ctx-&gt;sc_mp-&gt;mnt_syncer, MNT_LAZY, 0);\n            atomic_clear_int(&amp;ctx-&gt;syncer_trigger, 1);\n        }\n\n        // 2. Process current time slot\n        slp = &amp;ctx-&gt;syncer_workitem_pending[ctx-&gt;syncer_delayno];\n        while ((vp = LIST_FIRST(slp)) != NULL) {\n            vn_syncer_add(vp, retrydelay);  // Move to retry slot\n            if (vget(vp, LK_EXCLUSIVE | LK_NOWAIT) == 0) {\n                VOP_FSYNC(vp, MNT_LAZY, 0);\n                vput(vp);\n            }\n        }\n\n        // 3. Advance to next slot\n        ctx-&gt;syncer_delayno = (ctx-&gt;syncer_delayno + 1) &amp; ctx-&gt;syncer_mask;\n\n        // 4. Sleep until next second (or wakeup)\n        tsleep(ctx, PINTERLOCKED, \"syncer\", hz);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#syncer-control-functions","title":"Syncer Control Functions","text":"<p><code>speedup_syncer(mp)</code> - Request faster sync processing:</p> <pre><code>void speedup_syncer(struct mount *mp)\n{\n    atomic_add_int(&amp;rushjob, 1);\n    if (mp &amp;&amp; mp-&gt;mnt_syncer_ctx)\n        wakeup(mp-&gt;mnt_syncer_ctx);\n}\n</code></pre> <p><code>trigger_syncer(mp)</code> - Request immediate full sync:</p> <pre><code>void trigger_syncer(struct mount *mp)\n{\n    if (mp &amp;&amp; (ctx = mp-&gt;mnt_syncer_ctx) != NULL) {\n        atomic_set_int(&amp;ctx-&gt;syncer_trigger, 1);\n        wakeup(ctx);\n    }\n}\n</code></pre> <p><code>trigger_syncer_start(mp)</code> / <code>trigger_syncer_stop(mp)</code> - Continuous sync mode:</p> <p>Used by filesystems that need guaranteed sync progress (e.g., waiting for dirty data flush).</p>"},{"location":"sys/kern/vfs/vfs-extensions/#syncer-vnode","title":"Syncer Vnode","text":"<p>Each mount point has a special syncer vnode (<code>mp-&gt;mnt_syncer</code>) that: - Is always on the syncer worklist - Triggers <code>VFS_SYNC()</code> when its turn comes - Has minimal vnode operations (just fsync, inactive, reclaim)</p> <pre><code>int vfs_allocate_syncvnode(struct mount *mp)\n{\n    error = getspecialvnode(VT_VFS, mp, &amp;sync_vnode_vops_p, &amp;vp, 0, 0);\n    vp-&gt;v_type = VNON;\n    vn_syncer_add(vp, next % syncdelay);\n    mp-&gt;mnt_syncer = vp;\n    return 0;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#vsyncscan-efficient-dirty-vnode-iteration","title":"vsyncscan() - Efficient Dirty Vnode Iteration","text":"<p>For filesystems with many vnodes, iterating all mount vnodes is expensive. <code>vsyncscan()</code> iterates only vnodes on the syncer worklist:</p> <pre><code>int vsyncscan(struct mount *mp, int vmsc_flags,\n              int (*slowfunc)(struct mount *mp, struct vnode *vp, void *data),\n              void *data)\n</code></pre> <p>Flags: - <code>VMSC_NOWAIT</code> - Use non-blocking vnode acquisition - <code>VMSC_GETVP</code> - Acquire vnode lock before callback - <code>VMSC_GETVX</code> - Acquire VX lock before callback</p>"},{"location":"sys/kern/vfs/vfs-extensions/#synthetic-filesystem","title":"Synthetic Filesystem","text":"<p>The <code>vfs_synth.c</code> file provides a synthetic devfs mount used during early boot to locate root devices.</p> <p>Key files: - <code>sys/kern/vfs_synth.c</code> - Synthetic filesystem initialization</p>"},{"location":"sys/kern/vfs/vfs-extensions/#purpose","title":"Purpose","text":"<p>During boot, the kernel needs to find the root device before the normal filesystem hierarchy is mounted. The synthetic filesystem:</p> <ol> <li>Creates an internal devfs mount (<code>synth_mp</code>)</li> <li>Provides <code>getsynthvnode()</code> to look up device nodes by name</li> <li>Triggers <code>sync_devs()</code> to ensure devices are enumerated</li> </ol>"},{"location":"sys/kern/vfs/vfs-extensions/#initialization","title":"Initialization","text":"<pre><code>static void synthinit(void *arg __unused)\n{\n    // Create internal devfs mount\n    error = vfs_rootmountalloc(\"devfs\", \"dummy\", &amp;synth_mp);\n    error = VFS_MOUNT(synth_mp, NULL, NULL, proc0.p_ucred);\n    error = VFS_ROOT(synth_mp, &amp;synth_vp);\n\n    // Set up namecache for lookups\n    cache_allocroot(&amp;synth_mp-&gt;mnt_ncmountpt, synth_mp, synth_vp);\n    cache_unlock(&amp;synth_mp-&gt;mnt_ncmountpt);\n    vput(synth_vp);\n\n    synth_inited = 1;\n}\n\nSYSINIT(synthinit, SI_SUB_VFS, SI_ORDER_ANY, synthinit, NULL);\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#device-lookup","title":"Device Lookup","text":"<pre><code>struct vnode *getsynthvnode(const char *devname)\n{\n    KKASSERT(synth_inited != 0);\n\n    // Sync devfs twice to ensure devices are present\n    if (synth_synced &lt; 2) {\n        sync_devs();\n        ++synth_synced;\n    }\n\n    // Look up device in synthetic devfs\n    error = nlookup_init_root(&amp;nd, devname, UIO_SYSSPACE, NLC_FOLLOW,\n                              cred, &amp;synth_mp-&gt;mnt_ncmountpt,\n                              &amp;synth_mp-&gt;mnt_ncmountpt);\n    error = nlookup(&amp;nd);\n\n    if (error == 0) {\n        vp = nch.ncp-&gt;nc_vp;\n        error = vget(vp, LK_EXCLUSIVE);\n    }\n\n    nlookup_done(&amp;nd);\n    return vp;  // Returns VX-locked, referenced vnode\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#vfs-quota-system","title":"VFS Quota System","text":"<p>The <code>vfs_quota.c</code> file implements per-user and per-group space accounting and limits.</p> <p>Key files: - <code>sys/kern/vfs_quota.c</code> - Quota implementation - <code>sys/sys/vfs_quota.h</code> - Quota data structures</p>"},{"location":"sys/kern/vfs/vfs-extensions/#enabling-quotas","title":"Enabling Quotas","text":"<p>Quotas are disabled by default and controlled via: - Boot-time tunable: <code>vfs.quota_enabled</code> - Sysctl: <code>vfs.quota_enabled</code> (read-only)</p>"},{"location":"sys/kern/vfs/vfs-extensions/#data-structures","title":"Data Structures","text":"<p>Per-mount accounting uses red-black trees for efficient uid/gid lookup:</p> <pre><code>struct ac_unode {\n    RB_ENTRY(ac_unode) rb_entry;\n    uint32_t left_bits;        /* uid &gt;&gt; ACCT_CHUNK_BITS */\n    struct {\n        int64_t space;         /* bytes used */\n        int64_t limit;         /* byte limit (0 = unlimited) */\n    } uid_chunk[ACCT_CHUNK_NIDS];\n};\n\nstruct ac_gnode {\n    RB_ENTRY(ac_gnode) rb_entry;\n    uint32_t left_bits;        /* gid &gt;&gt; ACCT_CHUNK_BITS */\n    struct {\n        int64_t space;\n        int64_t limit;\n    } gid_chunk[ACCT_CHUNK_NIDS];\n};\n</code></pre> <p>Chunked storage reduces tree nodes: each node handles <code>ACCT_CHUNK_NIDS</code> consecutive uid/gids.</p>"},{"location":"sys/kern/vfs/vfs-extensions/#initialization_1","title":"Initialization","text":"<pre><code>void vq_init(struct mount *mp)\n{\n    if (!vfs_quota_enabled)\n        return;\n\n    RB_INIT(&amp;mp-&gt;mnt_acct.ac_uroot);\n    RB_INIT(&amp;mp-&gt;mnt_acct.ac_groot);\n    spin_init(&amp;mp-&gt;mnt_acct.ac_spin, \"vqinit\");\n\n    mp-&gt;mnt_acct.ac_bytes = 0;\n    mp-&gt;mnt_op-&gt;vfs_account = vfs_stdaccount;\n    mp-&gt;mnt_flag |= MNT_QUOTA;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#accounting-callback","title":"Accounting Callback","text":"<p>Filesystems call <code>vfs_account()</code> on space changes:</p> <pre><code>void vfs_stdaccount(struct mount *mp, uid_t uid, gid_t gid, int64_t delta)\n{\n    spin_lock(&amp;mp-&gt;mnt_acct.ac_spin);\n\n    mp-&gt;mnt_acct.ac_bytes += delta;\n\n    // Find or create uid node\n    ufind.left_bits = (uid &gt;&gt; ACCT_CHUNK_BITS);\n    if ((unp = RB_FIND(ac_utree, &amp;mp-&gt;mnt_acct.ac_uroot, &amp;ufind)) == NULL)\n        unp = unode_insert(mp, uid);\n\n    // Find or create gid node\n    gfind.left_bits = (gid &gt;&gt; ACCT_CHUNK_BITS);\n    if ((gnp = RB_FIND(ac_gtree, &amp;mp-&gt;mnt_acct.ac_groot, &amp;gfind)) == NULL)\n        gnp = gnode_insert(mp, gid);\n\n    // Update usage\n    unp-&gt;uid_chunk[(uid &amp; ACCT_CHUNK_MASK)].space += delta;\n    gnp-&gt;gid_chunk[(gid &amp; ACCT_CHUNK_MASK)].space += delta;\n\n    spin_unlock(&amp;mp-&gt;mnt_acct.ac_spin);\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#quota-enforcement","title":"Quota Enforcement","text":"<p><code>vq_write_ok()</code> - Check if write is allowed:</p> <pre><code>int vq_write_ok(struct mount *mp, uid_t uid, gid_t gid, uint64_t delta)\n{\n    spin_lock(&amp;mp-&gt;mnt_acct.ac_spin);\n\n    // Check filesystem limit\n    if (mp-&gt;mnt_acct.ac_limit &amp;&amp;\n        (mp-&gt;mnt_acct.ac_bytes + delta) &gt; mp-&gt;mnt_acct.ac_limit) {\n        rv = 0;\n        goto done;\n    }\n\n    // Check uid limit\n    if (unp &amp;&amp; unp-&gt;uid_chunk[...].limit &amp;&amp;\n        (space + delta) &gt; limit) {\n        rv = 0;\n        goto done;\n    }\n\n    // Check gid limit\n    if (gnp &amp;&amp; gnp-&gt;gid_chunk[...].limit &amp;&amp;\n        (space + delta) &gt; limit) {\n        rv = 0;\n    }\n\ndone:\n    spin_unlock(&amp;mp-&gt;mnt_acct.ac_spin);\n    return rv;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#system-call-interface","title":"System Call Interface","text":"<p><code>sys_vquotactl()</code> - Quota control system call:</p> <p>Commands (via proplib dictionary): - <code>\"get usage all\"</code> - Return all usage statistics - <code>\"set usage all\"</code> - Set usage statistics (for restore) - <code>\"set limit\"</code> - Set filesystem-wide limit - <code>\"set limit uid\"</code> - Set per-uid limit - <code>\"set limit gid\"</code> - Set per-gid limit</p>"},{"location":"sys/kern/vfs/vfs-extensions/#pfs-support","title":"PFS Support","text":"<p>For pseudo-filesystems (nullfs, etc.), <code>vq_vptomp()</code> returns the real mount point:</p> <pre><code>struct mount *vq_vptomp(struct vnode *vp)\n{\n    if ((vp-&gt;v_pfsmp != NULL) &amp;&amp; (mountlist_exists(vp-&gt;v_pfsmp)))\n        return vp-&gt;v_pfsmp;  /* Real mount for PFS */\n    else\n        return vp-&gt;v_mount;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#asynchronous-io-stubs","title":"Asynchronous I/O (Stubs)","text":"<p>The <code>vfs_aio.c</code> file contains stub implementations of POSIX AIO functions. These system calls are not implemented in DragonFly BSD and return <code>ENOSYS</code>:</p> <pre><code>int sys_aio_read(struct sysmsg *sysmsg, const struct aio_read_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_write(struct sysmsg *sysmsg, const struct aio_write_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_return(struct sysmsg *sysmsg, const struct aio_return_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_suspend(struct sysmsg *sysmsg, const struct aio_suspend_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_cancel(struct sysmsg *sysmsg, const struct aio_cancel_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_error(struct sysmsg *sysmsg, const struct aio_error_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_lio_listio(struct sysmsg *sysmsg, const struct lio_listio_args *uap)\n{\n    return ENOSYS;\n}\n\nint sys_aio_waitcomplete(struct sysmsg *sysmsg, const struct aio_waitcomplete_args *uap)\n{\n    return ENOSYS;\n}\n</code></pre> <p>The kevent filter for AIO also returns <code>ENXIO</code>:</p> <pre><code>static int filt_aioattach(struct knote *kn)\n{\n    return ENXIO;\n}\n\nstruct filterops aio_filtops =\n    { FILTEROP_MPSAFE, filt_aioattach, NULL, NULL };\n</code></pre>"},{"location":"sys/kern/vfs/vfs-extensions/#summary","title":"Summary","text":"Subsystem File Lines Purpose VFS Helpers <code>vfs_helper.c</code> 405 UNIX permission/attribute helpers Syncer <code>vfs_sync.c</code> 890 Per-mount syncer daemon Synthetic FS <code>vfs_synth.c</code> 137 Early boot devfs access Quotas <code>vfs_quota.c</code> 486 Space accounting and limits AIO <code>vfs_aio.c</code> 83 POSIX AIO stubs (not implemented) <p>Key design points:</p> <ol> <li>Helper functions provide consistent UNIX semantics across filesystems</li> <li>Per-mount syncers scale better than a single global syncer</li> <li>Quota system uses chunked RB-trees for efficient uid/gid tracking</li> <li>Synthetic filesystem enables device lookup before root mount</li> <li>AIO is unimplemented - applications should use alternatives</li> </ol>"},{"location":"sys/kern/vfs/vfs-locking/","title":"VFS Vnode Locking and Lifecycle","text":""},{"location":"sys/kern/vfs/vfs-locking/#overview","title":"Overview","text":"<p>The DragonFly BSD vnode locking subsystem (<code>vfs_lock.c</code>) manages vnode lifecycle states, reference counting, and synchronization. Unlike traditional BSD systems that use simple reference counts, DragonFly implements a sophisticated state machine with multiple vnode states and specialized locking primitives designed for SMP scalability.</p> <p>The system provides:</p> <ul> <li>State-based lifecycle management - Vnodes transition through CACHED, ACTIVE, INACTIVE, and DYING states</li> <li>Two-tier reference counting - Regular refs (<code>v_refcnt</code>) and auxiliary refs (<code>v_auxrefs</code>)</li> <li>Per-CPU vnode lists - Reduces lock contention by partitioning vnodes across CPUs</li> <li>Lockless fast paths - Many operations use atomic operations without acquiring locks</li> <li>VX locking - Special exclusive locks for reclamation and deactivation</li> </ul> <p>Key files: - <code>sys/kern/vfs_lock.c</code> - Vnode locking, state transitions, and recycling - <code>sys/kern/vfs_vnops.c</code> - Standard vnode lock functions (<code>vn_lock</code>, <code>vn_unlock</code>) - <code>sys/sys/vnode.h</code> - Vnode structure and state definitions</p>"},{"location":"sys/kern/vfs/vfs-locking/#vnode-states","title":"Vnode States","text":"<p>Vnodes exist in one of four states, managed through the <code>v_state</code> field:</p> <pre><code>flowchart LR\n    CACHED[\"VS_CACHED\"]\n    ACTIVE[\"VS_ACTIVE\"]\n    INACTIVE[\"VS_INACTIVE\"]\n    DYING[\"VS_DYING\"]\n    FREE[\"kfree()\"]\n\n    CACHED --&gt;|\"vget()\"| ACTIVE\n    ACTIVE --&gt;|\"vrele()\"| INACTIVE\n    ACTIVE --&gt;|\"vget()\"| CACHED\n    INACTIVE --&gt;|\"reclaim\"| DYING\n    DYING --&gt; FREE\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#state-definitions","title":"State Definitions","text":"State Value Description <code>VS_CACHED</code> 0 Vnode exists but has no references; eligible for reuse <code>VS_ACTIVE</code> 1 Vnode is actively referenced and in use <code>VS_INACTIVE</code> 2 Vnode has been deactivated; on inactive list awaiting reclamation <code>VS_DYING</code> 3 Vnode is being destroyed; removed from all lists"},{"location":"sys/kern/vfs/vfs-locking/#state-transition-locking-requirements","title":"State Transition Locking Requirements","text":"<p>From <code>vfs_lock.c:38-53</code>:</p> <pre><code>INACTIVE -&gt; CACHED|DYING    vx_lock(excl) + vi-&gt;spin\nDYING    -&gt; CACHED          vx_lock(excl)\nACTIVE   -&gt; INACTIVE        (none) + v_spin + vi-&gt;spin\nINACTIVE -&gt; ACTIVE          vn_lock(any) + v_spin + vi-&gt;spin\nCACHED   -&gt; ACTIVE          vn_lock(any) + v_spin + vi-&gt;spin\n</code></pre> <p>Key observations: - Transitions into ACTIVE can use shared locks - Transitions into CACHED or DYING require exclusive VX locks - The <code>v_spin</code> spinlock protects per-vnode state - The <code>vi-&gt;spin</code> spinlock protects the per-CPU vnode lists</p>"},{"location":"sys/kern/vfs/vfs-locking/#reference-counting","title":"Reference Counting","text":"<p>DragonFly uses a sophisticated reference counting scheme with special flags embedded in the reference count field.</p>"},{"location":"sys/kern/vfs/vfs-locking/#reference-count-fields","title":"Reference Count Fields","text":"<pre><code>struct vnode {\n    int v_refcnt;      /* vget/vput refs + flags */\n    int v_auxrefs;     /* vhold/vdrop refs */\n    // ...\n};\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#reference-count-flags","title":"Reference Count Flags","text":"<p>From <code>sys/vnode.h:252-256</code>:</p> <pre><code>#define VREF_TERMINATE  0x80000000  /* termination in progress */\n#define VREF_FINALIZE   0x40000000  /* deactivate on last vrele */\n#define VREF_MASK       0xBFFFFFFF  /* includes VREF_TERMINATE */\n\n#define VREFCNT(vp)     ((int)((vp)-&gt;v_refcnt &amp; VREF_MASK))\n</code></pre> Flag Purpose <code>VREF_TERMINATE</code> Set when vnode is undergoing termination; prevents reactivation <code>VREF_FINALIZE</code> Requests deactivation on the 1-&gt;0 ref transition <code>VREF_MASK</code> Extracts actual reference count (includes TERMINATE bit)"},{"location":"sys/kern/vfs/vfs-locking/#regular-references-vrefvrele","title":"Regular References (vref/vrele)","text":"<p><code>vref(vp)</code> - Add a reference to an active vnode:</p> <pre><code>void vref(struct vnode *vp)\n{\n    KASSERT((VREFCNT(vp) &gt; 0 &amp;&amp; vp-&gt;v_state != VS_INACTIVE),\n            (\"vref: bad refcnt %08x %d\", vp-&gt;v_refcnt, vp-&gt;v_state));\n    atomic_add_int(&amp;vp-&gt;v_refcnt, 1);\n}\n</code></pre> <ul> <li>Caller must already hold a reference</li> <li>Cannot be called on inactive vnodes (use <code>vget()</code> instead)</li> <li>Lock-free atomic operation</li> </ul> <p><code>vrele(vp)</code> - Release a reference:</p> <p>The 1-&gt;0 transition is critical and handles finalization:</p> <pre><code>void vrele(struct vnode *vp)\n{\n    // For refs &gt; 1: simple atomic decrement\n    if ((count &amp; VREF_MASK) &gt; 1) {\n        atomic_fcmpset_int(&amp;vp-&gt;v_refcnt, &amp;count, count - 1);\n        return;\n    }\n\n    // For 1-&gt;0 with VREF_FINALIZE: trigger termination\n    if (count &amp; VREF_FINALIZE) {\n        vx_lock(vp);\n        if (atomic_fcmpset_int(&amp;vp-&gt;v_refcnt, &amp;count, VREF_TERMINATE)) {\n            vnode_terminate(vp);  // Calls VOP_INACTIVE, moves to inactive list\n        }\n        vx_unlock(vp);\n    } else {\n        // Simple 1-&gt;0: vnode becomes cached\n        atomic_fcmpset_int(&amp;vp-&gt;v_refcnt, &amp;count, 0);\n        atomic_add_int(&amp;mycpu-&gt;gd_cachedvnodes, 1);\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#auxiliary-references-vholdvdrop","title":"Auxiliary References (vhold/vdrop)","text":"<p>Auxiliary references prevent vnode destruction but don't affect state:</p> <pre><code>void vhold(struct vnode *vp)\n{\n    atomic_add_int(&amp;vp-&gt;v_auxrefs, 1);\n}\n\nvoid vdrop(struct vnode *vp)\n{\n    atomic_add_int(&amp;vp-&gt;v_auxrefs, -1);\n}\n</code></pre> <p>Use cases: - Namecache entries holding references to vnodes - VM objects associated with vnodes - Temporary holds during complex operations</p> <p>A vnode cannot be freed (<code>kfree()</code>'d) while <code>v_auxrefs &gt; 0</code>.</p>"},{"location":"sys/kern/vfs/vfs-locking/#vx-locking","title":"VX Locking","text":"<p>VX locks are special exclusive locks used for vnode reclamation and deactivation. They combine the standard vnode lock with a spin lock update.</p>"},{"location":"sys/kern/vfs/vfs-locking/#vx-lock-functions","title":"VX Lock Functions","text":"<pre><code>void vx_lock(struct vnode *vp)\n{\n    lockmgr(&amp;vp-&gt;v_lock, LK_EXCLUSIVE);\n    spin_lock_update_only(&amp;vp-&gt;v_spin);\n}\n\nvoid vx_unlock(struct vnode *vp)\n{\n    spin_unlock_update_only(&amp;vp-&gt;v_spin);\n    lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n}\n</code></pre> <p>The <code>spin_lock_update_only()</code> is a special spinlock mode that: - Prevents readers from proceeding - Allows the holder to make atomic state changes - Is lighter weight than a full exclusive spinlock</p>"},{"location":"sys/kern/vfs/vfs-locking/#vx-vs-vn-locking","title":"VX vs VN Locking","text":"Aspect VN Lock (<code>vn_lock</code>) VX Lock (<code>vx_lock</code>) Lock type Can be shared or exclusive Always exclusive Spin lock Not held Holds <code>v_spin</code> in update mode Use case Normal vnode operations Reclamation, deactivation Reactivation Allowed Not applicable"},{"location":"sys/kern/vfs/vfs-locking/#downgrading-vx-to-vn","title":"Downgrading VX to VN","text":"<p>After allocating a new vnode, callers typically downgrade from VX to VN:</p> <pre><code>void vx_downgrade(struct vnode *vp)\n{\n    spin_unlock_update_only(&amp;vp-&gt;v_spin);\n    // Lock remains EXCLUSIVE, just without spin update mode\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#vnode-acquisition-vget","title":"Vnode Acquisition (vget)","text":"<p>The <code>vget()</code> function acquires a reference and lock on a vnode, potentially reactivating it:</p> <pre><code>int vget(struct vnode *vp, int flags)\n{\n    // 1. Add reference (may remove from cached count)\n    if ((atomic_fetchadd_int(&amp;vp-&gt;v_refcnt, 1) &amp; VREF_MASK) == 0)\n        atomic_add_int(&amp;mycpu-&gt;gd_cachedvnodes, -1);\n\n    // 2. Acquire lock (shared or exclusive based on flags)\n    if ((error = vn_lock(vp, flags | LK_FAILRECLAIM)) != 0) {\n        vrele(vp);\n        return error;\n    }\n\n    // 3. Check for reclaimed vnode\n    if (vp-&gt;v_flag &amp; VRECLAIMED) {\n        vn_unlock(vp);\n        vrele(vp);\n        return ENOENT;\n    }\n\n    // 4. Reactivate if needed\n    if (vp-&gt;v_state != VS_ACTIVE) {\n        _vclrflags(vp, VINACTIVE);\n        spin_lock(&amp;vp-&gt;v_spin);\n        _vactivate(vp);\n        atomic_clear_int(&amp;vp-&gt;v_refcnt, VREF_TERMINATE | VREF_FINALIZE);\n        spin_unlock(&amp;vp-&gt;v_spin);\n    }\n\n    return 0;\n}\n</code></pre> <p>Key points: - Can use shared locks for reactivation (important for scalability) - Clears <code>VREF_TERMINATE</code> and <code>VREF_FINALIZE</code> on success - Updates <code>v_act</code> activity counter for LRU decisions</p>"},{"location":"sys/kern/vfs/vfs-locking/#vput-combined-unlock-and-release","title":"vput() - Combined Unlock and Release","text":"<pre><code>void vput(struct vnode *vp)\n{\n    vn_unlock(vp);\n    vrele(vp);\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#per-cpu-vnode-lists","title":"Per-CPU Vnode Lists","text":"<p>Vnodes are distributed across per-CPU lists to reduce contention:</p> <pre><code>struct vnode_index {\n    struct freelst  active_list;     /* Active vnodes */\n    struct vnode    active_rover;    /* Rover for deactivation scan */\n    struct freelst  inactive_list;   /* Inactive vnodes awaiting reclaim */\n    struct spinlock spin;            /* Protects this CPU's lists */\n    int deac_rover;                  /* Deactivation scan position */\n    int free_rover;                  /* Free scan position */\n} __cachealign;\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#list-assignment","title":"List Assignment","text":"<p>Vnodes are assigned to lists using a hash of their address:</p> <pre><code>#define VLIST_HASH(vp)  (((uintptr_t)vp ^ VLIST_XOR) % \\\n                         VLIST_PRIME2 % (unsigned)ncpus)\n</code></pre> <p>This ensures: - Consistent list assignment for a given vnode - Even distribution across CPUs - Cache-line alignment for each <code>vnode_index</code></p>"},{"location":"sys/kern/vfs/vfs-locking/#activeinactive-list-management","title":"Active/Inactive List Management","text":"<p><code>_vactivate(vp)</code> - Move vnode to active list:</p> <pre><code>static void _vactivate(struct vnode *vp)\n{\n    struct vnode_index *vi = &amp;vnode_list_hash[VLIST_HASH(vp)];\n\n    spin_lock(&amp;vi-&gt;spin);\n\n    switch(vp-&gt;v_state) {\n    case VS_INACTIVE:\n        TAILQ_REMOVE(&amp;vi-&gt;inactive_list, vp, v_list);\n        atomic_add_int(&amp;mycpu-&gt;gd_inactivevnodes, -1);\n        break;\n    case VS_CACHED:\n    case VS_DYING:\n        break;\n    }\n\n    TAILQ_INSERT_TAIL(&amp;vi-&gt;active_list, vp, v_list);\n    vp-&gt;v_state = VS_ACTIVE;\n    spin_unlock(&amp;vi-&gt;spin);\n    atomic_add_int(&amp;mycpu-&gt;gd_activevnodes, 1);\n}\n</code></pre> <p><code>_vinactive(vp)</code> - Move vnode to inactive list:</p> <pre><code>static void _vinactive(struct vnode *vp)\n{\n    struct vnode_index *vi = &amp;vnode_list_hash[VLIST_HASH(vp)];\n\n    spin_lock(&amp;vi-&gt;spin);\n\n    if (vp-&gt;v_state == VS_ACTIVE) {\n        TAILQ_REMOVE(&amp;vi-&gt;active_list, vp, v_list);\n        atomic_add_int(&amp;mycpu-&gt;gd_activevnodes, -1);\n    }\n\n    // Reclaimed vnodes go to head (recycled first)\n    if (vp-&gt;v_flag &amp; VRECLAIMED) {\n        TAILQ_INSERT_HEAD(&amp;vi-&gt;inactive_list, vp, v_list);\n    } else {\n        TAILQ_INSERT_TAIL(&amp;vi-&gt;inactive_list, vp, v_list);\n    }\n    vp-&gt;v_state = VS_INACTIVE;\n    spin_unlock(&amp;vi-&gt;spin);\n    atomic_add_int(&amp;mycpu-&gt;gd_inactivevnodes, 1);\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#vnode-allocation-and-recycling","title":"Vnode Allocation and Recycling","text":""},{"location":"sys/kern/vfs/vfs-locking/#allocvnode-allocate-a-new-vnode","title":"allocvnode() - Allocate a New Vnode","text":"<pre><code>struct vnode *allocvnode(int lktimeout, int lkflags)\n{\n    struct vnode *vp;\n    struct vnode_index *vi = &amp;vnode_list_hash[mycpuid];\n\n    // 1. Try to reuse a reclaimed vnode from local inactive list\n    spin_lock(&amp;vi-&gt;spin);\n    vp = TAILQ_FIRST(&amp;vi-&gt;inactive_list);\n    if (vp &amp;&amp; (vp-&gt;v_flag &amp; VRECLAIMED)) {\n        // Fast path: reuse existing vnode structure\n        if (vx_get_nonblock(vp) == 0) {\n            // ... validation checks ...\n            TAILQ_REMOVE(&amp;vi-&gt;inactive_list, vp, v_list);\n            vp-&gt;v_state = VS_DYING;\n            spin_unlock(&amp;vi-&gt;spin);\n\n            bzero(vp, sizeof(*vp));  // Reuse structure\n            goto initialize;\n        }\n    }\n    spin_unlock(&amp;vi-&gt;spin);\n\n    // 2. Slow path: allocate new vnode\n    vp = kmalloc_obj(sizeof(*vp), M_VNODE, M_ZERO | M_WAITOK);\n    atomic_add_int(&amp;numvnodes, 1);\n\ninitialize:\n    // Initialize vnode fields\n    lwkt_token_init(&amp;vp-&gt;v_token, \"vnode\");\n    lockinit(&amp;vp-&gt;v_lock, \"vnode\", lktimeout, lkflags);\n    spin_init(&amp;vp-&gt;v_spin, \"allocvnode\");\n    // ... other initialization ...\n\n    vx_lock(vp);\n    vp-&gt;v_refcnt = 1;\n    vp-&gt;v_state = VS_CACHED;\n    _vactivate(vp);\n\n    return vp;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#cleanfreevnode-recycle-inactive-vnodes","title":"cleanfreevnode() - Recycle Inactive Vnodes","text":"<p>The <code>cleanfreevnode()</code> function scans inactive lists to find vnodes suitable for recycling:</p> <ol> <li>Deactivation scan: Moves vnodes from active to inactive list based on activity</li> <li>Reclamation scan: Finds fully reclaimable vnodes on inactive list</li> </ol> <pre><code>static struct vnode *cleanfreevnode(int maxcount)\n{\n    // Phase 1: Try to deactivate active vnodes\n    for (count = 0; count &lt; maxcount * 2; ++count) {\n        vi = &amp;vnode_list_hash[((unsigned)ri &gt;&gt; 4) % ncpus];\n        vp = TAILQ_NEXT(&amp;vi-&gt;active_rover, v_list);\n\n        // Skip if referenced\n        if ((vp-&gt;v_refcnt &amp; VREF_MASK) != 0)\n            continue;\n\n        // Decay activity counter\n        if (vp-&gt;v_act &gt; 0) {\n            vp-&gt;v_act -= VACT_INC;\n            continue;\n        }\n\n        // Trigger deactivation via finalize\n        atomic_set_int(&amp;vp-&gt;v_refcnt, VREF_FINALIZE);\n        vrele(vp);\n    }\n\n    // Phase 2: Find reclaimable inactive vnode\n    for (count = 0; count &lt; maxcount; ++count) {\n        vp = TAILQ_FIRST(&amp;vi-&gt;inactive_list);\n\n        // Must have no refs or auxrefs\n        if (vp-&gt;v_auxrefs != vp-&gt;v_namecache_count ||\n            (vp-&gt;v_refcnt &amp; ~VREF_FINALIZE) != VREF_TERMINATE + 1)\n            continue;\n\n        // Reclaim and return\n        if ((vp-&gt;v_flag &amp; VRECLAIMED) == 0) {\n            cache_inval_vp_nonblock(vp);\n            vgone_vxlocked(vp);\n        }\n\n        vp-&gt;v_state = VS_DYING;\n        return vp;\n    }\n    return NULL;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#activity-counter-v_act","title":"Activity Counter (v_act)","text":"<p>The <code>v_act</code> field implements LRU-like behavior:</p> <pre><code>#define VACT_MAX    10\n#define VACT_INC    2\n</code></pre> <ul> <li>Incremented on <code>vget()</code> (up to <code>VACT_MAX</code>)</li> <li>Decremented during deactivation scans</li> <li>Vnodes with <code>v_act == 0</code> are candidates for deactivation</li> <li>VM-heavy vnodes decay slower (based on <code>v_object-&gt;resident_page_count</code>)</li> </ul>"},{"location":"sys/kern/vfs/vfs-locking/#standard-vnode-lock-functions","title":"Standard Vnode Lock Functions","text":"<p>Located in <code>vfs_vnops.c</code>, these wrap the lockmgr:</p>"},{"location":"sys/kern/vfs/vfs-locking/#vn_lock-acquire-vnode-lock","title":"vn_lock() - Acquire Vnode Lock","text":"<pre><code>int vn_lock(struct vnode *vp, int flags)\n{\n    int error;\n\n    do {\n        error = lockmgr(&amp;vp-&gt;v_lock, flags);\n        if (error == 0)\n            break;\n    } while (flags &amp; LK_RETRY);\n\n    // Handle reclaimed vnodes\n    if (error == 0 &amp;&amp; (vp-&gt;v_flag &amp; VRECLAIMED)) {\n        if (flags &amp; LK_FAILRECLAIM) {\n            lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n            error = ENOENT;\n        }\n    }\n    return error;\n}\n</code></pre> <p>Flags: - <code>LK_SHARED</code> - Shared (read) lock - <code>LK_EXCLUSIVE</code> - Exclusive (write) lock - <code>LK_NOWAIT</code> - Don't block if unavailable - <code>LK_RETRY</code> - Retry on failure - <code>LK_FAILRECLAIM</code> - Fail if vnode is reclaimed</p>"},{"location":"sys/kern/vfs/vfs-locking/#vn_unlock-release-vnode-lock","title":"vn_unlock() - Release Vnode Lock","text":"<pre><code>void vn_unlock(struct vnode *vp)\n{\n    lockmgr(&amp;vp-&gt;v_lock, LK_RELEASE);\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#statistics-and-monitoring","title":"Statistics and Monitoring","text":"<p>Global vnode statistics are tracked per-CPU and aggregated:</p> <pre><code>int activevnodes;    /* sysctl debug.activevnodes */\nint cachedvnodes;    /* sysctl debug.cachedvnodes */\nint inactivevnodes;  /* sysctl debug.inactivevnodes */\n\nvoid synchronizevnodecount(void)\n{\n    for (i = 0; i &lt; ncpus; ++i) {\n        globaldata_t gd = globaldata_find(i);\n        nca += gd-&gt;gd_cachedvnodes;\n        act += gd-&gt;gd_activevnodes;\n        ina += gd-&gt;gd_inactivevnodes;\n    }\n    cachedvnodes = nca;\n    activevnodes = act;\n    inactivevnodes = ina;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#initialization","title":"Initialization","text":"<p>Called from <code>vfsinit()</code>:</p> <pre><code>void vfs_lock_init(void)\n{\n    kmalloc_obj_raise_limit(M_VNODE, 0);  /* unlimited */\n\n    vnode_list_hash = kmalloc(sizeof(*vnode_list_hash) * ncpus,\n                              M_VNODE_HASH, M_ZERO | M_WAITOK);\n\n    for (i = 0; i &lt; ncpus; ++i) {\n        struct vnode_index *vi = &amp;vnode_list_hash[i];\n        TAILQ_INIT(&amp;vi-&gt;inactive_list);\n        TAILQ_INIT(&amp;vi-&gt;active_list);\n        TAILQ_INSERT_TAIL(&amp;vi-&gt;active_list, &amp;vi-&gt;active_rover, v_list);\n        spin_init(&amp;vi-&gt;spin, \"vfslock\");\n    }\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-locking/#summary","title":"Summary","text":"<p>The DragonFly vnode locking system achieves scalability through:</p> <ol> <li>State machine design - Clear state transitions with well-defined locking requirements</li> <li>Embedded flags in refcount - Atomic flag manipulation without separate locks</li> <li>Per-CPU partitioning - Reduces cross-CPU cache line bouncing</li> <li>Activity-based LRU - Intelligent recycling decisions</li> <li>Lockless fast paths - Most operations use atomic CAS without locks</li> <li>Shared lock reactivation - Multiple readers can reactivate simultaneously</li> </ol> <p>Key invariants: - <code>v_refcnt &gt; 0</code> implies vnode cannot be recycled - <code>v_auxrefs &gt; 0</code> prevents <code>kfree()</code> of vnode structure - <code>VRECLAIMED</code> flag prevents reactivation - State transitions follow defined locking protocol</p>"},{"location":"sys/kern/vfs/vfs-operations/","title":"VFS Operations Framework","text":""},{"location":"sys/kern/vfs/vfs-operations/#overview","title":"Overview","text":"<p>The VFS operations framework provides a flexible, extensible architecture for implementing filesystem operations in DragonFly BSD. At its core is the VOP (Vnode Operation) dispatch mechanism, which allows different filesystem implementations to provide their own handlers for standard operations like open, read, write, and close.</p> <p>The framework consists of multiple layers:</p> <ol> <li>VOP Wrapper Layer (<code>vfs_vopops.c</code>) - High-level entry points with locking and journaling</li> <li>Journaling Layer (<code>vfs_journal.c</code>) - Optional transaction recording</li> <li>Filesystem Implementation Layer - Filesystem-specific operation handlers</li> <li>Compatibility Layer (<code>vfs_default.c</code>) - Default implementations and API translation</li> </ol> <p>This architecture enables: - Uniform interface across different filesystem types - Transparent journaling support - Gradual migration from legacy APIs - Per-mount locking strategies (MPLOCK vs fine-grained)</p> <p>Key files: - <code>sys/kern/vfs_vopops.c</code> - VOP wrapper functions and dispatch - <code>sys/kern/vfs_vnops.c</code> - High-level vnode operations - <code>sys/kern/vfs_default.c</code> - Default implementations and compatibility - <code>sys/sys/vfsops.h</code> - Operation structures and argument definitions</p>"},{"location":"sys/kern/vfs/vfs-operations/#vop-architecture","title":"VOP Architecture","text":""},{"location":"sys/kern/vfs/vfs-operations/#operation-vectors","title":"Operation Vectors","text":"<p>Every vnode has an associated <code>vop_ops</code> structure that defines handlers for filesystem operations:</p> <pre><code>struct vop_ops {\n    struct vop_generic_args **vop_ops_first_p;\n    struct vop_generic_args **vop_ops_last_p;\n\n    int (*vop_default)(struct vop_generic_args *);\n    int (*vop_old_lookup)(struct vop_old_lookup_args *);\n    int (*vop_old_create)(struct vop_old_create_args *);\n    // ... many more operations\n    int (*vop_nresolve)(struct vop_nresolve_args *);\n    int (*vop_nlookupdotdot)(struct vop_nlookupdotdot_args *);\n    int (*vop_ncreate)(struct vop_ncreate_args *);\n    int (*vop_nmkdir)(struct vop_nmkdir_args *);\n    // ... etc\n};\n</code></pre> <p>Key characteristics: - Each operation is a function pointer taking a typed argument structure - The <code>vop_default</code> handler catches unimplemented operations - Two API generations coexist: old (componentname) and new (nchandle) - Operation vectors are typically defined statically per filesystem type</p>"},{"location":"sys/kern/vfs/vfs-operations/#argument-structures","title":"Argument Structures","text":"<p>All VOP argument structures inherit from <code>vop_generic_args</code>:</p> <pre><code>struct vop_generic_args {\n    struct vop_ops *a_ops;\n    int a_reserved[3];\n    int a_desc_offset;\n};\n</code></pre> <p>Example operation-specific structures:</p> <pre><code>struct vop_open_args {\n    struct vop_ops *a_ops;\n    int a_reserved[3];\n    int a_desc_offset;\n    struct vnode *a_vp;\n    int a_mode;\n    struct ucred *a_cred;\n    struct file *a_fp;\n};\n\nstruct vop_read_args {\n    struct vop_ops *a_ops;\n    int a_reserved[3];\n    int a_desc_offset;\n    struct vnode *a_vp;\n    struct uio *a_uio;\n    int a_ioflag;\n    struct ucred *a_cred;\n};\n</code></pre> <p>Important fields: - <code>a_ops</code> - Points to the operation vector (used for dispatch) - <code>a_desc_offset</code> - Offset within vop_ops to find the correct handler - Operation-specific arguments follow the header</p>"},{"location":"sys/kern/vfs/vfs-operations/#dispatch-mechanism","title":"Dispatch Mechanism","text":"<p>VOP dispatch follows this path:</p> <ol> <li>Caller invokes wrapper (e.g., <code>vop_open()</code>)</li> <li>Wrapper handles MPLOCK if needed, sets up arguments</li> <li>Journal layer (if enabled) records operation</li> <li>Filesystem handler performs actual operation</li> <li>Return path releases locks, cleans up</li> </ol> <p>The actual dispatch uses the <code>a_desc_offset</code> to index into the <code>vop_ops</code> structure and call the appropriate handler.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vop-wrapper-functions","title":"VOP Wrapper Functions","text":""},{"location":"sys/kern/vfs/vfs-operations/#purpose-and-design","title":"Purpose and Design","text":"<p>The VOP wrapper functions in <code>vfs_vopops.c</code> provide:</p> <ol> <li>MPLOCK handling - Acquire/release Giant lock for non-MPSAFE filesystems</li> <li>Argument marshalling - Set up typed argument structures</li> <li>Journal integration - Optional operation recording</li> <li>Error checking - Validate return values</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#mplock-management","title":"MPLOCK Management","text":"<p>DragonFly supports both traditional (MPLOCK-protected) and modern (fine-grained locking) filesystems. Per-mount flags control locking behavior:</p> <p>Mount flags (from <code>sys/mount.h</code>): - <code>MNTK_MPSAFE</code> - Filesystem is fully SMP-safe - <code>MNTK_RD_MPSAFE</code> - Reads are SMP-safe, writes need MPLOCK - <code>MNTK_WR_MPSAFE</code> - Writes are SMP-safe, reads need MPLOCK - <code>MNTK_GA_MPSAFE</code> - Getattr is SMP-safe - <code>MNTK_IN_MPSAFE</code> - Inactive is SMP-safe - <code>MNTK_SG_MPSAFE</code> - Strategy is SMP-safe - <code>MNTK_NCALIASED</code> - Nchandle aliasing enabled</p> <p>Macros for MPLOCK handling:</p> <pre><code>#define VFS_MPLOCK_FLAG(MP, FLAG) \\\n    ((MP) == NULL || ((MP)-&gt;mnt_kern_flag &amp; (FLAG)))\n\n#define VFS_MPLOCK1(MP) \\\n    if (VFS_NEEDMPLOCK(MP)) get_mplock()\n\n#define VFS_MPLOCK2(MP) \\\n    if (VFS_NEEDMPLOCK(MP)) rel_mplock()\n</code></pre>"},{"location":"sys/kern/vfs/vfs-operations/#common-vop-wrappers","title":"Common VOP Wrappers","text":"<p>File access operations: - <code>vop_open()</code> - Open file/device - <code>vop_close()</code> - Close file/device - <code>vop_access()</code> - Check access permissions - <code>vop_read()</code> - Read from vnode - <code>vop_write()</code> - Write to vnode - <code>vop_ioctl()</code> - Device/file control - <code>vop_fsync()</code> - Sync file data to disk</p> <p>Namespace operations (old API): - <code>vop_old_lookup()</code> - Look up name in directory - <code>vop_old_create()</code> - Create regular file - <code>vop_old_mkdir()</code> - Create directory - <code>vop_old_rmdir()</code> - Remove directory - <code>vop_old_unlink()</code> - Remove file</p> <p>Namespace operations (new API): - <code>vop_nresolve()</code> - Resolve name to vnode (nchandle-based) - <code>vop_ncreate()</code> - Create file (nchandle-based) - <code>vop_nmkdir()</code> - Create directory (nchandle-based) - <code>vop_nremove()</code> - Remove file (nchandle-based) - <code>vop_nrmdir()</code> - Remove directory (nchandle-based) - <code>vop_nrename()</code> - Rename file (nchandle-based)</p> <p>Metadata operations: - <code>vop_getattr()</code> - Get file attributes - <code>vop_setattr()</code> - Set file attributes - <code>vop_getpages()</code> - Get VM pages for file - <code>vop_putpages()</code> - Write VM pages back</p> <p>Directory operations: - <code>vop_readdir()</code> - Read directory entries - <code>vop_readlink()</code> - Read symbolic link target</p>"},{"location":"sys/kern/vfs/vfs-operations/#example-vop_open","title":"Example: vop_open()","text":"<p>From <code>sys/kern/vfs_vopops.c:148</code>:</p> <pre><code>int\nvop_open(struct vop_ops *ops, struct vnode *vp, int mode,\n         struct ucred *cred, struct file *file)\n{\n    struct vop_open_args ap;\n    int error;\n\n    ap.a_head.a_ops = ops;\n    ap.a_head.a_desc = &amp;vop_open_desc;\n    ap.a_vp = vp;\n    ap.a_mode = mode;\n    ap.a_cred = cred;\n    ap.a_fp = file;\n\n    VFS_MPLOCK1(vp-&gt;v_mount);\n    error = vop_open_ap(&amp;ap);\n    VFS_MPLOCK2(vp-&gt;v_mount);\n\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-operations/#high-level-vnode-operations","title":"High-Level Vnode Operations","text":"<p>The file <code>sys/kern/vfs_vnops.c</code> provides high-level operations built on top of VOP primitives. These functions are used by system calls and kernel subsystems.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_open-complex-file-opening","title":"vn_open() - Complex File Opening","text":"<p>Located at <code>sys/kern/vfs_vnops.c:80</code>.</p> <p>Purpose: Open a file given a namecache path, handling permissions, device special files, and various flags.</p> <p>Key responsibilities: 1. Resolve path via namecache (<code>ncp-&gt;nc_vp</code>) 2. Check access permissions 3. Handle special cases (directories, block devices) 4. Call <code>VOP_OPEN()</code> on underlying filesystem 5. Set up sequential I/O heuristics if appropriate 6. Handle <code>O_TRUNC</code> flag for truncation after open</p> <p>Important checks: - Block opening directories for writing - Enforce read-only mounts - Handle device opens specially (pass to device driver) - Manage vnode reference counts</p> <p>Sequential I/O heuristics: - If opened with <code>FREAD | FWRITE</code> and not <code>FAPPEND</code>, sets <code>VSEQIO</code> flag - Helps buffer cache optimize for sequential access patterns</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_close-file-closing","title":"vn_close() - File Closing","text":"<p>Located at <code>sys/kern/vfs_vnops.c:229</code>.</p> <p>Purpose: Close an open file, synchronizing if needed and releasing resources.</p> <p>Operations: 1. Call <code>VOP_CLOSE()</code> on filesystem 2. Clear sequential I/O flag if set 3. Release vnode reference via <code>vrele()</code></p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_rdwr-and-vn_rdwr_inchunks-kernel-io","title":"vn_rdwr() and vn_rdwr_inchunks() - Kernel I/O","text":"<p>Located at <code>sys/kern/vfs_vnops.c:262</code> and <code>sys/kern/vfs_vnops.c:321</code>.</p> <p>Purpose: Perform read or write operations from kernel context.</p> <p>Key features: - Used for kernel-to-kernel I/O (e.g., loading executables, swap, core dumps) - <code>vn_rdwr_inchunks()</code> splits large I/O into manageable chunks (limited by <code>iosize_max()</code>) - Handles both UIO_USERSPACE and UIO_SYSSPACE addresses - Can perform synchronous I/O (<code>IO_SYNC</code> flag) - Manages file offset locking for concurrent access</p> <p>Common flags: - <code>IO_UNIT</code> - Atomic operation (all or nothing) - <code>IO_APPEND</code> - Append to end of file - <code>IO_SYNC</code> - Synchronous write (wait for disk) - <code>IO_NODELOCKED</code> - Node already locked - <code>IO_SEQMAX</code> - Maximum sequential I/O heuristic</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_read-read-from-vnode","title":"vn_read() - Read from Vnode","text":"<p>Located at <code>sys/kern/vfs_vnops.c:455</code>.</p> <p>Purpose: Read data from a vnode via VOP_READ.</p> <p>Sequential I/O detection: <pre><code>if ((fp-&gt;f_flag &amp; FSEQIO) || (vp-&gt;v_flag &amp; VSEQIO))\n    ioflag |= IO_SEQMAX;\n</code></pre></p> <p>If sequential I/O is detected, sets <code>IO_SEQMAX</code> flag to hint buffer cache to maximize read-ahead.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_write-write-to-vnode","title":"vn_write() - Write to Vnode","text":"<p>Located at <code>sys/kern/vfs_vnops.c:508</code>.</p> <p>Purpose: Write data to a vnode via VOP_WRITE.</p> <p>Key operations: 1. Check for read-only mount 2. Set <code>IO_SEQMAX</code> if sequential 3. Call <code>VOP_WRITE()</code> 4. Update access time if configured</p> <p>Mount-level write protection: <pre><code>if (vp-&gt;v_mount &amp;&amp; (vp-&gt;v_mount-&gt;mnt_flag &amp; MNT_RDONLY)) {\n    error = EROFS;\n    goto done;\n}\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-operations/#file-offset-locking","title":"File Offset Locking","text":"<p>Located at <code>sys/kern/vfs_vnops.c:571</code> and <code>sys/kern/vfs_vnops.c:599</code>.</p> <p>Functions: - <code>vn_get_fpf_offset(struct file *fp, off_t *offset)</code> - Atomically read file offset - <code>vn_set_fpf_offset(struct file *fp, off_t offset)</code> - Atomically set file offset</p> <p>Purpose: Safely manipulate file position in multi-threaded contexts.</p> <p>Implementation: - Uses <code>spin_lock()</code> on <code>fp-&gt;f_spin</code> - Returns offset via pointer argument - Essential for concurrent file access</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_stat-get-file-statistics","title":"vn_stat() - Get File Statistics","text":"<p>Located at <code>sys/kern/vfs_vnops.c:625</code>.</p> <p>Purpose: Fill in a <code>struct stat</code> from vnode attributes.</p> <p>Operations: 1. Call <code>VOP_GETATTR()</code> to get vnode attributes 2. Translate <code>struct vattr</code> to <code>struct stat</code> 3. Handle special fields (st_dev, st_ino, st_blocks, st_blksize) 4. Compute optimal I/O size based on filesystem block size</p> <p>Device number handling: - For device special files, uses <code>vp-&gt;v_rdev</code> as <code>st_rdev</code> - Regular files use mount device number</p>"},{"location":"sys/kern/vfs/vfs-operations/#vn_ioctl-io-control","title":"vn_ioctl() - I/O Control","text":"<p>Located at <code>sys/kern/vfs_vnops.c:741</code>.</p> <p>Purpose: Perform ioctl operations on vnodes and underlying devices.</p> <p>Special handling: - <code>FIOSEEKDATA</code> / <code>FIOSEEKHOLE</code> - Sparse file support (find next data/hole) - <code>FIOASYNC</code> - Enable/disable async I/O notifications - <code>FIOSETOWN</code> / <code>FIOGETOWN</code> - Manage signal recipient for async I/O - Falls through to <code>VOP_IOCTL()</code> for filesystem-specific operations</p>"},{"location":"sys/kern/vfs/vfs-operations/#other-helper-functions","title":"Other Helper Functions","text":"<p>vn_islocked() / vn_lock() - Query and acquire vnode locks - Wrappers around <code>vn_lock_shared()</code> and <code>vn_lock_exclusive()</code></p> <p>vn_fullpath() / vn_fullpath_global() - Reconstruct full pathname from vnode - Uses namecache to traverse parent directories</p> <p>vn_touser() / vn_touser_pgcache() - Copy file data to user buffer - Used for sendfile() and similar operations</p>"},{"location":"sys/kern/vfs/vfs-operations/#old-vs-new-api","title":"Old vs New API","text":"<p>DragonFly BSD is transitioning from an older nameiop-based API to a newer nchandle-based API. The two APIs coexist for backward compatibility.</p>"},{"location":"sys/kern/vfs/vfs-operations/#old-api-componentname-based","title":"Old API (componentname-based)","text":"<p>Characteristics: - Uses <code>struct componentname</code> to represent path components - Operations like <code>vop_old_lookup()</code>, <code>vop_old_create()</code>, <code>vop_old_mkdir()</code> - Directory operations split into multiple steps - More complex locking requirements</p> <p>Example operations: <pre><code>vop_old_lookup(struct vnode *dvp, struct vnode **vpp,\n               struct componentname *cnp)\nvop_old_create(struct vnode *dvp, struct vnode **vpp,\n               struct componentname *cnp, struct vattr *vap)\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-operations/#new-api-nchandle-based","title":"New API (nchandle-based)","text":"<p>Characteristics: - Uses <code>struct nchandle</code> from namecache - Operations like <code>vop_nresolve()</code>, <code>vop_ncreate()</code>, <code>vop_nmkdir()</code> - Better integration with namecache - Cleaner locking semantics - Supports namecache aliasing</p> <p>Example operations: <pre><code>vop_nresolve(struct nchandle *nch, struct vnode *dvp,\n             struct ucred *cred)\nvop_ncreate(struct nchandle *nch, struct vnode *dvp,\n            struct vnode **vpp, struct ucred *cred,\n            struct vattr *vap)\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-operations/#api-translation","title":"API Translation","text":"<p>Modern filesystems should implement the new API. The compatibility layer in <code>vfs_default.c</code> provides automatic translation for filesystems that only implement the old API.</p> <p>Translation functions: - <code>vop_compat_nresolve()</code> \u2192 <code>vop_old_lookup()</code> - <code>vop_compat_ncreate()</code> \u2192 <code>vop_old_create()</code> - <code>vop_compat_nmkdir()</code> \u2192 <code>vop_old_mkdir()</code> - <code>vop_compat_nlink()</code> \u2192 <code>vop_old_link()</code> - <code>vop_compat_nremove()</code> \u2192 <code>vop_old_unlink()</code> - <code>vop_compat_nrmdir()</code> \u2192 <code>vop_old_rmdir()</code> - <code>vop_compat_nrename()</code> \u2192 <code>vop_old_rename()</code></p> <p>These shims extract <code>componentname</code> from <code>nchandle</code> and call the old API, then update the namecache with results.</p>"},{"location":"sys/kern/vfs/vfs-operations/#compatibility-layer","title":"Compatibility Layer","text":"<p>The file <code>sys/kern/vfs_default.c</code> provides default implementations and compatibility shims.</p>"},{"location":"sys/kern/vfs/vfs-operations/#default-operation-vector","title":"Default Operation Vector","text":"<p>Located at <code>sys/kern/vfs_default.c:110</code>:</p> <pre><code>struct vop_ops default_vnode_vops = {\n    .vop_default         = vop_defaultop,\n    .vop_old_lookup      = vop_eopnotsupp,\n    .vop_old_create      = vop_eopnotsupp,\n    .vop_open            = vop_stdopen,\n    .vop_close           = vop_stdclose,\n    .vop_access          = vop_eopnotsupp,\n    .vop_nresolve        = vop_compat_nresolve,\n    .vop_ncreate         = vop_compat_ncreate,\n    // ... many more\n};\n</code></pre> <p>Purpose: Provide fallback handlers for filesystems that don't implement all operations.</p>"},{"location":"sys/kern/vfs/vfs-operations/#error-return-functions","title":"Error Return Functions","text":"<p>vop_eopnotsupp() - Returns <code>EOPNOTSUPP</code> - Used for unimplemented optional operations - Indicates operation not supported by this filesystem</p> <p>vop_einval() - Returns <code>EINVAL</code> - Used for operations that should never be called - Indicates programming error</p> <p>vop_enotty() - Returns <code>ENOTTY</code> - Used for ioctl operations on non-tty vnodes</p>"},{"location":"sys/kern/vfs/vfs-operations/#standard-implementations","title":"Standard Implementations","text":"<p>vop_stdopen() / vop_stdclose() - Minimal open/close handlers - Just return success (0)</p> <p>vop_stdgetpages() / vop_stdputpages() - Standard VM integration - Delegates to <code>vnode_pager_generic_getpages()</code> / <code>vnode_pager_generic_putpages()</code></p> <p>vop_stdpathconf() - Returns standard pathconf values - Handles <code>_PC_LINK_MAX</code>, <code>_PC_NAME_MAX</code>, <code>_PC_PIPE_BUF</code>, etc.</p> <p>vop_stdioctl() - Handles standard ioctls - <code>FIOSEEKDATA</code> / <code>FIOSEEKHOLE</code> via <code>vop_helper_seek_hole()</code></p> <p>vop_stdmountctl() - Default mount control operations - Returns <code>EOPNOTSUPP</code> for unsupported operations</p>"},{"location":"sys/kern/vfs/vfs-operations/#compatibility-shims","title":"Compatibility Shims","text":""},{"location":"sys/kern/vfs/vfs-operations/#vop_compat_nresolve","title":"vop_compat_nresolve()","text":"<p>Located at <code>sys/kern/vfs_default.c:557</code>.</p> <p>Purpose: Translate new-style <code>vop_nresolve()</code> to old-style <code>vop_old_lookup()</code>.</p> <p>Algorithm: 1. Extract component name from nchandle 2. Allocate and populate <code>struct componentname</code> 3. Call <code>VOP_OLD_LOOKUP()</code> on parent directory 4. Update namecache with result 5. Release resources</p> <p>Key code: <pre><code>struct componentname cn;\ncn.cn_nameiop = NAMEI_LOOKUP;\ncn.cn_flags = 0;\ncn.cn_cred = ap-&gt;a_cred;\ncn.cn_nameptr = ap-&gt;a_nch-&gt;ncp-&gt;nc_name;\ncn.cn_namelen = ap-&gt;a_nch-&gt;ncp-&gt;nc_nlen;\n\nerror = VOP_OLD_LOOKUP(ap-&gt;a_dvp, &amp;vp, &amp;cn);\nif (error == 0)\n    cache_setvp(ap-&gt;a_nch, vp);\n</code></pre></p>"},{"location":"sys/kern/vfs/vfs-operations/#vop_compat_ncreate","title":"vop_compat_ncreate()","text":"<p>Located at <code>sys/kern/vfs_default.c:623</code>.</p> <p>Purpose: Translate <code>vop_ncreate()</code> to <code>vop_old_create()</code>.</p> <p>Algorithm: 1. Similar to nresolve, but uses <code>NAMEI_CREATE</code> flag 2. Calls <code>VOP_OLD_CREATE()</code> 3. Updates namecache with newly created vnode 4. Returns vnode via <code>ap-&gt;a_vpp</code></p>"},{"location":"sys/kern/vfs/vfs-operations/#vop_compat_nremove","title":"vop_compat_nremove()","text":"<p>Located at <code>sys/kern/vfs_default.c:779</code>.</p> <p>Purpose: Translate <code>vop_nremove()</code> to <code>vop_old_unlink()</code>.</p> <p>Algorithm: 1. Extract vnode from nchandle (if cached) 2. Call <code>VOP_OLD_UNLINK()</code> with parent and component name 3. Call <code>cache_unlink()</code> to remove from namecache 4. Release vnode</p> <p>Important: Must handle case where nchandle has no cached vnode yet.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vop_compat_nrename","title":"vop_compat_nrename()","text":"<p>Located at <code>sys/kern/vfs_default.c:917</code>.</p> <p>Purpose: Translate <code>vop_nrename()</code> to <code>vop_old_rename()</code>.</p> <p>Complexity: Most complex shim due to: - Four directory/file combinations (source dir/file, target dir/file) - Namecache updates for both source and target - Handling cross-directory renames - Updating parent directory links (..)</p>"},{"location":"sys/kern/vfs/vfs-operations/#vfs-standard-operations","title":"VFS Standard Operations","text":"<p>These handle filesystem-level (not vnode-level) operations:</p> <p>vfs_stdroot() - Get root vnode of filesystem vfs_stdstatfs() - Get filesystem statistics vfs_stdsync() - Sync all dirty data on filesystem vfs_stdvptofh() - Convert vnode to file handle vfs_stdfhtovp() - Convert file handle to vnode</p> <p>These are used when a filesystem doesn't provide custom implementations.</p>"},{"location":"sys/kern/vfs/vfs-operations/#fileops-integration","title":"Fileops Integration","text":"<p>VOP operations integrate with file descriptor operations via <code>struct fileops</code>. The structure <code>vnode_fileops</code> (defined in <code>sys/kern/vfs_vnops.c:1354</code>) provides the glue between file descriptor operations and VOP operations.</p>"},{"location":"sys/kern/vfs/vfs-operations/#vnode_fileops-structure","title":"vnode_fileops Structure","text":"<pre><code>struct fileops vnode_fileops = {\n    .fo_read = vn_read,\n    .fo_write = vn_write,\n    .fo_ioctl = vn_ioctl,\n    .fo_kqfilter = vn_kqfilter,\n    .fo_stat = vn_stat,\n    .fo_close = vn_closefile,\n    .fo_shutdown = vn_shutdown\n};\n</code></pre> <p>Purpose: When a file descriptor refers to a vnode, these functions are called for file operations.</p>"},{"location":"sys/kern/vfs/vfs-operations/#mapping-fo_-to-vop_","title":"Mapping fo_ to VOP_","text":"<p>fo_read \u2192 vn_read() \u2192 VOP_READ() - Reads from file descriptor go through vnode read path</p> <p>fo_write \u2192 vn_write() \u2192 VOP_WRITE() - Writes to file descriptor go through vnode write path</p> <p>fo_ioctl \u2192 vn_ioctl() \u2192 VOP_IOCTL() - Ioctl operations on files/devices</p> <p>fo_stat \u2192 vn_stat() \u2192 VOP_GETATTR() - fstat() system call implementation</p> <p>fo_close \u2192 vn_closefile() \u2192 VOP_CLOSE() - Close file descriptor</p> <p>fo_shutdown \u2192 vn_shutdown() \u2192 VOP_SHUTDOWN() - Shutdown file descriptor (for socket-like operations)</p>"},{"location":"sys/kern/vfs/vfs-operations/#call-flow-examples","title":"Call Flow Examples","text":""},{"location":"sys/kern/vfs/vfs-operations/#opening-a-file-open-system-call","title":"Opening a File: open() System Call","text":"<ol> <li>System call handler calls <code>kern_open()</code> (<code>sys/kern/kern_descrip.c</code>)</li> <li>Namei lookup resolves path to namecache entry</li> <li>kern_open() calls <code>vn_open()</code> with nchandle</li> <li>vn_open() extracts vnode from namecache (<code>nch-&gt;ncp-&gt;nc_vp</code>)</li> <li>vn_open() performs access checks</li> <li>vn_open() calls <code>VOP_OPEN(vp-&gt;v_ops, ...)</code></li> <li>VOP wrapper handles MPLOCK, calls journal layer</li> <li>Filesystem handler (e.g., <code>hammer2_vop_open()</code>) performs open</li> <li>Return path propagates back to system call</li> <li>File descriptor set up with <code>vnode_fileops</code></li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#reading-from-a-file-read-system-call","title":"Reading from a File: read() System Call","text":"<ol> <li>System call handler calls <code>sys_read()</code> (<code>sys/kern/sys_generic.c</code>)</li> <li>sys_read() looks up file descriptor</li> <li>Calls <code>fo_read()</code> on file \u2192 <code>vnode_fileops.fo_read</code> \u2192 <code>vn_read()</code></li> <li>vn_read() sets up UIO structure with user buffer</li> <li>vn_read() detects sequential I/O, sets <code>IO_SEQMAX</code></li> <li>vn_read() calls <code>VOP_READ()</code></li> <li>VOP wrapper handles MPLOCK</li> <li>Filesystem handler (e.g., <code>hammer2_vop_read()</code>) performs read</li> <li>May call <code>cluster_read()</code> for buffer cache integration</li> <li>Data copied from kernel buffers to user space</li> <li>Return byte count to user</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#creating-a-file-open-with-o_creat","title":"Creating a File: open() with O_CREAT","text":"<ol> <li>System call handler calls <code>kern_open()</code> with <code>O_CREAT</code> flag</li> <li>Namei lookup with CREATE intent</li> <li>If file doesn't exist, calls <code>VOP_NCREATE()</code> via namecache</li> <li>VOP wrapper <code>vop_ncreate()</code> calls filesystem handler</li> <li>Filesystem (e.g., <code>hammer2_vop_ncreate()</code>) creates inode and directory entry</li> <li>Namecache updated with new vnode</li> <li>VOP_OPEN() called on newly created vnode</li> <li>File descriptor set up and returned to user</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#directory-lookup-stat-system-call","title":"Directory Lookup: stat() System Call","text":"<ol> <li>System call handler calls <code>kern_stat()</code> (<code>sys/kern/vfs_syscalls.c</code>)</li> <li>Namei lookup resolves path via namecache</li> <li>May call <code>VOP_NRESOLVE()</code> if not cached</li> <li>Filesystem handler resolves name to vnode</li> <li>vn_stat() called with vnode</li> <li>VOP_GETATTR() retrieves vnode attributes</li> <li>vn_stat() converts <code>struct vattr</code> to <code>struct stat</code></li> <li>Copy stat structure to user space</li> <li>Release vnode reference</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#filesystem-implementation-guide","title":"Filesystem Implementation Guide","text":""},{"location":"sys/kern/vfs/vfs-operations/#implementing-a-new-filesystem","title":"Implementing a New Filesystem","text":"<p>To implement a new filesystem, you need to:</p> <ol> <li>Define a <code>vop_ops</code> structure with handlers for all supported operations</li> <li>Implement new API operations (<code>vop_nresolve</code>, <code>vop_ncreate</code>, etc.)</li> <li>Mark filesystem as MPSAFE if using fine-grained locking</li> <li>Integrate with buffer cache via <code>cluster_read()</code> / <code>cluster_write()</code></li> <li>Handle reference counting properly (vnode lifecycle)</li> </ol>"},{"location":"sys/kern/vfs/vfs-operations/#minimal-operation-set","title":"Minimal Operation Set","text":"<p>A minimal filesystem must implement:</p> <p>Required: - <code>vop_nresolve()</code> - Name resolution - <code>vop_nlookupdotdot()</code> - Parent directory lookup (..) - <code>vop_open()</code> / <code>vop_close()</code> - File open/close - <code>vop_read()</code> / <code>vop_write()</code> - Data I/O - <code>vop_getattr()</code> / <code>vop_setattr()</code> - Attribute access - <code>vop_reclaim()</code> - Vnode cleanup</p> <p>For writable filesystems: - <code>vop_ncreate()</code> / <code>vop_nmkdir()</code> - File/directory creation - <code>vop_nremove()</code> / <code>vop_nrmdir()</code> - File/directory removal - <code>vop_fsync()</code> - Synchronize data</p> <p>For VM integration: - <code>vop_getpages()</code> / <code>vop_putpages()</code> - Page I/O</p>"},{"location":"sys/kern/vfs/vfs-operations/#example-implementing-vop_nresolve","title":"Example: Implementing vop_nresolve()","text":"<pre><code>static int\nmyfs_vop_nresolve(struct vop_nresolve_args *ap)\n{\n    struct nchandle *nch = ap-&gt;a_nch;\n    struct vnode *dvp = ap-&gt;a_dvp;\n    struct vnode *vp;\n    int error;\n\n    /* Look up name in parent directory */\n    error = myfs_lookup_name(dvp, nch-&gt;ncp-&gt;nc_name,\n                             nch-&gt;ncp-&gt;nc_nlen, &amp;vp);\n    if (error == 0) {\n        /* Found - cache positive result */\n        cache_setvp(nch, vp);\n        vrele(vp);  /* cache holds reference */\n    } else if (error == ENOENT) {\n        /* Not found - cache negative result */\n        cache_setvp(nch, NULL);\n    }\n\n    return error;\n}\n</code></pre>"},{"location":"sys/kern/vfs/vfs-operations/#mplock-considerations","title":"MPLOCK Considerations","text":"<p>For MPLOCK-based filesystems: - Set <code>MNTK_MPSAFE</code> to 0 during mount - VOP wrappers will automatically acquire/release MPLOCK - Simpler to implement, but less scalable</p> <p>For fine-grained locking: - Set appropriate <code>MNTK_*_MPSAFE</code> flags during mount - Use per-vnode, per-inode, or per-mount locks - More complex, but better SMP scalability - Must carefully order lock acquisition to avoid deadlock</p>"},{"location":"sys/kern/vfs/vfs-operations/#integration-checklist","title":"Integration Checklist","text":"<ul> <li>[ ] Define <code>struct vop_ops</code> with all handlers</li> <li>[ ] Implement name cache integration (nresolve, ncreate, etc.)</li> <li>[ ] Implement data I/O (read, write, strategy)</li> <li>[ ] Implement attribute operations (getattr, setattr)</li> <li>[ ] Implement VM integration (getpages, putpages)</li> <li>[ ] Handle vnode lifecycle (reclaim, inactive)</li> <li>[ ] Integrate with buffer cache (if applicable)</li> <li>[ ] Set appropriate MPLOCK flags</li> <li>[ ] Handle reference counting correctly</li> <li>[ ] Test with VFS test suite</li> </ul>"},{"location":"sys/kern/vfs/vfs-operations/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/kern/vfs/vfs-operations/#struct-vnode","title":"struct vnode","text":"<p>Defined in <code>sys/sys/vnode.h</code>. Represents a file or directory in the VFS layer.</p> <p>Key fields: - <code>v_ops</code> - Pointer to operation vector - <code>v_mount</code> - Mount point this vnode belongs to - <code>v_type</code> - Type (VREG, VDIR, VCHR, VBLK, etc.) - <code>v_flag</code> - Flags (VROOT, VSEQIO, VRECLAIMED, etc.) - <code>v_data</code> - Filesystem-private data (inode pointer) - <code>v_usecount</code> - Reference count</p>"},{"location":"sys/kern/vfs/vfs-operations/#struct-nchandle","title":"struct nchandle","text":"<p>Defined in <code>sys/sys/namecache.h</code>. Represents a namecache entry.</p> <p>Key fields: - <code>ncp</code> - Pointer to namecache entry (<code>struct namecache</code>) - <code>mount</code> - Associated mount point</p>"},{"location":"sys/kern/vfs/vfs-operations/#struct-namecache","title":"struct namecache","text":"<p>Defined in <code>sys/sys/namecache.h</code>. Represents a cached directory entry.</p> <p>Key fields: - <code>nc_vp</code> - Cached vnode (NULL for negative cache) - <code>nc_name</code> - Component name - <code>nc_nlen</code> - Name length - <code>nc_parent</code> - Parent directory nchandle</p>"},{"location":"sys/kern/vfs/vfs-operations/#struct-componentname","title":"struct componentname","text":"<p>Defined in <code>sys/sys/vnode.h</code>. Used by old API for name lookups (legacy).</p> <p>Key fields: - <code>cn_nameiop</code> - Operation (LOOKUP, CREATE, DELETE, RENAME) - <code>cn_flags</code> - Flags (FOLLOW, LOCKPARENT, etc.) - <code>cn_cred</code> - Credentials - <code>cn_nameptr</code> - Pointer to name string - <code>cn_namelen</code> - Length of name</p>"},{"location":"sys/kern/vfs/vfs-operations/#struct-vattr","title":"struct vattr","text":"<p>Defined in <code>sys/sys/vattr.h</code>. Represents file attributes.</p> <p>Key fields: - <code>va_type</code> - File type - <code>va_mode</code> - Permission bits - <code>va_uid</code> / <code>va_gid</code> - Owner/group - <code>va_size</code> - File size - <code>va_blocksize</code> - Preferred I/O block size - <code>va_atime</code> / <code>va_mtime</code> / <code>va_ctime</code> - Timestamps - <code>va_flags</code> - File flags (immutable, etc.)</p>"},{"location":"sys/kern/vfs/vfs-operations/#debugging-and-diagnostics","title":"Debugging and Diagnostics","text":""},{"location":"sys/kern/vfs/vfs-operations/#vop-call-tracing","title":"VOP Call Tracing","text":"<p>Enable VOP tracing with DDB:</p> <pre><code>db&gt; set vfs_debug_vop=1\n</code></pre> <p>This will log all VOP calls to the console.</p>"},{"location":"sys/kern/vfs/vfs-operations/#common-errors","title":"Common Errors","text":"<p>EOPNOTSUPP - Operation not supported - Filesystem doesn't implement this VOP - Check if default handler is being called</p> <p>EROFS - Read-only filesystem - Attempted write to read-only mount - Check mount flags</p> <p>ENOENT - File not found - Lookup failed - Check namecache and on-disk directory</p> <p>EINVAL - Invalid argument - Wrong operation for vnode type - Check vnode type (VREG, VDIR, etc.)</p> <p>Deadlock detection: - Use lock validation in DEBUG kernels - Check lock ordering in filesystem code</p>"},{"location":"sys/kern/vfs/vfs-operations/#useful-sysctls","title":"Useful Sysctls","text":"<pre><code>vfs.timestamp_precision - Timestamp resolution (0=sec, 1=ms, 2=us, 3=ns)\nvfs.read_max - Maximum read size\nvfs.write_max - Maximum write size\nvfs.hirunningspace - High water mark for async writes\n</code></pre>"},{"location":"sys/kern/vfs/vfs-operations/#summary","title":"Summary","text":"<p>The VFS operations framework provides a sophisticated, layered architecture for implementing filesystem operations in DragonFly BSD. Key takeaways:</p> <ol> <li>VOP dispatch provides uniform interface across filesystem types</li> <li>Wrapper layer handles locking (MPLOCK) and journaling transparently</li> <li>Two API generations coexist with automatic compatibility translation</li> <li>High-level operations (<code>vn_*</code>) built on VOP primitives</li> <li>Default implementations simplify filesystem development</li> <li>Fileops integration connects file descriptors to VOP operations</li> </ol> <p>The framework enables: - Clean separation between VFS layer and filesystem implementations - Gradual migration to modern APIs - Flexible locking strategies (MPLOCK vs fine-grained) - Transparent journaling support - Extensibility for new filesystem types</p> <p>Related documentation: - Name Cache - Pathname lookup and caching - Buffer Cache - Block buffer management - Mounting - Filesystem mounting and VFS infrastructure - VFS Overview - VFS subsystem introduction</p>"},{"location":"sys/vm/","title":"Virtual Memory Subsystem","text":"<p>The DragonFly BSD virtual memory subsystem manages virtual address spaces, physical memory allocation, paging, and swap. It derives from the Mach VM architecture as adopted by BSD but has been extensively modified for better SMP scalability and LWKT integration.</p>"},{"location":"sys/vm/#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TB\n    subgraph Process[\"Per-process address space\"]\n        vmspace[\"vmspace(vm_map + pmap)\"]\n    end\n\n    vmspace --&gt; vm_map[\"vm_map(vm_map_entry)\"]\n    vm_map --&gt; vm_map_backing[\"vm_map_backingBacking store chain\"]\n    vm_map_backing --&gt; vm_object[\"vm_objectContainer for pages\"]\n\n    vm_object --&gt; page1[\"vm_page(resident)\"]\n    vm_object --&gt; page2[\"vm_page(resident)\"]\n    vm_object --&gt; page3[\"vm_page(swapped)\"]\n\n    page3 --&gt; swap_pager[\"swap_pager\"]\n</code></pre>"},{"location":"sys/vm/#reading-guide","title":"Reading Guide","text":"<p>Start here based on what you're trying to do:</p> If you want to... Read this Why Understand how <code>mmap()</code> works Memory Mapping Traces syscall to VM layer Debug a page fault or crash Page Faults Explains fault handling and COW Investigate memory pressure/OOM Pageout and Swap Covers reclamation and OOM killer Add a new memory mapping type Address Space Shows map entry creation Work with page allocation Physical Pages Low-level page management Implement a new pager VM Objects Object-pager relationship <p>Recommended reading order for newcomers:</p> <ol> <li>This overview (understand the hierarchy)</li> <li>Memory Mapping (see how userspace enters the VM)</li> <li>Page Faults (see how pages get populated)</li> <li>Physical Pages (understand page lifecycle)</li> </ol>"},{"location":"sys/vm/#how-operations-flow-through-the-vm","title":"How Operations Flow Through the VM","text":"<pre><code>flowchart TB\n    subgraph User[\"USER SPACE\"]\n        mmap_call[\"mmap(file, ...)\"]\n        access[\"*ptr = 42(first access)\"]\n        pressure[\"(memory pressure)\"]\n    end\n\n    subgraph Kernel[\"KERNEL\"]\n        subgraph sys_mmap[\"vm_mmap.c: sys_mmap()\"]\n            validate[\"Validate parameters\"]\n            create_entry[\"Call vm_map_entry_create()to reserve address range\"]\n        end\n\n        subgraph vm_fault[\"vm_fault.c: vm_fault()\"]\n            lookup[\"Lookup vm_map_entryfor faulting address\"]\n            walk[\"Walk vm_map_backing chainto find/create vm_object\"]\n            call_pager[\"Call pager (vnode_pageror swap_pager) to load page\"]\n        end\n\n        subgraph vm_page_alloc[\"vm_page.c: vm_page_alloc()\"]\n            grab[\"Grab page from PQ_FREE queue\"]\n            associate[\"Associate with vm_objectat page index\"]\n            busy[\"Mark busy while I/O in progress\"]\n        end\n\n        subgraph pageout[\"vm_pageout.c: vm_pageout_daemon()\"]\n            scan[\"Scan inactive queue for victims\"]\n            write[\"Write dirty pages to swap/file\"]\n            move[\"Move clean pages to free queue\"]\n        end\n    end\n\n    mmap_call --&gt; sys_mmap\n    sys_mmap --&gt;|\"returns to userno pages yet\"| access\n    access --&gt; vm_fault\n    vm_fault --&gt; vm_page_alloc\n    vm_page_alloc --&gt;|\"page now resident\"| pressure\n    pressure --&gt; pageout\n</code></pre>"},{"location":"sys/vm/#subsystem-documentation","title":"Subsystem Documentation","text":"Document Description Physical Pages Page allocation, queues, coloring, busy states VM Objects Object lifecycle, reference counting, page management Address Space Address space management, entries, backing chains Page Faults Fault handling, COW, fast path optimization Pageout and Swap Memory reclamation, swap pager, OOM killer Memory Mapping mmap syscalls, vnode pager, file-backed I/O"},{"location":"sys/vm/#key-data-structures","title":"Key Data Structures","text":""},{"location":"sys/vm/#vm_page","title":"vm_page","text":"<p>Physical page descriptor (128 bytes). Each page tracks its owning object, physical address, queue membership, and busy/dirty state.</p> <p>Key fields: <code>object</code>, <code>pindex</code>, <code>phys_addr</code>, <code>queue</code>, <code>busy_count</code>, <code>wire_count</code>, <code>valid</code>, <code>dirty</code></p> <p>See Physical Pages for details.</p>"},{"location":"sys/vm/#vm_object","title":"vm_object","text":"<p>Container for pages with a specific backing store type. Objects can be anonymous (swap-backed), file-backed (vnode), or device-backed.</p> <p>Key fields: <code>rb_memq</code> (page tree), <code>ref_count</code>, <code>type</code>, <code>size</code>, <code>backing_list</code></p> <p>See VM Objects for details.</p>"},{"location":"sys/vm/#vm_map","title":"vm_map","text":"<p>Virtual address space containing a red-black tree of <code>vm_map_entry</code> structures. Each process has its own map within a <code>vmspace</code>.</p> <p>Key fields: <code>rb_root</code>, <code>pmap</code>, <code>min_addr</code>, <code>max_addr</code>, <code>nentries</code></p>"},{"location":"sys/vm/#vm_map_entry","title":"vm_map_entry","text":"<p>Single address range mapping with protection, inheritance, and backing store information. Contains an embedded <code>vm_map_backing</code> structure.</p>"},{"location":"sys/vm/#vm_map_backing-dragonfly-specific","title":"vm_map_backing (DragonFly-specific)","text":"<p>Backing store chain element. Unlike traditional BSD where entries point directly to objects, DragonFly interposes this structure for:</p> <ul> <li>Efficient shadow chains without modifying vm_object</li> <li>Per-entry backing relationships not shared across pmaps</li> <li>Cumulative offset calculation through the chain</li> </ul>"},{"location":"sys/vm/#page-queues","title":"Page Queues","text":"<p>Pages are organized into queues based on state, with 1024 sub-queues per major queue for cache optimization:</p> Queue Description <code>PQ_FREE</code> Available for allocation <code>PQ_ACTIVE</code> Recently referenced <code>PQ_INACTIVE</code> Candidates for reclamation <code>PQ_CACHE</code> Clean, quickly reclaimable <code>PQ_HOLD</code> Temporarily held"},{"location":"sys/vm/#locking-model","title":"Locking Model","text":"<p>The VM subsystem uses several locking strategies:</p> Lock Type Usage LWKT Tokens Soft locks on vm_objects (blocking allowed) lockmgr Hard locks on vm_map for structural changes Spinlocks Per-page and per-queue locks Per-CPU stats Cached vmstats avoid global contention"},{"location":"sys/vm/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":"<ul> <li>Page coloring with 1024 queues reduces lock contention</li> <li>vm_map_backing chains for efficient shadow objects</li> <li>LWKT token locking allows blocking while held</li> <li>Per-CPU vmstats caching avoids cache-line bouncing</li> <li>Fast fault path (<code>vm_fault_bypass</code>) for active pages</li> <li>UKSMAP - User-kernel shared memory with device callbacks</li> </ul>"},{"location":"sys/vm/#source-files","title":"Source Files","text":"File Lines Description <code>vm_page.c</code> ~4,200 Physical page management <code>vm_object.c</code> ~2,000 VM object management <code>vm_map.c</code> ~4,800 Address space management <code>vm_fault.c</code> ~3,200 Page fault handling <code>vm_pageout.c</code> ~2,900 Pageout daemon <code>swap_pager.c</code> ~2,600 Swap I/O <code>vnode_pager.c</code> ~800 File-backed I/O <code>vm_mmap.c</code> ~1,500 mmap() implementation"},{"location":"sys/vm/#see-also","title":"See Also","text":"<ul> <li>Memory Allocation - kmalloc/objcache</li> <li>Buffer Cache - Filesystem buffers</li> <li>Processes - Process and vmspace lifecycle</li> </ul>"},{"location":"sys/vm/vm_fault/","title":"Page Fault Handling","text":"<p>The page fault handler resolves virtual memory faults by locating or creating physical pages and establishing pmap mappings. DragonFly BSD's implementation emphasizes SMP scalability through shared locking, lockless fast paths, and burst faulting.</p> <p>Source file: <code>sys/vm/vm_fault.c</code> (~3,243 lines)</p>"},{"location":"sys/vm/vm_fault/#when-this-code-runs","title":"When This Code Runs","text":"<p>Page faults are triggered by hardware when a process accesses memory that:</p> Trigger Cause Typical Resolution First access after mmap() No PTE exists Allocate page, zero-fill or load from file Exec touches new code page Demand paging Load from executable via vnode_pager Write to COW page after fork() PTE is read-only Copy page, update PTE to writable Stack growth Access below current stack Expand stack via <code>vm_map_growstack()</code> Swapped-out page access Page not resident Load from swap via swap_pager MADV_DONTNEED region access Page was freed Zero-fill new page"},{"location":"sys/vm/vm_fault/#high-level-flow","title":"High-Level Flow","text":"<pre><code>flowchart TB\n    trap[\"HARDWARE TRAP\"]\n    vm_fault[\"trap() \u2192 vm_fault(map, vaddr, fault_type, fault_flags)\"]\n\n    subgraph lookup[\"1. vm_map_lookup()\"]\n        find[\"Find vm_map_entry for faulting address\"]\n        cow_setup[\"Handle COW setup (shadow object creation)\"]\n        ret_backing[\"Return backing chain and protection\"]\n    end\n\n    subgraph fast[\"2a. vm_fault_bypass()FAST PATH\"]\n        hash[\"Page in hash cache\"]\n        valid[\"Already valid/dirty\"]\n        nolock[\"No locks needed\"]\n        sbusy[\"Soft-busy only\"]\n    end\n\n    subgraph slow[\"2b. vm_fault_object()SLOW PATH\"]\n        walk[\"Walk backing chain\"]\n        pager[\"Call pager if needed\"]\n        cow_copy[\"Handle COW copy\"]\n        zero[\"Zero-fill if new\"]\n    end\n\n    pmap[\"3. pmap_enter()Install PTE for virtual\u2192physical mapping\"]\n\n    trap --&gt; vm_fault\n    vm_fault --&gt; lookup\n    lookup --&gt; fast\n    lookup --&gt; slow\n    fast --&gt; pmap\n    slow --&gt; pmap\n</code></pre>"},{"location":"sys/vm/vm_fault/#overview","title":"Overview","text":"<p>When a process accesses unmapped or protected memory, the hardware generates a page fault. The fault handler must:</p> <ol> <li>Find the vm_map_entry for the faulting address</li> <li>Walk the vm_map_backing chain to locate the page</li> <li>Handle copy-on-write if needed</li> <li>Page in from backing store (file/swap) if needed</li> <li>Enter the page into the pmap</li> </ol> <p>DragonFly optimizes this path with: - Lockless bypass for frequently accessed pages - Shared object tokens for concurrent read faults - Burst faulting to map multiple pages at once - Two-level prefaulting based on lock mode</p>"},{"location":"sys/vm/vm_fault/#data-structures","title":"Data Structures","text":""},{"location":"sys/vm/vm_fault/#struct-faultstate","title":"struct faultstate","text":"<p>Internal state maintained during fault processing:</p> <pre><code>struct faultstate {\n    vm_page_t mary[VM_FAULT_MAX_QUICK];  /* Burst pages (max 16) */\n    vm_map_backing_t ba;            /* Current backing during iteration */\n    vm_prot_t prot;                 /* Final protection for pmap */\n    vm_page_t first_m;              /* Allocated page for COW target */\n    vm_map_backing_t first_ba;      /* Top-level backing */\n    vm_prot_t first_prot;           /* Protection from map lookup */\n    vm_map_t map;                   /* Map being faulted */\n    vm_map_entry_t entry;           /* Entry being faulted */\n    int lookup_still_valid;         /* Map lock state */\n    int hardfault;                  /* I/O was required */\n    int fault_flags;                /* VM_FAULT_* flags */\n    int shared;                     /* Using shared object lock */\n    int msoftonly;                  /* Pages are soft-busied only */\n    int first_shared;               /* First object has shared lock */\n    int wflags;                     /* FW_* flags from lookup */\n    int first_ba_held;              /* Object lock state */\n    struct vnode *vp;               /* Locked vnode (if any) */\n};\n</code></pre>"},{"location":"sys/vm/vm_fault/#fault-flags","title":"Fault Flags","text":"Flag Description <code>VM_FAULT_NORMAL</code> Standard fault <code>VM_FAULT_WIRE_MASK</code> Wiring operation <code>VM_FAULT_USER_WIRE</code> User wiring (mlock) <code>VM_FAULT_CHANGE_WIRING</code> Kernel wiring <code>VM_FAULT_BURST</code> Enable prefaulting <code>VM_FAULT_DIRTY</code> Mark page dirty <code>VM_FAULT_UNSWAP</code> Remove swap backing <code>VM_FAULT_USERMODE</code> User-mode fault"},{"location":"sys/vm/vm_fault/#wiring-flags-wflags","title":"Wiring Flags (wflags)","text":"Flag Description <code>FW_WIRED</code> Entry is wired <code>FW_DIDCOW</code> COW was performed"},{"location":"sys/vm/vm_fault/#main-entry-point","title":"Main Entry Point","text":"<pre><code>int vm_fault(vm_map_t map, vm_offset_t vaddr, \n             vm_prot_t fault_type, int fault_flags);\n</code></pre> <p>Called from the trap handler when a page fault occurs.</p>"},{"location":"sys/vm/vm_fault/#algorithm","title":"Algorithm","text":"<p>1. Initialization: - Set <code>LWP_PAGING</code> flag on current LWP - Initialize faultstate with shared lock preference</p> <p>2. Map Lookup (RetryFault): <pre><code>result = vm_map_lookup(&amp;fs.map, vaddr, fault_type,\n                       &amp;fs.entry, &amp;fs.first_ba,\n                       &amp;first_pindex, &amp;first_count,\n                       &amp;fs.first_prot, &amp;fs.wflags);\n</code></pre></p> <p>The lookup may: - Create a shadow object for COW - Partition large entries for concurrency - Return protection and wiring state</p> <p>3. Handle Lookup Failures: - <code>KERN_INVALID_ADDRESS</code>: Try <code>vm_map_growstack()</code> for stack faults - <code>KERN_PROTECTION_FAILURE</code> with USER_WIRE: Retry with <code>VM_PROT_OVERRIDE_WRITE</code></p> <p>4. Special Entry Types: - <code>MAP_ENTRY_NOFAULT</code>: Panic (should never fault) - <code>MAP_ENTRY_KSTACK</code> guard page: Panic - <code>VM_MAPTYPE_UKSMAP</code>: Call device callback, map directly</p> <p>5. Fast Path (vm_fault_bypass): <pre><code>if (vm_fault_bypass_count &amp;&amp;\n    vm_fault_bypass(&amp;fs, first_pindex, first_count,\n                   &amp;mextcount, fault_type) == KERN_SUCCESS) {\n    goto success;\n}\n</code></pre></p> <p>6. Slow Path: - Acquire object lock (shared or exclusive) - Lock vnode if needed - Call <code>vm_fault_object()</code> to resolve page</p> <p>7. Success: - Set <code>PG_REFERENCED</code> on page - Enter page(s) into pmap - Handle wiring or activate page - Prefault nearby pages if <code>VM_FAULT_BURST</code></p> <p>8. Statistics: - Increment <code>v_vm_faults</code> - Update <code>ru_majflt</code> (hard) or <code>ru_minflt</code> (soft) - Check RSS limits, deactivate pages if exceeded</p>"},{"location":"sys/vm/vm_fault/#lockless-fast-path","title":"Lockless Fast Path","text":"<pre><code>static int vm_fault_bypass(struct faultstate *fs, vm_pindex_t first_pindex,\n                           vm_pindex_t first_count, int *mextcountp,\n                           vm_prot_t fault_type);\n</code></pre> <p>Attempts to resolve the fault without acquiring object locks:</p>"},{"location":"sys/vm/vm_fault/#requirements","title":"Requirements","text":"<ul> <li>No wiring operation in progress</li> <li>Page exists in object's page hash</li> <li>Object is not dead</li> <li>Page is fully valid, on <code>PQ_ACTIVE</code>, not <code>PG_SWAPPED</code></li> <li>For writes: object and page already marked writable/dirty</li> </ul>"},{"location":"sys/vm/vm_fault/#algorithm_1","title":"Algorithm","text":"<ol> <li>Look up page via <code>vm_page_hash_get()</code> (acquires soft-busy)</li> <li>Validate page state</li> <li>For writes: verify <code>OBJ_WRITEABLE | OBJ_MIGHTBEDIRTY</code> and <code>dirty == VM_PAGE_BITS_ALL</code></li> <li>Call <code>vm_page_soft_activate()</code> (passive queue manipulation)</li> <li>Optionally burst: get additional consecutive pages</li> <li>Return with soft-busied pages in <code>fs-&gt;mary[]</code></li> </ol>"},{"location":"sys/vm/vm_fault/#benefits","title":"Benefits","text":"<ul> <li>No object token acquisition</li> <li>No hard page busy</li> <li>Excellent for shared executables and libraries</li> <li>Multiple pages can be mapped in single operation</li> </ul>"},{"location":"sys/vm/vm_fault/#core-fault-logic","title":"Core Fault Logic","text":"<pre><code>static int vm_fault_object(struct faultstate *fs, vm_pindex_t first_pindex,\n                           vm_prot_t fault_type, int allow_nofault);\n</code></pre> <p>Walks the backing chain to find or create the target page.</p>"},{"location":"sys/vm/vm_fault/#protection-upgrade","title":"Protection Upgrade","text":"<p>For read faults, the code attempts to also enable write access if: - The mapping allows writes - The page is not COW - The pmap doesn't require A/M bit emulation (vkernel)</p>"},{"location":"sys/vm/vm_fault/#main-loop-backing-chain-walk","title":"Main Loop (Backing Chain Walk)","text":"<pre><code>for (;;) {\n    1. Check if object is dead\n    2. Look up page in current object\n    3. If found and valid \u2192 break to PAGE FOUND\n    4. If not found \u2192 try pager or allocate\n    5. Move to next backing object\n}\n</code></pre> <p>Page Lookup: <pre><code>fs-&gt;mary[0] = vm_page_lookup_busy_try(fs-&gt;ba-&gt;object, pindex, TRUE, &amp;error);\n</code></pre></p> <p>If the page is busy, sleep and return <code>KERN_TRY_AGAIN</code>.</p> <p>Page Not Resident:</p> <p>For <code>OBJT_SWAP</code> objects, check <code>swap_pager_haspage_locked()</code> before allocating.</p> <p>If the page might be in the pager (<code>TRYPAGER</code>) or this is the first object: 1. Require exclusive lock for allocation 2. Check <code>pindex &lt; object-&gt;size</code> 3. Allocate via <code>vm_page_alloc()</code></p> <p>Pager I/O: <pre><code>rv = vm_pager_get_page(object, pindex, &amp;fs-&gt;mary[0], seqaccess);\n</code></pre></p> Result Action <code>VM_PAGER_OK</code> Increment hardfault, re-lookup page <code>VM_PAGER_FAIL</code> Continue to next backing object <code>VM_PAGER_ERROR</code> Return <code>KERN_FAILURE</code> <code>VM_PAGER_BAD</code> Return <code>KERN_PROTECTION_FAILURE</code> <p>Continue to Next Object: <pre><code>next_ba = fs-&gt;ba-&gt;backing_ba;\nif (next_ba == NULL) {\n    /* Zero-fill the page */\n    vm_page_zero_fill(fs-&gt;mary[0]);\n    break;\n}\n/* Adjust pindex through offset chain */\npindex -= OFF_TO_IDX(fs-&gt;ba-&gt;offset);\npindex += OFF_TO_IDX(next_ba-&gt;offset);\nfs-&gt;ba = next_ba;\n</code></pre></p>"},{"location":"sys/vm/vm_fault/#copy-on-write","title":"Copy-On-Write","text":"<p>When the page is found in a backing object (<code>ba != first_ba</code>) and this is a write fault:</p> <pre><code>/* Copy from backing page to first_m */\nvm_page_copy(fs-&gt;mary[0], fs-&gt;first_m);\n\n/* Release backing page and object */\nrelease_page(fs);\nvm_object_pip_wakeup(fs-&gt;ba-&gt;object);\nvm_object_drop(fs-&gt;ba-&gt;object);\n\n/* Switch to the copy */\nfs-&gt;ba = fs-&gt;first_ba;\nfs-&gt;mary[0] = fs-&gt;first_m;\n</code></pre> <p>For read faults on backing pages, write permission is masked: <pre><code>fs-&gt;prot &amp;= ~VM_PROT_WRITE;\n</code></pre></p>"},{"location":"sys/vm/vm_fault/#finalization","title":"Finalization","text":"<ol> <li>Activate the page</li> <li>For writes:</li> <li><code>vm_object_set_writeable_dirty()</code></li> <li>Handle <code>PG_SWAPPED</code> (requires exclusive lock for <code>swap_pager_unswapped()</code>)</li> <li>Return <code>KERN_SUCCESS</code> with busied page</li> </ol>"},{"location":"sys/vm/vm_fault/#wiring-support","title":"Wiring Support","text":""},{"location":"sys/vm/vm_fault/#vm_fault_wire","title":"vm_fault_wire","text":"<pre><code>int vm_fault_wire(vm_map_t map, vm_map_entry_t entry,\n                  boolean_t user_wire, int kmflags);\n</code></pre> <p>Wires a range by simulating faults:</p> <ol> <li>Entry must be marked <code>IN_TRANSITION</code></li> <li>Unlock map during faults</li> <li>For each page: call <code>vm_fault()</code> with wire flags</li> <li>On failure: unwire already-wired pages</li> </ol>"},{"location":"sys/vm/vm_fault/#vm_fault_unwire","title":"vm_fault_unwire","text":"<pre><code>void vm_fault_unwire(vm_map_t map, vm_map_entry_t entry);\n</code></pre> <p>Unwires a range: - Calls <code>pmap_unwire()</code> to get page - Calls <code>vm_page_unwire()</code> to decrement wire count - Skips guard page for <code>MAP_ENTRY_KSTACK</code></p>"},{"location":"sys/vm/vm_fault/#shadow-collapse","title":"Shadow Collapse","text":"<pre><code>int vm_fault_collapse(vm_map_t map, vm_map_entry_t entry);\n</code></pre> <p>Used during fork when the backing chain exceeds <code>vm.map_backing_limit</code>:</p> <ol> <li>For each pindex in entry range:</li> <li>Skip if page already in head object</li> <li>Call <code>vm_fault_object()</code> with write permission</li> <li>Activates and wakes page</li> <li>If any pages copied: <code>pmap_remove()</code> entire range</li> </ol> <p>This brings all pages into the head object, allowing the backing chain to be freed.</p>"},{"location":"sys/vm/vm_fault/#physical-page-copy","title":"Physical Page Copy","text":"<pre><code>void vm_fault_copy_entry(vm_map_t dst_map, vm_map_t src_map,\n                         vm_map_entry_t dst_entry, vm_map_entry_t src_entry);\n</code></pre> <p>Physically copies pages between entries when COW is not possible (wired pages):</p> <ol> <li>Allocate destination object</li> <li>For each page:</li> <li>Allocate page in destination</li> <li>Look up page in source (must exist)</li> <li><code>vm_page_copy()</code> contents</li> <li><code>pmap_enter()</code> into destination pmap</li> </ol>"},{"location":"sys/vm/vm_fault/#prefaulting","title":"Prefaulting","text":"<p>Prefaulting maps nearby pages after a fault to reduce future faults.</p>"},{"location":"sys/vm/vm_fault/#full-prefault","title":"Full Prefault","text":"<pre><code>static void vm_prefault(pmap_t pmap, vm_offset_t addra,\n                        vm_map_entry_t entry, int prot, int fault_flags);\n</code></pre> <p>Used when holding exclusive object lock:</p> <ol> <li>Scan \u00b1<code>vm_prefault_pages</code> (default 8) around fault address</li> <li>Skip already-mapped pages (<code>pmap_prefault_ok()</code>)</li> <li>Walk backing chain for each address</li> <li>If not found and <code>vm_fast_fault</code>: allocate zero-fill page</li> <li>Enter page into pmap</li> </ol>"},{"location":"sys/vm/vm_fault/#quick-prefault","title":"Quick Prefault","text":"<pre><code>static void vm_prefault_quick(pmap_t pmap, vm_offset_t addra,\n                              vm_map_entry_t entry, int prot, int fault_flags);\n</code></pre> <p>Used when holding shared object lock:</p> <ul> <li>Only works on terminal objects (no backing chain)</li> <li>Uses <code>vm_page_lookup_sbusy_try()</code> for soft-busy</li> <li>Only maps existing valid pages, no allocation</li> <li>Much lower overhead than full prefault</li> </ul>"},{"location":"sys/vm/vm_fault/#selection","title":"Selection","text":"<pre><code>if (fs.first_shared == 0 &amp;&amp; fs.shared == 0) {\n    vm_prefault(pmap, vaddr, entry, prot, fault_flags);\n} else {\n    vm_prefault_quick(pmap, vaddr, entry, prot, fault_flags);\n}\n</code></pre>"},{"location":"sys/vm/vm_fault/#alternative-entry-points","title":"Alternative Entry Points","text":""},{"location":"sys/vm/vm_fault/#vm_fault_page","title":"vm_fault_page","text":"<pre><code>vm_page_t vm_fault_page(vm_map_t map, vm_offset_t vaddr, vm_prot_t fault_type,\n                        int fault_flags, int *errorp, int *busyp);\n</code></pre> <p>Returns a held (and optionally busied) page without pmap update:</p> <ol> <li>First tries <code>pmap_fault_page_quick()</code> for fast lookup</li> <li>Falls back to full <code>vm_fault_object()</code> path</li> <li>Returns held page, optionally busied for writes</li> <li>Used by vkernel, ptrace, and similar</li> </ol>"},{"location":"sys/vm/vm_fault/#vm_fault_page_quick","title":"vm_fault_page_quick","text":"<pre><code>vm_page_t vm_fault_page_quick(vm_offset_t va, vm_prot_t fault_type,\n                              int *errorp, int *busyp);\n</code></pre> <p>Convenience wrapper using current process vmspace.</p>"},{"location":"sys/vm/vm_fault/#vm_fault_object_page","title":"vm_fault_object_page","text":"<pre><code>vm_page_t vm_fault_object_page(vm_object_t object, vm_ooffset_t offset,\n                               vm_prot_t fault_type, int fault_flags,\n                               int *sharedp, int *errorp);\n</code></pre> <p>Faults a page directly from an object (no map involvement): - Creates fake <code>vm_map_entry</code> - Used internally for direct object access</p>"},{"location":"sys/vm/vm_fault/#sysctls","title":"Sysctls","text":"Sysctl Default Description <code>vm.shared_fault</code> 1 Allow shared object token for faults <code>vm.fault_bypass</code> 1 Enable lockless fast path <code>vm.prefault_pages</code> 8 Pages to prefault each direction <code>vm.fast_fault</code> 1 Allow zero-fill allocation during prefault <code>vm.debug_fault</code> 0 Debug output for faults <code>vm.debug_cluster</code> 0 Debug output for I/O clustering"},{"location":"sys/vm/vm_fault/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_fault/#lockless-bypass","title":"Lockless Bypass","text":"<p><code>vm_fault_bypass()</code> uses the page hash table to find pages without acquiring object locks. Pages are soft-busied only, allowing concurrent access. This dramatically improves performance for shared libraries and executables.</p>"},{"location":"sys/vm/vm_fault/#shared-object-tokens","title":"Shared Object Tokens","text":"<p>The <code>vm_shared_fault</code> sysctl (default on) allows read faults to use shared object tokens. This enables concurrent faults on the same object from different processes, important for fork-heavy workloads.</p>"},{"location":"sys/vm/vm_fault/#exclusive-lock-heuristics","title":"Exclusive Lock Heuristics","text":"<p><code>VM_MAP_BACK_EXCL_HEUR</code> tracks when exclusive locks were needed, avoiding unnecessary shared\u2192exclusive upgrades on subsequent faults.</p>"},{"location":"sys/vm/vm_fault/#burst-faulting","title":"Burst Faulting","text":"<p>The <code>mary[]</code> array (max 16 pages) allows multiple pages to be faulted in a single operation. Combined with prefaulting, this reduces per-page overhead.</p>"},{"location":"sys/vm/vm_fault/#rss-enforcement","title":"RSS Enforcement","text":"<p>After user faults, the code checks process RSS against <code>RLIMIT_RSS</code>: <pre><code>if (size &gt; limit) {\n    vm_pageout_map_deactivate_pages(map, limit);\n}\n</code></pre></p>"},{"location":"sys/vm/vm_fault/#mgtdevice-support","title":"MGTDEVICE Support","text":"<p>For managed device objects (GPU/DRM), pages are not indexed in the VM object. The pager returns pages directly for pmap entry without object insertion.</p>"},{"location":"sys/vm/vm_fault/#see-also","title":"See Also","text":"<ul> <li>VM Subsystem Overview - Architecture overview</li> <li>Physical Pages - Page allocation and states</li> <li>VM Objects - Object lifecycle</li> <li>Address Space - Map lookup and entry management</li> </ul>"},{"location":"sys/vm/vm_map/","title":"Address Space Management","text":"<p>The VM map subsystem manages virtual address spaces in DragonFly BSD. It handles address range mappings, copy-on-write (COW), process forking, stack management, and the relationship between virtual addresses and backing objects.</p> <p>Source file: <code>sys/vm/vm_map.c</code> (~4,781 lines)</p>"},{"location":"sys/vm/vm_map/#common-operations","title":"Common Operations","text":"<p>Understanding when this code runs helps navigate the 4,781 lines. Here's what happens for common scenarios:</p> User Action Syscall Key Functions What Happens <code>mmap()</code> <code>sys_mmap</code> <code>vm_map_find()</code> \u2192 <code>vm_map_insert()</code> Creates vm_map_entry, may coalesce with previous <code>munmap()</code> <code>sys_munmap</code> <code>vm_map_remove()</code> \u2192 <code>vm_map_delete()</code> Clips entries, removes pmap mappings, frees objects <code>mprotect()</code> <code>sys_mprotect</code> <code>vm_map_protect()</code> Two-pass: validate then apply to entries and pmap <code>mlock()</code> <code>sys_mlock</code> <code>vm_map_user_wiring()</code> Faults in pages, sets USER_WIRED flag <code>fork()</code> <code>sys_fork</code> <code>vmspace_fork()</code> \u2192 <code>vm_map_copy_entry()</code> Clones entries, sets up COW sharing Stack growth (fault) <code>vm_map_growstack()</code> Expands stack entry into reserved space Page fault (trap) <code>vm_map_lookup()</code> Finds entry, handles COW, returns backing info <pre><code>flowchart LR\n    subgraph User[\"USER\"]\n        mmap[\"mmap(NULL, 4096, ...)\"]\n        access[\"*ptr = 42\"]\n        fork[\"fork()\"]\n        munmap[\"munmap(ptr, 4096)\"]\n    end\n\n    subgraph vmmap[\"vm_map.c\"]\n        findspace[\"vm_map_findspace()\"]\n        insert[\"vm_map_insert()\"]\n        lookup[\"vm_map_lookup()\"]\n        fault[\"(vm_fault handles rest)\"]\n        vmfork[\"vmspace_fork()\"]\n        copyentry[\"vm_map_copy_entry()\"]\n        delete[\"vm_map_delete()\"]\n        pmapremove[\"pmap_remove()\"]\n    end\n\n    subgraph Result[\"RESULT\"]\n        addr[\"0x7f0000000000\"]\n        newentry[\"new vm_map_entry\"]\n        entryinfo[\"entry + backing info\"]\n        childvm[\"child vmspace\"]\n        cowentries[\"COW entries\"]\n        removed[\"entry removed\"]\n        cleared[\"PTEs cleared\"]\n    end\n\n    mmap --&gt; findspace --&gt; addr\n    mmap --&gt; insert --&gt; newentry\n    access --&gt; lookup --&gt; entryinfo\n    access --&gt; fault\n    fork --&gt; vmfork --&gt; childvm\n    fork --&gt; copyentry --&gt; cowentries\n    munmap --&gt; delete --&gt; removed\n    munmap --&gt; pmapremove --&gt; cleared\n</code></pre>"},{"location":"sys/vm/vm_map/#overview","title":"Overview","text":"<p>A virtual address space is represented by two key structures:</p> <ul> <li>vmspace - Per-process address space containing both <code>vm_map</code> and <code>pmap</code></li> <li>vm_map - Collection of address range mappings organized in a red-black tree</li> </ul> <p>Each mapping (<code>vm_map_entry</code>) describes a contiguous virtual address range with its protection, inheritance, and backing store. DragonFly uses <code>vm_map_backing</code> chains to link entries to objects, enabling efficient shadow object handling without modifying the objects themselves.</p>"},{"location":"sys/vm/vm_map/#data-structures","title":"Data Structures","text":""},{"location":"sys/vm/vm_map/#struct-vmspace","title":"struct vmspace","text":"<p>Per-process address space descriptor:</p> <pre><code>struct vmspace {\n    struct vm_map vm_map;       /* Embedded address map */\n    struct pmap vm_pmap;        /* Embedded page table */\n\n    /* Copied on fork (vm_startcopy to vm_endcopy) */\n    segsz_t vm_swrss;          /* Swap reserved for process */\n    segsz_t vm_tsize;          /* Text size (pages) */\n    segsz_t vm_dsize;          /* Data size (pages) */\n    segsz_t vm_ssize;          /* Stack size (pages) */\n    caddr_t vm_taddr;          /* Text address */\n    caddr_t vm_daddr;          /* Data address */\n    caddr_t vm_maxsaddr;       /* Max stack address */\n\n    int vm_flags;              /* VMSPACE_EXIT1/EXIT2 */\n    int vm_exitingcnt;         /* Threads currently exiting */\n    int vm_refcnt;             /* Reference count */\n    int vm_holdcnt;            /* Hold count (prevents stage-2 exit) */\n};\n</code></pre>"},{"location":"sys/vm/vm_map/#struct-vm_map","title":"struct vm_map","text":"<p>Virtual address space:</p> <pre><code>struct vm_map {\n    struct vm_map_rb_tree rb_root;  /* RB tree of entries */\n    struct pmap *pmap;              /* Physical address map */\n\n    vm_offset_t min_addr;           /* Lowest valid address */\n    vm_offset_t max_addr;           /* Highest valid address */\n    vm_size_t size;                 /* Total mapped size */\n    int nentries;                   /* Number of entries */\n\n    unsigned int timestamp;         /* Modification counter */\n    struct lock lock;               /* Hard lock (100ms timeout) */\n    struct lwkt_token token;        /* Soft serializer */\n\n    vm_flags_t flags;               /* MAP_WIREFUTURE, etc. */\n    struct vm_map_freehint freehint[VM_MAP_FFCOUNT];  /* Findspace hints */\n    struct spinlock ilock_spin;     /* For range interlocks */\n};\n</code></pre>"},{"location":"sys/vm/vm_map/#struct-vm_map_entry","title":"struct vm_map_entry","text":"<p>Single address range mapping:</p> <pre><code>struct vm_map_entry {\n    /* RB tree linkage */\n    RB_ENTRY(vm_map_entry) rb_entry;\n\n    /* Embedded backing store */\n    struct vm_map_backing ba;       /* pmap, start, end, offset, object */\n\n    /* Properties */\n    vm_prot_t protection;           /* Current protection */\n    vm_prot_t max_protection;       /* Maximum allowed */\n    vm_inherit_t inheritance;       /* Fork behavior */\n    u_int wired_count;              /* Wiring reference count */\n    vm_eflags_t eflags;             /* Entry flags */\n    vm_maptype_t maptype;           /* NORMAL, SUBMAP, UKSMAP */\n    vm_subsys_t id;                 /* Subsystem identifier */\n\n    union {\n        vm_offset_t avail_ssize;    /* Stack: available growth */\n        int dev_prot;               /* Device protection */\n    } aux;\n};\n</code></pre>"},{"location":"sys/vm/vm_map/#struct-vm_map_backing","title":"struct vm_map_backing","text":"<p>Backing store chain element (DragonFly-specific):</p> <pre><code>struct vm_map_backing {\n    /* Chain linkage */\n    struct vm_map_backing *backing_ba;   /* Next in shadow chain */\n    int backing_count;                   /* Depth counter */\n\n    /* Address range (mirrored from entry but can differ in chain) */\n    vm_offset_t start;\n    vm_offset_t end;\n    vm_ooffset_t offset;\n\n    /* Backing store */\n    struct pmap *pmap;                   /* Page table reference */\n    union {\n        struct vm_object *object;        /* NORMAL: backing object */\n        struct vm_map *sub_map;          /* SUBMAP: nested map */\n        int (*uksmap)(struct vm_map_backing *ba, int op,\n                      cdev_t dev, vm_page_t page);  /* UKSMAP: callback */\n    };\n    void *aux_info;                      /* UKSMAP auxiliary */\n\n    /* Object linkage */\n    TAILQ_ENTRY(vm_map_backing) entry;   /* On object's backing_list */\n};\n</code></pre>"},{"location":"sys/vm/vm_map/#entry-flags-vm_eflags_t","title":"Entry Flags (vm_eflags_t)","text":"Flag Description <code>MAP_ENTRY_COW</code> Copy-on-write enabled <code>MAP_ENTRY_NEEDS_COPY</code> COW not yet performed <code>MAP_ENTRY_NOFAULT</code> No faults allowed <code>MAP_ENTRY_USER_WIRED</code> User-level wiring (mlock) <code>MAP_ENTRY_IN_TRANSITION</code> Entry being modified <code>MAP_ENTRY_NEEDS_WAKEUP</code> Wake waiters when done <code>MAP_ENTRY_NOSYNC</code> Don't sync to filesystem <code>MAP_ENTRY_NOCOREDUMP</code> Exclude from core dumps <code>MAP_ENTRY_STACK</code> User stack mapping <code>MAP_ENTRY_KSTACK</code> Kernel stack mapping"},{"location":"sys/vm/vm_map/#map-types-vm_maptype_t","title":"Map Types (vm_maptype_t)","text":"Type Description <code>VM_MAPTYPE_NORMAL</code> Standard mapping with backing object <code>VM_MAPTYPE_SUBMAP</code> Nested map (kernel use) <code>VM_MAPTYPE_UKSMAP</code> User-kernel shared memory with device callback"},{"location":"sys/vm/vm_map/#vmspace-lifecycle","title":"vmspace Lifecycle","text":""},{"location":"sys/vm/vm_map/#two-stage-termination","title":"Two-Stage Termination","text":"<p>DragonFly uses a two-stage termination for vmspace cleanup:</p> <p>Stage 1 (ref_count reaches 0): - Triggered by <code>vmspace_rel()</code> or <code>vmspace_relexit()</code> - Sets <code>VMSPACE_EXIT1</code> flag - Detaches SysV shared memory - Removes pmap mappings and frees most VM resources - Process can still run (using cached TLB entries)</p> <p>Stage 2 (hold_count reaches 0): - Triggered by <code>vmspace_drop()</code> or <code>vmspace_exitfree()</code> - Sets <code>VMSPACE_EXIT2</code> flag - Final cleanup via <code>vm_map_delete()</code> - Releases pmap resources - Returns vmspace to objcache</p> <p>This separation allows efficient process exit - the heavyweight cleanup happens while threads are still runnable.</p>"},{"location":"sys/vm/vm_map/#allocation-and-initialization","title":"Allocation and Initialization","text":"<pre><code>/* Allocate new vmspace */\nstruct vmspace *vmspace_alloc(vm_offset_t min, vm_offset_t max);\n</code></pre> <p>Initialization sequence: 1. Get vmspace from objcache 2. Zero the copyable region (<code>vm_startcopy</code> to <code>vm_endcopy</code>) 3. Initialize embedded <code>vm_map</code> with RB tree, locks, freehints 4. Set refs=1, holds=1 5. Initialize pmap via <code>pmap_pinit()</code> 6. Architecture-specific setup via <code>cpu_vmspace_alloc()</code></p>"},{"location":"sys/vm/vm_map/#reference-counting","title":"Reference Counting","text":"Function Description <code>vmspace_ref(vm)</code> Add reference (increments both ref and hold) <code>vmspace_rel(vm)</code> Release reference (triggers stage-1 on 1\u21920) <code>vmspace_hold(vm)</code> Add hold only (acquires map token) <code>vmspace_drop(vm)</code> Release hold (triggers stage-2 on 1\u21920) <code>vmspace_relexit(p)</code> Exit path: add hold, release ref <code>vmspace_exitfree(p)</code> Reap path: clear p_vmspace, drop hold"},{"location":"sys/vm/vm_map/#map-operations","title":"Map Operations","text":""},{"location":"sys/vm/vm_map/#entry-lookup","title":"Entry Lookup","text":"<pre><code>boolean_t vm_map_lookup_entry(vm_map_t map, vm_offset_t address, \n                               vm_map_entry_t *entry);\n</code></pre> <p>Performs RB tree lookup: - Returns TRUE if address is within an entry - Sets <code>*entry</code> to containing entry or closest predecessor - <code>*entry = NULL</code> if address is before all entries</p>"},{"location":"sys/vm/vm_map/#finding-free-space","title":"Finding Free Space","text":"<pre><code>int vm_map_findspace(vm_map_t map, vm_offset_t start, vm_size_t length,\n                     vm_size_t align, int flags, vm_offset_t *addr);\n</code></pre> <p>Finds a hole of <code>length</code> bytes starting at or after <code>start</code>: - Respects alignment requirements - <code>MAP_32BIT</code> flag restricts to low 4GB - Uses freehint cache for O(1) common case - Handles stack entry reserved space (<code>avail_ssize</code>) - For kernel_map: calls <code>pmap_growkernel()</code> if needed</p> <p>Freehint Optimization:</p> <p>The map maintains <code>VM_MAP_FFCOUNT</code> hints for recently successful allocations:</p> <pre><code>struct vm_map_freehint {\n    vm_offset_t start;\n    vm_offset_t length;\n    vm_offset_t align;\n};\n</code></pre> <p>When findspace succeeds, it updates the hint cache. Subsequent requests with matching (length, align) get O(1) lookup.</p>"},{"location":"sys/vm/vm_map/#inserting-mappings","title":"Inserting Mappings","text":"<pre><code>int vm_map_insert(vm_map_t map, int *countp, vm_object_t object,\n                  void *aux, vm_ooffset_t offset,\n                  vm_offset_t start, vm_offset_t end,\n                  vm_maptype_t maptype, vm_subsys_t id,\n                  vm_prot_t prot, vm_prot_t max, int cow);\n</code></pre> <p>COWF_* flags:</p> Flag Effect <code>COWF_COPY_ON_WRITE</code> Enable COW (sets COW + NEEDS_COPY) <code>COWF_NOFAULT</code> No faults allowed (object must be NULL) <code>COWF_PREFAULT</code> Prepopulate page tables <code>COWF_32BIT</code> Restrict to 32-bit addresses <code>COWF_DISABLE_SYNCER</code> Set NOSYNC flag <code>COWF_DISABLE_COREDUMP</code> Set NOCOREDUMP flag <code>COWF_IS_STACK</code> Mark as stack <p>Coalescing Optimization:</p> <p>When no object is specified and the previous entry is compatible (same flags, id, maptype, no backing chain), insert attempts to extend the previous entry's object via <code>vm_object_coalesce()</code> instead of creating a new entry.</p>"},{"location":"sys/vm/vm_map/#high-level-find-and-insert","title":"High-Level Find and Insert","text":"<pre><code>int vm_map_find(vm_map_t map, vm_object_t object, void *aux,\n                vm_ooffset_t offset, vm_offset_t *addr,\n                vm_size_t length, vm_size_t align,\n                boolean_t fitit, vm_maptype_t maptype,\n                vm_subsys_t id, vm_prot_t prot, vm_prot_t max, int cow);\n</code></pre> <p>Combines findspace + insert: - If <code>fitit</code> is TRUE: finds space starting at <code>*addr</code> - If <code>fitit</code> is FALSE: uses exact address <code>*addr</code> - Handles UKSMAP aux_info setup for upmap/kpmap/lpmap</p>"},{"location":"sys/vm/vm_map/#entry-clipping","title":"Entry Clipping","text":"<pre><code>void vm_map_clip_start(vm_map_t map, vm_map_entry_t entry,\n                       vm_offset_t startaddr, int *countp);\nvoid vm_map_clip_end(vm_map_t map, vm_map_entry_t entry,\n                     vm_offset_t endaddr, int *countp);\n</code></pre> <p>Clips split an entry at a given address: - <code>clip_start</code>: Creates new entry for front portion - <code>clip_end</code>: Creates new entry for tail portion - Both replicate the backing chain via <code>vm_map_backing_replicated()</code></p> <p>Partition Optimization:</p> <p>For large anonymous mappings, clip may allocate an object to enable 32MB partitioning for concurrent faults:</p> <pre><code>#define MAP_ENTRY_PARTITION_SIZE  (32 * 1024 * 1024)\n</code></pre>"},{"location":"sys/vm/vm_map/#range-operations","title":"Range Operations","text":"<p>Many operations work on address ranges using clip_range/unclip_range:</p> <pre><code>vm_map_entry_t vm_map_clip_range(vm_map_t map, vm_offset_t start,\n                                  vm_offset_t end, int *countp, int flags);\nvoid vm_map_unclip_range(vm_map_t map, vm_map_entry_t entry,\n                         vm_offset_t start, vm_offset_t end,\n                         int *countp, int flags);\n</code></pre> <p><code>clip_range</code>: 1. Clips entries to exact range boundaries 2. Sets <code>IN_TRANSITION</code> on all covered entries 3. Returns first entry (or NULL) 4. <code>MAP_CLIP_NO_HOLES</code> fails if gaps exist</p> <p><code>unclip_range</code>: 1. Clears <code>IN_TRANSITION</code> flags 2. Wakes any waiters (<code>NEEDS_WAKEUP</code>) 3. Simplifies entries (merges compatible neighbors)</p>"},{"location":"sys/vm/vm_map/#removing-mappings","title":"Removing Mappings","text":"<pre><code>int vm_map_remove(vm_map_t map, vm_offset_t start, vm_offset_t end);\n</code></pre> <p>Internal workhorse <code>vm_map_delete()</code>: 1. Clips entries to range 2. For each entry:    - Unwires if wired    - Removes pmap mappings    - For anonymous with ONEMAPPING: removes pages + swap    - Deletes entry 3. Updates freehint with new hole</p>"},{"location":"sys/vm/vm_map/#protection-changes","title":"Protection Changes","text":"<pre><code>int vm_map_protect(vm_map_t map, vm_offset_t start, vm_offset_t end,\n                   vm_prot_t new_prot, boolean_t set_max);\n</code></pre> <p>Two-pass algorithm: 1. Validation: Check submaps, verify new protection fits max 2. Apply: Clip entries, update protection, call <code>pmap_protect()</code></p> <p>For COW entries, write protection is masked from pmap even if logically allowed.</p>"},{"location":"sys/vm/vm_map/#wiring","title":"Wiring","text":"<p>User wiring (mlock/munlock):</p> <pre><code>int vm_map_user_wiring(vm_map_t map, vm_offset_t start, vm_offset_t end,\n                       boolean_t new_pageable);\n</code></pre> <ul> <li>Sets <code>USER_WIRED</code> flag</li> <li>Increments <code>wired_count</code></li> <li>Calls <code>vm_fault_wire()</code> to fault in pages</li> <li>On failure: backs out changes</li> </ul> <p>Kernel wiring:</p> <pre><code>int vm_map_kernel_wiring(vm_map_t map, vm_offset_t start, vm_offset_t end,\n                         int kmflags);\n</code></pre> <ul> <li><code>KM_KRESERVE</code> avoids zone allocation recursion</li> <li>Two-pass: prepare entries, then wire</li> </ul>"},{"location":"sys/vm/vm_map/#msync-implementation","title":"msync Implementation","text":"<pre><code>int vm_map_clean(vm_map_t map, vm_offset_t start, vm_offset_t end,\n                 boolean_t syncio, boolean_t invalidate);\n</code></pre> <p>For each entry: - SUBMAP: recurse - NORMAL: follow backing chain to vnode object, call <code>vm_object_page_clean()</code> - If invalidate: remove pmap mappings</p>"},{"location":"sys/vm/vm_map/#copy-on-write","title":"Copy-On-Write","text":""},{"location":"sys/vm/vm_map/#shadow-object-creation","title":"Shadow Object Creation","text":"<p>When a COW fault occurs on a <code>NEEDS_COPY</code> entry:</p> <pre><code>void vm_map_entry_shadow(vm_map_entry_t entry);\n</code></pre> <p>Optimization case - If source object is: - ref_count == 1 - No vnode - OBJT_DEFAULT or OBJT_SWAP - No handle</p> <p>Then: Just clear NEEDS_COPY, no shadow needed.</p> <p>General case: 1. Allocate new result object (OBJT_DEFAULT) 2. Create new <code>vm_map_backing</code> for old object 3. Chain: <code>entry-&gt;ba.object = new</code>, <code>entry-&gt;ba.backing_ba-&gt;object = old</code> 4. Increment <code>backing_count</code> 5. Clear <code>OBJ_ONEMAPPING</code> on source</p>"},{"location":"sys/vm/vm_map/#shadow-chain-depth-limiting","title":"Shadow Chain Depth Limiting","text":"<pre><code>sysctl vm.map_backing_limit=5  /* Default max depth */\n</code></pre> <p>During fork, if <code>backing_count &gt;= limit</code>: - Collapse via <code>vm_fault_collapse()</code> to reduce depth - Prevents unbounded chain growth</p>"},{"location":"sys/vm/vm_map/#process-fork","title":"Process Fork","text":"<pre><code>struct vmspace *vmspace_fork(struct vmspace *vm1, struct proc *p2,\n                             struct lwp *lp2);\n</code></pre> <p>Creates child address space:</p> <ol> <li>Allocate new vmspace</li> <li>Copy size/address fields (<code>vm_startcopy</code> to <code>vm_endcopy</code>)</li> <li>For each parent entry, based on inheritance:</li> </ol> Inheritance Action <code>VM_INHERIT_NONE</code> Skip entry <code>VM_INHERIT_SHARE</code> Clone entry, share backing (allocate object if needed, shadow if NEEDS_COPY) <code>VM_INHERIT_COPY</code> Clone entry, set up COW via <code>vm_map_copy_entry()</code> <p><code>vm_map_copy_entry()</code> for COW: - If wired: physically copy pages via <code>vm_fault_copy_entry()</code> - If not wired:   - Write-protect parent PTEs   - Set COW + NEEDS_COPY on both   - Copy PTEs via <code>pmap_copy()</code></p>"},{"location":"sys/vm/vm_map/#uksmap-fork-handling","title":"UKSMAP Fork Handling","text":"<p>For user-kernel shared mappings: - upmap/kpmap: Fork normally with updated aux_info - lpmap: Only fork if thread ID matches new LWP</p>"},{"location":"sys/vm/vm_map/#stack-management","title":"Stack Management","text":""},{"location":"sys/vm/vm_map/#stack-creation","title":"Stack Creation","text":"<pre><code>int vm_map_stack(vm_map_t map, vm_offset_t addrbos, vm_size_t max_ssize,\n                 int flags, vm_prot_t prot, vm_prot_t max, int cow);\n</code></pre> <p>Creates auto-grow stack: 1. Initial size = min(max_ssize, sgrowsiz) 2. Find space for full <code>max_ssize</code> 3. Insert mapping at top (grows down) 4. Set <code>entry-&gt;aux.avail_ssize = max_ssize - init_ssize</code></p>"},{"location":"sys/vm/vm_map/#stack-growth","title":"Stack Growth","text":"<pre><code>int vm_map_growstack(vm_map_t map, vm_offset_t addr);\n</code></pre> <p>Called on stack fault: 1. Verify faulting address is in reserved stack space 2. Calculate growth amount (page-aligned) 3. Check against:    - Available space (<code>avail_ssize</code>)    - Previous entry gap    - <code>RLIMIT_STACK</code>    - <code>RLIMIT_VMEM</code> 4. Insert new mapping below stack entry 5. Update <code>avail_ssize</code> and <code>vm_ssize</code> 6. If <code>MAP_WIREFUTURE</code>: wire new region</p>"},{"location":"sys/vm/vm_map/#fault-path-lookup","title":"Fault Path Lookup","text":"<pre><code>int vm_map_lookup(vm_map_t *var_map, vm_offset_t vaddr, vm_prot_t fault_type,\n                  vm_map_entry_t *out_entry, struct vm_map_backing **bap,\n                  vm_pindex_t *pindex, vm_pindex_t *pcount,\n                  vm_prot_t *out_prot, int *wflags);\n</code></pre> <p>Core function called from page fault handler:</p> <ol> <li>Reserve entries (with recursion protection)</li> <li>Lock map (read or write)</li> <li>Lookup entry for address</li> <li>Handle submaps: switch to submap, retry</li> <li>Check protection:</li> <li><code>OVERRIDE_WRITE</code> uses max_protection</li> <li>Normal uses current protection</li> <li>Handle <code>NEEDS_COPY</code>:</li> <li>Write fault: upgrade lock, shadow entry, set <code>FW_DIDCOW</code></li> <li>Read fault: mask out write permission</li> <li>Partition large entries for concurrent faults</li> <li>Allocate object if needed</li> <li>Return backing, page index, protection, flags</li> </ol> <p>Entry Partitioning:</p> <p>For entries larger than 32MB, <code>vm_map_entry_partition()</code> clips to the 32MB partition containing the fault address. This allows concurrent faults on different partitions.</p>"},{"location":"sys/vm/vm_map/#range-interlocks","title":"Range Interlocks","text":"<p>For <code>MADV_INVAL</code> coordination with faults:</p> <pre><code>void vm_map_interlock(vm_map_t map, struct vm_map_ilock *ilock,\n                      vm_offset_t ran_beg, vm_offset_t ran_end);\nvoid vm_map_deinterlock(vm_map_t map, struct vm_map_ilock *ilock);\n</code></pre> <p>Interlocks wait for overlapping operations to complete, preventing races between pmap manipulation and fault handling.</p>"},{"location":"sys/vm/vm_map/#per-cpu-entry-cache","title":"Per-CPU Entry Cache","text":"<p>To avoid zone allocation in hot paths:</p> <pre><code>/* Per-CPU freelist */\ngd-&gt;gd_vme_base  /* Entry freelist head */\ngd-&gt;gd_vme_avail /* Available count */\n</code></pre> <p>Boot initialization: - BSP gets <code>MAPENTRYBSP_CACHE</code> (MAXCPU+1) entries - Each AP gets <code>MAPENTRYAP_CACHE</code> (8) entries</p> <p>Reserve/release: - <code>vm_map_entry_reserve(count)</code>: Ensure count available - <code>vm_map_entry_release(count)</code>: Return entries, trim if excessive - <code>kreserve/krelease</code>: Special versions for kernel_map (zalloc recursion avoidance)</p>"},{"location":"sys/vm/vm_map/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_map/#vm_map_backing-chains","title":"vm_map_backing Chains","text":"<p>Unlike traditional BSD where entries point directly to objects, DragonFly interposes <code>vm_map_backing</code>:</p> <ul> <li>Shadow chains via <code>backing_ba</code> linkage</li> <li>Objects unchanged when shadowed</li> <li>Per-entry backing not shared across pmaps</li> <li>Efficient fork without object manipulation</li> </ul>"},{"location":"sys/vm/vm_map/#two-stage-vmspace-termination","title":"Two-Stage vmspace Termination","text":"<p>Separates heavyweight cleanup (stage-1) from final release (stage-2): - Stage-1 on ref\u21920: bulk pmap/object cleanup - Stage-2 on hold\u21920: final map delete, return to cache</p>"},{"location":"sys/vm/vm_map/#freehint-cache","title":"Freehint Cache","text":"<p>O(1) findspace for repeated similar allocations (common in mmap patterns).</p>"},{"location":"sys/vm/vm_map/#entry-partitioning","title":"Entry Partitioning","text":"<p>32MB partitions enable concurrent faults on large anonymous mappings without serializing on the entire entry.</p>"},{"location":"sys/vm/vm_map/#uksmap","title":"UKSMAP","text":"<p>User-kernel shared memory with device callbacks: - <code>/dev/upmap</code> (minor 5): Per-process shared page - <code>/dev/kpmap</code> (minor 6): Kernel-wide shared page - <code>/dev/lpmap</code> (minor 7): Per-LWP shared page</p> <p>Device provides callback for mapping operations.</p>"},{"location":"sys/vm/vm_map/#coalescing-on-insert","title":"Coalescing on Insert","text":"<p>Automatically extends previous entry's object when possible, reducing entry count.</p>"},{"location":"sys/vm/vm_map/#sysctls","title":"Sysctls","text":"Sysctl Default Description <code>vm.randomize_mmap</code> 0 Enable mmap ASLR <code>vm.map_relock_enable</code> 1 Relock optimization for prefault <code>vm.map_partition_enable</code> 1 Enable 32MB entry partitioning <code>vm.map_backing_limit</code> 5 Max shadow chain depth <code>vm.map_backing_shadow_test</code> 1 Test backing object shadows"},{"location":"sys/vm/vm_map/#debugging","title":"Debugging","text":"<p>DDB commands:</p> Command Description <code>show map &lt;addr&gt;</code> Print map entries with protection and backing <code>show procvm</code> Print current process vmspace"},{"location":"sys/vm/vm_map/#see-also","title":"See Also","text":"<ul> <li>VM Subsystem Overview - Architecture overview</li> <li>Physical Pages - Page allocation and queues</li> <li>VM Objects - Object lifecycle and management</li> </ul>"},{"location":"sys/vm/vm_mmap/","title":"Memory Mapping and Pagers","text":"<p>The memory mapping subsystem provides the mmap() interface for applications to map files and anonymous memory into their address space. The vnode pager handles file-backed I/O for memory-mapped files.</p> <p>Source files: <code>sys/vm/vm_mmap.c</code> (~1,530 lines), <code>sys/vm/vnode_pager.c</code> (~832 lines)</p>"},{"location":"sys/vm/vm_mmap/#common-use-cases","title":"Common Use Cases","text":"What You Want Syscall Flags What Happens Allocate heap memory <code>mmap(NULL, size, RW, MAP_ANON\\|MAP_PRIVATE, -1, 0)</code> Anonymous, private Creates vm_map_entry, pages zero-filled on demand Map a file read-only <code>mmap(NULL, size, R, MAP_SHARED, fd, 0)</code> File-backed, shared Creates entry pointing to vnode's vm_object Shared memory (IPC) <code>shm_open()</code> + <code>mmap(MAP_SHARED)</code> Anonymous, shared Multiple processes share same vm_object Memory-mapped I/O <code>mmap(/dev/mem or device)</code> Device-backed Uses dev_pager, may be uncached Copy-on-write fork (internal) Private after fork Parent/child share until write triggers COW"},{"location":"sys/vm/vm_mmap/#syscall-to-vm-layer-flow","title":"Syscall to VM Layer Flow","text":"<pre><code>flowchart TB\n    subgraph APP[\"APPLICATION\"]\n        MMAP[\"mmap(addr, len, prot, flags, fd, offset)\"]\n    end\n\n    subgraph KERNEL[\"KERNEL\"]\n        subgraph SYSMMAP[\"sys_mmap() \u2192 kern_mmap()\"]\n            K1[\"1. Validate parameters (alignment, flags combinations)\"]\n            K2[\"2. If file: get vnode, check permissions\"]\n            K3[\"3. If anonymous: object = NULL (deferred) or new OBJT_DEFAULT\"]\n        end\n\n        subgraph VMMMAP[\"vm_mmap() \u2192 vm_map_find()\"]\n            V1[\"1. Find free address range (or use MAP_FIXED address)\"]\n            V2[\"2. Create vm_map_entry with backing info\"]\n            V3[\"3. For files: entry points to vnode's shared vm_object\"]\n            V4[\"4. For anon: entry has NULL object (allocated on first fault)\"]\n        end\n\n        RET[\"Returns address to user\"]\n\n        subgraph FAULT[\"vm_fault()\"]\n            F1[\"For files: vnode_pager_getpage() \u2192 VOP_READ()\"]\n            F2[\"For anon: zero-fill page, create OBJT_DEFAULT object if needed\"]\n        end\n    end\n\n    MMAP --&gt; SYSMMAP\n    SYSMMAP --&gt; VMMMAP\n    VMMMAP --&gt; RET\n    RET --&gt;|\"later: first access\"| FAULT\n</code></pre>"},{"location":"sys/vm/vm_mmap/#overview","title":"Overview","text":"<p>Memory mapping connects user address space to backing store:</p> <pre><code>flowchart LR\n    subgraph USER[\"User Address Space\"]\n        MA[\"mmap()MAP_ANON\"]\n        MF[\"mmap()file fd\"]\n        MD[\"mmap()device\"]\n    end\n\n    subgraph VM[\"VM Layer\"]\n        VA[\"vm_map_findvm_object\"]\n        VF[\"vm_map_findvm_object\"]\n        VD[\"vm_map_findvm_object\"]\n    end\n\n    subgraph BACKING[\"Backing Store\"]\n        BA[\"Anonymous(swap)\"]\n        BF[\"vnode_pager(file)\"]\n        BD[\"dev_pager(device)\"]\n    end\n\n    MA --&gt; VA --&gt; BA\n    MF --&gt; VF --&gt; BF\n    MD --&gt; VD --&gt; BD\n</code></pre>"},{"location":"sys/vm/vm_mmap/#vnode-pager-vnode_pagerc","title":"Vnode Pager (vnode_pager.c)","text":"<p>The vnode pager provides VM object backing for regular files, allowing memory-mapped file I/O.</p>"},{"location":"sys/vm/vm_mmap/#pager-operations","title":"Pager Operations","text":"<pre><code>struct pagerops vnodepagerops = {\n    .pgo_dealloc  = vnode_pager_dealloc,\n    .pgo_getpage  = vnode_pager_getpage,\n    .pgo_putpages = vnode_pager_putpages,\n    .pgo_haspage  = vnode_pager_haspage\n};\n</code></pre> Operation Description <code>dealloc</code> Clean up when object destroyed <code>getpage</code> Page in from file <code>putpages</code> Page out to file <code>haspage</code> Check if file has backing for page"},{"location":"sys/vm/vm_mmap/#object-allocation","title":"Object Allocation","text":"<p><code>vnode_pager_alloc(handle, length, prot, offset, blksize, boff)</code></p> <p>Creates or references a VM object for a vnode:</p> <ol> <li>Acquire vnode token for serialization</li> <li>If object exists: reference it, validate size</li> <li>If no object: create new <code>OBJT_VNODE</code> object</li> <li>Set <code>vp-&gt;v_object</code>, <code>vp-&gt;v_filesize</code></li> <li>If mount has <code>MNTK_NOMSYNC</code>: set <code>OBJ_NOMSYNC</code></li> <li>Take vnode reference</li> </ol> <p>Object sizing: <pre><code>/* Round up to next block, then to page boundary */\nif (boff &lt; 0)\n    boff = (int)(length % blksize);\nif (boff)\n    loffset = length + (blksize - boff);\nelse\n    loffset = length;\nlsize = OFF_TO_IDX(round_page64(loffset));\n</code></pre></p> <p>The object size includes any partial buffer cache block straddling EOF.</p> <p><code>vnode_pager_reference(vp)</code></p> <p>Adds a reference to an existing vnode's VM object without creating a new one. Returns NULL if no object exists.</p>"},{"location":"sys/vm/vm_mmap/#page-in-vnode_pager_getpage","title":"Page-In (vnode_pager_getpage)","text":"<p><code>vnode_pager_getpage(object, pindex, mpp, seqaccess)</code></p> <p>Wrapper that calls <code>VOP_GETPAGES()</code> on the vnode.</p> <p><code>vnode_pager_generic_getpages(vp, mpp, bytecount, reqpage, seqaccess)</code></p> <p>Generic implementation for filesystems that don't implement <code>VOP_GETPAGES</code>:</p> <ol> <li>Validate vnode mount state</li> <li>Discard pages past file EOF</li> <li>For block/char devices: round up to sector size</li> <li>Release page busy state temporarily (deadlock avoidance)</li> <li>Issue <code>VOP_READ()</code> with <code>IO_VMIO</code> flag</li> <li>Re-acquire page busy state</li> <li>Handle results per page:</li> <li>Non-requested pages: activate if referenced, else deactivate</li> <li>Requested page: validate, zero-fill partial pages</li> </ol> <p>I/O flags: <pre><code>ioflags = IO_VMIO;\nif (seqaccess)\n    ioflags |= IO_SEQMAX &lt;&lt; IO_SEQSHIFT;\n</code></pre></p>"},{"location":"sys/vm/vm_mmap/#page-out-vnode_pager_putpages","title":"Page-Out (vnode_pager_putpages)","text":"<p><code>vnode_pager_putpages(object, m, count, flags, rtvals)</code></p> <p>Wrapper that calls <code>VOP_PUTPAGES()</code> on the vnode.</p> <p>Low memory handling: <pre><code>if ((vmstats.v_free_count + vmstats.v_cache_count) &lt;\n    vmstats.v_pageout_free_min) {\n    flags |= OBJPC_SYNC;  /* Force synchronous */\n}\n</code></pre></p> <p><code>vnode_pager_generic_putpages(vp, m, bytecount, flags, rtvals)</code></p> <p>Generic implementation:</p> <ol> <li>Truncate write to file EOF</li> <li>Set I/O flags based on <code>OBJPC_*</code> flags</li> <li>Issue <code>VOP_WRITE()</code> with <code>IO_VMIO</code></li> <li>Mark pages clean on success</li> </ol> <p>I/O clustering: <pre><code>ioflags = IO_VMIO;\nif (flags &amp; (OBJPC_SYNC | OBJPC_INVAL))\n    ioflags |= IO_SYNC;\nelse if ((flags &amp; OBJPC_CLUSTER_OK) == 0)\n    ioflags |= IO_ASYNC;\nioflags |= IO_SEQMAX &lt;&lt; IO_SEQSHIFT;\n</code></pre></p>"},{"location":"sys/vm/vm_mmap/#file-size-changes","title":"File Size Changes","text":"<p><code>vnode_pager_setsize(vp, nsize)</code></p> <p>Called when file size changes (truncate, extend):</p> <ol> <li>Acquire object hold</li> <li>If shrinking:</li> <li>Update <code>object-&gt;size</code> and <code>vp-&gt;v_filesize</code></li> <li>Remove pages beyond new EOF via <code>vm_object_page_remove()</code></li> <li>Zero partial page at new EOF</li> <li>Clear dirty bits for truncated portion</li> <li>If extending:</li> <li>Update <code>vp-&gt;v_filesize</code></li> </ol> <p>Partial page handling on truncate: <pre><code>if (nsize &amp; PAGE_MASK) {\n    m = vm_page_lookup_busy_wait(object, OFF_TO_IDX(nsize), TRUE, \"vsetsz\");\n    if (m &amp;&amp; m-&gt;valid) {\n        /* Zero trailing bytes */\n        bzero((caddr_t)kva + base, PAGE_SIZE - base);\n        /* Unmap to sync all CPUs */\n        vm_page_protect(m, VM_PROT_NONE);\n        /* Clear partial dirty bits */\n        vm_page_clear_dirty_beg_nonincl(m, base, size);\n    }\n}\n</code></pre></p>"},{"location":"sys/vm/vm_mmap/#vnode-locking-helper","title":"Vnode Locking Helper","text":"<p><code>vnode_pager_lock(ba)</code></p> <p>Walks backing chain and locks the bottom-most vnode:</p> <ol> <li>Find deepest backing_ba in chain</li> <li>Get object from that backing</li> <li>If object is <code>OBJT_VNODE</code> and not dead:</li> <li>Call <code>vget(vp, LK_SHARED | LK_RETRY | LK_CANRECURSE)</code></li> <li>Retry on failure with 1-second sleep</li> </ol> <p>Returns locked vnode or NULL.</p>"},{"location":"sys/vm/vm_mmap/#memory-mapping-vm_mmapc","title":"Memory Mapping (vm_mmap.c)","text":""},{"location":"sys/vm/vm_mmap/#system-calls-overview","title":"System Calls Overview","text":"Syscall Function Description <code>mmap</code> <code>sys_mmap()</code> Create memory mapping <code>munmap</code> <code>sys_munmap()</code> Remove mapping <code>mprotect</code> <code>sys_mprotect()</code> Change protection <code>msync</code> <code>sys_msync()</code> Synchronize to backing store <code>madvise</code> <code>sys_madvise()</code> Advise kernel about usage <code>mlock</code> <code>sys_mlock()</code> Wire pages in memory <code>munlock</code> <code>sys_munlock()</code> Unwire pages <code>mlockall</code> <code>sys_mlockall()</code> Wire entire address space <code>munlockall</code> <code>sys_munlockall()</code> Unwire entire address space <code>mincore</code> <code>sys_mincore()</code> Query page residency <code>minherit</code> <code>sys_minherit()</code> Set inheritance"},{"location":"sys/vm/vm_mmap/#mmap-implementation","title":"mmap Implementation","text":"<p><code>sys_mmap(sysmsg, uap)</code></p> <p>Entry point for mmap() system call:</p> <ol> <li>Handle <code>MAP_STACK</code> \u2192 convert to <code>MAP_ANON</code> (stack auto-grow disabled for userland)</li> <li>Call <code>kern_mmap()</code></li> </ol> <p><code>kern_mmap(vms, uaddr, ulen, uprot, uflags, fd, upos, res)</code></p> <p>Main mmap implementation:</p> <p>Validation: <pre><code>if ((flags &amp; MAP_ANON) &amp;&amp; (fd != -1 || pos != 0))\n    return (EINVAL);\nif (size == 0)\n    return (EINVAL);\nif (flags &amp; MAP_STACK) {\n    if (fd != -1)\n        return (EINVAL);\n    if ((prot &amp; (PROT_READ|PROT_WRITE)) != (PROT_READ|PROT_WRITE))\n        return (EINVAL);\n    flags |= MAP_ANON;\n}\n</code></pre></p> <p>Address alignment: <pre><code>pageoff = (pos &amp; PAGE_MASK);\npos -= pageoff;\nsize += pageoff;\nsize = round_page(size);\n</code></pre></p> <p>File mapping setup:</p> <p>For <code>fd != -1</code>: 1. Get file pointer via <code>holdfp()</code> 2. Validate file type is <code>DTYPE_VNODE</code> 3. Handle <code>FPOSIXSHM</code> \u2192 add <code>MAP_NOSYNC</code> 4. Check vnode type (VREG, VCHR allowed) 5. Validate protections against file open mode 6. Handle <code>/dev/zero</code> as anonymous</p> <p>Protection calculation: <pre><code>maxprot = VM_PROT_EXECUTE;\nif (fp-&gt;f_flag &amp; FREAD)\n    maxprot |= VM_PROT_READ;\nif ((flags &amp; MAP_SHARED) &amp;&amp; (fp-&gt;f_flag &amp; FWRITE))\n    maxprot |= VM_PROT_WRITE;  /* Check IMMUTABLE/APPEND */\n</code></pre></p> <p>Entry limit: <pre><code>if (max_proc_mmap &amp;&amp; vms-&gt;vm_map.nentries &gt;= max_proc_mmap)\n    return (ENOMEM);\n</code></pre></p> <p><code>vm_mmap(map, addr, size, prot, maxprot, flags, handle, foff, fp)</code></p> <p>Internal mmap implementation:</p> <ol> <li>Check RLIMIT_VMEM</li> <li>Validate page-aligned file offset</li> <li>Calculate alignment:</li> <li><code>MAP_SIZEALIGN</code>: align to size (must be power of 2)</li> <li>Large mappings (\u2265 SEG_SIZE or &gt; 16\u00d7SEG_SIZE): SEG_SIZE align</li> <li>Otherwise: PAGE_SIZE align</li> </ol> <p>Object lookup: <pre><code>if (flags &amp; MAP_ANON) {\n    if (handle)\n        object = default_pager_alloc(handle, objsize, prot, foff);\n    else\n        object = NULL;  /* Deferred allocation */\n} else {\n    vp = (struct vnode *)handle;\n    if (vp-&gt;v_type == VCHR &amp;&amp; vp-&gt;v_rdev-&gt;si_ops-&gt;d_uksmap) {\n        /* UKSMAP device mapping */\n        uksmap = vp-&gt;v_rdev-&gt;si_ops-&gt;d_uksmap;\n        object = NULL;\n    } else if (vp-&gt;v_type == VCHR) {\n        /* Device mapping */\n        object = dev_pager_alloc(...);\n    } else {\n        /* Regular file */\n        object = vnode_pager_reference(vp);\n    }\n}\n</code></pre></p> <p>Map entry creation: <pre><code>if (uksmap) {\n    rv = vm_map_find(map, uksmap, vp-&gt;v_rdev, foff, addr, size,\n                     align, fitit, VM_MAPTYPE_UKSMAP, ...);\n} else if (flags &amp; MAP_STACK) {\n    rv = vm_map_stack(map, addr, size, flags, prot, maxprot, docow);\n} else {\n    rv = vm_map_find(map, object, NULL, foff, addr, size,\n                     align, fitit, VM_MAPTYPE_NORMAL, ...);\n}\n</code></pre></p> <p>Post-processing: - Set <code>VM_INHERIT_SHARE</code> for <code>MAP_SHARED</code>/<code>MAP_INHERIT</code> - Wire if <code>MAP_WIREFUTURE</code> is set - Update vnode access time</p>"},{"location":"sys/vm/vm_mmap/#munmap-implementation","title":"munmap Implementation","text":"<p><code>sys_munmap(sysmsg, uap)</code></p> <ol> <li>Page-align address and size</li> <li>Validate address range within user space</li> <li>Check entire range is allocated via <code>vm_map_check_protection()</code></li> <li>Call <code>vm_map_remove()</code></li> </ol>"},{"location":"sys/vm/vm_mmap/#mprotect-implementation","title":"mprotect Implementation","text":"<p><code>sys_mprotect(sysmsg, uap)</code></p> <ol> <li>Page-align address and size</li> <li>Call <code>vm_map_protect()</code> with new protection</li> <li>Return appropriate errno for kernel result</li> </ol>"},{"location":"sys/vm/vm_mmap/#msync-implementation","title":"msync Implementation","text":"<p><code>sys_msync(sysmsg, uap)</code></p> <ol> <li>Page-align address and size</li> <li>Validate flags (MS_ASYNC|MS_INVALIDATE mutually exclusive)</li> <li>If size == 0: find containing map entry, use its range</li> <li>Call <code>vm_map_clean()</code> with sync/invalidate flags</li> </ol>"},{"location":"sys/vm/vm_mmap/#madvisemcontrol-implementation","title":"madvise/mcontrol Implementation","text":"<p><code>sys_madvise(sysmsg, uap)</code></p> <p>Calls <code>vm_map_madvise()</code> with behavior:</p> Behavior Action <code>MADV_NORMAL</code> Reset to default <code>MADV_SEQUENTIAL</code> Expect sequential access <code>MADV_RANDOM</code> Expect random access <code>MADV_WILLNEED</code> Prefault pages <code>MADV_DONTNEED</code> May discard pages <code>MADV_FREE</code> May free pages <code>MADV_NOSYNC</code> Don't sync to disk <code>MADV_AUTOSYNC</code> Resume sync <code>MADV_NOCORE</code> Exclude from core dump <code>MADV_CORE</code> Include in core dump <code>MADV_INVAL</code> Invalidate pages <p><code>sys_mcontrol(sysmsg, uap)</code></p> <p>Extended madvise with value parameter.</p>"},{"location":"sys/vm/vm_mmap/#mincore-implementation","title":"mincore Implementation","text":"<p><code>sys_mincore(sysmsg, uap)</code></p> <p>Reports page residency:</p> <ol> <li>Lock map for reading</li> <li>For each page in range:</li> <li>Check pmap first (<code>pmap_mincore()</code>)</li> <li>If not in pmap, check VM object for resident page</li> <li>Build result flags: <code>MINCORE_INCORE</code>, <code>MINCORE_MODIFIED_OTHER</code>, <code>MINCORE_REFERENCED_OTHER</code></li> <li>Write results to user byte vector</li> <li>Restart if map changed during scan</li> </ol>"},{"location":"sys/vm/vm_mmap/#mlockmunlock-implementation","title":"mlock/munlock Implementation","text":"<p><code>sys_mlock(sysmsg, uap)</code></p> <ol> <li>Check against <code>vm_page_max_wired</code> limit</li> <li>Check privilege or <code>RLIMIT_MEMLOCK</code></li> <li>Call <code>vm_map_user_wiring()</code> with <code>FALSE</code> (wire)</li> </ol> <p><code>sys_munlock(sysmsg, uap)</code></p> <ol> <li>Check privilege</li> <li>Call <code>vm_map_user_wiring()</code> with <code>TRUE</code> (unwire)</li> </ol>"},{"location":"sys/vm/vm_mmap/#mlockallmunlockall-implementation","title":"mlockall/munlockall Implementation","text":"<p><code>sys_mlockall(sysmsg, uap)</code></p> <ol> <li>Check privilege</li> <li>If <code>MCL_CURRENT</code>: wire all existing entries</li> <li>If <code>MCL_FUTURE</code>: set <code>MAP_WIREFUTURE</code> flag</li> </ol> <p><code>sys_munlockall(sysmsg, uap)</code></p> <ol> <li>Clear <code>MAP_WIREFUTURE</code></li> <li>Unwire all user-wired entries</li> <li>Handle in-transition entries with retry</li> </ol>"},{"location":"sys/vm/vm_mmap/#minherit-implementation","title":"minherit Implementation","text":"<p><code>sys_minherit(sysmsg, uap)</code></p> <p>Sets fork inheritance via <code>vm_map_inherit()</code>:</p> Inheritance Behavior <code>VM_INHERIT_NONE</code> Not inherited <code>VM_INHERIT_COPY</code> COW copy (default) <code>VM_INHERIT_SHARE</code> Share with child"},{"location":"sys/vm/vm_mmap/#key-sysctls","title":"Key Sysctls","text":"Sysctl Default Description <code>vm.max_proc_mmap</code> 1000000 Max map entries per process <code>vm.vkernel_enable</code> 0 Enable vkernel features"},{"location":"sys/vm/vm_mmap/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_mmap/#uksmap-device-mappings","title":"UKSMAP Device Mappings","text":"<p>Devices can provide direct user-kernel shared memory via <code>d_uksmap</code>:</p> <pre><code>if (vp-&gt;v_type == VCHR &amp;&amp; vp-&gt;v_rdev-&gt;si_ops-&gt;d_uksmap) {\n    uksmap = vp-&gt;v_rdev-&gt;si_ops-&gt;d_uksmap;\n    object = NULL;  /* No VM object */\n    flags |= MAP_SHARED;\n}\n</code></pre> <p>Used for <code>/dev/upmap</code>, <code>/dev/kpmap</code>, <code>/dev/lpmap</code>.</p>"},{"location":"sys/vm/vm_mmap/#map_stack-handling","title":"MAP_STACK Handling","text":"<p>User MAP_STACK is converted to MAP_ANON:</p> <pre><code>if (flags &amp; MAP_STACK) {\n    flags &amp;= ~MAP_STACK;\n    flags |= MAP_ANON;\n}\n</code></pre> <p>Only the exec-created user stack uses true MAP_STACK internally.</p>"},{"location":"sys/vm/vm_mmap/#alignment-optimization","title":"Alignment Optimization","text":"<p>Large mappings are aligned to SEG_SIZE for MMU optimization:</p> <pre><code>if ((flags &amp; MAP_FIXED) == 0 &amp;&amp;\n    ((size &amp; SEG_MASK) == 0 || size &gt; SEG_SIZE * 16)) {\n    align = SEG_SIZE;\n}\n</code></pre>"},{"location":"sys/vm/vm_mmap/#address-hint","title":"Address Hint","text":"<p>Non-fixed mappings get ASLR-randomized hint:</p> <pre><code>addr = vm_map_hint(p, addr, prot, flags);\n</code></pre>"},{"location":"sys/vm/vm_mmap/#posix-shared-memory","title":"POSIX Shared Memory","text":"<p>Files opened with <code>shm_open()</code> set <code>FPOSIXSHM</code>:</p> <pre><code>if (fp-&gt;f_flag &amp; FPOSIXSHM)\n    flags |= MAP_NOSYNC;\n</code></pre>"},{"location":"sys/vm/vm_mmap/#function-reference","title":"Function Reference","text":""},{"location":"sys/vm/vm_mmap/#vnode-pager","title":"Vnode Pager","text":"Function Description <code>vnode_pager_alloc()</code> Create/reference vnode object <code>vnode_pager_reference()</code> Reference existing object <code>vnode_pager_dealloc()</code> Destroy vnode object <code>vnode_pager_haspage()</code> Check file backing <code>vnode_pager_getpage()</code> Page in from file <code>vnode_pager_putpages()</code> Page out to file <code>vnode_pager_generic_getpages()</code> Generic page-in implementation <code>vnode_pager_generic_putpages()</code> Generic page-out implementation <code>vnode_pager_setsize()</code> Handle file size change <code>vnode_pager_freepage()</code> Release page from getpages <code>vnode_pager_lock()</code> Lock backing vnode"},{"location":"sys/vm/vm_mmap/#memory-mapping","title":"Memory Mapping","text":"Function Description <code>kern_mmap()</code> Internal mmap implementation <code>vm_mmap()</code> Core mapping function <code>sys_mmap()</code> mmap syscall handler <code>sys_munmap()</code> munmap syscall handler <code>sys_mprotect()</code> mprotect syscall handler <code>sys_msync()</code> msync syscall handler <code>sys_madvise()</code> madvise syscall handler <code>sys_mcontrol()</code> mcontrol syscall handler <code>sys_mincore()</code> mincore syscall handler <code>sys_mlock()</code> mlock syscall handler <code>sys_munlock()</code> munlock syscall handler <code>sys_mlockall()</code> mlockall syscall handler <code>sys_munlockall()</code> munlockall syscall handler <code>sys_minherit()</code> minherit syscall handler <code>vm_mmap_to_errno()</code> Convert VM return to errno"},{"location":"sys/vm/vm_mmap/#see-also","title":"See Also","text":"<ul> <li>Address Space - Map entry management</li> <li>VM Objects - Object lifecycle</li> <li>Page Faults - Fault handling for mapped pages</li> <li>Pageout and Swap - Anonymous page backing</li> </ul>"},{"location":"sys/vm/vm_object/","title":"VM Objects","text":"<p>VM objects are the fundamental abstraction for managing virtual memory contents in DragonFly BSD. Each object represents a contiguous range of virtual memory that can be backed by files, swap space, devices, or anonymous memory.</p> <p>Source file: <code>sys/vm/vm_object.c</code> (~2,034 lines)</p>"},{"location":"sys/vm/vm_object/#where-objects-fit-in-the-vm-hierarchy","title":"Where Objects Fit in the VM Hierarchy","text":"<pre><code>flowchart LR\n    subgraph User[\"USER PROCESS\"]\n        mmap[\"ptr = mmap(file)\"]\n    end\n\n    subgraph VM[\"VM SUBSYSTEM\"]\n        entry[\"vm_map_entry\"]\n        backing[\"vm_map_backing\"]\n        object[\"vm_object\"]\n        pager[\"vnode_pager\"]\n\n        entry --&gt; backing\n        backing --&gt; object\n        pager --&gt; object\n\n        subgraph pages[\"RB tree by page index\"]\n            page1[\"vm_page\"]\n            page2[\"vm_page\"]\n            page3[\"vm_page\"]\n        end\n\n        object --&gt; pages\n    end\n\n    mmap --&gt; entry\n</code></pre> <p>Key insight: Objects are the container for pages. They don't manage address spaces (that's <code>vm_map</code>) or physical allocation (that's <code>vm_page</code>). They manage:</p> <ul> <li>Which pages belong together logically</li> <li>How to populate missing pages (via pager)</li> <li>Sharing between processes (file-backed)</li> </ul>"},{"location":"sys/vm/vm_object/#common-scenarios","title":"Common Scenarios","text":"Scenario Object Type What Happens <code>malloc(large)</code> OBJT_DEFAULT\u2192OBJT_SWAP Anonymous object created, pages added on fault, swapped under pressure <code>mmap(file)</code> OBJT_VNODE Object tied to vnode, pages loaded from file on fault <code>fork()</code> Parent's objects Child gets new vm_map_backing pointing to same objects (COW) GPU memory OBJT_MGTDEVICE Device manages pages, object tracks mappings <code>shm_open()</code> OBJT_SWAP Swap-backed object shared between processes"},{"location":"sys/vm/vm_object/#overview","title":"Overview","text":"<p>A VM object maintains:</p> <ul> <li>A red-black tree of resident physical pages (<code>rb_memq</code>)</li> <li>Reference count for lifetime management</li> <li>Type-specific backing store (file, swap, device)</li> <li>A list of <code>vm_map_backing</code> structures that reference this object</li> </ul> <p>Objects are the bridge between address space mappings (<code>vm_map_entry</code>) and physical pages (<code>vm_page</code>). A single page exists within exactly one object at any given time.</p>"},{"location":"sys/vm/vm_object/#object-types","title":"Object Types","text":"<pre><code>enum obj_type {\n    OBJT_DEFAULT,    /* Anonymous memory, initially no backing */\n    OBJT_SWAP,       /* Backed by swap blocks */\n    OBJT_VNODE,      /* Backed by file (vnode) */\n    OBJT_DEVICE,     /* Device-backed pages */\n    OBJT_MGTDEVICE,  /* Managed device pager */\n    OBJT_PHYS,       /* Physical pages (no paging) */\n    OBJT_DEAD,       /* Being destroyed */\n    OBJT_MARKER,     /* List iteration marker */\n};\n</code></pre>"},{"location":"sys/vm/vm_object/#type-characteristics","title":"Type Characteristics","text":"Type Backing Store Pages in rb_memq Swappable OBJT_DEFAULT None initially Yes Converts to SWAP OBJT_SWAP Swap blocks Yes Yes OBJT_VNODE File Yes Via file I/O OBJT_DEVICE Device memory No (typically) No OBJT_MGTDEVICE Managed device Via backing_list No"},{"location":"sys/vm/vm_object/#data-structures","title":"Data Structures","text":""},{"location":"sys/vm/vm_object/#struct-vm_object","title":"struct vm_object","text":"<pre><code>struct vm_object {\n    struct lwkt_token token;           /* Soft-lock for object */\n    struct lock backing_lk;            /* Lock for backing_list only */\n    struct vm_page_rb_tree rb_memq;    /* Resident pages (RB tree) */\n    TAILQ_HEAD(,vm_map_backing) backing_list;  /* Who references us */\n\n    vm_pindex_t size;                  /* Size in pages */\n    int ref_count;                     /* Reference count */\n    int hold_count;                    /* Destruction prevention */\n    u_int paging_in_progress;          /* Active I/O operations */\n\n    objtype_t type;                    /* OBJT_* type */\n    u_short flags;                     /* OBJ_* flags */\n    vm_memattr_t memattr;              /* Memory attributes (PAT) */\n    u_short pg_color;                  /* Base page color */\n\n    void *handle;                      /* Type-specific (vnode, dev) */\n    long resident_page_count;          /* Cached page count */\n    int generation;                    /* Modification counter */\n\n    /* Swap support */\n    struct swblock_rb_tree swblock_root;\n    long swblock_count;\n};\n</code></pre>"},{"location":"sys/vm/vm_object/#object-flags","title":"Object Flags","text":"Flag Description <code>OBJ_ACTIVE</code> Object is active <code>OBJ_DEAD</code> Being destroyed <code>OBJ_NOSPLIT</code> Don't split this object <code>OBJ_ONEMAPPING</code> Each page maps to at most one vm_map_entry <code>OBJ_WRITEABLE</code> Has been made writeable <code>OBJ_MIGHTBEDIRTY</code> May have dirty pages <code>OBJ_CLEANING</code> Page cleaning in progress <code>OBJ_DEADWNT</code> Waiter for object death"},{"location":"sys/vm/vm_object/#global-hash-table","title":"Global Hash Table","text":"<p>Objects are tracked in a 256-bucket hash table for global enumeration:</p> <pre><code>struct vm_object_hash vm_object_hash[VMOBJ_HSIZE];  /* VMOBJ_HSIZE = 256 */\n</code></pre> <p>Each bucket contains a TAILQ list protected by an LWKT token. The hash function uses two large primes for distribution.</p>"},{"location":"sys/vm/vm_object/#locking-model","title":"Locking Model","text":"<p>DragonFly uses LWKT tokens (soft-locks) for VM objects, allowing blocking while held:</p> Function Description <code>vm_object_hold(obj)</code> Acquire hold + exclusive token <code>vm_object_hold_shared(obj)</code> Acquire hold + shared token <code>vm_object_hold_try(obj)</code> Non-blocking hold attempt <code>vm_object_drop(obj)</code> Release hold + token"},{"location":"sys/vm/vm_object/#hold-count-vs-reference-count","title":"Hold Count vs Reference Count","text":"<ul> <li>ref_count: Logical references (from mappings, etc.)</li> <li>hold_count: Prevents object from being freed while working with it</li> </ul> <p>The hold/drop pattern is critical:</p> <pre><code>/* Must increment hold_count BEFORE blocking on token */\nrefcount_acquire(&amp;obj-&gt;hold_count);  /* Makes object stable */\nvm_object_lock(obj);                 /* May block */\n/* ... work with object ... */\nvm_object_unlock(obj);\nif (refcount_release(&amp;obj-&gt;hold_count)) {\n    if (obj-&gt;ref_count == 0 &amp;&amp; (obj-&gt;flags &amp; OBJ_DEAD))\n        kfree_obj(obj, M_VM_OBJECT);  /* Final free */\n}\n</code></pre>"},{"location":"sys/vm/vm_object/#object-lifecycle","title":"Object Lifecycle","text":""},{"location":"sys/vm/vm_object/#allocation","title":"Allocation","text":"<pre><code>/* Returns unheld object */\nvm_object_t vm_object_allocate(objtype_t type, vm_pindex_t size);\n\n/* Returns held object for atomic initialization */\nvm_object_t vm_object_allocate_hold(objtype_t type, vm_pindex_t size);\n</code></pre> <p>Initialization (<code>_vm_object_allocate()</code>) performs:</p> <ol> <li>Initialize page RB tree and token</li> <li>Initialize <code>backing_list</code> and <code>backing_lk</code></li> <li>Set type, size, ref_count=1</li> <li>For DEFAULT/SWAP: set <code>OBJ_ONEMAPPING</code></li> <li>Assign random page color via <code>vm_quickcolor()</code></li> <li>Initialize swap block tree</li> <li>Insert into global hash table</li> </ol>"},{"location":"sys/vm/vm_object/#reference-counting","title":"Reference Counting","text":"<p>Adding references:</p> <pre><code>/* Must hold object token */\nvoid vm_object_reference_locked(vm_object_t object);\n\n/* Safe without token when object is deterministically referenced */\nvoid vm_object_reference_quick(vm_object_t object);\n</code></pre> <p>For <code>OBJT_VNODE</code> objects, these also call <code>vref()</code> on the vnode.</p> <p>Releasing references:</p> <pre><code>void vm_object_deallocate(vm_object_t object);\n</code></pre> <p>The deallocation path optimizes for the common case:</p> <ul> <li>Fast path (ref_count &gt; 3): Atomic decrement without locking</li> <li>Slow path (ref_count &lt;= 3): Hold object, handle termination</li> </ul> <p>This avoids exclusive lock contention on highly-shared binaries during exec/exit.</p>"},{"location":"sys/vm/vm_object/#termination","title":"Termination","text":"<p>When ref_count reaches zero, <code>vm_object_terminate()</code> is called:</p> <ol> <li>Set <code>OBJ_DEAD</code> flag</li> <li>Wait for <code>paging_in_progress</code> to reach 0</li> <li>For <code>OBJT_VNODE</code>:</li> <li><code>vinvalbuf()</code> - flush buffers</li> <li><code>vm_object_page_clean()</code> - write dirty pages</li> <li><code>vinvalbuf()</code> again (TMPFS special case)</li> <li>Free all resident pages via callback</li> <li><code>vm_pager_deallocate()</code> - notify pager</li> <li>Remove from hash table</li> <li>Object freed when hold_count reaches 0</li> </ol>"},{"location":"sys/vm/vm_object/#page-management","title":"Page Management","text":""},{"location":"sys/vm/vm_object/#page-cleaning","title":"Page Cleaning","text":"<p><code>vm_object_page_clean()</code> writes dirty pages to backing store:</p> <pre><code>void vm_object_page_clean(vm_object_t object, \n                          vm_pindex_t start, \n                          vm_pindex_t end,\n                          int flags);\n</code></pre> <p>Flags:</p> Flag Description <code>OBJPC_SYNC</code> Synchronous I/O <code>OBJPC_INVAL</code> Invalidate after cleaning <code>OBJPC_NOSYNC</code> Skip PG_NOSYNC pages <code>OBJPC_CLUSTER_OK</code> Allow I/O clustering <p>Two-pass algorithm:</p> <ol> <li>Pass 1: Mark all pages read-only (<code>vm_page_protect(VM_PROT_READ)</code>)</li> <li>Sets <code>PG_CLEANCHK</code> flag on each page</li> <li> <p>If entire object cleaned: clears <code>OBJ_WRITEABLE|OBJ_MIGHTBEDIRTY</code></p> </li> <li> <p>Pass 2: Write dirty pages</p> </li> <li>Skips pages without <code>PG_CLEANCHK</code> (inserted after pass 1)</li> <li>Clusters adjacent dirty pages for efficient I/O</li> <li>Repeats if object's generation changes</li> </ol>"},{"location":"sys/vm/vm_object/#page-removal","title":"Page Removal","text":"<pre><code>void vm_object_page_remove(vm_object_t object,\n                           vm_pindex_t start,\n                           vm_pindex_t end,\n                           boolean_t clean_only);\n</code></pre> <p>This function:</p> <ol> <li>Scans <code>backing_list</code> to remove pmap mappings (important for MGTDEVICE)</li> <li>Scans <code>rb_memq</code> to free pages</li> <li>Frees related swap blocks</li> </ol> <p>The <code>clean_only</code> flag preserves dirty pages.</p>"},{"location":"sys/vm/vm_object/#madvise-support","title":"madvise Support","text":"<pre><code>void vm_object_madvise(vm_object_t object,\n                       vm_pindex_t pindex,\n                       vm_pindex_t count,\n                       int advise);\n</code></pre> Advise Action <code>MADV_WILLNEED</code> Activate pages (move to active queue) <code>MADV_DONTNEED</code> Deactivate pages (candidate for reclaim) <code>MADV_FREE</code> Mark clean + deactivate + free swap <p><code>MADV_FREE</code> is restricted to <code>OBJT_DEFAULT</code>/<code>OBJT_SWAP</code> objects with <code>OBJ_ONEMAPPING</code>.</p>"},{"location":"sys/vm/vm_object/#object-coalescing","title":"Object Coalescing","text":"<pre><code>boolean_t vm_object_coalesce(vm_object_t prev_object,\n                             vm_pindex_t prev_pindex,\n                             vm_size_t prev_size,\n                             vm_size_t next_size);\n</code></pre> <p>Extends an object into adjacent virtual memory:</p> <ul> <li>Only for <code>OBJT_DEFAULT</code>/<code>OBJT_SWAP</code></li> <li>Requires single reference (or extending into new space)</li> <li>Removes any existing pages in the new region</li> <li>Updates <code>object-&gt;size</code></li> </ul>"},{"location":"sys/vm/vm_object/#vnode-object-handling","title":"Vnode Object Handling","text":"<p><code>OBJT_VNODE</code> objects have special handling:</p> <ul> <li>Reference counting: <code>vref()</code>/<code>vrele()</code> called alongside object refs</li> <li>VTEXT flag: Cleared on last reference (executable text)</li> <li>Dirty tracking: <code>VOBJDIRTY</code> flag on vnode for syncer</li> <li>Page cleaning: Double <code>vinvalbuf()</code> for TMPFS compatibility</li> </ul> <p>The <code>vm_object_vndeallocate()</code> function handles the complex 1-&gt;0 transition:</p> <pre><code>/* Atomically handle ref_count with retry loop */\nif (count == 1) {\n    vm_object_upgrade(object);      /* Need exclusive for VTEXT */\n    if (atomic_fcmpset_int(&amp;object-&gt;ref_count, &amp;count, 0)) {\n        vclrflags(vp, VTEXT);\n        break;\n    }\n}\n</code></pre>"},{"location":"sys/vm/vm_object/#dirty-flag-management","title":"Dirty Flag Management","text":"<pre><code>void vm_object_set_writeable_dirty(vm_object_t object);\n</code></pre> <p>Called from the fault path when a page becomes writeable:</p> <ol> <li>Sets <code>OBJ_WRITEABLE | OBJ_MIGHTBEDIRTY</code> on object</li> <li>For <code>OBJT_VNODE</code>: sets <code>VOBJDIRTY</code> on vnode</li> <li>Uses <code>vsetobjdirty()</code> for <code>MNTK_THR_SYNC</code> mounts</li> <li>Uses <code>vsetflags()</code> for traditional mounts</li> </ol> <p>The flags check before atomic operation avoids contention in the fault path.</p>"},{"location":"sys/vm/vm_object/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_object/#lwkt-token-locking","title":"LWKT Token Locking","text":"<p>Unlike traditional BSD mutexes, LWKT tokens allow:</p> <ul> <li>Blocking while held</li> <li>Other threads to \"squeeze in\" work</li> <li>Shared/exclusive modes</li> <li>Token swapping for lock ordering</li> </ul>"},{"location":"sys/vm/vm_object/#backing_list","title":"backing_list","text":"<p>Each object maintains a list of <code>vm_map_backing</code> structures:</p> <pre><code>TAILQ_HEAD(, vm_map_backing) backing_list;\nstruct lock backing_lk;  /* Separate lock for this list */\n</code></pre> <p>This enables:</p> <ul> <li>Efficient pmap removal during page removal</li> <li>Support for <code>OBJT_MGTDEVICE</code> (pages not in rb_memq)</li> <li>Tracking all mappings of an object</li> </ul>"},{"location":"sys/vm/vm_object/#page-coloring","title":"Page Coloring","text":"<p><code>vm_quickcolor()</code> provides semi-random initial page colors:</p> <pre><code>int vm_quickcolor(void) {\n    globaldata_t gd = mycpu;\n    int pg_color = (int)(intptr_t)gd-&gt;gd_curthread &gt;&gt; 10;\n    pg_color += gd-&gt;gd_quick_color;\n    gd-&gt;gd_quick_color += PQ_PRIME2;\n    return pg_color;\n}\n</code></pre> <p>This spreads page allocations across queues for SMP scalability.</p>"},{"location":"sys/vm/vm_object/#debugging","title":"Debugging","text":"<p>DDB commands for object inspection:</p> Command Description <code>show vmochk</code> Verify internal objects are mapped <code>show object &lt;addr&gt;</code> Print object details and pages <code>show vmopag</code> Print page runs for all objects"},{"location":"sys/vm/vm_object/#see-also","title":"See Also","text":"<ul> <li>VM Subsystem Overview - Architecture overview</li> <li>Physical Page Management - Page allocation and queues</li> </ul>"},{"location":"sys/vm/vm_page/","title":"Physical Page Management","text":"<p>This document describes DragonFly BSD's physical page management subsystem, implemented in <code>sys/vm/vm_page.c</code>. The subsystem manages all physical memory pages in the system, including allocation, freeing, queue management, and state transitions.</p> <p>Source file: <code>sys/vm/vm_page.c</code> (~4,200 lines)</p>"},{"location":"sys/vm/vm_page/#when-you-need-this","title":"When You Need This","text":"Scenario Key Functions Section Allocating a page during fault handling <code>vm_page_alloc()</code>, <code>vm_page_grab()</code> Page Allocation Understanding why a page can't be freed Wire count, hold count, busy state Wire/Unwire, Hold, Busy State Implementing a new pager <code>vm_page_set_valid()</code>, <code>vm_page_dirty()</code> Valid/Dirty Bits Debugging memory pressure issues Page queues, vmstats Page Queues, Memory Pressure Writing DMA-capable driver code <code>vm_page_alloc_contig()</code> Contiguous Allocation Understanding pageout victim selection Queue transitions Page State Transitions"},{"location":"sys/vm/vm_page/#overview","title":"Overview","text":"<p>Every physical page in the system is represented by a <code>struct vm_page</code> (128 bytes). These structures are stored in a global array (<code>vm_page_array</code>) and indexed by physical page number. The VM system organizes pages into multiple queues based on their state and uses sophisticated coloring and NUMA-aware algorithms to optimize memory locality.</p>"},{"location":"sys/vm/vm_page/#key-concepts","title":"Key Concepts","text":"<ul> <li>Page coloring: Pages are distributed across 1024 queues per queue type to reduce lock contention and improve cache behavior</li> <li>NUMA awareness: Page allocation considers CPU topology to prefer local memory</li> <li>Busy state: Pages use atomic busy/soft-busy counts instead of traditional locks</li> <li>Per-CPU statistics: Reduces cache-line bouncing by caching vmstats locally</li> </ul>"},{"location":"sys/vm/vm_page/#data-structures","title":"Data Structures","text":""},{"location":"sys/vm/vm_page/#page-array","title":"Page Array","text":"<pre><code>vm_page_t vm_page_array;           // Global array of all vm_page structures\nvm_pindex_t vm_page_array_size;    // Number of entries\nvm_pindex_t first_page;            // First physical page index\n</code></pre> <p>The macro <code>PHYS_TO_VM_PAGE(pa)</code> converts a physical address to its corresponding <code>vm_page</code> pointer.</p>"},{"location":"sys/vm/vm_page/#page-queues","title":"Page Queues","text":"<pre><code>struct vpgqueues vm_page_queues[PQ_COUNT];\n</code></pre> <p>Five queue types, each with 1024 color variants (<code>PQ_L2_SIZE</code>):</p> Queue Type Purpose <code>PQ_FREE</code> Available for allocation <code>PQ_CACHE</code> Clean pages, immediately reusable <code>PQ_INACTIVE</code> Low activity, candidates for reclamation <code>PQ_ACTIVE</code> Recently referenced pages <code>PQ_HOLD</code> Temporarily held (prevents freeing) <p>Each <code>struct vpgqueues</code> contains: - <code>spin</code> - Per-queue spinlock - <code>pl</code> - Page list (TAILQ) - <code>lcnt</code> - Local count - <code>lastq</code> - Heuristic for skipping empty queues</p>"},{"location":"sys/vm/vm_page/#page-hash-table","title":"Page Hash Table","text":"<p>A lockless heuristic cache for fast page lookups:</p> <pre><code>struct vm_page_hash_elm {\n    vm_page_t   m;\n    vm_object_t object;   // Cached for fast comparison\n    vm_pindex_t pindex;   // Cached for fast comparison\n    int         ticks;    // LRU timestamp\n};\n</code></pre> <ul> <li>4-way set associative (<code>VM_PAGE_HASH_SET</code>)</li> <li>Maximum 8 million entries</li> <li>Only caches pages with <code>PG_MAPPEDMULTI</code> flag</li> </ul>"},{"location":"sys/vm/vm_page/#boot-time-initialization","title":"Boot-Time Initialization","text":""},{"location":"sys/vm/vm_page/#vm_page_startup","title":"<code>vm_page_startup()</code>","text":"<p>Called early in boot to initialize the page management subsystem:</p> <ol> <li>Aligns physical memory ranges - Rounds <code>phys_avail[]</code> to page boundaries</li> <li>Initializes page queues - Creates 5120 queue structures (5 types \u00d7 1024 colors)</li> <li>Allocates minidump bitmap - For crash dump support</li> <li>Allocates vm_page_array - One <code>struct vm_page</code> per physical page</li> <li>Initializes page structures - Sets up spinlocks and physical addresses</li> <li>Populates free queues - Adds pages in ascending physical order</li> </ol>"},{"location":"sys/vm/vm_page/#page-color-calculation","title":"Page Color Calculation","text":"<p>During boot, page colors are calculated with CPU locality twisting:</p> <pre><code>m-&gt;pc = (pa &gt;&gt; PAGE_SHIFT);\nm-&gt;pc ^= ((pa &gt;&gt; PAGE_SHIFT) / PQ_L2_SIZE);\nm-&gt;pc ^= ((pa &gt;&gt; PAGE_SHIFT) / (PQ_L2_SIZE * PQ_L2_SIZE));\nm-&gt;pc &amp;= PQ_L2_MASK;\n</code></pre> <p>This distributes pages across queues while maintaining locality.</p>"},{"location":"sys/vm/vm_page/#numa-organization","title":"NUMA Organization","text":"<p><code>vm_numa_organize()</code> reorganizes page colors based on physical socket ID:</p> <pre><code>socket_mod = PQ_L2_SIZE / cpu_topology_phys_ids;\nsocket_value = (physid % cpu_topology_phys_ids) * socket_mod;\n</code></pre> <p><code>vm_numa_organize_finalize()</code> then balances queues to prevent empty queues that would force cross-socket borrowing.</p>"},{"location":"sys/vm/vm_page/#dma-reserve","title":"DMA Reserve","text":"<p>Low physical memory is reserved for DMA operations:</p> <ul> <li><code>vm_low_phys_reserved</code>: Threshold for DMA reserve (default 65536 pages)</li> <li><code>vm_dma_reserved</code>: Tunable amount to keep reserved (default 128MB on 2G+ systems)</li> <li>Pages in this range are marked <code>PG_FICTITIOUS | PG_UNQUEUED</code> and managed by <code>vm_contig_alist</code></li> </ul>"},{"location":"sys/vm/vm_page/#page-allocation","title":"Page Allocation","text":""},{"location":"sys/vm/vm_page/#vm_page_alloc","title":"<code>vm_page_alloc()</code>","text":"<p>The primary page allocation function.</p> <pre><code>vm_page_t vm_page_alloc(vm_object_t object, vm_pindex_t pindex, int page_req);\n</code></pre> <p>Allocation flags:</p> Flag Description <code>VM_ALLOC_NORMAL</code> Can use cache pages <code>VM_ALLOC_QUICK</code> Free queue only, skip cache <code>VM_ALLOC_SYSTEM</code> Can exhaust most of free list <code>VM_ALLOC_INTERRUPT</code> Can exhaust entire free list <code>VM_ALLOC_CPU(n)</code> CPU localization hint <code>VM_ALLOC_ZERO</code> Zero page if allocated <code>VM_ALLOC_NULL_OK</code> Return NULL on collision <p>Algorithm:</p> <ol> <li>Calculate page color via <code>vm_get_pg_color(cpuid, object, pindex)</code></li> <li>Check free count against thresholds</li> <li>Search free queue (and optionally cache queue)</li> <li>If using cache page, free it first then retry</li> <li>Insert into object if provided</li> <li>Return BUSY page</li> </ol>"},{"location":"sys/vm/vm_page/#cpu-localized-color-selection","title":"CPU-Localized Color Selection","text":"<p><code>vm_get_pg_color()</code> calculates colors considering CPU topology:</p> <pre><code>// General format: [phys_id][core_id][cpuid][set-associativity]\nphyscale = PQ_L2_SIZE / cpu_topology_phys_ids;\ngrpscale = physcale / cpu_topology_core_ids;\ncpuscale = grpscale / cpu_topology_ht_ids;\n\npg_color = phys_id * physcale;\npg_color += core_id * grpscale;\npg_color += ht_id * cpuscale;\npg_color += (pindex + object_pg_color) % cpuscale;\n</code></pre>"},{"location":"sys/vm/vm_page/#queue-search-algorithm","title":"Queue Search Algorithm","text":"<p><code>_vm_page_list_find()</code> searches for pages with widening locality:</p> <ol> <li>Try exact color queue first</li> <li>Widen search: 16 \u2192 32 \u2192 64 \u2192 128 \u2192 ... \u2192 1024 queues</li> <li>Track <code>lastq</code> to skip known-empty queues</li> <li>Return spinlocked page removed from queue</li> </ol>"},{"location":"sys/vm/vm_page/#contiguous-allocation","title":"Contiguous Allocation","text":"<p>For DMA and device drivers requiring physically contiguous memory:</p> <pre><code>vm_page_t vm_page_alloc_contig(vm_paddr_t low, vm_paddr_t high,\n                               unsigned long alignment,\n                               unsigned long boundary,\n                               unsigned long size,\n                               vm_memattr_t memattr);\n</code></pre> <p>Uses the <code>vm_contig_alist</code> allocator for low-memory DMA pages.</p>"},{"location":"sys/vm/vm_page/#other-allocation-functions","title":"Other Allocation Functions","text":"Function Description <code>vm_page_alloczwq()</code> Allocate without object, returns wired page <code>vm_page_grab()</code> Lookup-or-allocate with object"},{"location":"sys/vm/vm_page/#page-freeing","title":"Page Freeing","text":""},{"location":"sys/vm/vm_page/#vm_page_free_toq","title":"<code>vm_page_free_toq()</code>","text":"<p>The main page freeing function:</p> <ol> <li>Assert page not mapped (calls <code>pmap_mapped_sync()</code> if needed)</li> <li>Remove from object via <code>vm_page_remove()</code></li> <li>For fictitious pages: just wakeup and return</li> <li>Remove from current queue</li> <li>Clear valid/dirty bits</li> <li>Place on appropriate queue:</li> <li><code>PQ_HOLD</code> if <code>hold_count != 0</code></li> <li><code>PQ_FREE</code> otherwise (at head for cache-hot)</li> <li>Wake up page waiters</li> <li>Wake memory-waiting threads via <code>vm_page_free_wakeup()</code></li> </ol>"},{"location":"sys/vm/vm_page/#free-wakeup-logic","title":"Free Wakeup Logic","text":"<p><code>vm_page_free_wakeup()</code> signals: - Pageout daemon: If it needs pages and threshold met - Memory-waiting processes: If above hysteresis threshold</p>"},{"location":"sys/vm/vm_page/#page-state-transitions","title":"Page State Transitions","text":"<pre><code>flowchart TB\n    PQ_FREE[\"PQ_FREE\"]\n    PQ_ACTIVE[\"PQ_ACTIVE\"]\n    PQ_INACTIVE[\"PQ_INACTIVE\"]\n    PQ_CACHE[\"PQ_CACHEclean, not mapped\"]\n    PQ_FREE_OR_HOLD[\"PQ_FREE or PQ_HOLD\"]\n\n    PQ_FREE --&gt;|alloc| PQ_ACTIVE\n    PQ_ACTIVE --&gt;|deactivate| PQ_INACTIVE\n    PQ_INACTIVE --&gt;|clean| PQ_CACHE\n    PQ_CACHE --&gt;|free| PQ_FREE_OR_HOLD\n</code></pre>"},{"location":"sys/vm/vm_page/#activation","title":"Activation","text":"<p><code>vm_page_activate()</code> moves a page to <code>PQ_ACTIVE</code>: - Sets <code>act_count</code> to at least <code>ACT_INIT</code> - Wakes pagedaemon if page was on cache/free queue</p>"},{"location":"sys/vm/vm_page/#deactivation","title":"Deactivation","text":"<p><code>vm_page_deactivate()</code> moves a page to <code>PQ_INACTIVE</code>: - Clears <code>PG_WINATCFLS</code> flag - Optional <code>athead</code> for pseudo-cache behavior (MADV_DONTNEED)</p>"},{"location":"sys/vm/vm_page/#caching","title":"Caching","text":"<p><code>vm_page_cache()</code> moves a clean page to <code>PQ_CACHE</code>: - Removes all pmap mappings first - Dirty pages are deactivated instead</p>"},{"location":"sys/vm/vm_page/#busy-state-management","title":"Busy State Management","text":"<p>DragonFly uses atomic busy counts instead of traditional locks.</p>"},{"location":"sys/vm/vm_page/#hard-busy-pbusy_locked","title":"Hard Busy (<code>PBUSY_LOCKED</code>)","text":"<p>Exclusive access to the page.</p> <pre><code>void vm_page_busy_wait(vm_page_t m, int also_m_busy, const char *msg);\nint  vm_page_busy_try(vm_page_t m, int also_m_busy);\nvoid vm_page_wakeup(vm_page_t m);\n</code></pre> <ul> <li><code>vm_page_busy_wait()</code>: Blocks until page not busy</li> <li><code>vm_page_busy_try()</code>: Non-blocking attempt, returns TRUE on failure</li> <li><code>vm_page_wakeup()</code>: Clears busy and wakes waiters</li> </ul>"},{"location":"sys/vm/vm_page/#soft-busy-pbusy_mask","title":"Soft Busy (<code>PBUSY_MASK</code>)","text":"<p>Shared access for compatible operations (e.g., read-only mapping during write).</p> <pre><code>void vm_page_io_start(vm_page_t m);   // Increment soft-busy (requires hard-busy)\nvoid vm_page_io_finish(vm_page_t m);  // Decrement soft-busy\nint  vm_page_sbusy_try(vm_page_t m);  // Non-blocking soft-busy acquire\n</code></pre>"},{"location":"sys/vm/vm_page/#waiting","title":"Waiting","text":"<p><code>vm_page_sleep_busy()</code> sleeps until page not busy without acquiring it.</p>"},{"location":"sys/vm/vm_page/#wireunwire","title":"Wire/Unwire","text":"<p>Wiring prevents a page from being paged out.</p> <pre><code>void vm_page_wire(vm_page_t m);\nvoid vm_page_unwire(vm_page_t m, int activate);\n</code></pre> <ul> <li><code>vm_page_wire()</code>: Increments <code>wire_count</code>, adjusts vmstats on 0\u21921</li> <li><code>vm_page_unwire()</code>: Decrements <code>wire_count</code>, activates or deactivates on 1\u21920</li> <li>Fictitious pages ignore wire operations</li> </ul>"},{"location":"sys/vm/vm_page/#holdunhold","title":"Hold/Unhold","text":"<p>Holding prevents page reuse but not disassociation from object.</p> <pre><code>void vm_page_hold(vm_page_t m);\nvoid vm_page_unhold(vm_page_t m);\n</code></pre> <p>On last unhold, if page is on <code>PQ_HOLD</code>, it moves to <code>PQ_FREE</code>.</p>"},{"location":"sys/vm/vm_page/#page-lookup","title":"Page Lookup","text":""},{"location":"sys/vm/vm_page/#standard-lookup","title":"Standard Lookup","text":"<pre><code>vm_page_t vm_page_lookup(vm_object_t object, vm_pindex_t pindex);\n</code></pre> <p>Requires object token held. Does RB-tree lookup and populates hash cache.</p>"},{"location":"sys/vm/vm_page/#lookup-busy","title":"Lookup + Busy","text":"<pre><code>// Blocking\nvm_page_t vm_page_lookup_busy_wait(vm_object_t object, vm_pindex_t pindex,\n                                   int also_m_busy, const char *msg);\n\n// Non-blocking\nvm_page_t vm_page_lookup_busy_try(vm_object_t object, vm_pindex_t pindex,\n                                  int also_m_busy, int *errorp);\n</code></pre>"},{"location":"sys/vm/vm_page/#fast-heuristic-lookup","title":"Fast Heuristic Lookup","text":"<pre><code>vm_page_t vm_page_hash_get(vm_object_t object, vm_pindex_t pindex);\n</code></pre> <p>Lockless lookup returning soft-busied page on hit.</p>"},{"location":"sys/vm/vm_page/#validdirty-bit-management","title":"Valid/Dirty Bit Management","text":"<p>Each page has 8 valid and 8 dirty bits (one per DEV_BSIZE chunk, typically 512 bytes).</p>"},{"location":"sys/vm/vm_page/#functions","title":"Functions","text":"Function Description <code>vm_page_bits(base, size)</code> Convert range to bit mask <code>vm_page_set_valid()</code> Set valid bits, zero invalid portions <code>vm_page_set_validclean()</code> Set valid, clear dirty <code>vm_page_set_validdirty()</code> Set both valid and dirty <code>vm_page_clear_dirty()</code> Clear dirty bits <code>vm_page_dirty()</code> Set all dirty bits <code>vm_page_test_dirty()</code> Sync dirty from pmap <code>vm_page_zero_invalid()</code> Zero invalid portions before mapping <code>vm_page_is_valid()</code> Check if range is valid"},{"location":"sys/vm/vm_page/#memory-pressure-handling","title":"Memory Pressure Handling","text":""},{"location":"sys/vm/vm_page/#waiting-functions","title":"Waiting Functions","text":"Function Description <code>vm_wait()</code> Block until memory available (I/O path) <code>vm_wait_pfault()</code> Block in page fault path (nice-aware) <code>vm_wait_nominal()</code> Block for kernel heavy operations <code>vm_test_nominal()</code> Test if vm_wait_nominal would block"},{"location":"sys/vm/vm_page/#nice-aware-paging","title":"Nice-Aware Paging","text":"<p>Process nice value affects paging thresholds: - Higher nice = earlier blocking - Prevents nice'd memory hogs from impacting normal processes</p>"},{"location":"sys/vm/vm_page/#low-memory-kill","title":"Low Memory Kill","text":"<p>Processes with <code>P_LOWMEMKILL</code> flag can break out of wait loops.</p>"},{"location":"sys/vm/vm_page/#madvise-support","title":"madvise Support","text":""},{"location":"sys/vm/vm_page/#vm_page_dontneed","title":"<code>vm_page_dontneed()</code>","text":"<p>Implements <code>MADV_DONTNEED</code>: - 3/32 chance: deactivate page - 28/32 chance: deactivate at head (pseudo-cache) - Clears <code>PG_REFERENCED</code></p>"},{"location":"sys/vm/vm_page/#special-page-types","title":"Special Page Types","text":""},{"location":"sys/vm/vm_page/#fictitious-pages","title":"Fictitious Pages","text":"<p>Pages with <code>PG_FICTITIOUS</code> flag: - Not in normal page array - Created via <code>vm_page_initfake()</code> - Wire/unwire operations ignored - Used for device mappings (GPU, etc.)</p>"},{"location":"sys/vm/vm_page/#pages-requiring-commit","title":"Pages Requiring Commit","text":"<p>Pages with <code>PG_NEED_COMMIT</code> flag: - Cannot be reclaimed even if clean - Used by tmpfs, NFS - Set via <code>vm_page_need_commit()</code></p>"},{"location":"sys/vm/vm_page/#locking-rules","title":"Locking Rules","text":""},{"location":"sys/vm/vm_page/#queue-operations","title":"Queue Operations","text":"<p>Locking order: Page spinlock first, then queue spinlock</p> <pre><code>vm_page_spin_lock(m);           // Lock page\n_vm_page_queue_spin_lock(m);    // Then lock its queue\n// ... manipulate queue ...\n_vm_page_queue_spin_unlock(m);  // Unlock queue first\nvm_page_spin_unlock(m);         // Then unlock page\n</code></pre>"},{"location":"sys/vm/vm_page/#per-cpu-statistics","title":"Per-CPU Statistics","text":"<p>Queue adjustments update per-CPU vmstats: - <code>mycpu-&gt;gd_vmstats_adj</code> - Accumulated adjustments - <code>mycpu-&gt;gd_vmstats</code> - Current view - Synchronized to global <code>vmstats</code> periodically or when threshold exceeded</p>"},{"location":"sys/vm/vm_page/#debugging","title":"Debugging","text":""},{"location":"sys/vm/vm_page/#ddb-commands","title":"DDB Commands","text":"Command Description <code>show page</code> Display vmstats counters <code>show pageq</code> Display queue lengths per color"},{"location":"sys/vm/vm_page/#sysctls","title":"Sysctls","text":"Sysctl Description <code>vm.dma_reserved</code> Memory reserved for DMA <code>vm.dma_free_pages</code> Available DMA pages <code>vm.page_hash_vnode_only</code> Only hash vnode pages"},{"location":"sys/vm/vm_page/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":"<ol> <li>1024-color page queues - Extreme SMP scalability</li> <li>NUMA-aware coloring - Automatic per-socket page distribution</li> <li>Per-CPU vmstats caching - Reduces cache-line bouncing</li> <li>Heuristic page hash - Lockless fast path for lookups</li> <li>Nice-aware paging - Fair memory allocation under pressure</li> <li>DMA alist reserve - Efficient contiguous allocation</li> </ol>"},{"location":"sys/vm/vm_page/#see-also","title":"See Also","text":"<ul> <li>VM Architecture Overview</li> <li><code>sys/vm/vm_page.h</code> - Page structure and flags</li> <li><code>sys/vm/vm_page2.h</code> - Inline functions and thresholds</li> </ul>"},{"location":"sys/vm/vm_pageout/","title":"Pageout and Swap","text":"<p>The pageout daemon manages memory reclamation when physical memory becomes scarce, while the swap pager provides backing store for anonymous memory. DragonFly BSD features dual pageout threads for deadlock recovery, a radix bitmap swap allocator, and aggressive RSS enforcement.</p> <p>Source files: <code>sys/vm/vm_pageout.c</code> (~2,895 lines), <code>sys/vm/swap_pager.c</code> (~2,600 lines)</p>"},{"location":"sys/vm/vm_pageout/#two-subsystems-one-goal","title":"Two Subsystems, One Goal","text":"<p>This document covers two related but distinct subsystems:</p> <pre><code>flowchart TB\n    subgraph PAGEOUT[\"PAGEOUT DAEMON (vm_pageout.c)When do we reclaim memory?\"]\n        P1[\"Monitors free memory against thresholds\"]\n        P2[\"Scans page queues to find victims\"]\n        P3[\"Decides: activate, deactivate, cache, or launder\"]\n        P4[\"Calls pagers to write dirty pages\"]\n        P5[\"Triggers OOM killer when all else fails\"]\n    end\n\n    subgraph SWAP[\"SWAP PAGER (swap_pager.c)Where do we store anonymous pages?\"]\n        S1[\"Allocates blocks on swap device\"]\n        S2[\"Tracks which page \u2192 which block (RB-tree metadata)\"]\n        S3[\"Handles I/O for pageout (write) and page fault (read)\"]\n        S4[\"Also used by: swapcache (file page caching on swap)\"]\n    end\n\n    PAGEOUT --&gt;|\"Write this dirty page\"| SWAP\n</code></pre> <p>When you need to understand: - Why the system is low on memory \u2192 Pageout daemon (thresholds, scanning) - Why a process was OOM killed \u2192 Pageout daemon (OOM killer) - Why swap I/O is slow \u2192 Swap pager (async limits, clustering) - Why swap space is exhausted \u2192 Swap pager (allocation, hysteresis)</p>"},{"location":"sys/vm/vm_pageout/#overview","title":"Overview","text":"<p>When free memory falls below critical thresholds, the pageout daemon scans page queues to:</p> <ol> <li>Deactivate referenced pages from active to inactive queue</li> <li>Launder dirty pages by writing to backing store</li> <li>Free clean pages to replenish the free pool</li> <li>Kill processes when swap is exhausted (OOM)</li> </ol> <p>The swap pager provides block storage for anonymous pages:</p> <ol> <li>Allocate swap space using a radix bitmap allocator</li> <li>Track swap assignments per-object in RB-tree metadata</li> <li>Page out dirty pages with clustering and async I/O</li> <li>Page in with burst reading for sequential access</li> </ol>"},{"location":"sys/vm/vm_pageout/#pageout-daemon-vm_pageoutc","title":"Pageout Daemon (vm_pageout.c)","text":""},{"location":"sys/vm/vm_pageout/#dual-pageout-threads","title":"Dual Pageout Threads","text":"<p>DragonFly runs two pageout threads for deadlock recovery:</p> Thread Purpose <code>pagedaemon</code> Primary pageout daemon for all page types <code>emergpager</code> Emergency pager for swap-only pages when primary deadlocks <p>The emergency pager activates when the primary daemon blocks on a vnode lock for more than 2 seconds. It only processes anonymous pages (<code>OBJT_DEFAULT</code>, <code>OBJT_SWAP</code>) to avoid the vnode deadlock.</p>"},{"location":"sys/vm/vm_pageout/#memory-thresholds","title":"Memory Thresholds","text":"<p>The pageout daemon maintains several free memory thresholds, calculated from <code>v_free_min</code> in <code>vm_pageout_free_page_calc()</code>:</p> <pre><code>v_interrupt_free_min   Low-level allocation reserve (swap structures)\n         \u2193\nv_pageout_free_min     Pageout daemon allocation reserve\n         \u2193\nv_free_reserved        System allocation reserve\n         \u2193\nv_free_min             Normal allocation minimum\n         \u2193\nv_free_target          Target free pages (2x v_free_min)\n         \u2193\nv_paging_wait          Start blocking allocators (3x v_free_min)\n         \u2193\nv_paging_start         Begin paging to target1 (3.5x v_free_min)\n         \u2193\nv_paging_target1       Aggressive paging target (4x v_free_min)\n         \u2193\nv_paging_target2       Lazy paging target (5x v_free_min)\n</code></pre>"},{"location":"sys/vm/vm_pageout/#paging-states","title":"Paging States","text":"<p>The daemon operates in three states:</p> State Behavior <code>PAGING_IDLE</code> No paging needed, daemon sleeps <code>PAGING_TARGET1</code> Aggressive paging toward <code>v_paging_target1</code> <code>PAGING_TARGET2</code> Lazy paging toward <code>v_paging_target2</code> <p>State transitions prevent thrashing while ensuring responsiveness:</p> <pre><code>flowchart LR\n    IDLE --&gt;|\"shortage detected\"| TARGET1\n    TARGET1 --&gt;|\"target1 reached\"| TARGET2\n    TARGET2 --&gt;|\"target2 reached\"| IDLE\n</code></pre>"},{"location":"sys/vm/vm_pageout/#key-sysctls","title":"Key Sysctls","text":"Sysctl Default Description <code>vm.anonmem_decline</code> ACT_DECLINE Active\u2192inactive rate for anon pages <code>vm.filemem_decline</code> ACT_DECLINE*2 Active\u2192inactive rate for file pages <code>vm.max_launder</code> physmem/256+16 Max dirty pages to flush per pass <code>vm.emerg_launder</code> 100 Emergency pager minimum launder <code>vm.pageout_memuse_mode</code> 2 RSS enforcement (0=off, 1=passive, 2=active) <code>vm.pageout_allow_active</code> 1 Allow scanning active queue <code>vm.swap_enabled</code> 1 Enable swap pageouts <code>vm.defer_swapspace_pageouts</code> 0 Prefer keeping dirty pages in memory <code>vm.disable_swapspace_pageouts</code> 0 Completely disable swap writes"},{"location":"sys/vm/vm_pageout/#queue-scanning","title":"Queue Scanning","text":"<p>The daemon scans page queues in order: inactive \u2192 active \u2192 cache.</p>"},{"location":"sys/vm/vm_pageout/#inactive-queue-scan","title":"Inactive Queue Scan","text":"<p><code>vm_pageout_scan_inactive()</code> processes candidates for reclamation:</p> <ol> <li>Scan ~1/10 of queue per pass using per-queue markers</li> <li>Calculate <code>max_launder</code> budget from <code>vm_max_launder</code></li> <li>For each page, call <code>vm_pageout_page()</code> for decision</li> </ol> <p>Per-page decision in <code>vm_pageout_page()</code>:</p> <pre><code>Page state              Action\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nWired                   Remove from queue\nHeld                    Requeue at tail\nReferenced (pmap/flag)  Activate with boosted act_count\nInvalid (valid==0)      Free directly\nClean (dirty==0)        Cache via vm_page_cache()\nDirty, first pass       Set PG_WINATCFLS, requeue (double-LRU)\nDirty, second pass      Attempt pageout via vm_pageout_clean_helper()\n</code></pre> <p>Double-LRU for Dirty Pages:</p> <p>Dirty pages cycle through the inactive queue twice before laundering. The <code>PG_WINATCFLS</code> (\"win at cache flush\") flag tracks the first pass. This reduces unnecessary I/O for pages that may be freed or become clean naturally.</p> <p>When <code>vm_pageout_memuse_mode &gt;= 3</code>, single-LRU mode is used instead.</p>"},{"location":"sys/vm/vm_pageout/#active-queue-scan","title":"Active Queue Scan","text":"<p><code>vm_pageout_scan_active()</code> moves pages to the inactive queue:</p> <ol> <li>Scan ~1/10 of queue per iteration</li> <li>Check page activity via <code>pmap_ts_referenced()</code> and <code>PG_REFERENCED</code></li> <li>Active pages: bump <code>act_count</code>, leave in active queue</li> <li>Inactive pages: decrement <code>act_count</code>, deactivate when threshold reached</li> </ol> <p>Activity Decline Rates:</p> <ul> <li>Anonymous pages decline at <code>vm_anonmem_decline</code> per pass</li> <li>File-backed pages decline at <code>vm_filemem_decline</code> (2x anon by default)</li> </ul> <p>Faster file page decay implements the \"scan resistance\" principle\u2014file scans shouldn't evict working set.</p>"},{"location":"sys/vm/vm_pageout/#cache-queue-scan","title":"Cache Queue Scan","text":"<p><code>vm_pageout_scan_cache()</code> frees clean cached pages:</p> <ul> <li>Uses two rovers (primary and emergency) to avoid contention</li> <li>Frees pages until <code>v_free_target</code> reached</li> <li>Stops early if target satisfied</li> </ul>"},{"location":"sys/vm/vm_pageout/#page-clustering","title":"Page Clustering","text":"<p><code>vm_pageout_clean_helper()</code> clusters dirty pages for efficient I/O:</p> <ol> <li>Find clusterable neighbors (dirty, not wired/held, inactive or allowed-active)</li> <li>Align cluster to <code>BLIST_MAX_ALLOC</code> for swap optimization</li> <li>Set <code>PG_WINATCFLS</code> on cluster pages to match primary</li> <li>Call <code>vm_pageout_flush()</code> for actual I/O</li> </ol> <p><code>vm_pageout_flush()</code> handles the write:</p> <ol> <li>Mark pages read-only via <code>vm_page_protect()</code></li> <li>Clear pmap modified bits</li> <li>Call <code>vm_pager_put_pages()</code> for I/O</li> <li>Handle results and update page states</li> </ol>"},{"location":"sys/vm/vm_pageout/#rss-enforcement","title":"RSS Enforcement","text":"<p>When <code>vm_pageout_memuse_mode &gt;= 1</code>, the daemon enforces process RSS limits:</p> <p><code>vm_pageout_map_deactivate_pages()</code>:</p> <ol> <li>Called for processes exceeding <code>RLIMIT_RSS</code></li> <li>Walks address space from <code>map-&gt;pgout_offset</code></li> <li>Scans each mapped page via <code>pmap_pgscan()</code></li> <li>Removes unreferenced pages from pmap</li> <li>Deactivates unmapped pages, optionally launders dirty ones</li> <li>Continues until RSS below limit</li> </ol> <p>RSS enforcement modes:</p> Mode Behavior 0 Disabled 1 Passive\u2014enforce during memory pressure 2 Active\u2014enforce proactively (default) 3 Active + single-LRU for dirty pages"},{"location":"sys/vm/vm_pageout/#oom-killer","title":"OOM Killer","text":"<p>When swap exhausts and pages cannot be reclaimed:</p> <ol> <li>Rate-limited to once per second</li> <li>Scan all processes via <code>allproc_scan()</code></li> <li>Select victim with largest memory footprint</li> <li>Skip: system processes, init (pid 1), low-pid processes with swap</li> <li>Set <code>P_LOWMEMKILL</code> flag and call <code>killproc()</code></li> </ol> <p>Victim size calculation: <pre><code>size = vmspace_anonymous_count(vm) + vmspace_swap_count(vm)\n</code></pre></p>"},{"location":"sys/vm/vm_pageout/#main-loop","title":"Main Loop","text":"<p><code>vm_pageout_thread()</code> main loop:</p> <pre><code>Initialize markers, thresholds, swap pager\n\nwhile (TRUE) {\n    1. Sleep until vm_pages_needed or timeout\n    2. Calculate avail_shortage from targets\n    3. vm_pageout_scan_inactive() - launder/free pages\n    4. Calculate inactive_shortage\n    5. vm_pageout_scan_active() - feed inactive queue\n    6. vm_pageout_scan_cache() - free cached pages\n    7. Determine next state (IDLE/TARGET1/TARGET2)\n    8. Wakeup memory waiters if appropriate\n}\n</code></pre> <p>Emergency pager differences:</p> <ul> <li>Sleeps on <code>&amp;vm_pagedaemon_uptime</code> instead of <code>&amp;vm_pages_needed</code></li> <li>Activates if primary hasn't updated uptime for 2+ seconds</li> <li>Only processes anonymous/swap pages</li> <li>Iterates queues in reverse direction</li> </ul>"},{"location":"sys/vm/vm_pageout/#swap-pager-swap_pagerc","title":"Swap Pager (swap_pager.c)","text":""},{"location":"sys/vm/vm_pageout/#architecture","title":"Architecture","text":"<p>The swap pager provides block storage for anonymous memory:</p> <pre><code>flowchart TB\n    OBJ[\"vm_object\"]\n\n    subgraph RBTREE[\"swblock_root (RB-tree)RB-tree of swap metadata\"]\n        SWB1[\"swblock(16 pgs)\"]\n        SWB2[\"swblock(16 pgs)\"]\n        SWB3[\"swblock(16 pgs)\"]\n    end\n\n    BLIST[\"swapblist (blist)Radix bitmap allocatorSwap block allocation\"]\n\n    DISK[\"Swap devices (disks)\"]\n\n    OBJ --&gt; RBTREE\n    SWB1 --&gt; BLIST\n    SWB2 --&gt; BLIST\n    SWB3 --&gt; BLIST\n    BLIST --&gt; DISK\n</code></pre>"},{"location":"sys/vm/vm_pageout/#key-data-structures","title":"Key Data Structures","text":"<p>struct swblock - Swap metadata per 16-page range:</p> <pre><code>struct swblock {\n    RB_ENTRY(swblock) swb_entry;         /* RB-tree linkage */\n    vm_pindex_t       swb_index;         /* Base page index (aligned) */\n    int               swb_count;         /* Valid entries */\n    swblk_t           swb_pages[SWAP_META_PAGES];  /* Block numbers (16) */\n};\n</code></pre> <p>Global state:</p> <pre><code>struct blist *swapblist;      /* Radix bitmap allocator */\nint swap_pager_full;          /* Swap exhausted flag (triggers OOM) */\nint swap_pager_almost_full;   /* Near exhaustion (with hysteresis) */\nswblk_t vm_swap_anon_use;     /* Swap used for anonymous pages */\nswblk_t vm_swap_cache_use;    /* Swap used for swapcache */\n</code></pre>"},{"location":"sys/vm/vm_pageout/#swap-space-allocation","title":"Swap Space Allocation","text":"<p>Radix bitmap (blist) allocator:</p> <p>The blist provides O(log n) allocation and deallocation with efficient fragmentation handling. Swap is allocated in contiguous runs when possible.</p> <p><code>swp_pager_getswapspace(npages, object_type)</code>:</p> <ol> <li>Try <code>blist_allocat()</code> with iterator hint</li> <li>Fall back to start of swap if hint fails</li> <li>Update <code>vm_swap_anon_use</code> or <code>vm_swap_cache_use</code></li> <li>Set <code>swap_pager_full=2</code> on allocation failure</li> </ol> <p>Hysteresis thresholds:</p> <ul> <li><code>nswap_lowat</code> = 4% of total swap (minimum 128 pages)</li> <li><code>nswap_hiwat</code> = 6% of total swap (minimum 512 pages)</li> <li><code>swap_pager_almost_full</code> set when below lowat</li> <li>Cleared when above hiwat</li> </ul>"},{"location":"sys/vm/vm_pageout/#swap-io-limits","title":"Swap I/O Limits","text":"<pre><code>nsw_rcount         = nswbuf_kva / 2   /* Read buffer limit */\nnsw_wcount_sync    = nswbuf_kva / 4   /* Sync write limit */\nnsw_wcount_async   = swap_async_max   /* Async write limit (default 4) */\nnsw_cluster_max    = MAXPHYS / PAGE_SIZE  /* Max cluster size */\n</code></pre>"},{"location":"sys/vm/vm_pageout/#page-in-swap_pager_getpage","title":"Page-In (swap_pager_getpage)","text":"<p>Burst reading for sequential access:</p> <ol> <li>If page already valid, switch to read-ahead only mode</li> <li>Scan for contiguous swap blocks on same stripe</li> <li>Allocate pages for burst (up to <code>swap_burst_read</code>)</li> <li>Set <code>PG_RAM</code> on last page for pipeline continuation</li> <li>Issue I/O and wait for <code>PBUSY_SWAPINPROG</code> to clear</li> </ol> <p>Read process:</p> <pre><code>1. Verify object match\n2. Look up swap block for requested page\n3. Scan for contiguous swap blocks (same stripe)\n4. Allocate pages for burst read\n5. Map pages to KVA, issue I/O\n6. Wait for completion\n7. Return VM_PAGER_OK or VM_PAGER_ERROR\n</code></pre>"},{"location":"sys/vm/vm_pageout/#page-out-swap_pager_putpages","title":"Page-Out (swap_pager_putpages)","text":"<p>Object conversion:</p> <p>First pageout converts <code>OBJT_DEFAULT</code> to <code>OBJT_SWAP</code>.</p> <p>Write process:</p> <ol> <li>Allocate swap blocks (fall back to smaller chunks on failure)</li> <li>Validate stripe boundary, adjust if crossing</li> <li>Build swap metadata entries</li> <li>Issue I/O (async or sync based on flags)</li> <li>For sync: wait and call completion handler directly</li> </ol> <p>Async throttling:</p> <ul> <li><code>nsw_wcount_async_max</code> controlled by <code>vm.swap_async_max</code></li> <li>Prevents swap I/O from starving other disk I/O</li> <li>Non-pageout threads forced sync unless <code>swap_user_async=1</code></li> </ul>"},{"location":"sys/vm/vm_pageout/#io-completion-swp_pager_async_iodone","title":"I/O Completion (swp_pager_async_iodone)","text":"<p>Read success:</p> <ol> <li>Set <code>m-&gt;valid = VM_PAGE_BITS_ALL</code></li> <li>Clear dirty bits</li> <li>Set <code>PG_SWAPPED</code> flag</li> <li>Deactivate non-requested pages</li> </ol> <p>Read error:</p> <ol> <li>Set <code>m-&gt;valid = 0</code></li> <li>Deactivate non-requested pages</li> <li>Leave requested page busy for caller</li> </ol> <p>Write success:</p> <ol> <li>Clear dirty bits (OBJT_SWAP only)</li> <li>Set <code>PG_SWAPPED</code> flag</li> <li>Deactivate if <code>vm_paging_severe()</code></li> <li>Try to cache if <code>SWBIO_TTC</code> flag</li> </ol> <p>Write error:</p> <ol> <li>Remove swap assignment</li> <li>Re-dirty OBJT_SWAP pages (no other backing)</li> <li>Activate page to prevent data loss</li> <li>Don't dirty non-OBJT_SWAP (has vnode backing)</li> </ol>"},{"location":"sys/vm/vm_pageout/#swap-metadata-management","title":"Swap Metadata Management","text":"<p>RB-tree operations:</p> Function Purpose <code>swp_pager_lookup()</code> Find swblock by page index <code>swp_pager_meta_build()</code> Add/update swap entry <code>swp_pager_meta_free()</code> Free range of entries <code>swp_pager_meta_free_all()</code> Destroy all metadata <code>swp_pager_meta_ctl()</code> Control (free/pop) single entry <p>Metadata control flags:</p> Flag Action <code>SWM_FREE</code> Remove and free swap block <code>SWM_POP</code> Remove but keep block (for transfer)"},{"location":"sys/vm/vm_pageout/#swapoff-support","title":"Swapoff Support","text":"<p><code>swap_pager_swapoff()</code> removes a device from use:</p> <ol> <li>Scan all <code>OBJT_SWAP</code> and <code>OBJT_VNODE</code> objects</li> <li>Skip <code>OBJ_NOPAGEIN</code> objects</li> <li>Page in all blocks on target device</li> <li>Return 1 if blocks remain (partial success)</li> </ol>"},{"location":"sys/vm/vm_pageout/#key-sysctls_1","title":"Key Sysctls","text":"Sysctl Default Description <code>vm.swap_async_max</code> 4 Max concurrent async writes <code>vm.swap_burst_read</code> 16 Pages to read-ahead on swap-in <code>vm.swap_user_async</code> 0 Allow user async swap writes"},{"location":"sys/vm/vm_pageout/#dragonfly-specific-features","title":"DragonFly-Specific Features","text":""},{"location":"sys/vm/vm_pageout/#dual-pageout-threads_1","title":"Dual Pageout Threads","text":"<p>The emergency pager (<code>emergpager</code>) provides deadlock recovery:</p> <ul> <li>Activates when primary daemon blocks on vnode for 2+ seconds</li> <li>Only processes anonymous pages to avoid the deadlock source</li> <li>Enables memory reclamation even during filesystem stalls</li> </ul>"},{"location":"sys/vm/vm_pageout/#queue-distribution","title":"Queue Distribution","text":"<p>Pageout work is distributed across 1024 sub-queues per major queue type:</p> <ul> <li>Each sub-queue has its own marker for incremental scanning</li> <li><code>PQAVERAGE()</code> distributes work across queues</li> <li>Reduces lock contention on SMP systems</li> </ul>"},{"location":"sys/vm/vm_pageout/#three-state-paging","title":"Three-State Paging","text":"<p>The IDLE \u2192 TARGET1 \u2192 TARGET2 state machine:</p> <ul> <li>Prevents thrashing between paging and normal operation</li> <li>TARGET1 aggressive paging ensures quick recovery</li> <li>TARGET2 lazy paging maintains headroom without wasting I/O</li> </ul>"},{"location":"sys/vm/vm_pageout/#radix-bitmap-allocator","title":"Radix Bitmap Allocator","text":"<p>The blist provides:</p> <ul> <li>O(log n) allocation and deallocation</li> <li>Efficient handling of fragmentation</li> <li>Scalability to arbitrary swap sizes</li> <li>Per-device stripe awareness</li> </ul>"},{"location":"sys/vm/vm_pageout/#rb-tree-swap-metadata","title":"RB-Tree Swap Metadata","text":"<p>Per-object swap tracking via RB-tree:</p> <ul> <li>16 pages per swblock entry reduces overhead</li> <li>Efficient range operations for large frees</li> <li>Separate anon vs cache accounting</li> </ul>"},{"location":"sys/vm/vm_pageout/#kvabio-support","title":"KVABIO Support","text":"<p>Swap I/O uses KVABIO to avoid pmap synchronization:</p> <ul> <li><code>pmap_qenter_noinval()</code> for mapping</li> <li>Reduces IPI overhead on SMP</li> </ul>"},{"location":"sys/vm/vm_pageout/#stripe-aware-clustering","title":"Stripe-Aware Clustering","text":"<p>I/O operations respect device stripe boundaries:</p> <ul> <li><code>SWB_DMMASK</code> for boundary detection</li> <li>Prevents I/O from crossing stripes</li> <li>Optimizes disk access patterns</li> </ul>"},{"location":"sys/vm/vm_pageout/#function-reference","title":"Function Reference","text":""},{"location":"sys/vm/vm_pageout/#pageout-daemon","title":"Pageout Daemon","text":"Function Description <code>vm_pageout_thread()</code> Main daemon loop <code>vm_pageout_scan_inactive()</code> Scan inactive queue <code>vm_pageout_scan_active()</code> Scan active queue <code>vm_pageout_scan_cache()</code> Scan cache queue <code>vm_pageout_page()</code> Per-page decision <code>vm_pageout_clean_helper()</code> Cluster and flush dirty pages <code>vm_pageout_flush()</code> Write pages to backing store <code>vm_pageout_map_deactivate_pages()</code> RSS enforcement <code>pagedaemon_wakeup()</code> Wake pageout daemon"},{"location":"sys/vm/vm_pageout/#swap-pager","title":"Swap Pager","text":"Function Description <code>swap_pager_getpage()</code> Page in from swap <code>swap_pager_putpages()</code> Page out to swap <code>swap_pager_haspage()</code> Check for swap backing <code>swap_pager_unswapped()</code> Remove swap when dirtied <code>swap_pager_freespace()</code> Free swap range <code>swap_pager_reserve()</code> Pre-allocate swap <code>swap_pager_copy()</code> Transfer swap metadata <code>swap_pager_swapoff()</code> Remove device from use <code>swp_pager_async_iodone()</code> I/O completion handler"},{"location":"sys/vm/vm_pageout/#see-also","title":"See Also","text":"<ul> <li>Physical Pages - Page allocation and queue management</li> <li>VM Objects - Object lifecycle and page containers</li> <li>Page Faults - Fault handling and COW</li> <li>Memory Management - kmalloc/objcache</li> </ul>"}]}